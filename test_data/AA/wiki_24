{"url": "https://en.wikipedia.org/wiki?curid=3833", "text": "Beowulf\n\nBeowulf (; ) is an Old English epic poem consisting of 3182 alliterative lines. It may be the oldest surviving long poem in Old English and is commonly cited as one of the most important works of Old English literature. A date of composition is a matter of contention among scholars; the only certain dating pertains to the manuscript, which was produced between 975 and 1025. The author was an anonymous Anglo-Saxon poet, referred to by scholars as the \"\"Beowulf\" poet\".\n\nThe poem is set in Scandinavia. Beowulf, a hero of the Geats, comes to the aid of Hrothgar, the king of the Danes, whose mead hall in Heorot has been under attack by a monster known as Grendel. After Beowulf slays him, Grendel's mother attacks the hall and is then also defeated. Victorious, Beowulf goes home to Geatland (Götaland in modern Sweden) and later becomes king of the Geats. After a period of fifty years has passed, Beowulf defeats a dragon, but is fatally wounded in the battle. After his death, his attendants cremate his body and erect a tower on a headland in his memory.\n\nThe full poem survives in the manuscript known as the Nowell Codex. It has no title in the original manuscript, but has become known by the name of the story's protagonist. In 1731, the manuscript was badly damaged by a fire that swept through Ashburnham House in London that had a collection of medieval manuscripts assembled by Sir Robert Bruce Cotton. The Nowell Codex is currently housed in the British Library.\n\nThe events in the poem take place over most of the sixth century, after the Anglo-Saxons had started their journey to England and before the beginning of the seventh century, a time when the Anglo-Saxons were either newly arrived or were still in close contact with their Germanic kinsmen in Northern Germany. The poem may have been brought to England by people of Geatish origins. It has been suggested that \"Beowulf\" was first composed in the 7th century at Rendlesham in East Anglia, as the Sutton Hoo ship-burial also shows close connections with Scandinavia, and also that the East Anglian royal dynasty, the Wuffingas, may have been descendants of the Geatish Wulfings. Others have associated this poem with the court of King Alfred the Great or with the court of King Cnut the Great.\nThe poem deals with legends, was composed for entertainment, and does not separate between fictional elements and real historic events, such as the raid by King Hygelac into Frisia. Though Beowulf himself is not mentioned in any other Anglo-Saxon manuscript, scholars generally agree that many of the other personalities of \"Beowulf\" also appear in Scandinavian sources. (Specific works are designated in the following section). This does not only concern people (e.g., Healfdene, Hroðgar, Halga, Hroðulf, Eadgils and Ohthere), but also clans (e.g., Scyldings, Scylfings and Wulfings) and some of the events (e.g., the Battle on the Ice of Lake Vänern). The dating of the events in the poem has been confirmed by archaeological excavations of the barrows indicated by Snorri Sturluson and by Swedish tradition as the graves of Ohthere (dated to c. 530) and his son Eadgils (dated to c. 575) in Uppland, Sweden.\n\nIn Denmark, recent archaeological excavations at Lejre, where Scandinavian tradition located the seat of the Scyldings, i.e., Heorot, have revealed that a hall was built in the mid-6th century, exactly the time period of \"Beowulf\". Three halls, each about long, were found during the excavation.\nThe majority view appears to be that people such as King Hroðgar and the Scyldings in \"Beowulf\" are based on real historical people from 6th-century Scandinavia. Like the \"Finnesburg Fragment\" and several shorter surviving poems, \"Beowulf\" has consequently been used as a source of information about Scandinavian personalities such as Eadgils and Hygelac, and about continental Germanic personalities such as Offa, king of the continental Angles.\n\n19th-century archeological evidence may confirm elements of the \"Beowulf\" story. Eadgils was buried at Uppsala according to Snorri Sturluson. When Eadgils' mound (to the left in the photo) was excavated in 1874, the finds supported \"Beowulf\" and the sagas. They showed that a powerful man was buried in a large barrow, c 575, on a bear skin with two dogs and rich grave offerings. These remains include a Frankish sword adorned with gold and garnets and a tafl game with Roman pawns of ivory. He was dressed in a costly suit made of Frankish cloth with golden threads, and he wore a belt with a costly buckle. There were four cameos from the Middle East which were probably part of a casket. This would have been a burial fitting a king who was famous for his wealth in Old Norse sources. Ongentheow's barrow has not been excavated.\n\nThe main protagonist Beowulf, a hero of the Geats, comes to the aid of Hrothgar, king of the Danes, whose great hall, Heorot, is plagued by the monster Grendel. Beowulf kills Grendel with his bare hands and Grendel's mother with a giant's sword that he found in her lair.\n\nLater in his life, Beowulf becomes king of the Geats, and finds his realm terrorized by a dragon, some of whose treasure had been stolen from his hoard in a burial mound. He attacks the dragon with the help of his \"thegns\" or servants, but they do not succeed. Beowulf decides to follow the dragon to its lair at Earnanæs, but only his young Swedish relative Wiglaf, whose name means \"remnant of valour\", dares to join him. Beowulf finally slays the dragon, but is mortally wounded in the struggle. He is cremated and a burial mound by the sea is erected in his honor.\n\n\"Beowulf\" is considered an epic poem in that the main character is a hero who travels great distances to prove his strength at impossible odds against supernatural demons and beasts. The poem also begins \"in medias res\" or simply, \"in the middle of things\", which is a characteristic of the epics of antiquity. Although the poem begins with Beowulf's arrival, Grendel's attacks have been an ongoing event. An elaborate history of characters and their lineages is spoken of, as well as their interactions with each other, debts owed and repaid, and deeds of valour. The warriors form a kind of brotherhood linked by loyalty to their lord.\n\n\"Beowulf\" begins with the story of Hrothgar, who constructed the great hall Heorot for himself and his warriors. In it he, his wife Wealhtheow, and his warriors spend their time singing and celebrating. Grendel, a troll-like monster said to be descended from the biblical Cain, is pained by the sounds of a joy he cannot share, attacks the hall, and kills and devours many of Hrothgar's warriors while they sleep. Hrothgar and his people, helpless against Grendel, abandon Heorot.\n\nBeowulf, a young warrior from Geatland, hears of Hrothgar's troubles and with his king's permission leaves his homeland to assist Hrothgar.\n\nBeowulf and his men spend the night in Heorot. Beowulf refuses to use any weapon because he holds himself to be the equal of Grendel. When Grendel enters the hall, Beowulf, who has been feigning sleep, leaps up to clench Grendel's hand. Grendel and Beowulf battle each other violently. Beowulf's retainers draw their swords and rush to his aid, but their blades cannot pierce Grendel's skin. Finally, Beowulf tears Grendel's arm from his body at the shoulder and Grendel runs to his home in the marshes where he dies.\n\nThe next night, after celebrating Grendel's defeat, Hrothgar and his men sleep in Heorot. Grendel's mother, angry that her son has been killed, sets out to get revenge. She violently kills Æschere, who is Hrothgar's most loyal fighter.\n\nHrothgar, Beowulf and their men track Grendel's mother to her lair under a lake. Unferth, a warrior who had doubted him and wishes to make amends, presents Beowulf with his sword Hrunting. After stipulating a number of conditions to Hrothgar in case of his death (including the taking in of his kinsmen and the inheritance by Unferth of Beowulf's estate), Beowulf jumps into the lake, at the bottom of which he finds a cavern containing Grendel's body and the remains of men that the two have killed. Grendel's mother and Beowulf engage in fierce combat.\n\nAt first, Grendel's mother appears to prevail. Beowulf, finding that Hrunting cannot harm his foe, puts it aside in fury. Beowulf is again saved from his opponent's attack by his armour. Beowulf takes another sword from Grendel's mother and slices her head off with it. Travelling further into Grendel's mother's lair, Beowulf discovers Grendel and severs his head. The blade of Beowulf's sword touches Grendel's toxic blood, and instantly dissolves so that only the hilt remains. Beowulf swims back up to the rim of the pond where his men wait in growing despair. Carrying the hilt of the sword and Grendel's head, he presents them to Hrothgar upon his return to Heorot. Hrothgar gives Beowulf many gifts, including the sword Nægling, his family's heirloom. The events prompt a long reflection by the king, sometimes referred to as \"Hrothgar's sermon\", in which he urges Beowulf to be wary of pride and to reward his thegns.\n\nBeowulf returns home and eventually becomes king of his own people. One day, fifty years after Beowulf's battle with Grendel's mother, a slave steals a golden cup from the lair of a dragon at Earnanæs. When the dragon sees that the cup has been stolen, it leaves its cave in a rage, burning everything in sight. Beowulf and his warriors come to fight the dragon, but Beowulf tells his men that he will fight the dragon alone and that they should wait on the barrow. Beowulf descends to do battle with the dragon, but finds himself outmatched. His men, upon seeing this and fearing for their lives, retreat into the woods. One of his men, Wiglaf, however, in great distress at Beowulf's plight, comes to his aid. The two slay the dragon, but Beowulf is mortally wounded. After Beowulf's death, he is ritually burned on a great pyre in Geatland while his people wail and mourn him, fearing that without him, the Geates are defenseless against attacks from surrounding tribes. Afterwards, a barrow, visible from the sea, is built in his memory. (\"Beowulf\" lines 2712–3182).\n\n\"Beowulf\" was written in England, but is set in Scandinavia; its dating has attracted considerable scholarly attention. The poem has been dated to between the 8th and the early 11th centuries, with some recent scholarship offering what has been called \"a cohesive and compelling case for Beowulf's early composition.\" However, opinion differs as to whether the composition of the poem is nearly contemporary with its transcription, whether it was first written in the 8th century, or if a proto-version of the poem was perhaps composed at an even earlier time (possibly as one of the Bear's Son Tales) and orally transmitted for many years, then transcribed in its present form at a later date. Albert Lord felt strongly that the manuscript represents the transcription of a performance, though likely taken at more than one sitting. J. R. R. Tolkien believed that the poem retains too genuine a memory of Anglo-Saxon paganism to have been composed more than a few generations after the completion of the Christianisation of England around AD 700, and Tolkien's conviction that the poem dates to the 8th century has been defended by Tom Shippey, Leonard Neidorf, Rafael J. Pascual, and R.D. Fulk, among others.\n\nThe claim to an early 11th-century date depends in part on scholars who argue that, rather than the transcription of a tale from the oral tradition by an earlier literate monk, \"Beowulf\" reflects an original interpretation of an earlier version of the story by the manuscript's two scribes. On the other hand, some scholars argue that linguistic, paleographical, metrical, and onomastic considerations align to support a date of composition in the first half of the eighth century; in particular, the poem's regular observation of etymological length distinctions (Max Kaluza's law) has been thought to demonstrate a date of composition in the first half of the eighth century. However, scholars disagree about whether the metrical phenomena described by Kaluza's Law prove an early date of composition or are evidence of a longer prehistory of the Beowulf meter; B.R. Hutcheson, for instance, does not believe Kaluza's Law can be used to date the poem, while claiming that \"the weight of all the evidence Fulk presents in his book tells strongly in favor of an eighth-century date.\"\n\n\"Beowulf\" survives in a single manuscript dated on paleographical grounds to the late 10th or early 11th century. The manuscript measures 245 × 185 mm.\n\nThe poem is known only from a single manuscript, which is estimated to date from close to AD 1000, in which it appears with other works. The \"Beowulf\" manuscript is known as the Nowell Codex, gaining its name from 16th-century scholar Laurence Nowell. The official designation is \"British Library, Cotton Vitellius A.XV\" because it was one of Sir Robert Bruce Cotton's holdings in the Cotton library in the middle of the 17th century. Many private antiquarians and book collectors, such as Sir Robert Cotton, used their own library classification systems. \"Cotton Vitellius A.XV\" translates as: the 15th book from the left on shelf A (the top shelf) of the bookcase with the bust of Roman Emperor Vitellius standing on top of it, in Cotton's collection. Kevin Kiernan argues that Nowell most likely acquired it through William Cecil, 1st Baron Burghley, in 1563, when Nowell entered Cecil's household as a tutor to his ward, Edward de Vere, 17th Earl of Oxford.\n\nThe earliest extant reference to the first foliation of the Nowell Codex was made sometime between 1628 and 1650 by Franciscus Junius (the younger). The ownership of the codex before Nowell remains a mystery.\n\nThe Reverend Thomas Smith (1638–1710) and Humfrey Wanley (1672–1726) both catalogued the Cotton library (in which the Nowell Codex was held). Smith's catalogue appeared in 1696, and Wanley's in 1705. The \"Beowulf\" manuscript itself is identified by name for the first time in an exchange of letters in 1700 between George Hickes, Wanley's assistant, and Wanley. In the letter to Wanley, Hickes responds to an apparent charge against Smith, made by Wanley, that Smith had failed to mention the \"Beowulf\" script when cataloguing Cotton MS. Vitellius A. XV. Hickes replies to Wanley \"I can find nothing yet of Beowulph.\" Kiernan theorised that Smith failed to mention the \"Beowulf\" manuscript because of his reliance on previous catalogues or because either he had no idea how to describe it or because it was temporarily out of the codex.\n\nIt suffered damage in the Cotton Library fire at Ashburnham House in 1731. Since then, parts of the manuscript have crumbled along with many of the letters. Rebinding efforts, though saving the manuscript from much degeneration, have nonetheless covered up other letters of the poem, causing further loss. Kevin Kiernan, in preparing his electronic edition of the manuscript, used fibre-optic backlighting and ultraviolet lighting to reveal letters in the manuscript lost from binding, erasure, or ink blotting.\n\nThe \"Beowulf\" manuscript was transcribed from an original by two scribes, one of whom wrote the first 1939 lines and a second who wrote the remainder, with a difference in handwriting noticeable after line 1939. The script of the second scribe is archaic. While both scribes appear to proofread their work, there are nevertheless many errors. The second scribe toiled over the poem for many years \"with great reverence and care to restoration\". The work of the second scribe bears a striking resemblance to the work of the first scribe of the Blickling homilies, and so much so that it is believed they derive from the same scriptorium. From knowledge of books held in the library at Malmesbury Abbey and available as source works, and from the identification of certain words particular to the local dialect found in the text, the transcription may have been made there.\n\nIcelandic scholar Grímur Jónsson Thorkelin made the first transcriptions of the manuscript in 1786 and published the results in 1815, working as part of a Danish government historical research commission. He made one himself, and had another done by a professional copyist who knew no Anglo-Saxon. Since that time, however, the manuscript has crumbled further, making these transcripts a prized witness to the text. While the recovery of at least 2000 letters can be attributed to them, their accuracy has been called into question, and the extent to which the manuscript was actually more readable in Thorkelin's time is uncertain.\n\nIn 1805, the historian Sharon Turner translated selected verses into modern English. This was followed in 1814 by John Josias Conybeare who published an edition \"in English paraphrase and Latin verse translation.\" In 1815, Grímur Jónsson Thorkelin published the first complete edition in Latin. N. F. S. Grundtvig reviewed this edition in 1815 and created the first complete verse translation in Danish in 1820. In 1837, John Mitchell Kemble created an important literal translation in English. In 1895, William Morris & A. J. Wyatt published the ninth English translation. Many retellings of \"Beowulf\" for children also appeared around the beginning of the 20th century. In 1909, Francis Barton Gummere's full translation in \"English imitative meter\" was published, and was used as the text of Gareth Hinds's graphic novel based on \"Beowulf\" in 2007.\n\nDuring the early 20th century, Frederick Klaeber's \"Beowulf and The Fight at Finnsburg\" (which included the poem in Old English, an extensive glossary of Old English terms, and general background information) became the \"central source used by graduate students for the study of the poem and by scholars and teachers as the basis of their translations.\"\n\nA great number of translations are available, in poetry and prose. Andy Orchard, in \"A Critical Companion to Beowulf\", lists 33 \"representative\" translations in his bibliography, and it has been translated into at least 23 other languages.\n\nSeamus Heaney's 1999 translation of the poem (referred to by Howell Chickering and many others as \"Heaneywulf\") was widely publicized. Translating \"Beowulf\" is one of the subjects of the 2012 publication \"Beowulf at Kalamazoo\", containing a section with 10 essays on translation, and a section with 22 reviews of Heaney's translation (some of which compare Heaney's work with that of Anglo-Saxon scholar Roy Liuzza). R. D. Fulk, of Indiana University, published the first facing-page edition and translation of the entire manuscript in the Dumbarton Oaks Medieval Library series in 2010.\n\nJ. R. R. Tolkien's long-awaited translation (edited by his son, Christopher) was published in 2014 as \"\". This also includes Tolkien's own retelling of the story of Beowulf in his tale, \"Sellic Spell\".\n\nThe question of whether \"Beowulf\" was passed down through oral tradition prior to its present manuscript form has been the subject of much debate, and involves more than simply the issue of its composition. Rather, given the implications of the theory of oral-formulaic composition and oral tradition, the question concerns how the poem is to be understood, and what sorts of interpretations are legitimate.\n\nScholarly discussion about \"Beowulf\" in the context of the oral tradition was extremely active throughout the 1960s and 1970s. The debate might be framed starkly as follows: on the one hand, we can hypothesise a poem put together from various tales concerning the hero (the Grendel episode, the Grendel's mother story, and the fire drake narrative). These fragments would have been told for many years in tradition, and learned by apprenticeship from one generation of illiterate poets to the next. The poem is composed orally and extemporaneously, and the archive of tradition on which it draws is oral, pagan, Germanic, heroic, and tribal. On the other hand, one might posit a poem which is composed by a literate scribe, who acquired literacy by way of learning Latin (and absorbing Latinate culture and ways of thinking), probably a monk and therefore profoundly Christian in outlook. On this view, the pagan references would be a sort of decorative archaising. There is a third view that sees merit in both arguments above and attempts to bridge them, and so cannot be articulated as starkly as they can; it sees more than one Christianity and more than one attitude towards paganism at work in the poem; it sees the poem as initially the product of a literate Christian author with one foot in the pagan world and one in the Christian, himself perhaps a convert (or one whose forebears had been pagan), a poet who was conversant in both oral and literary composition and was capable of a masterful \"repurposing\" of poetry from the oral tradition.\n\nHowever, scholars such as D.K. Crowne have proposed the idea that the poem was passed down from reciter to reciter under the theory of oral-formulaic composition, which hypothesises that epic poems were (at least to some extent) improvised by whoever was reciting them, and only much later written down. In his landmark work, \"The Singer of Tales\", Albert Lord refers to the work of Francis Peabody Magoun and others, saying \"the documentation is complete, thorough, and accurate. This exhaustive analysis is in itself sufficient to prove that Beowulf was composed orally.\"\n\nExamination of \"Beowulf\" and other Old English literature for evidence of oral-formulaic composition has met with mixed response. While \"themes\" (inherited narrative subunits for representing familiar classes of event, such as the \"arming the hero\", or the particularly well-studied \"hero on the beach\" theme) do exist across Anglo-Saxon and other Germanic works, some scholars conclude that Anglo-Saxon poetry is a mix of oral-formulaic and literate patterns, arguing that the poems both were composed on a word-by-word basis and followed larger formulae and patterns.\n\nLarry Benson argued that the interpretation of \"Beowulf\" as an entirely formulaic work diminishes the ability of the reader to analyze the poem in a unified manner, and with due attention to the poet's creativity. Instead, he proposed that other pieces of Germanic literature contain \"kernels of tradition\" from which \"Beowulf\" borrows and expands upon. A few years later, Ann Watts argued against the imperfect application of one theory to two different traditions: traditional, Homeric, oral-formulaic poetry and Anglo-Saxon poetry. Thomas Gardner agreed with Watts, arguing that the \"Beowulf\" text is of too varied a nature to be completely constructed from set formulae and themes.\n\nJohn Miles Foley wrote, referring to the \"Beowulf\" debate, that while comparative work was both necessary and valid, it must be conducted with a view to the particularities of a given tradition; Foley argued with a view to developments of oral traditional theory that do not assume, or depend upon, ultimately unverifiable assumptions about composition, and instead delineate a more fluid continuum of traditionality and textuality.\n\nFinally, in the view of Ursula Schaefer, the question of whether the poem was \"oral\" or \"literate\" becomes something of a red herring. In this model, the poem is created, and is interpretable, within both noetic horizons. Schaefer's concept of \"vocality\" offers neither a compromise nor a synthesis of the views which see the poem as on the one hand Germanic, pagan, and oral and on the other Latin-derived, Christian, and literate, but, as stated by Monika Otter: \"... a 'tertium quid', a modality that participates in both oral and literate culture yet also has a logic and aesthetic of its own.\"\n\nNeither identified sources nor analogues for \"Beowulf\" can be definitively proven, but many conjectures have been made. These are important in helping historians understand the \"Beowulf\" manuscript, as possible source-texts or influences would suggest time-frames of composition, geographic boundaries within which it could be composed, or range (both spatial and temporal) of influence (i.e. when it was \"popular\" and where its \"popularity\" took it). There are five main categories in which potential sources and/or analogues are included: Scandinavian parallels, early Irish literature sources and analogues, classical sources, ecclesiastical sources, and echoes in other Old English texts.\n\nEarly studies into Scandinavian sources and analogues proposed that \"Beowulf\" was a translation of an original Scandinavian work, but this idea has been discarded. In 1878, Guðbrandur Vigfússon made the connection between \"Beowulf\" and the \"Grettis saga\". This is currently one of the few Scandinavian analogues to receive a general consensus of potential connection. Tales concerning the Skjöldungs, possibly originating as early as the 6th century were later used as a narrative basis in such texts as \"Gesta Danorum\" by Saxo Grammaticus and \"Hrólfs saga kraka\". Some scholars see \"Beowulf\" as a product of these early tales along with \"Gesta Danorum\" and \"Hrólfs saga kraka\", and some early scholars of the poem proposed that the latter saga and \"Beowulf\" share a common legendary ancestry, \"Beowulf\"s Hrothulf being identified with Hrólf Kraki. Paul Beekman Taylor argued that the \"Ynglinga saga\" was proof that the \"Beowulf\" poet was likewise working from Germanic tradition.\n\nFriedrich Panze attempted to contextualise \"Beowulf\" and other Scandinavian works, including \"Grettis saga\", under the international folktale type 301B, or \"The Bear's Son\" tale. However, although this folkloristic approach was seen as a step in the right direction, \"The Bear's Son\" tale was seen as too universal. Later, Peter Jørgensen, looking for a more concise frame of reference, coined a \"two-troll tradition\" that covers both \"Beowulf\" and \"Grettis saga\": \"a Norse 'ecotype' in which a hero enters a cave and kills two giants, usually of different sexes\".\n\nScholars who favored Irish parallels directly spoke out against pro-Scandinavian theories, citing them as unjustified. Wilhelm Grimm is noted to be the first person to link \"Beowulf\" with Irish folklore; however, Max Deutschbein is the first person to present the argument in academic form. He suggested the Irish \"Feast of Bricriu\" as a source for \"Beowulf\"—a theory that was soon denied by Oscar Olson. Swedish folklorist Carl Wilhelm Von Sydow argued against both Scandinavian translation and source material due to his theory that \"Beowulf\" is fundamentally Christian and written at a time when any Norse tale would have most likely been pagan.\n\nIn the late 1920s, Heinzer Dehmer suggested \"Beowulf\" as contextually based in the folktale type \"The Hand and the Child,\" due to the motif of the \"monstrous arm\"—a motif that distances \"Grettis saga\" and \"Beowulf\" and further aligns \"Beowulf\" with Irish parallelism. James Carney and Martin Puhvel also agree with this \"Hand and the Child\" contextualisation. Carney also ties \"Beowulf\" to Irish literature through the \"Táin Bó\" \"Fráech\" story. Puhvel supported the \"Hand and the Child\" theory through such motifs as (in Andersson's words) \"the more powerful giant mother, the mysterious light in the cave, the melting of the sword in blood, the phenomenon of battle rage, swimming prowess, combat with water monsters, underwater adventures, and the bear-hug style of wrestling.\"\n\nAttempts to find classical or Late Latin influence or analogue in \"Beowulf\" are almost exclusively linked with Homer's \"Odyssey\" or Virgil's \"Aeneid\". In 1926, Albert Stanburrough Cook suggested a Homeric connection due to equivalent formulas, metonymies, and analogous voyages. In 1930, James A. Work also supported the Homeric influence, stating that encounter between Beowulf and Unferth was parallel to the encounter between Odysseus and Euryalus in Books 7–8 of the \"Odyssey,\" even to the point of both characters giving the hero the same gift of a sword upon being proven wrong in their initial assessment of the hero's prowess. This theory of Homer's influence on \"Beowulf\" remained very prevalent in the 1920s, but started to die out in the following decade when a handful of critics stated that the two works were merely \"comparative literature\", although Greek was known in late 7th century England: Bede states that Theodore of Tarsus, a Greek, was appointed Archbishop of Canterbury in 668, and he taught Greek. Several English scholars and churchmen are described by Bede as being fluent in Greek due to being taught by him; Bede claims to be fluent in Greek himself.\n\nFrederick Klaeber, among others, argued for a connection between \"Beowulf\" and Virgil near the start of the 20th century, claiming that the very act of writing a secular epic in a Germanic world represents Virgilian influence. Virgil was seen as the pinnacle of Latin literature, and Latin was the dominant literary language of England at the time, therefore making Virgilian influence highly likely. Similarly, in 1971, Alistair Campbell stated that the apologue technique used in \"Beowulf\" is so rare in epic poetry aside from Virgil that the poet who composed \"Beowulf\" could not have written the poem in such a manner without first coming across Virgil's writings.\n\nIt cannot be denied that Biblical parallels occur in the text, whether seen as a pagan work with \"Christian colouring\" added by scribes or as a \"Christian historical novel, with selected bits of paganism deliberately laid on as 'local colour',\" as Margaret E. Goldsmith did in \"The Christian Theme of \"Beowulf,\"\". \"Beowulf\" channels the books of \"Genesis\", \"Exodus\", and \"Daniel\" in its inclusion of references to the Genesis creation narrative, the story of Cain and Abel, Noah and the flood myth, the Devil, Hell, and the Last Judgment.\n\nThe poem mixes the West Saxon and Anglian dialects of Old English, though it predominantly uses West Saxon, as do other Old English poems copied at the time.\n\nThere is a wide array of linguistic forms in the \"Beowulf\" manuscript. It is this fact that leads some scholars to believe that \"Beowulf\" has endured a long and complicated transmission through all the main dialect areas. The poem retains a complicated mix of the following dialectical forms: Mercian, Northumbrian, Early West Saxon, Kentish and Late West Saxon. There are in \"Beowulf\" more than thirty-one hundred distinct words, and almost thirteen hundred occur exclusively, or almost exclusively, in this poem and in the other poetical texts. Considerably more than one-third of the total vocabulary is alien from ordinary prose use. There are, in round numbers, three hundred and sixty uncompounded verbs in \"Beowulf\", and forty of them are poetical words in the sense that they are unrecorded or rare in the existing prose writings. One hundred and fifty more occur with the prefix \"ge\"- (reckoning a few found only in the past-participle), but of these one hundred occur also as simple verbs, and the prefix is employed to render a shade of meaning which was perfectly known and thoroughly familiar except in the latest Anglo-Saxon period. The nouns number sixteen hundred. Seven hundred of them, including those formed with prefixes, of which fifty (or considerably more than half) have \"ge\"-, are simple nouns, at the highest reckoning not more than one-fourth is absent in prose. That this is due in some degree to accident is clear from the character of the words, and from the fact that several reappear and are common after the Norman Conquest.\n\nAn Old English poem such as \"Beowulf\" is very different from modern poetry. Anglo-Saxon poets typically used alliterative verse, a form of verse in which the first half of the line (the a-verse) is linked to the second half (the b-verse) through similarity in initial sound. In addition, the two halves are divided by a caesura: \"Oft Scyld Scefing \\\\ sceaþena þreatum\" (l. 4). This verse form maps stressed and unstressed syllables onto abstract entities known as metrical positions. There is no fixed number of beats per line: the first one cited has three (Oft SCYLD SCEFING, with ictus on the suffix -ING) whereas the second has two (SCEAþena ÞREATum).\n\nThe poet has a choice of epithets or formulae to use in order to fulfill the alliteration. When speaking or reading Old English poetry, it is important to remember for alliterative purposes that many of the letters are not pronounced in the same way as in modern English. The letter , for example, is always pronounced (\"Hroðgar\": ), and the digraph is pronounced , as in the word \"edge\". Both and vary in pronunciation depending on their phonetic environment. Between vowels or voiced consonants, they are voiced, sounding like modern and , respectively. Otherwise they are unvoiced, like modern in \"fat\" and in \"sat\". Some letters which are no longer found in modern English, such as thorn, , and eth, – representing both pronunciations of modern English , as in \"thing\" and \"this\" – are used extensively both in the original manuscript and in modern English editions. The voicing of these characters echoes that of and . Both are voiced (as in \"this\") between other voiced sounds: \"oðer\", \"laþleas\", \"suþern\". Otherwise they are unvoiced (as in \"thing\"): \"þunor\", \"suð\", \"soþfæst\".\n\nKennings are also a significant technique in \"Beowulf\". They are evocative poetic descriptions of everyday things, often created to fill the alliterative requirements of the metre. For example, a poet might call the sea the \"swan-road\" or the \"whale-road\"; a king might be called a \"ring-giver.\" There are many kennings in \"Beowulf\", and the device is typical of much of classic poetry in Old English, which is heavily formulaic. The poem also makes extensive use of elided metaphors.\n\nJ. R. R. Tolkien argued that the poem is an elegy.\n\nThe history of modern \"Beowulf\" criticism is often said to begin with J. R. R. Tolkien, author and Merton professor of Anglo-Saxon at University of Oxford, who in his 1936 lecture to the British Academy criticised his contemporaries' excessive interest in its historical implications. He noted in \"\" that as a result the poem's literary value had been largely overlooked and argued that the poem \"is in fact so interesting as poetry, in places poetry so powerful, that this quite overshadows the historical content...\"\n\nIn historical terms, the poem's characters would have been Norse pagans (the historical events of the poem took place before the Christianisation of Scandinavia), yet the poem was recorded by Christian Anglo-Saxons who had largely converted from their native Anglo-Saxon paganism around the 7th century – both Anglo-Saxon paganism and Norse paganism share a common origin as both are forms of Germanic paganism. \"Beowulf\" thus depicts a Germanic warrior society, in which the relationship between the lord of the region and those who served under him was of paramount importance.\n\nStanley B. Greenfield has suggested that references to the human body throughout \"Beowulf\" emphasise the relative position of thanes to their lord. He argues that the term \"shoulder-companion\" could refer to both a physical arm as well as a thane (Aeschere) who was very valuable to his lord (Hrothgar). With Aeschere's death, Hrothgar turns to Beowulf as his new \"arm.\" In addition, Greenfield argues the foot is used for the opposite effect, only appearing four times in the poem. It is used in conjunction with Unferð (a man described by Beowulf as weak, traitorous, and cowardly). Greenfield notes that Unferð is described as \"at the king's feet\" (line 499). Unferð is also a member of the foot troops, who, throughout the story, do nothing and \"generally serve as backdrops for more heroic action.\"\n\nAt the same time, Richard North argues that the \"Beowulf\" poet interpreted \"Danish myths in Christian form\" (as the poem would have served as a form of entertainment for a Christian audience), and states: \"As yet we are no closer to finding out why the first audience of \"Beowulf\" liked to hear stories about people routinely classified as damned. This question is pressing, given... that Anglo-Saxons saw the Danes as 's' rather than as foreigners.\" Grendel's mother and Grendel are described as descendants of Cain, a fact which some scholars link to the Cain tradition.\n\nOther scholars disagree, however, as to the meaning and nature of the poem: is it a Christian work set in a Germanic pagan context? The question suggests that the conversion from the Germanic pagan beliefs to Christian ones was a very slow and gradual process over several centuries, and it remains unclear the ultimate nature of the poem's message in respect to religious belief at the time it was written. Robert F. Yeager notes the facts that form the basis for these questions:\nThe location of the composition of the poem is also deeply disputed. In 1914, F.W. Moorman, the first professor of English Language at Leeds University, claimed the \"Beowulf\" was composed in Yorkshire, but E. Talbot Donaldson claimed that it was probably composed more than twelve hundred years ago, during the first half of the eighth century, and that the writer was a native of what was then called West Mercia, located in the Western Midlands of England. However, the late tenth-century manuscript \"which alone preserves the poem\" originated in the kingdom of the West Saxons – as it is more commonly known. Donaldson wrote that \"the poet who put the materials into their present form was a Christian and ... poem reflects a Christian tradition\".\n\n\n\n", "id": "3833", "title": "Beowulf"}
{"url": "https://en.wikipedia.org/wiki?curid=3836", "text": "Barb Wire\n\nBarb Wire was a superhero published by Comics Greatest World, an imprint of Dark Horse Comics. A regular series was published for 9 issues between 1994-1995, followed by a mini-series in 1996. In March 2015, Dark Horse announced they would be planning a new series starring the heroine.\n\nRegular Series:\n\nAce Of Spades (minseries):\n\n1–4: Chris Warner, script and pencils/Tim Bradstreet, inks.\n\nIn Steel Harbor, a bombed-out wreck of a town, thrill-junkie Barbara Kopetski—better known as Barb Wire—is a bar owner and part-time bounty hunter (in order to pay for her bar, the Hammerhead). She is skilled in many areas, but excels in combat-related abilities. While she has a brother and several allies, she is essentially a loner, although this is something which is uncomfortable for her to think about.\n\n\n\n\nThe movie adaptation was released in 1996 starring Pamela Anderson as Barb Wire. The film, panned by critics and fans alike, was nominated for the Razzie Award for Worst Picture, but lost to the Demi Moore film \"Striptease\".\n\n", "id": "3836", "title": "Barb Wire"}
{"url": "https://en.wikipedia.org/wiki?curid=3837", "text": "Blazing Saddles\n\nBlazing Saddles is a 1974 American satirical Western comedy film directed by Mel Brooks. Starring Cleavon Little and Gene Wilder, the film was written by Brooks, Andrew Bergman, Richard Pryor, Norman Steinberg, and Al Uger, and was based on Bergman's story and draft. The film received generally positive reviews from critics and audiences, was nominated for three Academy Awards, and is ranked No. 6 on the American Film Institute's \"100 Years...100 Laughs\" list.\n\nBrooks appears in two supporting roles, Governor William J. Le Petomane and a Yiddish-speaking Indian chief; he also dubs lines for one of Lili von Shtupp's backing troupe. The supporting cast includes Slim Pickens, Alex Karras, and David Huddleston, as well as Brooks regulars Dom DeLuise, Madeline Kahn, and Harvey Korman. Bandleader Count Basie has a cameo as himself.\n\nThe film satirizes the racism obscured by myth-making Hollywood accounts of the American West, with the hero being a black sheriff in an all-white town. The film is full of deliberate anachronisms, from the Count Basie Orchestra playing \"April in Paris\" in the Wild West, to Slim Pickens referring to the \"Wide World of Sports\", to the German army of World War II.\n\nIn the American Old West of 1874, construction on a new railroad will soon be going through Rock Ridge, a frontier town inhabited exclusively by white people with the surname Johnson. The conniving State Attorney General Hedley Lamarr (Harvey Korman) wants to force Rock Ridge's residents to abandon their town, thereby lowering land prices. After he sends a gang of thugs, led by his flunky assistant Taggart (Slim Pickens), to shoot the sheriff and trash the town, the townspeople demand that Governor William J. Le Petomane (Mel Brooks) appoint a new sheriff to protect them. Lamarr persuades the dim-witted Le Petomane to appoint Bart (Cleavon Little), a black railroad worker who was about to be hanged. A black sheriff, he reasons, will offend the townspeople, create chaos, and leave the town at his mercy.\n\nWith his quick wits and the assistance of recovering alcoholic gunslinger Jim, the Waco Kid (Gene Wilder), Bart works to overcome the townspeople's hostile reception. He subdues Mongo (Alex Karras), an immensely strong, dim-witted, but philosophical henchman sent to kill him, and then beats German seductress-for-hire Lili von Shtupp (Madeline Kahn) at her own game. Lamarr, furious that his schemes have backfired, hatches a larger plan involving a recruited army of thugs, including common criminals, Ku Klux Klansmen, and Nazi soldiers.\n\nThree miles east of Rock Ridge, Bart introduces the white townspeople to the black and Chinese railroad workers—who have agreed to help in exchange for acceptance by the community—and explains his plan to defeat Lamarr's army. They labor all night to build a perfect replica of their town, as a diversion; but with no people in it, Bart realizes it won't fool the villains. While the townspeople construct replicas of themselves, Bart, Jim, and Mongo buy time by constructing the \"William J. Le Pétomane Memorial Thruway\", forcing the raiding party to turn back for \"a shitload of dimes\" to pay the toll. Once through the tollbooth, the raiders attack the fake town populated with dummies, which are boobytrapped with dynamite bombs. After Jim detonates the bombs with his sharpshooting, launching bad guys and horses skyward, the Rock Ridgers storm the villains.\n\nThe resulting brawl between townsfolk, railroad workers, and Lamarr's thugs breaks the fourth wall—literally—spilling onto a neighboring set where director Buddy Bizarre (Dom DeLuise) is directing a Busby Berkeley-style top-hat-and-tails musical number; then into the studio commissary for a food fight; and then out of the Warner Bros. film lot into the streets of Burbank. Lamarr, realizing he has been beaten again, hails a taxi and orders the driver to \"get me out of this picture\". He ducks into Grauman's Chinese Theatre, which is playing the premiere of \"Blazing Saddles\". As he settles into his seat, he sees Bart arriving on horseback outside the theatre. Bart blocks Lamarr's escape, and then, in a spoof of a classic cinematic gunfight, shoots him in the groin. Bart and Jim then go into Grauman's to watch the end of the film, in which Bart announces to the townspeople that he is moving on, for his work there is done (and he is bored). Riding out of town, he finds Jim (finishing his popcorn), and invites him along to \"nowhere special\". The two friends ride off into the sunset—in a chauffeured stretch limousine.\n\n\nCast notes\n\nThe idea for the film came from a story outline written by Andrew Bergman that he originally intended to develop and produce himself. \"I wrote a first draft called \"Tex-X\"\" (a play on Malcolm X's name), he said. \"Alan Arkin was hired to direct and James Earl Jones was going to play the sheriff. That fell apart, as things often do.\" Brooks was taken with the story, which he described as \"hip talk—1974 talk and expressions—happening in 1874 in the Old West\", and purchased the film rights from Bergman. Though he had not worked with a writing team since \"Your Show of Shows\", he hired a group of writers (including Bergman) to expand the outline, and posted a large sign: \"Please do not write a polite script\". Brooks described the writing process as chaotic: \"\"Blazing Saddles\" was more or less written in the middle of a drunken fistfight. There were five of us all yelling loudly for our ideas to be put into the movie. Not only was I the loudest, but luckily I also had the right as director to decide what was in or out.\" Bergman remembers the room being just as chaotic, telling \"Creative Screenwriting\", \"In the beginning, we had five people. One guy left after a couple of weeks. Then, it was basically me, Mel, Richie Pryor and Norman Steinberg. Richie left after the first draft and then Norman, Mel and I wrote the next three or four drafts. It was a riot. It was a rioter’s room!\"\n\nThe original title, \"Tex X\", was rejected, as were \"Black Bart\" and \"Purple Sage\". Brooks said he finally conceived \"Blazing Saddles\" one morning while taking a shower. For the movie's title song, Brooks advertised in the trade papers for a \"Frankie Laine-type\" singer; to his surprise, Laine himself offered his services. \"Frankie sang his heart out ... and we didn't have the heart to tell him it was a spoof. He never heard the whip cracks; we put those in later. We got so lucky with his serious interpretation of the song.\"\n\nCasting was problematic. Richard Pryor was Brooks' original choice to play the sheriff, but the studio, claiming his history of drug arrests made him uninsurable, refused to approve financing with Pryor as the star. Cleavon Little was cast in the role, and Pryor remained as a writer. Brooks offered the other leading role, the Waco Kid, to John Wayne; he declined, deeming the film \"too blue\" for his family-oriented image, but assured Brooks that \"he would be the first one in line to see it\". Gig Young was cast, but he collapsed during his first scene from what was later determined to be alcohol withdrawal syndrome, and Gene Wilder was flown in to replace him. Johnny Carson and Wilder both turned down the Hedley Lamarr role before Harvey Korman was cast. Madeline Kahn objected when Brooks asked to see her legs during her audition. \"She said, 'So it’s THAT kind of an audition?'\" Brooks recalled. \"I explained that I was a happily married man and that I needed someone who could straddle a chair with her legs like Marlene Dietrich in \"Destry Rides Again.\" So she lifted her skirt and said, 'No touching.'\"\n\nBrooks had numerous conflicts over content with Warner Bros. executives, including frequent use of the word \"nigger\", Lili Von Shtupp's seduction scene, the cacophony of flatulence around the campfire, and Mongo punching out a horse. Brooks, whose contract gave him final content control, declined to make any substantive changes, with the exception of cutting Bart's final line during Lili's seduction: \"I hate to disappoint you, ma'am, but you're sucking my arm.\" When asked later about the many \"nigger\" references, Brooks said he received consistent support from Pryor and Little. He added, \"If they did a remake of \"Blazing Saddles\" today, they would leave out the N-word. And then, you've got no movie.\" Brooks said he received many letters of complaint after the film's release, \"... but of course, most of them were from white people.\"\n\nThe film was almost not released. “When we screened it for executives, there were few laughs,\" said Brooks. \"The head of distribution said, 'Let’s dump it and take a loss.’ But [studio president John] Calley insisted they open it in New York, Los Angeles and Chicago as a test. It became the studio’s top moneymaker that summer.” The world premiere took place on February 7, 1974, at the Pickwick Drive-In Theater in Burbank; 250 invited guests—including Little and Wilder—watched the film on horseback.\n\nHedy Lamarr sued Warner Bros., charging that the film's running parody of her name infringed on her right to privacy. Brooks said he was flattered; the studio settled out of court for a small sum and an apology for “almost using her name\". Brooks said that Lamarr \"never got the joke\".\n\nWhile the film is now considered a classic comedy, critical reaction was mixed when the film was released. Vincent Canby wrote:\nRoger Ebert gave the film four stars and called it a \"crazed grabbag of a movie that does everything to keep us laughing except hit us over the head with a rubber chicken. Mostly, it succeeds. It's an audience picture; it doesn't have a lot of classy polish and its structure is a total mess. But of course! What does that matter while Alex Karras is knocking a horse cold with a right cross to the jaw?\"\n\nThe film grossed $119.5 million at the box office, becoming only the tenth film up to that time to pass the $100 million mark.\n\nOn the film-critics aggregator Rotten Tomatoes, the film has 90% positive reviews, saying \"Daring, provocative, and laugh-out-loud funny, Blazing Saddles is a gleefully vulgar spoof of Westerns that marks a high point in Mel Brooks' storied career.\" \n\nThe film received three Academy Award nominations in 1974: Best Actress in a Supporting Role (Madeline Kahn), Best Film Editing, and Best Music, Original Song (the title song). The film also earned two BAFTA awards nominations, for Best Newcomer (Cleavon Little) and Best Screenplay.\n\nThe film won the Writers Guild of America Award for \"Best Comedy Written Directly for the Screen\" for writers Mel Brooks, Norman Steinberg, Andrew Bergman, Richard Pryor, and Alan Uger.\n\nIn 2006, \"Blazing Saddles\" was deemed \"culturally, historically, or aesthetically significant\" by the Library of Congress and was selected for preservation in the National Film Registry.\n\nThe film is recognized by American Film Institute in these lists:\n\nA television series titled \"Black Bart\" was produced for CBS based on Bergman's original story. It featured Louis Gossett, Jr. as Bart and Steve Landesberg as his drunkard sidekick, a former Confederate officer named \"Reb Jordan\". Other cast members included Millie Slavin and Noble Willingham. Bergman is listed as the sole creator.\n\nCBS aired the pilot once on April 4, 1975. The pilot episode featured guest appearances by Gerrit Graham and Brooke Adams and was written by Michael Elias and Rich Eustis. Elias and Eutis later created and executive produced the ABC sitcom \"Head of the Class\" (1986–1991).\n\nInterviewed in 1996, Steve Landesberg said \"Black Bart\" \"was like a joke. ... We did the pilot, and CBS dumped it at the end of the 1975 season in April or May on a Friday. We thought it was done, then CBS tells us to come back and film six more episodes. And then another six. Six episodes each season, when an order was usually for 24 or 26. I was on \"Barney Miller\" by that point, and we'd film during the winter break when all other TV shows were on hiatus. And they never aired any of them. It was like a sick joke. If I wasn't under contract I would have walked, but they were paying me so I can't complain.\"\n\nIn 1989, Louis Gossett, Jr. told \"Entertainment Tonight\", \"CBS and Warner Bros. made a deal. ... The deal was that CBS would get to air \"Blazing Saddles\", and any sequels from the movie, in exchange for co-producing a TV show. At the time Warners wanted to make \"Blazing Saddles\" into a comedy series of films, a new one coming out every year or so. They wanted to use the model that the Brits had for the \"Carry On\" films. But [Mel] Brooks had a clause in his contract that said Warner had to keep producing \"Blazing Saddles\" stories, in the movies or TV, or they'd lose the rights to make sequels. The TV show was a way to keep the rights. They didn't have to air it, just keep producing it. So for four years I spent my winter on a soundstage being paid to be in show that would never see the light of day, just so Warners could keep the sequel rights to \"Blazing Saddles\". By 1979 they finally figured out the market had changed and they weren't going to make any sequels, so we were cancelled, if a show that never was supposed to air can be cancelled.\"\n\nMel Brooks addressed the existence of the \"Black Bart\" series in 2005: \"My lawyers, bless their souls, came to me and said, 'Warner Bros. is going to try and take away your control of the movie. Let's put in a crazy condition that says they can't do any sequels unless they make it right away or make a TV show out of it within six months.' Which is brilliant. They couldn't make a sequel in six months, and the movie was too vulgar to be a TV show. Now it would air in family hour if that was still a thing. So the lawyers put that in, never thinking they'd make a TV show. ... In 1977, three years later, Warner Bros comes to me and says they want to make another \"Blazing Saddles\", and I say, 'No. You don't have the right to do that.' They say, 'Yes we do, we've been making a TV series and still control the rights.' What TV series? I haven't seen a TV show. They take me onto the lot, into a projection booth, and show me three episodes. My lawyers never thought to put in language that said they had to air the damn thing, only that they had to make it. Oy gevalt! Well, management changed and they never did \"Blazing Saddles 2\", and as far as I know they're still making that stupid show to this day.\"\n\nThe pilot episode of \"Black Bart\" was later included as a bonus feature on the \"Blazing Saddles\" 30th Anniversary DVD and the Blu-ray disc. To date, the pilot episode is the only episode of \"Black Bart\" that has ever been released publicly.\n\nThe first studio-licensed release of the full music soundtrack to \"Blazing Saddles\" was on La-La Land Records on August 26, 2008. Remastered from original studio vault elements, the limited edition CD (a run of 3000) features the songs from the film as well as composer John Morris's score. Instrumental versions of all the songs are bonus tracks on the disc. The disc features exclusive liner notes featuring comments from Mel Brooks and John Morris.\n\nThe 2017 animated film \"Blazing Samurai,\" starring Michael Cera, Samuel L. Jackson, Michelle Yeoh, and Ricky Gervais, has been characterized by its creators as \"equally inspired by and an homage to \"Blazing Saddles\"\". Brooks served as an executive producer for the production, and voiced one of the characters.\n\nThe film was first released on DVD in 1997. In 2006, the film was released on Blu-ray. A 40th Anniversary Blu-Ray set was released in 2014.\n\n\n", "id": "3837", "title": "Blazing Saddles"}
{"url": "https://en.wikipedia.org/wiki?curid=3838", "text": "Bruce Sterling\n\nMichael Bruce Sterling (born April 14, 1954) is an American science fiction author known for his novels and work on the \"Mirrorshades\" anthology. This work helped to define the cyberpunk genre.\n\nSterling, along with William Gibson, Rudy Rucker, John Shirley, Lewis Shiner, and Pat Cadigan, is one of the founders of the cyberpunk movement in science fiction. In addition, he is one of the subgenre's chief ideological promulgators. This has earned him the nickname \"Chairman Bruce\". He was also one of the first organizers of the Turkey City Writer's Workshop, and is a frequent attendee at the Sycamore Hill Writer's Workshop. He won Hugo Awards for his novelettes \"Bicycle Repairman\" and \"Taklamakan\". His first novel, \"Involution Ocean\", published in 1977, features the world Nullaqua where all the atmosphere is contained in a single, miles-deep crater. The story concerns a ship sailing on the ocean of dust at the bottom, which hunts creatures called dustwhales that live beneath the surface. It is partially a science-fictional pastiche of \"Moby-Dick\" by Herman Melville.\n\nFrom the late 1970s onwards, Sterling wrote a series of stories set in the Shaper/Mechanist universe: the solar system is colonised, with two major warring factions. The Mechanists use a great deal of computer-based mechanical technologies; the Shapers do genetic engineering on a massive scale. The situation is complicated by the eventual contact with alien civilizations; humanity eventually splits into many subspecies, with the implication that many of these effectively vanish from the galaxy, reminiscent of The Singularity in the works of Vernor Vinge. The Shaper/Mechanist stories can be found in the collection \"Crystal Express\" and the collection \"Schismatrix Plus\", which contains the original novel \"Schismatrix\" and all of the stories set in the Shaper/Mechanist universe. Alastair Reynolds identified \"Schismatrix\" and the other Shaper/Mechanist stories as one of the greatest influences on his own work.\nIn the 1980s, Sterling edited the science fiction critical fanzine \"Cheap Truth\" under the alias of Vincent Omniaveritas. He wrote a column called \"Catscan\" for the now-defunct science fiction critical magazine \"SF Eye\".\n\nHe contributed a chapter to \"Sound Unbound: Sampling Digital Music and Culture\" (The MIT Press, 2008) edited by Paul D. Miller a.k.a. DJ Spooky. He also contributed, along with Lewis Shiner, to the short story \"Mozart in Mirrorshades\".\n\nFrom April 2009 through May 2009, he was an editor at Cool Tools.\n\nSince October 2003 Sterling has blogged at \"Beyond the Beyond\", which is hosted by \"Wired\".\n\nHis most recent novel (as of 2013) is \"Love Is Strange\" (December 2012), a Paranormal Romance (40k).\n\nHe has been the instigator of three projects which can be found on the Web -\n\nSterling has a habit of coining neologisms to describe things that he believes will be common in the future, especially items which already exist in limited numbers.\n\nIn childhood, Sterling spent several years in India; today he has a notable fondness for Bollywood films. In 2003 he was appointed Professor at the European Graduate School where he is teaching summer intensive courses on media and design. In 2005, he became \"visionary in residence\" at Art Center College of Design in Pasadena, California. He lived in Belgrade with Serbian author and film-maker Jasmina Tešanović for several years, and married her in 2005. In September 2007 he moved to Turin, Italy. He also travels the world extensively giving speeches and attending conferences.\n\n\nSterling has been interviewed for documentaries like Freedom Downtime, TechnoCalyps and Traceroute.\n\n", "id": "3838", "title": "Bruce Sterling"}
{"url": "https://en.wikipedia.org/wiki?curid=3840", "text": "Brain abscess\n\nBrain abscess (or cerebral abscess) is an abscess caused by inflammation and collection of infected material, coming from local (ear infection, dental abscess, infection of paranasal sinuses, infection of the mastoid air cells of the temporal bone, epidural abscess) or remote (lung, heart, kidney etc.) infectious sources, within the brain tissue. The infection may also be introduced through a skull fracture following a head trauma or surgical procedures. Brain abscess is usually associated with congenital heart disease in young children. It may occur at any age but is most frequent in the third decade of life.\n\nFever, headache, and neurological problems, while classic, only occur in 20% of people.\n\nThe symptoms of brain abscess are caused by a combination of increased intracranial pressure due to a space-occupying lesion (headache, vomiting, confusion, coma), infection (fever, fatigue etc.) and focal neurologic brain tissue damage (hemiparesis, aphasia etc.). The most frequent presenting symptoms are headache, drowsiness, confusion, seizures, hemiparesis or speech difficulties together with fever with a rapidly progressive course. The symptoms and findings depend largely on the specific location of the abscess in the brain. An abscess in the cerebellum, for instance, may cause additional complaints as a result of brain stem compression and hydrocephalus. Neurological examination may reveal a stiff neck in occasional cases (erroneously suggesting meningitis). The famous triad of fever, headache and focal neurologic findings are highly suggestive of brain abscess.\n\nAnaerobic and microaerophilic cocci and gram-negative and gram-positive anaerobic bacilli are the predominate bacterial isolates. Many brain abscesses are polymicrobical. The predominant organisms include: \"Staphylococcus aureus\", aerobic and anaerobic streptococci (especially \"Streptococcus intermedius\"), \"Bacteroides\", \"Prevotella\", and \"Fusobacterium\" species, Enterobacteriaceae, \"Pseudomonas\" species, and other anaerobes. Less common organisms include: \"Haemophillus influenzae\", \"Streptococcus pneumoniae\" and \"Neisseria meningitides\".\n\nBacterial abscesses rarely (if ever) arise \"de novo\" within the brain, although establishing a cause can be difficult in many cases. There is almost always a primary lesion elsewhere in the body that must be sought assiduously, because failure to treat the primary lesion will result in relapse. In cases of trauma, for example in compound skull fractures where fragments of bone are pushed into the substance of the brain, the cause of the abscess is obvious. Similarly, bullets and other foreign bodies may become sources of infection if left in place. The location of the primary lesion may be suggested by the location of the abscess: infections of the middle ear result in lesions in the middle and posterior cranial fossae; congenital heart disease with right-to-left shunts often result in abscesses in the distribution of the middle cerebral artery; and infection of the frontal and ethmoid sinuses usually results in collection in the subdural sinuses.\n\nFungi and parasites may also cause the disease. Fungi and parasites are especially associated with immunocompromised patients. Other causes include: \"Nocardia asteroides\", \"Mycobacterium\", Fungi (e.g. \"Aspergillus\", \"Candida\", \"Cryptococcus\", \"Mucorales\", \"Coccidioides\", \"Histoplasma capsulatum\", \"Blastomyces dermatitidis\", \"Bipolaris\", \"Exophiala dermatitidis\", \"Curvularia pallescens\", \"Ochroconis gallopava\", \"Ramichloridium mackenziei\", \"Pseudallescheria boydii\"), Protozoa (e.g. \"Toxoplasma gondii\", \"Entamoeba histolytica\", \"Trypanosoma cruzi\", \"Schistosoma\", \"Paragonimus\"), and Helminths (e.g. \"Taenia solium\"). Organisms that are most frequently associated with brain abscess in patients with AIDS are poliovirus, \"Toxoplasma gondii\", and \"Cryptococcus neoformans\", though in infection with the latter organism, symptoms of meningitis generally predominate.\n\nThese organisms are associated with certain predisposing conditions: \n\nThe diagnosis is established by a computed tomography (CT) (with contrast) examination. At the initial phase of the inflammation (which is referred to as cerebritis), the immature lesion does not have a capsule and it may be difficult to distinguish it from other space-occupying lesions or infarcts of the brain. Within 4–5 days the inflammation and the concomitant dead brain tissue are surrounded with a capsule, which gives the lesion the famous ring-enhancing lesion appearance on CT examination with contrast (since intravenously applied contrast material can not pass through the capsule, it is collected around the lesion and looks as a ring surrounding the relatively dark lesion). Lumbar puncture procedure, which is performed in many infectious disorders of the central nervous system is contraindicated in this condition (as it is in all space-occupying lesions of the brain) because removing a certain portion of the cerebrospinal fluid may alter the concrete intracranial pressure balances and causes the brain tissue to move across structures within the skull (brain herniation).\n\nRing enhancement may also be observed in cerebral hemorrhages (bleeding) and some brain tumors. However, in the presence of the rapidly progressive course with fever, focal neurologic findings (hemiparesis, aphasia etc.) and signs of increased intracranial pressure, the most likely diagnosis should be the brain abscess.\n\nThe treatment includes lowering the increased intracranial pressure and starting intravenous antibiotics (and meanwhile identifying the causative organism mainly by blood culture studies).\n\nHyperbaric oxygen therapy (HBO2 or HBOT) is indicated as a primary and adjunct treatment which provides four primary functions.\nFirstly, HBOT reduces intracranial pressure. Secondly, high partial pressures of oxygen act as a bactericide and thus inhibits the anaerobic and functionally anaerobic flora common in brain abscess. Third, HBOT optimizes the immune function thus enhancing the host defense mechanisms and fourth, HBOT has been found to be of benefit when brain abscess is concomitant with cranial osteomyleitis.\n\nSecondary functions of HBOT include increased stem cell production and up-regulation of VEGF which aid in the healing and recovery process.\n\nSurgical drainage of the abscess remains part of the standard management of bacterial brain abscesses. The location and treatment of the primary lesion also crucial, as is the removal of any foreign material (bone, dirt, bullets, and so forth).\n\nThere are few exceptions to this rule: \"Haemophilus influenzae\" meningitis is often associated with subdural effusions that are mistaken for subdural empyemas. These effusions resolve with antibiotics and require no surgical treatment. Tuberculosis can produce brain abscesses that look identical to conventional bacterial abscesses on CT imaging. Surgical drainage or aspiration is often necessary to identify \"Mycobacterium tuberculosis\", but once the diagnosis is made no further surgical intervention is necessary.\n\nCT guided stereotactic aspiration is also indicated in the treatment of brain abscess.\n\nDeath occurs in about 10% of cases and people do well about 70% of the time. This is a large improvement from the 1960s due to improved ability to image the head, better neurosurgery and better antibiotics.\n\n", "id": "3840", "title": "Brain abscess"}
{"url": "https://en.wikipedia.org/wiki?curid=3845", "text": "Brigitte Bardot\n\nBrigitte Anne-Marie Bardot (; born 28 September 1934) is a French actress, singer and fashion model, who later became an animal rights activist. She was one of the best known sex symbols of the 1950s and 1960s and was widely referred to by her initials, B.B.\n\nBardot was an aspiring ballerina in her early life. She started her acting career in 1952. After appearing in 16 routine comedy films that had limited international release, she became world-famous in 1957 after starring in the controversial film \"And God Created Woman\". Bardot caught the attention of French intellectuals. She was the subject of Simone de Beauvoir's 1959 essay, \"The Lolita Syndrome\", which described Bardot as a \"locomotive of women's history\" and built upon existentialist themes to declare her the first and most liberated woman of post-war France. She later starred in Jean-Luc Godard's 1963 film \"Le Mépris\". For her role in Louis Malle's 1965 film \"Viva Maria!\" Bardot was nominated for a BAFTA Award for Best Foreign Actress. From 1969 to 1978, Bardot was the official face of Marianne (who had previously been anonymous) to represent the liberty of France.\n\nBardot retired from the entertainment industry in 1973. During her career in show business, she starred in 47 films, performed in several musical shows and recorded over 60 songs. She was awarded the Legion of Honour in 1985 but refused to accept it. After her retirement, she established herself as an animal rights activist. During the 2000s, she generated controversy by criticizing immigration and Islam in France and has been fined five times for inciting racial hatred.\n\nBardot was born in Paris, the daughter of Louis Bardot (1896–1975) and Anne-Marie \"Toty\" Bardot (née Mucel; 1912–1978). Louis had an engineering degree and worked with his father, Charles Bardot, in the family business. Louis and Anne-Marie married in 1933. Bardot grew up in an upper middle-class Roman Catholic observant home. When she was seven she was admitted to the Cours Hattemer, a private school. She went to school three days a week, and otherwise studied at home. This gave time for lessons at Madame Bourget's dance studio three days a week.\nBrigitte's mother also enrolled Brigitte's younger sister, Marie-Jeanne (born 5 May 1938), in dance. Marie-Jeanne eventually gave up dancing lessons and did not tell her mother, whereas Brigitte concentrated on ballet. In 1947, Bardot was accepted to the Conservatoire de Paris. For three years she attended ballet classes by Russian choreographer Boris Knyazev. One of her classmates was Leslie Caron. The other ballerinas nicknamed Bardot \"Bichette\" (\"Little Doe\").\n\nAt the invitation of an acquaintance of her mother, she modelled in a fashion show in 1949. In the same year, she modelled for a fashion magazine \"\"Jardin des Modes\"\" managed by journalist Hélène Lazareff. Aged 15, she appeared on an 8 March 1950 cover of \"Elle\" and was noticed by a young film director, Roger Vadim, while babysitting. He showed an issue of the magazine to director and screenwriter Marc Allégret, who offered Bardot the opportunity to audition for \"Les lauriers sont coupés\". Although Bardot got the role, the film was cancelled but made her consider becoming an actress. Her acquaintance with Vadim, who attended the audition, influenced her further life and career.\n\nAlthough the European film industry was then in its ascendancy, Bardot was one of the few European actresses to have the mass media's attention in the United States, an interest which she did not enjoy. She debuted in a 1952 comedy film, \"Le Trou Normand\" (English title: \"Crazy for Love\"). From 1952 to 1956, she appeared in seventeen films; in 1953 she played a role in Jean Anouilh's stageplay \"L'Invitation au Château\" (\"Invitation to the Castle\"). She received media attention when she attended the Cannes Film Festival in April 1953.\n\nHer films of the early and mid 1950s were generally lightweight romantic dramas, some historical, in which she was cast as ingénue or siren, often appearing nude or nearly so. She played bit parts in three English-language films, the British comedy \"Doctor at Sea\" (1955) with Dirk Bogarde, \"Helen of Troy\" (1954), in which she was understudy for the title role but appears only as Helen's handmaid and \"Act of Love\" (1954) with Kirk Douglas. Her French-language films were dubbed for international release. Director (and then soon-to-be ex-husband) Roger Vadim showcased her in \"And God Created Woman\" (1956) opposite Jean-Louis Trintignant. The film, about an immoral teenager in a respectable small-town setting, was a huge success and turned Bardot into an international star. In 1958 the moniker \"sex kitten\" was invented for her.\n\nDuring her early career, professional photographer Sam Lévin's photos contributed to her image of Bardot's sensuality. One showed Bardot from behind, dressed in a white corset. British photographer Cornel Lucas made images of Bardot in the 1950s and 1960s, that have become representative of her public persona.\n\nBardot was awarded a David di Donatello Award for Best Foreign actress for her role in \"A Very Private Affair\" (\"Vie privée\", 1962), directed by Louis Malle.\n\nIn May 1958, Bardot withdrew to the seclusion of Southern France, where she had bought the house \"La Madrague\" in Saint-Tropez. In 1963, she starred in Jean-Luc Godard's film \"Le Mépris\". Bardot was featured in many other films along with notable actors such as Alain Delon (\"Famous Love Affairs\"; \"Spirits of the Dead\"); Jean Gabin (\"In Case of Adversity\"); Sean Connery (\"Shalako\"); Jean Marais (\"Royal Affairs in Versailles\"; \"School for Love\"); Lino Ventura (\"Rum Runners\"); Annie Girardot (\"The Novices\"); Claudia Cardinale (\"The Legend of Frenchie King\"); Jeanne Moreau (\"Viva Maria!\"); Jane Birkin (\"Don Juan, or If Don Juan Were a Woman\"). Her career had traversed epochs where it was possible to say, \"In the Sixties and early Seventies, there was no better known – or more scandalous – movie star on earth. — Not since the death of Valentino had a star aroused such insane devotion in their fans.\" In 1973, Bardot announced she was retiring from acting as \"a way to get out elegantly\".\n\nShe participated in several musical shows and recorded many popular songs in the 1960s and 1970s, mostly in collaboration with Serge Gainsbourg, Bob Zagury and Sacha Distel, including \"Harley Davidson\"; \"Je Me Donne À Qui Me Plaît\"; \"Bubble gum\"; \"Contact\"; \"Je Reviendrai Toujours Vers Toi\"; \"L'Appareil À Sous\"; \"La Madrague\"; \"On Déménage\"; \"Sidonie\"; \"Tu Veux, Ou Tu Veux Pas?\"; \"Le Soleil De Ma Vie\" (the cover of Stevie Wonder's \"You Are the Sunshine of My Life\"); and the notorious \"Je t'aime... moi non-plus\". Bardot pleaded with Gainsbourg not to release this duet and he complied with her wishes; the following year, he rerecorded a version with British-born model and actress Jane Birkin that became a massive hit all over Europe. The version with Bardot was issued in 1986 and became a popular download hit in 2006 when Universal Music made its back catalogue available to purchase online, with this version of the song ranking as the third most popular download.\n\nOn 21 December 1952, aged 18, Bardot married director Roger Vadim. They divorced in 1957, after less than five years of marriage; they had no children together, but remained in touch, and even collaborated on later projects. The stated reason for the divorce was Bardot's affair with two other men. While married to Vadim, Bardot had an affair with Jean-Louis Trintignant, who was her co-star in \"And God Created Woman.\" Trintignant was also a married man, being at the time married to actress Stéphane Audran. The two lived together for about two years, spanning the period before and after Bardot's divorce from Vadim, but they never married. Their relationship was complicated by Trintignant's frequent absence due to military service and Bardot's affair with musician Gilbert Bécaud.\n\nIn early 1958, after her divorce from Vadim, it was followed in quick order by her break-up with Trintignant and Bardot suffered a reported nervous breakdown in Italy, according to newspaper reports. A suicide attempt with sleeping pills two days earlier was also noted, but was denied by her public relations manager. She recovered within weeks and then began an affair with the actor Jacques Charrier. She became pregnant well before they were married on 18 June 1959. Bardot's only child, her son Nicolas-Jacques Charrier, was born on 11 January 1960. After she and Charrier divorced in 1962, Nicolas was raised in the Charrier family and had little contact with his biological mother until his adulthood.\n\nBardot's third marriage was to German millionaire playboy Gunter Sachs, and it lasted from 14 July 1966 to 1 October 1969. In 1968, she began dating Patrick Gilles, who went on to costar with her in \"The Bear and the Doll\" (1970); but she ended their relationship in the spring of 1971. Over the next few years, Bardot dated in succession the bartender/ski instructor Christian Kalt; club owner Luigi Rizzi; musician (later producer) Bob Zagury; singer Serge Gainsbourg; writer John Gilmore; actor Warren Beatty, and Laurent Vergez, who was her co-star in \"Don Juan, or If Don Juan Were a Woman\". The longest of these casual relationships was with sculptor Miroslav Brozek. She lived with him from 1975 to December 1979, posed for some of his sculptures. After breaking up with Brozek, she was involved in a long-term relationship with French TV producer Allain Bougrain-duBourg.\n\nIn 1974, Bardot appeared in a nude photo shoot in \"Playboy\" magazine, which celebrated her 40th birthday. On 28 September 1983, her 49th birthday, Bardot took an overdose of sleeping pills or tranquilizers with red wine. She had to be rushed to hospital, where her life was saved after a stomach pump was used to evacuate the pills from her body. Bardot is also a breast cancer survivor.\n\nBardot's fourth and current husband is Bernard d'Ormale, a former adviser of Jean-Marie Le Pen, former leader of the far right party Front National; they were married on 16 August 1992.\n\nIn 1973, before her 39th birthday, Bardot announced her retirement. After appearing in more than forty motion pictures and recording several music albums, most notably with Serge Gainsbourg, she used her fame to promote animal rights.\n\nIn 1986, she established the Brigitte Bardot Foundation for the Welfare and Protection of Animals. She became a vegetarian and raised three million francs to fund the foundation by auctioning off jewellery and personal belongings.\n\nShe is a strong animal rights activist and a major opponent of the consumption of horse meat. In support of animal protection, she condemned seal hunting in Canada during a visit to that country with Paul Watson of the Sea Shepherd Conservation Society. On 25 May 2011 the Sea Shepherd Conservation Society renamed its fast interceptor vessel, MV \"Gojira\", as MV \"Brigitte Bardot\" in appreciation of her support.\n\nShe once had a neighbour's donkey castrated while looking after it, on the grounds of its \"sexual harassment\" of her own donkey and mare, for which she was taken to court by the donkey's owner in 1989. Bardot wrote a 1999 letter to Chinese President Jiang Zemin, published in French magazine VSD, in which she accused the Chinese of \"torturing bears and killing the world's last tigers and rhinos to make aphrodisiacs\".\n\nShe has donated more than $140,000 over two years for a mass sterilization and adoption program for Bucharest's stray dogs, estimated to number 300,000.\n\nIn August 2010, Bardot addressed a letter to the Queen of Denmark, Margrethe II of Denmark, appealing for the sovereign to halt the killing of dolphins in the Faroe Islands. In the letter, Bardot describes the activity as a \"macabre spectacle\" that \"is a shame for Denmark and the Faroe Islands ... This is not a hunt but a mass slaughter ... an outmoded tradition that has no acceptable justification in today's world\".\n\nOn 22 April 2011, French culture minister Frédéric Mitterrand officially included bullfighting in the country's cultural heritage. Bardot wrote him a highly critical letter of protest.\n\nFrom 2013 onwards the Brigitte Bardot Foundation in collaboration with Kagyupa International Monlam Trust of India has operated annual Veterinary Care Camp. She has committed to the cause of animal welfare in Bodhgaya year after year.\n\nBardot expressed support for President Charles de Gaulle in the 1960s. Her husband Bernard d'Ormale is a former adviser of the Front National, the main far right party in France, known for its nationalist and conservative beliefs.\n\nIn her 1999 book \"Le Carré de Pluton\" (\"\"Pluto's Square\"\"), Bardot criticizes the procedure used in the ritual slaughter of sheep during the Muslim festival of Eid al-Adha. Additionally, in a section in the book entitled, \"Open Letter to My Lost France\", Bardot writes that \"\"my country, France, my homeland, my land is again invaded by an overpopulation of foreigners, especially Muslims\"\". For this comment, a French court fined her 30,000 francs in June 2000. She had been fined in 1997 for the original publication of this open letter in \"Le Figaro\" and again in 1998 for making similar remarks.\n\nIn her 2003 book, \"Un cri dans le silence\" (\"\"A Scream in the Silence\"\"), she warned of an \"Islamicization of France\", and said of Muslim immigration:\nOver the last twenty years, we have given in to a subterranean, dangerous, and uncontrolled infiltration, which not only resists adjusting to our laws and customs but which will, as the years pass, attempt to impose its own.\n\nIn the book, she contrasted her close gay friends with today's homosexuals, who \"jiggle their bottoms, put their little fingers in the air and with their little castrato voices moan about what those ghastly heteros put them through\" and that some contemporary homosexuals behave like \"fairground freaks\". In her own defence, Bardot wrote in a letter to a French gay magazine: \"Apart from my husband — who maybe will cross over one day as well — I am entirely surrounded by homos. For years, they have been my support, my friends, my adopted children, my confidants.\"\n\nIn her book she wrote about issues such as racial mixing, immigration, the role of women in politics and Islam. The book also contained a section attacking what she called the mixing of genes and praised previous generations who, she said, had given their lives to push out invaders.\n\nOn 10 June 2004, Bardot was convicted for a fourth time by a French court for \"inciting racial hatred\" and fined €5,000. Bardot denied the racial hatred charge and apologized in court, saying: \"I never knowingly wanted to hurt anybody. It is not in my character.\"\n\nIn 2008, Bardot was convicted of inciting racial/religious hatred in relation to a letter she wrote, a copy of which she sent to Nicolas Sarkozy when he was Interior Minister of France. The letter stated her objections to Muslims in France ritually slaughtering sheep by slitting their throats without anesthetizing them first. She also said, in reference to Muslims, that she was \"fed up with being under the thumb of this population which is destroying us, destroying our country and imposing its habits\". The trial concluded on 3 June 2008, with a conviction and fine of €15,000, the largest of her fines to date. The prosecutor stated that she was tired of charging Bardot with offences related to racial hatred.\n\nDuring the 2008 United States presidential election, she branded the Republican Party vice-presidential candidate Sarah Palin as \"stupid\" and a \"disgrace to women\". She criticized the former governor of Alaska for her stance on global warming and gun control. She was also offended by Palin's support for Arctic oil exploration and for her lack of consideration in protecting polar bears.\n\nOn 13 August 2010, Bardot lashed out at director Kyle Newman regarding his plan to make a biographical film on her life. She told him, \"Wait until I'm dead before you make a movie about my life!\" otherwise \"sparks will fly\".\n\nIn fashion, the Bardot neckline (a wide open neck that exposes both shoulders) is named after her. Bardot popularized this style which is especially used for knitted sweaters or jumpers although it is also used for other tops and dresses. Bardot popularized the bikini in her early films such as \"Manina\" (1952) (released in France as \"Manina, la fille sans voiles\"). The following year she was also photographed in a bikini on every beach in the south of France during the Cannes Film Festival. She gained additional attention when she filmed \"...And God Created Woman\" (1956) with Jean-Louis Trintignant (released in France as \"Et Dieu Créa La Femme\"). Bardot portrayed an immoral teenager cavorting in a bikini who seduces men in a respectable small-town setting. The film was an international success. The bikini was in the 1950s relatively well accepted in France but was still considered risqué in the United States. As late as 1959, Anne Cole, one of the United States' largest swimsuit designers, said, \"It's nothing more than a G-string. It's at the razor's edge of decency.\"\n\nShe also brought into fashion the \"choucroute\" (\"Sauerkraut\") hairstyle (a sort of beehive hair style) and gingham clothes after wearing a checkered pink dress, designed by Jacques Esterel, at her wedding to Charrier. She was the subject for an Andy Warhol painting. \n\nA modelling pose shot around 1960 where Bardot is dressed only in a pair of black pantyhose and cross-legged over her front and cross-armed over her breasts known as the Bardot pose, is an iconic one and has been emulated numerous times by models and celebrities such as Lindsay Lohan, Elle Macpherson and Monica Bellucci, among others.\n\nThe Australian pop group Bardot was named after her.\n\nIn addition to popularizing the bikini swimming suit, Bardot has been credited with popularizing the city of St. Tropez and the town of Armação dos Búzios in Brazil, which she visited in 1964 with her boyfriend at the time, Brazilian musician Bob Zagury. The place where she stayed in Búzios is today a small hotel, Pousada do Sol, and also a French restaurant, Cigalon.\n\nA statue by Christina Motta honours Brigitte Bardot in Armação dos Búzios.\n\nBardot was idolized by the young John Lennon and Paul McCartney. They made plans to shoot a film featuring The Beatles and Bardot, similar to \"A Hard Day's Night\", but the plans were never fulfilled. Lennon's first wife Cynthia Powell lightened her hair color to more closely resemble Bardot, while George Harrison made comparisons between Bardot and his first wife Pattie Boyd, as Cynthia wrote later in \"A Twist of Lennon\". Lennon and Bardot met in person once, in 1968 at the Mayfair Hotel, introduced by Beatles press agent Derek Taylor; a nervous Lennon took LSD before arriving, and neither star impressed the other. (Lennon recalled in a memoir, \"I was on acid, and she was on her way out.\") According to the liner notes of his first (self-titled) album, musician Bob Dylan dedicated the first song he ever wrote to Bardot. He also mentioned her by name in \"I Shall Be Free\", which appeared on his second album, \"The Freewheelin' Bob Dylan\". The first-ever official exhibition spotlighting Bardot's influence and legacy opened in Paris on 29 September 2009 – a day after her 75th birthday.\n\nBardot released several albums and singles during the 1960s and 1970s\n\nBardot has also written five books:\n\n\n", "id": "3845", "title": "Brigitte Bardot"}
{"url": "https://en.wikipedia.org/wiki?curid=3846", "text": "Banjo\n\nThe banjo is a four-, five- or six-stringed instrument with a thin membrane stretched over a frame or cavity as a resonator, called the head. The membrane, or head, is typically made of plastic, although animal skin is still occasionally but rarely used, and the frame is typically circular. Early forms of the instrument were fashioned by Africans in America, adapted from African instruments of similar design.\n\nThe banjo is frequently associated with country, folk, Irish traditional and bluegrass music. Historically, the banjo occupied a central place in African American traditional music, before becoming popular in the minstrel shows of the 19th century. The banjo, with the fiddle, is a mainstay of American old-time music. It is also very frequently used in Traditional (\"Trad\") Jazz.\n\nThe modern banjo derives from instruments that had been used in the Caribbean since the 17th century by enslaved people taken from West Africa. Written references to the banjo in North America appear in the 18th century, and the instrument became increasingly available commercially from around the second quarter of the 19th century. \n\nThe etymology of the name \"banjo\" is uncertain. The word could have come from the Yoruba word \"Bami jo,\" which means \"dance for me.\" It may derive from the Kimbundu word \"mbanza\". Some etymologists believe it comes from a dialectal pronunciation of the Portuguese \"bandore\" or from an early anglicisation of the Spanish word \"bandurria\", though other research suggests that it may come from a West African term for a bamboo stick formerly used for the instrument's neck.\n\nA Banza: a five double string courses Portuguese viuhela with two short strings. Mbanza is a string African instrument that has been built after the Portuguese Banza. \"Banza\" is quite similar to \"Banjo\".\n\nVarious instruments in Africa, chief among them the kora, feature a skin head and gourd (or similar shell) body. The African instruments differ from early African American banjos in that the necks do not possess a Western-style fingerboard and tuning pegs, instead having stick necks, with strings attached to the neck with loops for tuning. Banjos with fingerboards and tuning pegs are known from the Caribbean as early as the 17th century. 18th- and early 19th-century writers transcribed the name of these instruments variously as \"bangie\", \"banza\", \"bonjaw\", \"banjer\" and \"banjar\". Instruments similar to the banjo (e.g., the Japanese \"shamisen\", Persian \"tar\", and Moroccan \"sintir\") have been played in many countries. Another likely relative of the banjo is the \"akonting\", a spike folk lute played by the Jola tribe of Senegambia, and the \"ubaw-akwala\" of the Igbo. Similar instruments include the \"xalam\" of Senegal and the \"ngoni\" of the Wassoulou region including parts of Mali, Guinea, and Ivory Coast, as well as a larger variation of the \"ngoni\" developed in Morocco by sub-Saharan Africans known as the \"gimbri\".\n\nEarly, African-influenced banjos were built around a gourd body and a wooden stick neck. These instruments had varying numbers of strings, though often including some form of drone. The five-string banjo was popularized by Joel Walker Sweeney, an American minstrel performer from Appomattox Court House, Virginia.\n\nIn the 1830s Sweeney became the first white performer to play the banjo on stage. His version of the instrument replaced the gourd with a drum-like sound box and included four full-length strings alongside a short fifth string. This new banjo was at first tuned d'Gdf♯a, though by the 1890s this had been transposed up to g'cgbd'. Banjos were introduced in Britain by Sweeney's group, the American Virginia Minstrels, in the 1840s, and became very popular in music halls.\n\nIn the Antebellum South, many black slaves played the banjo and taught their masters how to play. For example, in his memoir entitled \"With Sabre and Scalpel: The Autobiography of a Soldier and Surgeon\", Confederate veteran and surgeon John Allan Wyeth recalls learning it from a slave as a child on his family plantation.\n\nTwo techniques closely associated with the five-string banjo are rolls and drones. Rolls are right hand accompanimental fingering pattern[s] that consist of eight (eighth) notes that subdivide each measure. Drone notes are quick little notes [typically eighth notes], usually played on the 5th (short) string to fill in around the melody notes [typically eighth notes]. These techniques are both idiomatic to the banjo in all styles, and their sound is characteristic of bluegrass.\n\nHistorically, the banjo was played in the clawhammer style by the Africans who brought their version of the banjo with them. Several other styles of play were developed from this. Clawhammer consists of downward striking of one or more of the four main strings with the index, middle or both finger(s)while the drone or fifth string is played with a 'lifting' (as opposed to downward pluck) motion of the thumb. The notes typically sounded by the thumb in this fashion are, usually, on the off beat. Melodies can be quite intricate adding techniques such as double thumbing and drop thumb. In old time Appalachian Mountain music, there is also a style called two finger up-pick, and a three finger version that Earl Scruggs developed into the famous \"Scruggs\" style picking, nationally aired in 1945 on the Grand Ole Opry.\n\nWhile five-string banjos are traditionally played with either fingerpicks or the fingers themselves, tenor banjos and plectrum banjos are played with a pick, either to strum full chords or, most commonly in Irish Traditional Music, play single note melodies.\n\nThe modern banjo comes in a variety of forms, including four- and five-string versions. A six-string version, tuned and played similarly to a guitar, has gained popularity. In almost all of its forms, banjo playing is characterized by a fast arpeggiated plucking, though there are many different playing styles.\n\nThe body, or \"pot\", of a modern banjo typically consists of a circular rim (generally made of wood, though metal was also common on older banjos) and a tensioned head, similar to a drum head. Traditionally the head was made from animal skin, but today is often made of various synthetic materials. Most modern banjos also have a metal \"tone ring\" assembly that helps further clarify and project the sound, however many older banjos do not include a tone ring.\n\nThe banjo is usually tuned with friction tuning pegs or planetary gear tuners, rather than the worm gear machine head used on guitars. Frets have become standard since the late 19th century, though fretless banjos are still manufactured and played by those wishing to execute glissando, play quarter tones, or otherwise achieve the sound and feeling of early playing styles.\n\nModern banjos are typically strung with metal strings. Usually the fourth string is wound with either steel or bronze-phosphor alloy. Some players may string their banjos with nylon or gut strings to achieve a more mellow, old-time tone.\n\nSome banjos have a separate resonator plate on the back of the pot to project the sound forward and give the instrument more volume. This type of banjo is usually used in bluegrass music, though resonator banjos are played by players of all styles, and are also used in old-time, sometimes as a substitute for electric amplification when playing in large venues.\n\nOpen-back banjos generally have a mellower tone and weigh less than resonator banjos. They usually have a different setup than a resonator banjo, often with a higher string action.\n\nThe modern five-string banjo is a variation on Sweeney's original design. The fifth string is usually the same gauge as the first, but starts from the fifth fret, three quarters the length of the other strings. This lets the string be tuned to a higher open pitch than possible for the full-length strings. Because of the short fifth string, the five-string banjo uses a reentrant tuning—the string pitches don't proceed lowest to highest across the fingerboard. Instead, the fourth string is lowest, then third, second, first, and the fifth string is highest.\n\nThe short fifth string presents special problems for a capo. For small changes (going up or down one or two semitones, for example) it is possible simply to re-tune the fifth string. Otherwise, various devices called \"fifth string capos\" effectively shorten the vibrating part of the string. Many banjo players use model railroad spikes or titanium spikes (usually installed at the seventh fret and sometimes at others), that they hook the string under to press it down on the fret.\n\nFive-string banjo players use many tunings. Probably the most common, particularly in bluegrass, is the Open-G tuning G4 D3 G3 B3 D4. In earlier times, the tuning G4 C3 G3 B3 D4 was commonly used instead, and this is still the preferred tuning for some types of folk music and for classic banjo. Other tunings found in old-time music include double C (G4 C3 G3 C4 D4), \"sawmill\" (G4 D3 G3 C4 D4) also called \"mountain modal\" and open D (F#4D3 F#3 A3 D4). These tunings are often taken up a tone, either by tuning up or using a capo. For example, \"old-time D\" tuning (A4 D3 A3 D4 E4) – commonly reached by tuning up from double C – is often played to accompany fiddle tunes in the key of D and Open-A (A4 E3 A3 C#4 E4) is usually used for playing tunes in the key of A. There are dozens of other banjo tunings, used mostly in old-time music. These tunings are used to make it easier to play specific, usually, fiddle tunes, or groups of fiddle tunes.\n\nThe size of the five string banjo is largely standardized—but smaller and larger sizes exist, including the long-neck or \"Seeger neck\" variation designed by Pete Seeger. Petite variations on the five-string banjo have been available since the 1890s. S.S. Stewart introduced the banjeaurine, tuned one fourth above a standard five-string. Piccolo banjos are smaller, and tuned one octave above a standard banjo. Between these sizes and standard lies the A-scale banjo, which is two frets shorter and usually tuned one full step above standard tunings. Many makers have produced banjos of other scale lengths, and with various innovations.\n\nAmerican old-time music typically uses the five-string open back banjo. It is played in a number of different styles, the most common being clawhammer or frailing, characterized by the use of a downward rather than upward stroke when striking the strings with a fingernail. Frailing techniques use the thumb to catch the fifth string for a drone after most strums or after each stroke (\"double thumbing\"), or to pick out additional melody notes in what is known as \"drop-thumb.\" Pete Seeger popularised a folk style by combining clawhammer with \"up picking\", usually without the use of fingerpicks. Another common style of old-time banjo playing is \"Fingerpicking banjo\" or \"classic banjo\". This style is based upon parlor-style guitar.\n\nBluegrass music, which uses the five-string resonator banjo almost exclusively, is played in several common styles. These include Scruggs style, named after Earl Scruggs; melodic, or Keith style, named for Bill Keith; and three-finger style with single string work, also called Reno style after Don Reno. In these styles the emphasis is on arpeggiated figures played in a continuous eighth-note rhythm, known as rolls. All of these styles are typically played with fingerpicks.\n\nThe first five-string electric solid-body banjo was developed by Charles (Buck) Wilburn Trent, Harold \"Shot\" Jackson, and David Jackson in 1960.\n\nThe five-string banjo has been used in classical music since before the turn of the 20th century. Contemporary and modern works have been written or arranged for the instrument by Buck Trent, Béla Fleck, Tony Trischka, Ralph Stanley, Steve Martin, George Crumb, Modest Mouse, Jo Kondo, Paul Elwood, Hans Werner Henze (notably in his Sixth Symphony), Daniel Mason of Hank Williams III's Damn Band, Beck, the Water Tower Bucket Boys, Todd Taylor, J.P. Pickens, Peggy Honeywell, Norfolk & Western, Putnam Smith, Iron & Wine, The Avett Brothers, Punch Brothers and Sufjan Stevens.\n\nFrederick Delius wrote for a banjo in his opera Koanga.\n\nErnst Krenek includes two banjos in his Kleine Symphonie (Little Symphony).\n\nKurt Weill has a banjo in his opera The Rise and Fall of the City of Mahagonny.\n\nFour-string banjos, both plectrum and tenor, can be used for chordal accompaniment (as in early jazz), for single string melody playing (as in Irish traditional music), in \"chord melody\" style (a succession of chords in which the highest notes carry the melody), in tremolo style (both on chords and single strings), and a mixed technique called duo style that combines single string tremolo and rhythm chords.\n\nThe plectrum banjo is a standard banjo without the short drone string. It usually has 22 frets on the neck and a scale length of 26 to 28 inches, and was originally tuned C3 G3 B3 D4. It can also be tuned like the top four strings of a guitar, which is known as \"Chicago tuning.\" As the name suggests, it is usually played with a guitar-style pick (that is, a single one held between thumb and forefinger), unlike the five-string banjo, which is either played with a thumbpick and two fingerpicks, or with bare fingers. The plectrum banjo evolved out of the five-string banjo, to cater to styles of music involving strummed chords. The plectrum is also featured in many early jazz recordings and arrangements.\n\nThe four-string banjo is used from time to time in musical theater. Examples include: \"Hello, Dolly!\", \"Mame\", \"Chicago\", \"Cabaret\", \"Oklahoma!\", \"Half a Sixpence\", \"Annie\", \"Barnum\", \"The Threepenny Opera\", \"Monty Python's Spamalot\", and countless others. Joe Raposo had used it variably in the imaginative 7-piece orchestration for the long-running TV show \"Sesame Street\", and has sometimes had it overdubbed with itself or an electric guitar. The banjo is still (albeit rarely) in use in the show's arrangement currently.\n\nThe shorter-necked, tenor banjo, with 17 (\"short scale\") or 19 frets, is also typically played with a plectrum. It became a popular instrument after about 1910. Early models used for melodic picking typically had 17 frets on the neck and a scale length of 19½ to 21½ inches. By the mid-1920s, when the instrument was used primarily for strummed chordal accompaniment, 19-fret necks with a scale length of 21¾ to 23 inches became standard. The usual tuning is the all-fifths tuning C3 G3 D4 A4, in which there are exactly seven semitones (a perfect fifth) between the open notes of consecutive strings. Other players (particularly in Irish traditional music) tune the banjo G2 D3 A3 E4 like an octave mandolin, which lets the banjoist duplicate fiddle and mandolin fingering. The popularisation of this tuning was usually attributed to the late Barney McKenna, banjoist with The Dubliners. Fingerstyle on tenor banjo retuned to open G tuning dgd'g' or lower open D tuning Adad' (three finger picking, frailing) have been explored by Mirek Patek.\n\nThe tenor banjo was a common rhythm-instrument in early 20th-century dance-bands. Its volume and timbre suited early jazz (and jazz-influenced popular music styles) and could both compete with other instruments (such as brass instruments and saxophones) and be heard clearly on acoustic recordings. George Gershwin's \"Rhapsody in Blue\", in Ferde Grofe's original jazz orchestra arrangement, includes tenor banjo, with widely spaced chords not easily playable on plectrum banjo in its conventional tuning(s). With development of the archtop and electric guitar, the tenor banjo largely disappeared from jazz and popular music, though keeping its place in traditional \"Dixieland\" jazz.\n\nSome 1920s Irish banjo players picked out the melodies of jigs, reels and hornpipes on tenor banjos, decorating the tunes with snappy triplet ornaments. The most important Irish banjo player of this era was Mike Flanagan of the New York-based Flanagan Brothers, one of the most popular Irish-American groups of the day. Other pre-WW2 Irish banjo players included Neil Nolan, who recorded with Dan Sullivan's Shamrock Band in Boston, and Jimmy McDade, who recorded with the Four Provinces Orchestra in Philadelphia. Meanwhile, in Ireland the rise of \"ceili\" bands provided a new market for a loud instrument like the tenor banjo. Use of the tenor banjo in Irish music has increased greatly since the folk revival of the 1960s.\n\nIn the late 19th and early 20th centuries there was a vogue in plucked-string instrument ensembles—guitar orchestras, mandolin orchestras, banjo orchestras—in which the instrumentation was made to parallel that of the string section in symphony orchestras. Thus \"violin, viola, 'cello, bass\" became \"mandolin, mandola, mandocello, mandobass\", or in the case of banjos, \"banjolin, banjola, banjo cello, bass banjo\". Because the range of pluck stringed instrument generally isn't as great as that of comparably-size bowed string instruments, other instruments were often added to these plucked orchestras to extend the range of the ensemble upwards and downwards.\n\nRarer than either the tenor or plectrum banjo is the cello banjo (also \"banjo cello\"). It's normally tuned C2-G2-D3-A3, one octave below the tenor banjo like the cello and mandocello. It played a role in banjo orchestras in the late nineteenth and early twentieth centuries. A five-string cello banjo, set up like a bluegrass banjo (with the short 5th string), but tuned one octave lower, has been produced by the Goldtone company.\n\n Bass banjos have been produced in both upright bass formats and with standard, horizontally carried banjo bodies. Contrabass banjos with either three or four strings have also been made; some of these had headstocks similar to those of bass violins. Tuning varies on these large instruments, with four-string models sometimes being tuned in 4ths like a bass violin—E1-A1-D2-G2, and sometimes in 5ths, like a four-string cello banjo, one octave lower—C1-G1-D2-A2. Other variants are also used.\n\nThe six-string banjo began as a British innovation by William Temlet, one of England's earliest banjo makers. He opened a shop in London in 1846, and sold banjos with closed backs and up to seven strings. He marketed these as \"zither\" Banjos from his 1869 patent. American Alfred Davis Cammeyer (1862–1949), a young violinist-turned banjo concert player, devised the five or six-string zither banjo around 1880. It had a wood resonator and metal \"wire\" strings (the 1st and 2nd melody strings and 5th \"thumb\" string. The 3rd melody string was gut and the 4th was silk covered) as well as frets and guitar-style tuning machines.\n\nA zither banjo usually has a closed back and sides with the drum body (usually metal) and skin tensioning system suspended inside the wooden rim/back, the neck and string tailpiece was mounted on the wooden outer rim, the short string usually led through a tube in the neck so that the tuning peg could be mounted on the peg head. They were often made by builders who used guitar tuners that came in banks of three and so if 5 stringed had a redundant tuner. The banjos could also be somewhat easily converted over to a six-string banjo. British opera diva Adelina Patti advised Cammeyer that the zither-banjo might be popular with English audiences (it was invented there), and Cammeyer went to London in 1888. With his virtuoso playing, he helped show that banjos could make sophisticated music than normally played by blackface minstrels. He was soon performing for London society, where he met Sir Arthur Sullivan, who recommended that Cammeyer progress from arranging the music of others for banjo to composing his own music. (Supposedly unknown to Cammeyer, William Temlett had patented a seven-string closed back banjo in 1869, and was already marketing it as a \"zither-banjo.\")\n\nIn the late 1890s Banjo maker F.C Wilkes developed a six-string version of the banjo, with the 6th string \"tunnelled\" through the neck. It is arguable that Arthur O. Windsor influenced development and perfection of the zither banjo and created the open-back banjo along with other modifications to the banjo type instruments, such as the modern non-solid attached resonator. (Gibson claims credit for this modification on the American Continent.) Windsor claimed he created the hollow neck banjo with a truss rod, and buried the 5th string in the neck after the 5th fret so to put the tuning peg on the peg-head rather than in the neck. Gibson claims credit for perfecting the tone ring.\n\nModern six-string bluegrass banjos have been made. These add a bass string between the lowest string and the drone string on a five-string banjo, and are usually tuned G4 G2 D3 G3 B3 D4. Sonny Osborne played one of these instruments for several years. It was modified by luthier Rual Yarbrough from a Vega five-string model. A picture of Sonny with this banjo appears in Pete Wernick's \"Bluegrass Banjo\" method book.\n\nSix-string banjos having a guitar neck and a banjo body have become quite popular since the mid-1990s. See under \"Banjo Hybrids and variants\", below.\n\nA number of hybrid instruments exist, crossing the banjo with other stringed instruments. Most of these use the body of a banjo, often with a resonator, and the neck of the other instrument. Examples include the banjo mandolin (first patented in 1882) and the banjo ukulele or banjolele, most famously played by the English comedian George Formby. These were especially popular in the early decades of the twentieth century, and were probably a result of a desire either to allow players of other instruments to jump on the banjo bandwagon at the height of its popularity, or to get the natural amplification benefits of the banjo resonator in an age before electric amplification.\n\nThe six-string banjo guitar basically consists of a six-string guitar neck attached to a bluegrass or plectrum banjo body. This was the instrument of the early jazz great Johnny St. Cyr, jazzmen Django Reinhardt, Danny Barker, Papa Charlie Jackson and Clancy Hayes, as well as the blues and gospel singer The Reverend Gary Davis. Nowadays, it appears under various names such as guitanjo, guitjoe, ganjo, banjitar, or bantar. Today, musicians as diverse as Keith Urban, Rod Stewart, Taj Mahal, Joe Satriani, David Hidalgo, Larry Lalonde and Doc Watson play the six-string guitar banjo.\nRhythm guitarist Dave Day of 1960s proto-punks The Monks replaced his guitar with a six-string, gut-strung guitar banjo on which he played guitar chords. This instrument sounds much more metallic, scratchy and wiry than a standard electric guitar, due to its amplification via a small microphone stuck inside the banjo's body.\n\nInstruments that have a five-string banjo neck on a wooden body (for example, a guitar, bouzouki, or dobro body) have also been made, such as the banjola. A 20th-Century Turkish instrument similar to the banjo is called the cümbüş—which has been made into eight different hybrid instruments, including guitar, mandolin, ukulele, and oud. At the end of the twentieth century, a development of the five-string banjo was the BanSitar. This features a bone bridge, giving the instrument a sitar-like resonance. A recent innovation is the patented Banjo-Tam, invented by Frank Abrams of Asheville North Carolina combining a traditional five string banjo neck with a tambourine as a rim or \"pot\".\n\n\n\n\n", "id": "3846", "title": "Banjo"}
{"url": "https://en.wikipedia.org/wiki?curid=3850", "text": "Baseball\n\nBaseball is a bat-and-ball game played between two teams of nine players each, who take turns batting and fielding.\n\nThe batting team attempts to score runs by hitting a ball that is thrown by the pitcher with a bat swung by the batter, then running counter-clockwise around a series of four bases: first, second, third, and home plate. A run is scored when a player advances around the bases and returns to home plate.\n\nPlayers on the batting team take turns hitting against the pitcher of the fielding team, which tries to prevent runs by getting hitters out in any of several ways. A player on the batting team who reaches a base safely can later attempt to advance to subsequent bases during teammates' turns batting, such as on a hit or by other means. The teams switch between batting and fielding whenever the fielding team records three outs. One turn batting for both teams, beginning with the visiting team, constitutes an inning. A game is composed of nine innings, and the team with the greater number of runs at the end of the game wins. Baseball has no game clock, although almost all games end in the ninth inning.\n\nBaseball evolved from older bat-and-ball games already being played in England by the mid-18th century. This game was brought by immigrants to North America, where the modern version developed. By the late 19th century, baseball was widely recognized as the national sport of the United States. Baseball is currently popular in North America and parts of Central and South America, the Caribbean, and East Asia, particularly Japan.\n\nIn the United States and Canada, professional Major League Baseball (MLB) teams are divided into the National League (NL) and American League (AL), each with three divisions: East, West, and Central. The major league champion is determined by playoffs that culminate in the World Series. The top level of play is similarly split in Japan between the Central and Pacific Leagues and in Cuba between the West League and East League.\n\nThe evolution of baseball from older bat-and-ball games is difficult to trace with precision. A French manuscript from 1344 contains an illustration of clerics playing a game, possibly \"la soule\", with similarities to baseball. Other old French games such as \"thèque\", \"la balle au bâton\", and \"la balle empoisonnée\" also appear to be related. Consensus once held that today's baseball is a North American development from the older game rounders, popular in Great Britain and Ireland. \"Baseball Before We Knew It: A Search for the Roots of the Game\" (2005), by David Block, suggests that the game originated in England; recently uncovered historical evidence supports this position. Block argues that rounders and early baseball were actually regional variants of each other, and that the game's most direct antecedents are the English games of stoolball and \"tut-ball.\" It has long been believed that cricket also descended from such games, though evidence uncovered in early 2009 suggests that cricket may have been imported to England from Flanders.\n\nThe earliest known reference to baseball is in a 1744 British publication, \"A Little Pretty Pocket-Book\", by John Newbery. It contains a rhymed description of \"base-ball\" and a woodcut that shows a field set-up somewhat similar to the modern game—though in a triangular rather than diamond configuration, and with posts instead of ground-level bases. David Block discovered that the first recorded game of \"Bass-Ball\" took place in 1749 in Surrey, and featured the Prince of Wales as a player. William Bray, an English lawyer, recorded a game of baseball on Easter Monday 1755 in Guildford, Surrey. This early form of the game was apparently brought to Canada by English immigrants. Rounders was also brought to the USA by Canadians of both British and Irish ancestry. The first known American reference to baseball appears in a 1791 Pittsfield, Massachusetts, town bylaw prohibiting the playing of the game near the town's new meeting house. By 1796, a version of the game was well-known enough to earn a mention in a German scholar's book on popular pastimes. As described by Johann Gutsmuths, \"\"englische Base-ball\"\" involved a contest between two teams, in which \"the batter has three attempts to hit the ball while at the home plate.\" Only one out was required to retire a side.\nBy the early 1830s, there were reports of a variety of uncodified bat-and-ball games recognizable as early forms of baseball being played around North America. These games were often referred to locally as \"town ball\", though other names such as \"round-ball\" and \"base-ball\" were also used. Among the earliest examples to receive a detailed description—albeit five decades after the fact, in a letter from an attendee to \"Sporting Life\" magazine—took place in Beachville, Ontario, in 1838. There were many similarities to modern baseball, and some crucial differences: five bases (or \"byes\"); first bye just from the home bye; batter out if a hit ball was caught after the first bounce. The once widely accepted story that Abner Doubleday invented baseball in Cooperstown, New York, in 1839 has been conclusively debunked by sports historians.\n\nIn 1845, Alexander Cartwright, a member of New York City's Knickerbocker Club, led the codification of the so-called Knickerbocker Rules. The practice, common to bat-and-ball games of the day, of \"soaking\" or \"plugging\"—effecting a putout by hitting a runner with a thrown ball—was barred. The rules thus facilitated the use of a smaller, harder ball than had been common. Several other rules also brought the Knickerbockers' game close to the modern one, though a ball caught on the first bounce was, again, an out and only underhand pitching was allowed. While there are reports that the New York Knickerbockers played games in 1845, the contest long recognized as the first officially recorded baseball game in U.S. history took place on June 19, 1846, in Hoboken, New Jersey: the \"New York Nine\" defeated the Knickerbockers, 23–1, in four innings (three earlier games have recently been discovered). With the Knickerbocker code as the basis, the rules of modern baseball continued to evolve over the next half-century.\n\nIn the mid-1850s, a baseball craze hit the New York metropolitan area. By 1856, local journals were referring to baseball as the \"national pastime\" or \"national game.\" A year later, sixteen area clubs formed the sport's first governing body, the National Association of Base Ball Players. In 1858 in Corona, Queens New York, at the Fashion Race Course, the first games of baseball to charge admission took place. The games, which took place between the all stars of Brooklyn, including players from the Brooklyn Atlantics, Excelsior of Brooklyn, Putnams and Eckford of Brooklyn, and the All Stars of New York (Manhattan), including players from the New York Knickerbockers, Gothams (predecessors of the San Francisco Giants), Eagles and Empire, are commonly believed to be the first all-star baseball games. In 1863, the organization disallowed putouts made by catching a fair ball on the first bounce. Four years later, it barred participation by African Americans. The game's commercial potential was developing: in 1869 the first fully professional baseball club, the Cincinnati Red Stockings, was formed and went undefeated against a schedule of semipro and amateur teams. The first professional league, the National Association of Professional Base Ball Players, lasted from 1871 to 1875; scholars dispute its status as a major league.\n\nThe more formally structured National League was founded in 1876. As the oldest surviving major league, the National League is sometimes referred to as the \"senior circuit.\" Several other major leagues formed and failed. In 1884, African American Moses Walker (and, briefly, his brother Welday) played in one of these, the American Association. An injury ended Walker's major league career, and by the early 1890s, a gentlemen's agreement in the form of the baseball color line effectively barred black players from the white-owned professional leagues, major and minor. Professional Negro leagues formed, but quickly folded. Several independent African American teams succeeded as barnstormers. Also in 1884, overhand pitching was legalized. In 1887, softball, under the name of indoor baseball or indoor-outdoor, was invented as a winter version of the parent game. Virtually all of the modern baseball rules were in place by 1893; the last major change—counting foul balls as strikes—was instituted in 1901. The National League's first successful counterpart, the American League, which evolved from the minor Western League, was established that year. The two leagues, each with eight teams, were rivals that fought for the best players, often disregarding each other's contracts and engaging in bitter legal disputes.\nA modicum of peace was eventually established, leading to the National Agreement of 1903. The pact formalized relations both between the two major leagues and between them and the National Association of Professional Base Ball Leagues, representing most of the country's minor professional leagues. The World Series, pitting the two major league champions against each other, was inaugurated that fall, albeit without express major league sanction: The Boston Americans of the American League defeated the Pittsburgh Pirates of the National League. The next year, the series was not held, as the National League champion New York Giants, under manager John McGraw, refused to recognize the major league status of the American League and its champion. In 1905, the Giants were National League champions again and team management relented, leading to the establishment of the World Series as the major leagues' annual championship event.\n\nAs professional baseball became increasingly profitable, players frequently raised grievances against owners over issues of control and equitable income distribution. During the major leagues' early decades, players on various teams occasionally attempted strikes, which routinely failed when their jobs were sufficiently threatened. In general, the strict rules of baseball contracts and the reserve clause, which bound players to their teams even when their contracts had ended, tended to keep the players in check. Motivated by dislike for particularly stingy owner Charles Comiskey and gamblers' payoffs, real and promised, members of the Chicago White Sox conspired to throw the 1919 World Series. The Black Sox Scandal led to the formation of a new National Commission of baseball that drew the two major leagues closer together. The first major league baseball commissioner, Kenesaw Mountain Landis, was elected in 1920. That year also saw the founding of the Negro National League; the first significant Negro league, it would operate until 1931. For part of the 1920s, it was joined by the Eastern Colored League.\n\nProfessional baseball was played in northeastern cities with a large immigrant-ethnic population; they gave strong support to the new sport. The Irish Catholics dominated in the late 19th century, comprising a third or more of the players and many of the top stars and managers. Historian Jerrold Casway argues that:\n\nCompared with the present, professional baseball in the early 20th century was lower-scoring and pitchers, the likes of Walter Johnson and Christy Mathewson, were more dominant. The \"inside game,\" which demanded that players \"scratch for runs\", was played much more aggressively than it is today: the brilliant and often violent Ty Cobb epitomized this style. The so-called dead-ball era ended in the early 1920s with several changes in rule and circumstance that were advantageous to hitters. Strict new regulations governing the ball's size, shape and composition along with a new rule officially banning the spitball, along with other pitches that depended on the ball being treated or roughed-up with foreign substances after the death of Ray Chapman who was hit by a pitch in August 1920, coupled with superior materials available after World War I, resulted in a ball that traveled farther when hit. The construction of additional seating to accommodate the rising popularity of the game often had the effect of bringing the outfield fences closer in, making home runs more common. The rise of the legendary player Babe Ruth, the first great power hitter of the new era, helped permanently alter the nature of the game. The club with which Ruth set most of his slugging records, the New York Yankees, built a reputation as the majors' premier team. In the late 1920s and early 1930s, St. Louis Cardinals general manager Branch Rickey invested in several minor league clubs and developed the first modern \"farm system\". A new Negro National League was organized in 1933; four years later, it was joined by the Negro American League. The first elections to the Baseball Hall of Fame took place in 1936. In 1939 Little League Baseball was founded in Pennsylvania. By the late 1940s, it was the organizing body for children's baseball leagues across the United States.\nWith America's entry into World War II, many professional players had left to serve in the armed forces. A large number of minor league teams disbanded as a result and the major league game seemed under threat as well. Chicago Cubs owner Philip K. Wrigley led the formation of a new professional league with women players to help keep the game in the public eye – the All-American Girls Professional Baseball League existed from 1943 to 1954. The inaugural College World Series was held in 1947, and the Babe Ruth League youth program was founded. This program soon became another important organizing body for children's baseball. The first crack in the unwritten agreement barring blacks from white-controlled professional ball occurred the previous year: Jackie Robinson was signed by the National League's Brooklyn Dodgers—where Branch Rickey had become general manager—and began playing for their minor league team in Montreal. In 1947, Robinson broke the major leagues' color barrier when he debuted with the Dodgers. Larry Doby debuted with the American League's Cleveland Indians the same year. Latin American players, largely overlooked before, also started entering the majors in greater numbers. In 1951, two Chicago White Sox, Venezuelan-born Chico Carrasquel and black Cuban-born Minnie Miñoso, became the first Hispanic All-Stars.\n\nFacing competition as varied as television and football, baseball attendance at all levels declined. While the majors rebounded by the mid-1950s, the minor leagues were gutted and hundreds of semipro and amateur teams dissolved. Integration proceeded slowly: by 1953, only six of the 16 major league teams had a black player on the roster. That year, the Major League Baseball Players Association was founded. It was the first professional baseball union to survive more than briefly, but it remained largely ineffective for years. No major league team had been located west of St. Louis until 1958, when the Brooklyn Dodgers and New York Giants relocated to Los Angeles and San Francisco, respectively. The majors' final all-white bastion, the Boston Red Sox, added a black player in 1959. With the integration of the majors drying up the available pool of players, the last Negro league folded the following year. In 1961, the American League reached the West Coast with the Los Angeles Angels expansion team, and the major league season was extended from 154 games to 162. This coincidentally helped Roger Maris break Babe Ruth's long-standing single-season home run record, one of the most celebrated marks in baseball. Along with the Angels, three other new franchises were launched during 1961–62. With this, the first major league expansion in 60 years, each league now had ten teams.\nThe players' union became bolder under the leadership of former United Steelworkers chief economist and negotiator Marvin Miller, who was elected executive director in 1966. On the playing field, major league pitchers were becoming increasingly dominant again. After the 1968 season, in an effort to restore balance, the strike zone was reduced and the height of the pitcher's mound was lowered from 15 to 10 inches (38.1 - 25.4 cm) . In 1969, both the National and American leagues added two more expansion teams, the leagues were reorganized into two divisions each, and a post-season playoff system leading to the World Series was instituted. Also that same year, Curt Flood of the St. Louis Cardinals made the first serious legal challenge to the reserve clause. The major leagues' first general players' strike took place in 1972. In another effort to add more offense to the game, the American League adopted the designated hitter rule the following year. In 1975, the union's power—and players' salaries—began to increase greatly when the reserve clause was effectively struck down, leading to the free agency system. In 1977, two more expansion teams joined the American League. Significant work stoppages occurred again in 1981 and 1994, the latter forcing the cancellation of the World Series for the first time in 90 years. Attendance had been growing steadily since the mid-1970s and in 1994, before the stoppage, the majors were setting their all-time record for per-game attendance.\nThe addition of two more expansion teams after the 1993 season had facilitated another restructuring of the major leagues, this time into three divisions each. Offensive production—the number of home runs in particular—had surged that year, and again in the abbreviated 1994 season. After play resumed in 1995, this trend continued and non-division-winning wild card teams became a permanent fixture of the post-season. Regular-season interleague play was introduced in 1997 and the second-highest attendance mark for a full season was set. The next year, Mark McGwire and Sammy Sosa both surpassed Maris's decades-old single season home run record and two more expansion franchises were added. In 2000, the National and American leagues were dissolved as legal entities. While their identities were maintained for scheduling purposes (and the designated hitter distinction), the regulations and other functions—such as player discipline and umpire supervision—they had administered separately were consolidated under the rubric of Major League Baseball (MLB).\n\nIn 2001, Barry Bonds established the current record of 73 home runs in a single season. There had long been suspicions that the dramatic increase in power hitting was fueled in large part by the abuse of illegal steroids (as well as by the dilution of pitching talent due to expansion), but the issue only began attracting significant media attention in 2002 and there was no penalty for the use of performance-enhancing drugs before 2004. In 2007, Bonds became MLB's all-time home run leader, surpassing Hank Aaron, as total major league and minor league attendance both reached all-time highs. Even though McGwire, Sosa, and Bonds—as well as many other players, including storied pitcher Roger Clemens—have been implicated in the steroid abuse scandal, their feats and those of other sluggers had become the major leagues' defining attraction. In contrast to the professional game's resurgence in popularity after the 1994 interruption, Little League enrollment was in decline: after peaking in 1996, it dropped 1 percent a year over the following decade. With more rigorous testing and penalties for performance-enhancing drug use a possible factor, the balance between bat and ball swung markedly in 2010, which became known as the \"Year of the Pitcher\". Runs per game fell to their lowest level in 18 years, and the strikeout rate was higher than it had been in half a century.\n\nBefore the start of the 2012 season, MLB altered its rules to double the number of wild card teams admitted into the playoffs to two per league. The playoff expansion resulted in the addition of annual one-game playoffs between the wild card teams in each league.\n\nBaseball, widely known as America's pastime, is well established in several other countries as well. The history of baseball in Canada has remained closely linked with that of the sport in the United States. As early as 1877, a professional league, the International Association, featured teams from both countries. While baseball is widely played in Canada and many minor league teams have been based in the country, the American major leagues did not include a Canadian club until 1969, when the Montreal Expos joined the National League as an expansion team. In 1977, the expansion Toronto Blue Jays joined the American League. The Blue Jays won the World Series in 1992 and 1993, the first and still the only club from outside the United States to do so. After the 2004 season, Major League Baseball relocated the Expos to Washington, D.C., where the team is now known as the Nationals.\nIn 1847, American soldiers played what may have been the first baseball game in Mexico at Parque Los Berros in Xalapa, Veracruz. A few days after the Battle of Cerro Gordo, they used the \"wooden leg captured (by the Fourth Illinois regiment) from General Santa Anna\". The first formal baseball league outside of the United States and Canada was founded in 1878 in Cuba, which maintains a rich baseball tradition and whose national team has been one of the world's strongest since international play began in the late 1930s (all organized baseball in the country has officially been amateur since the Cuban Revolution). The Dominican Republic held its first islandwide championship tournament in 1912. Professional baseball tournaments and leagues began to form in other countries between the world wars, including the Netherlands (formed in 1922), Australia (1934), Japan (1936), Mexico (1937), and Puerto Rico (1938). The Japanese major leagues—the Central League and Pacific League—have long been considered the highest quality professional circuits outside of the United States. Japan has a professional minor league system as well, though it is much smaller than the American version—each team has only one farm club in contrast to MLB teams' four or five.\n\nAfter World War II, professional leagues were founded in many Latin American nations, most prominently Venezuela (1946) and the Dominican Republic (1955). Since the early 1970s, the annual Caribbean Series has matched the championship clubs from the four leading Latin American winter leagues: the Dominican Professional Baseball League, Mexican Pacific League, Puerto Rican Professional Baseball League, and Venezuelan Professional Baseball League. In Asia, South Korea (1982), Taiwan (1990), and China (2003) all have professional leagues.\n\nMany European countries have professional leagues as well, the most successful, other than the Dutch league, being the Italian league founded in 1948. Compared to those in Asia and Latin America, the various European leagues and the one in Australia historically have had no more than niche appeal. In 2004, Australia won a surprise silver medal at the Olympic Games. The Israel Baseball League, launched in 2007, folded after one season. The Confédération Européene de Baseball (European Baseball Confederation), founded in 1953, organizes a number of competitions between clubs from different countries, as well as national squads. Other competitions between national teams, such as the Baseball World Cup and the Olympic baseball tournament, were administered by the International Baseball Federation (IBAF) from its formation in 1938 until its 2013 merger with the International Softball Federation to create the current joint governing body for both sports, the World Baseball Softball Confederation (WBSC). By 2009, the IBAF had 117 member countries. Women's baseball is played on an organized amateur basis in many of the countries where it is a leading men's sport. Since 2004, the IBAF and now WBSC have sanctioned the Women's Baseball World Cup, featuring national teams.\n\nAfter being admitted to the Olympics as a medal sport beginning with the 1992 Games, baseball was dropped from the 2012 Summer Olympic Games at the 2005 International Olympic Committee meeting. It remained part of the 2008 Games. The elimination of baseball, along with softball, from the 2012 Olympic program enabled the IOC to consider adding two different sports, but none received the votes required for inclusion. While the sport's lack of a following in much of the world was a factor, more important was Major League Baseball's reluctance to have a break during the Games to allow its players to participate, as the National Hockey League now does during the Winter Olympic Games. Such a break is more difficult for MLB to accommodate because it would force the playoffs deeper into cold weather. Seeking reinstatement for the 2016 Summer Olympics, the IBAF proposed an abbreviated competition designed to facilitate the participation of top players, but the effort failed. Major League Baseball initiated the World Baseball Classic, scheduled to precede the major league season, partly as a replacement, high-profile international tournament. The inaugural Classic, held in March 2006, was the first tournament involving national teams to feature a significant number of MLB participants. The Baseball World Cup was discontinued after its 2011 edition in favor of an expanded World Baseball Classic.\n\nA game is played between two teams, each composed of nine players, that take turns playing offense (batting and baserunning) and defense (pitching and fielding). A pair of turns, one at bat and one in the field, by each team constitutes an inning. A game consists of nine innings (seven innings at the high school level and in doubleheaders in college and minor leagues). One team—customarily the visiting team—bats in the top, or first half, of every inning. The other team—customarily the home team—bats in the bottom, or second half, of every inning. The goal of the game is to score more points (runs) than the other team. The players on the team at bat attempt to score runs by circling or completing a tour of the four bases set at the corners of the square-shaped baseball diamond. A player bats at home plate and must proceed counterclockwise to first base, second base, third base, and back home in order to score a run. The team in the field attempts both to prevent runs from scoring and to record outs, which remove opposing players from offensive action until their turn in their team's batting order comes up again. When three outs are recorded, the teams switch roles for the next half-inning. If the score of the game is tied after nine innings, extra innings are played to resolve the contest. Many amateur games, particularly unorganized ones, involve different numbers of players and innings.\n\nThe game is played on a field whose primary boundaries, the foul lines, extend forward from home plate at 45-degree angles. The 90-degree area within the foul lines is referred to as fair territory; the 270-degree area outside them is foul territory. The part of the field enclosed by the bases and several yards beyond them is the infield; the area farther beyond the infield is the outfield. In the middle of the infield is a raised pitcher's mound, with a rectangular rubber plate (the rubber) at its center. The outer boundary of the outfield is typically demarcated by a raised fence, which may be of any material and height (many amateur games are played on unfenced fields). Fair territory between home plate and the outfield boundary is baseball's field of play, though significant events can take place in foul territory, as well.\n\nThere are three basic tools of baseball: the ball, the bat, and the glove or mitt:\nProtective helmets are also standard equipment for all batters.\n\nAt the beginning of each half-inning, the nine players on the fielding team arrange themselves around the field. One of them, the pitcher, stands on the pitcher's mound. The pitcher begins the pitching delivery with one foot on the rubber, pushing off it to gain velocity when throwing toward home plate. Another player, the catcher, squats on the far side of home plate, facing the pitcher. The rest of the team faces home plate, typically arranged as four infielders—who set up along or within a few yards outside the imaginary lines between first, second, and third base—and three outfielders. In the standard arrangement, there is a first baseman positioned several steps to the left of first base, a second baseman to the right of second base, a shortstop to the left of second base, and a third baseman to the right of third base. The basic outfield positions are left fielder, center fielder, and right fielder. A neutral umpire sets up behind the catcher. Other umpires will be distributed around the field as well, though the number will vary depending on the level of play, amateur or children's games may only have an umpire behind the plate, while as many as six umpires can be used for important Major League Baseball games.\nPlay starts with a batter standing at home plate, holding a bat. The batter waits for the pitcher to throw a pitch (the ball) toward home plate, and attempts to hit the ball with the bat. The catcher catches pitches that the batter does not hit—as a result of either electing not to swing or failing to connect—and returns them to the pitcher. A batter who hits the ball into the field of play must drop the bat and begin running toward first base, at which point the player is referred to as a \"runner\" (or, until the play is over, a \"batter-runner\"). A batter-runner who reaches first base without being put out (see below) is said to be \"safe\" and is now on base. A batter-runner may choose to remain at first base or attempt to advance to second base or even beyond—however far the player believes can be reached safely. A player who reaches base despite proper play by the fielders has recorded a hit. A player who reaches first base safely on a hit is credited with a single. If a player makes it to second base safely as a direct result of a hit, it is a double; third base, a triple. If the ball is hit in the air within the foul lines over the entire outfield (and outfield fence, if there is one), it is a home run: the batter and any runners on base may all freely circle the bases, each scoring a run. This is the most desirable result for the batter. A player who reaches base due to a fielding mistake is not credited with a hit—instead, the responsible fielder is charged with an error.\n\nAny runners already on base may attempt to advance on batted balls that land, or contact the ground, in fair territory, before or after the ball lands. A runner on first base \"must\" attempt to advance if a ball lands in play. If a ball hit into play rolls foul before passing through the infield, it becomes dead and any runners must return to the base they were at when the play began. If the ball is hit in the air and caught before it lands, the batter has flied out and any runners on base may attempt to advance only if they tag up or touch the base they were at when the play began, as or after the ball is caught. Runners may also attempt to advance to the next base while the pitcher is in the process of delivering the ball to home plate—a successful effort is a stolen base.\n\nA pitch that is not hit into the field of play is called either a strike or a ball. A batter against whom three strikes are recorded strikes out. A batter against whom four balls are recorded is awarded a base on balls or walk, a free advance to first base. (A batter may also freely advance to first base if the batter's body or uniform is struck by a pitch outside the strike zone, provided the batter does not swing and attempts to avoid being hit.) Crucial to determining balls and strikes is the umpire's judgment as to whether a pitch has passed through the strike zone, a conceptual area above home plate extending from the midpoint between the batter's shoulders and belt down to the hollow of the knee.\n\nA strike is called when one of the following happens:\nA ball is called when the pitcher throws a pitch that is outside the strike zone, provided the batter has not swung at it.\nWhile the team at bat is trying to score runs, the team in the field is attempting to record outs. Among the various ways a member of the batting team may be put out, five are most common:\nIt is possible to record two outs in the course of the same play. This is called a double play. Even three outs in one play, a triple play, is possible, though this is very rare. Players put out or retired must leave the field, returning to their team's dugout or bench. A runner may be stranded on base when a third out is recorded against another player on the team. Stranded runners do not benefit the team in its next turn at bat as every half-inning begins with the bases empty of runners.\n\nAn individual player's turn batting or plate appearance is complete when the player reaches base, hits a home run, makes an out, or hits a ball that results in the team's third out, even if it is recorded against a teammate. On rare occasions, a batter may be at the plate when, without the batter's hitting the ball, a third out is recorded against a teammate—for instance, a runner getting caught stealing (tagged out attempting to steal a base). A batter with this sort of incomplete plate appearance starts off the team's next turn batting; any balls or strikes recorded against the batter the previous inning are erased. A runner may circle the bases only once per plate appearance and thus can score at most a single run per batting turn. Once a player has completed a plate appearance, that player may not bat again until the eight other members of the player's team have all taken their turn at bat. The batting order is set before the game begins, and may not be altered except for substitutions. Once a player has been removed for a substitute, that player may not reenter the game. Children's games often have more liberal substitution rules.\n\nIf the designated hitter (DH) rule is in effect, each team has a tenth player whose sole responsibility is to bat (and run). The DH takes the place of another player—almost invariably the pitcher—in the batting order, but does not field. Thus, even with the DH, each team still has a batting order of nine players and a fielding arrangement of nine players.\n\nRoster, or squad, sizes differ between different leagues and different levels of organized play. Major League Baseball teams maintain 25-player active rosters. A typical 25-man roster in a league without the DH rule, such as MLB's National League, features:\n\nIn the American League and others with the DH rule, there will usually be nine offensive regulars (including the DH), five starting pitchers, seven or eight relievers, a backup catcher and two or three other reserves; the need for late inning pinch-hitters (usually in the pitcher's spot) is reduced by the DH.\n\nThe manager, or head coach of a team, oversees the team's major strategic decisions, such as establishing the starting rotation, setting the lineup, or batting order, before each game, and making substitutions during games—in particular, bringing in relief pitchers. Managers are typically assisted by two or more coaches; they may have specialized responsibilities, such as working with players on hitting, fielding, pitching, or strength and conditioning. At most levels of organized play, two coaches are stationed on the field when the team is at bat: the first base coach and third base coach, occupying designated coaches' boxes just outside the foul lines, assist in the direction of baserunners when the ball is in play, and relay tactical signals from the manager to batters and runners during pauses in play. In contrast to many other team sports, baseball managers and coaches generally wear their team's uniforms; coaches must be in uniform in order to be allowed on the playing field during a game.\n\nAny baseball game involves one or more umpires, who make rulings on the outcome of each play. At a minimum, one umpire will stand behind the catcher, to have a good view of the strike zone, and call balls and strikes. Additional umpires may be stationed near the other bases, thus making it easier to judge plays such as attempted force outs and tag outs. In Major League Baseball, four umpires are used for each game, one near each base. In the playoffs, six umpires are used: one at each base and two in the outfield along the foul lines.\n\nMany of the pre-game and in-game strategic decisions in baseball revolve around a fundamental fact: in general, right-handed batters tend to be more successful against left-handed pitchers and, to an even greater degree, left-handed batters tend to be more successful against right-handed pitchers. A manager with several left-handed batters in the regular lineup who knows the team will be facing a left-handed starting pitcher may respond by starting one or more of the right-handed backups on the team's roster. During the late innings of a game, as relief pitchers and pinch hitters are brought in, the opposing managers will often go back and forth trying to create favorable matchups with their substitutions: the manager of the fielding team trying to arrange same-handed pitcher-batter matchups, the manager of the batting team trying to arrange opposite-handed matchups. With a team that has the lead in the late innings, a manager may remove a starting position player—especially one whose turn at bat is not likely to come up again—for a more skillful fielder.\n\nThe tactical decision that precedes almost every play in a baseball game involves pitch selection. By gripping and then releasing the baseball in a certain manner, and by throwing it at a certain speed, pitchers can cause the baseball to break to either side, or downward, as it approaches the batter. Among the resulting wide variety of pitches that may be thrown, the four basic types are the fastball, the changeup (or off-speed pitch), and two breaking balls—the curveball and the slider. Pitchers have different repertoires of pitches they are skillful at throwing. Conventionally, before each pitch, the catcher signals the pitcher what type of pitch to throw, as well as its general vertical and/or horizontal location. If there is disagreement on the selection, the pitcher may shake off the sign and the catcher will call for a different pitch. With a runner on base and taking a lead, the pitcher may attempt a pickoff, a quick throw to a fielder covering the base to keep the runner's lead in check or, optimally, effect a tag out. Pickoff attempts, however, are subject to rules that severely restrict the pitcher's movements before and during the pickoff attempt. Violation of any one of these rules could result in the umpire calling a balk against the pitcher, with the result being runners on base, if any, advance one base with impunity. If an attempted stolen base is anticipated, the catcher may call for a pitchout, a ball thrown deliberately off the plate, allowing the catcher to catch it while standing and throw quickly to a base. Facing a batter with a strong tendency to hit to one side of the field, the fielding team may employ a shift, with most or all of the fielders moving to the left or right of their usual positions. With a runner on third base, the infielders may play in, moving closer to home plate to improve the odds of throwing out the runner on a ground ball, though a sharply hit grounder is more likely to carry through a drawn-in infield.\n\nSeveral basic offensive tactics come into play with a runner on first base, including the fundamental choice of whether to attempt a steal of second base. The hit and run is sometimes employed with a skillful contact hitter: the runner takes off with the pitch drawing the shortstop or second baseman over to second base, creating a gap in the infield for the batter to poke the ball through. The sacrifice bunt calls for the batter to focus on making contact with the ball so that it rolls a short distance into the infield, allowing the runner to advance into scoring position even at the expense of the batter being thrown out at first—a batter who succeeds is credited with a sacrifice. (A batter, particularly one who is a fast runner, may also attempt to bunt for a hit.) A sacrifice bunt employed with a runner on third base, aimed at bringing that runner home, is known as a squeeze play. With a runner on third and fewer than two outs, a batter may instead concentrate on hitting a fly ball that, even if it is caught, will be deep enough to allow the runner to tag up and score—a successful batter in this case gets credit for a sacrifice fly. The manager will sometimes signal a batter who is ahead in the count (i.e., has more balls than strikes) to take, or not swing at, the next pitch.\n\nBaseball has certain attributes that set it apart from the other popular team sports in the countries where it has a following, including American and Canadian football, basketball, ice hockey, and soccer. All of these sports use a clock; in all of them, play is less individual and more collective; and in none of them is the variation between playing fields nearly as substantial or important. The comparison between cricket and baseball demonstrates that many of baseball's distinctive elements are shared in various ways with its cousin sports.\n\nIn clock-limited sports, games often end with a team that holds the lead killing the clock rather than competing aggressively against the opposing team. In contrast, baseball has no clock; a team cannot win without getting the last batter out and rallies are not constrained by time. At almost any turn in any baseball game, the most advantageous strategy is some form of aggressive strategy. In contrast, again, the clock comes into play even in the case of multi-day Test and first-class cricket: the possibility of a draw often encourages a team that is batting last and well behind to bat defensively, giving up any faint chance at a win to avoid a loss. Baseball offers no such reward for conservative batting.\n\nWhile nine innings has been the standard since the beginning of professional baseball, the duration of the average major league game has increased steadily through the years. At the turn of the 20th century, games typically took an hour and a half to play. In the 1920s, they averaged just less than two hours, which eventually ballooned to 2:38 in 1960. By 1997, the average American League game lasted 2:57 (National League games were about 10 minutes shorter—pitchers at the plate making for quicker outs than designated hitters). In 2004, Major League Baseball declared that its goal was an average game of merely 2:45. By 2014, though, the average MLB game took over three hours to complete. The lengthening of games is attributed to longer breaks between half-innings for television commercials, increased offense, more pitching changes, and a slower pace of play with pitchers taking more time between each delivery, and batters stepping out of the box more frequently. Other leagues have experienced similar issues. In 2008, Nippon Professional Baseball took steps aimed at shortening games by 12 minutes from the preceding decade's average of 3:18.\n\nAlthough baseball is a team sport, individual players are often placed under scrutiny and pressure. In 1915, a baseball instructional manual pointed out that every single pitch, of which there are often more than two hundred in a game, involves an individual, one-on-one contest: \"the pitcher and the batter in a battle of wits\". Contrasting the game with both football and basketball, scholar Michael Mandelbaum argues that \"baseball is the one closest in evolutionary descent to the older individual sports\". Pitcher, batter, and fielder all act essentially independent of each other. While coaching staffs can signal pitcher or batter to pursue certain tactics, the execution of the play itself is a series of solitary acts. If the batter hits a line drive, the outfielder is solely responsible for deciding to try to catch it or play it on the bounce and for succeeding or failing. The statistical precision of baseball is both facilitated by this isolation and reinforces it. As described by Mandelbaum,\n\nIt is impossible to isolate and objectively assess the contribution each [football] team member makes to the outcome of the play ... [E]very basketball player is interacting with all of his teammates all the time. In baseball, by contrast, every player is more or less on his own ... Baseball is therefore a realm of complete transparency and total responsibility. A baseball player lives in a glass house, and in a stark moral universe ... Everything that every player does is accounted for and everything accounted for is either good or bad, right or wrong.\n\nCricket is more similar to baseball than many other team sports in this regard: while the individual focus in cricket is mitigated by the importance of the batting partnership and the practicalities of tandem running, it is enhanced by the fact that a batsman may occupy the wicket for an hour or much more. There is no statistical equivalent in cricket for the fielding error and thus less emphasis on personal responsibility in this area of play.\n\nUnlike those of most sports, baseball playing fields can vary significantly in size and shape. While the dimensions of the infield are specifically regulated, the only constraint on outfield size and shape for professional teams following the rules of Major League and Minor League Baseball is that fields built or remodeled since June 1, 1958, must have a minimum distance of from home plate to the fences in left and right field and to center. Major league teams often skirt even this rule. For example, at Minute Maid Park, which became the home of the Houston Astros in 2000, the Crawford Boxes in left field are only from home plate. There are no rules at all that address the height of fences or other structures at the edge of the outfield. The most famously idiosyncratic outfield boundary is the left-field wall at Boston's Fenway Park, in use since 1912: the Green Monster is from home plate down the line and tall.\nSimilarly, there are no regulations at all concerning the dimensions of foul territory. Thus a foul fly ball may be entirely out of play in a park with little space between the foul lines and the stands, but a foulout in a park with more expansive foul ground. A fence in foul territory that is close to the outfield line will tend to direct balls that strike it back toward the fielders, while one that is farther away may actually prompt more collisions, as outfielders run full speed to field balls deep in the corner. These variations can make the difference between a double and a triple or inside-the-park home run. The surface of the field is also unregulated. While the adjacent image shows a traditional field surfacing arrangement (and the one used by virtually all MLB teams with naturally surfaced fields), teams are free to decide what areas will be grassed or bare. Some fields—including several in MLB—use an artificial surface, such as AstroTurf. Surface variations can have a significant effect on how ground balls behave and are fielded as well as on baserunning. Similarly, the presence of a roof (seven major league teams play in stadiums with permanent or retractable roofs) can greatly affect how fly balls are played. While football and soccer players deal with similar variations of field surface and stadium covering, the size and shape of their fields are much more standardized. The area out-of-bounds on a football or soccer field does not affect play the way foul territory in baseball does, so variations in that regard are largely insignificant.\n\nThese physical variations create a distinctive set of playing conditions at each ballpark. Other local factors, such as altitude and climate, can also significantly affect play. A given stadium may acquire a reputation as a pitcher's park or a hitter's park, if one or the other discipline notably benefits from its unique mix of elements. The most exceptional park in this regard is Coors Field, home of the Colorado Rockies. Its high altitude— above sea level—is responsible for giving it the strongest hitter's park effect in the major leagues. Wrigley Field, home of the Chicago Cubs, is known for its fickle disposition: a hitter's park when the strong winds off Lake Michigan are blowing out, it becomes more of a pitcher's park when they are blowing in. The absence of a standardized field affects not only how particular games play out, but the nature of team rosters and players' statistical records. For example, hitting a fly ball into right field might result in an easy catch on the warning track at one park, and a home run at another. A team that plays in a park with a relatively short right field, such as the New York Yankees, will tend to stock its roster with left-handed pull hitters, who can best exploit it. On the individual level, a player who spends most of his career with a team that plays in a hitter's park will gain an advantage in batting statistics over time—even more so if his talents are especially suited to the park.\n\nOrganized baseball lends itself to statistics to a greater degree than many other sports. Each play is discrete and has a relatively small number of possible outcomes. In the late 19th century, a former cricket player, English-born Henry Chadwick of Brooklyn, New York, was responsible for the \"development of the box score, tabular standings, the annual baseball guide, the batting average, and most of the common statistics and tables used to describe baseball.\" The statistical record is so central to the game's \"historical essence\" that Chadwick came to be known as Father Baseball. In the 1920s, American newspapers began devoting more and more attention to baseball statistics, initiating what journalist and historian Alan Schwarz describes as a \"tectonic shift in sports, as intrigue that once focused mostly on teams began to go to individual players and their statistics lines.\"\n\nThe Official Baseball Rules administered by Major League Baseball require the official scorer to categorize each baseball play unambiguously. The rules provide detailed criteria to promote consistency. The score report is the official basis for both the box score of the game and the relevant statistical records. General managers, managers, and baseball scouts use statistics to evaluate players and make strategic decisions.\nCertain traditional statistics are familiar to most baseball fans. The basic batting statistics include:\nThe basic baserunning statistics include:\n\nThe basic pitching statistics include:\nThe basic fielding statistics include:\n\nAmong the many other statistics that are kept are those collectively known as \"situational statistics\". For example, statistics can indicate which specific pitchers a certain batter performs best against. If a given situation statistically favors a certain batter, the manager of the fielding team may be more likely to change pitchers or have the pitcher intentionally walk the batter in order to face one who is less likely to succeed.\n\n\"Sabermetrics\" refers to the field of baseball statistical study and the development of new statistics and analytical tools. The term is also used to refer directly to new statistics themselves. The term was coined around 1980 by one of the field's leading proponents, Bill James, and derives from the Society for American Baseball Research (SABR).\n\nThe growing popularity of sabermetrics since the early 1980s has brought more attention to two batting statistics that sabermetricians argue are much better gauges of a batter's skill than batting average:\n\nSome of the new statistics devised by sabermetricians have gained wide use:\n\nWriting in 1919, philosopher Morris Raphael Cohen described baseball as America's national religion. In the words of sports columnist Jayson Stark, baseball has long been \"a unique paragon of American culture\"—a status he sees as devastated by the steroid abuse scandal. Baseball has an important place in other national cultures as well: Scholar Peter Bjarkman describes \"how deeply the sport is ingrained in the history and culture of a nation such as Cuba, [and] how thoroughly it was radically reshaped and nativized in Japan.\" Since the early 1980s, the Dominican Republic, in particular the city of San Pedro de Macorís, has been the major leagues' primary source of foreign talent. Hall-of-Famer Roberto Clemente remains one of the greatest national heroes in Puerto Rico's history. While baseball has long been the island's primary athletic pastime, its once well-attended professional winter league has declined in popularity since 1990, when young Puerto Rican players began to be included in the major leagues' annual first-year player draft. In the Western Hemisphere, baseball is also one of the leading sports in Canada, Colombia, Mexico, the Netherlands Antilles, Nicaragua, Panama, and Venezuela. In Asia, it is among the most popular sports in Japan, South Korea and Taiwan.\n\nThe major league game in the United States was originally targeted toward a middle-class, white-collar audience: relative to other spectator pastimes, the National League's set ticket price of 50 cents in 1876 was high, while the location of playing fields outside the inner city and the workweek daytime scheduling of games were also obstacles to a blue-collar audience. A century later, the situation was very different. With the rise in popularity of other team sports with much higher average ticket prices—football, basketball, and hockey—professional baseball had become among the most blue-collar-oriented of leading American spectator sports.\n\nIn the late 1900s and early 2000s, baseball's position compared to football in the United States moved in contradictory directions. In 2008, Major League Baseball set a revenue record of $6.5 billion, matching the NFL's revenue for the first time in decades. A new MLB revenue record of $6.6 billion was set in 2009. On the other hand, the percentage of American sports fans polled who named baseball as their favorite sport was 16%, compared to pro football at 31%. In 1985, the respective figures were pro football 24%, baseball 23%. Because there are so many more major league baseball games played, there is no comparison in overall attendance. In 2008, total attendance at major league games was the second-highest in history: 78.6 million, 0.7% off the record set the previous year. The following year, amid the U.S. recession, attendance fell by 6.6% to 73.4 million. Attendance at games held under the Minor League Baseball umbrella also set a record in 2007, with 42.8 million; this figure does not include attendance at games of the several independent minor leagues.\nIn Japan, where baseball is inarguably the leading spectator team sport, combined revenue for the twelve teams in Nippon Professional Baseball (NPB), the body that oversees both the Central and Pacific leagues, was estimated at $1 billion in 2007. Total NPB attendance for the year was approximately 20 million. While in the preceding two decades, MLB attendance grew by 50 percent and revenue nearly tripled, the comparable NPB figures were stagnant. There are concerns that MLB's growing interest in acquiring star Japanese players will hurt the game in their home country. In Cuba, where baseball is by every reckoning the national sport, the national team overshadows the city and provincial teams that play in the top-level domestic leagues. Revenue figures are not released for the country's amateur system. Similarly, according to one official pronouncement, the sport's governing authority \"has never taken into account attendance ... because its greatest interest has always been the development of athletes\".\n\nAs of 2007, Little League Baseball oversees more than 7,000 children's baseball leagues with more than 2.2 million participants–2.1 million in the United States and 123,000 in other countries. Babe Ruth League teams have over 1 million participants. According to the president of the International Baseball Federation, between 300,000 and 500,000 women and girls play baseball around the world, including Little League and the introductory game of Tee Ball.\n\nA varsity baseball team is an established part of physical education departments at most high schools and colleges in the United States. In 2008, nearly half a million high schoolers and over 35,000 collegians played on their schools' baseball teams. The number of Americans participating in baseball has declined since the late 1980s, falling well behind the number of soccer participants. By early in the 20th century, intercollegiate baseball was Japan's leading sport. Today, high school baseball in particular is immensely popular there. The final rounds of the two annual tournaments—the National High School Baseball Invitational Tournament in the spring, and the even more important National High School Baseball Championship in the summer—are broadcast around the country. The tournaments are known, respectively, as Spring Koshien and Summer Koshien after the 55,000-capacity stadium where they are played. In Cuba, baseball is a mandatory part of the state system of physical education, which begins at age six. Talented children as young as seven are sent to special district schools for more intensive training—the first step on a ladder whose acme is the national baseball team.\n\nBaseball has had a broad impact on popular culture, both in the United States and elsewhere. Dozens of English-language idioms have been derived from baseball; in particular, the game is the source of a number of widely used sexual euphemisms. The first networked radio broadcasts in North America were of the 1922 World Series: famed sportswriter Grantland Rice announced play-by-play from New York City's Polo Grounds on WJZ–Newark, New Jersey, which was connected by wire to WGY–Schenectady, New York, and WBZ–Springfield, Massachusetts. The baseball cap has become a ubiquitous fashion item not only in the United States and Japan, but also in countries where the sport itself is not particularly popular, such as the United Kingdom.\n\nBaseball has inspired many works of art and entertainment. One of the first major examples, Ernest Thayer's poem \"Casey at the Bat\", appeared in 1888. A wry description of the failure of a star player in what would now be called a \"clutch situation\", the poem became the source of vaudeville and other staged performances, audio recordings, film adaptations, and an opera, as well as a host of sequels and parodies in various media. There have been many baseball movies, including the Academy Award–winning \"The Pride of the Yankees\" (1942) and the Oscar nominees \"The Natural\" (1984) and \"Field of Dreams\" (1989). The American Film Institute's selection of the ten best sports movies includes \"The Pride of the Yankees\" at number 3 and \"Bull Durham\" (1988) at number 5. Baseball has provided thematic material for hits on both stage—the Adler–Ross musical \"Damn Yankees\"—and record—George J. Gaskin's \"Slide, Kelly, Slide\", Simon and Garfunkel's \"Mrs. Robinson\", and John Fogerty's \"Centerfield\". The baseball-inspired comedic sketch \"Who's on First\", popularized by Abbott and Costello in 1938, quickly became famous. Six decades later, \"Time\" named it the best comedy routine of the 20th century. Baseball is also featured in various video games including \"\", \"Wii Sports\", \"\" and \"Mario Baseball\".\n\nLiterary works connected to the game include the short fiction of Ring Lardner and novels such as Bernard Malamud's \"The Natural\" (the source for the movie), Robert Coover's \"The Universal Baseball Association, Inc., J. Henry Waugh, Prop.\", and W. P. Kinsella's \"Shoeless Joe\" (the source for \"Field of Dreams\"). Baseball's literary canon also includes the beat reportage of Damon Runyon; the columns of Grantland Rice, Red Smith, Dick Young, and Peter Gammons; and the essays of Roger Angell. Among the celebrated nonfiction books in the field are Lawrence S. Ritter's \"The Glory of Their Times\", Roger Kahn's \"The Boys of Summer\", and Michael Lewis's \"Moneyball\". The 1970 publication of major league pitcher Jim Bouton's tell-all chronicle \"Ball Four\" is considered a turning point in the reporting of professional sports.\n\nBaseball has also inspired the creation of new cultural forms. Baseball cards were introduced in the late 19th century as trade cards. A typical example would feature an image of a baseball player on one side and advertising for a business on the other. In the early 1900s they were produced widely as promotional items by tobacco and confectionery companies. The 1930s saw the popularization of the modern style of baseball card, with a player photograph accompanied on the rear by statistics and biographical data. Baseball cards—many of which are now prized collectibles—are the source of the much broader trading card industry, involving similar products for different sports and non-sports-related fields.\n\nModern fantasy sports began in 1980 with the invention of Rotisserie League Baseball by New York writer Daniel Okrent and several friends. Participants in a Rotisserie league draft notional teams from the list of active Major League Baseball players and play out an entire imaginary season with game outcomes based on the players' latest real-world statistics. Rotisserie-style play quickly became a phenomenon. Now known more generically as fantasy baseball, it has inspired similar games based on an array of different sports. The field boomed with increasing Internet access and new fantasy sports–related websites. By 2008, 29.9 million people in the United States and Canada were playing fantasy sports, spending $800 million on the hobby. The burgeoning popularity of fantasy baseball is also credited with the increasing attention paid to sabermetrics—first among fans, only later among baseball professionals.\n\n\n\n\n\n\n\n\n", "id": "3850", "title": "Baseball"}
{"url": "https://en.wikipedia.org/wiki?curid=3851", "text": "Baseball positions\n\nThere are nine fielding positions in baseball. Each position conventionally has an associated number, which is used to score putouts: 1 (pitcher), 2 (catcher), 3 (first baseman), 4 (second baseman), 5 (third baseman), 6 (shortstop), 7 (left fielder), 8 (center fielder), and 9 (right fielder).\n\nFor example:\n\n", "id": "3851", "title": "Baseball positions"}
{"url": "https://en.wikipedia.org/wiki?curid=3856", "text": "History of baseball in the United States\n\nThe history of baseball in the United States can be traced to the 19th century, when amateurs played a baseball-like game by their own informal rules using home made equipment. The popularity of the sport inspired the semipro national baseball clubs in the 1860s.\n\nThe earliest known mention of baseball in the U.S was a 1791 Pittsfield, Massachusetts, ordinance banning the playing of the game within of the town meeting house. In 1903, the British sportswriter Henry Chadwick published an article speculating that baseball derived from a British game called rounders, which Chadwick had played as a boy in England. But baseball executive Albert Spalding disagreed. Baseball, said Spalding, was fundamentally an American sport and began on American soil. To settle the matter, the two men appointed a commission, headed by Abraham Mills, the fourth president of the National League of Professional Baseball Clubs. The commission, which also included six other sports executives, labored for three years, after which it declared that Abner Doubleday invented the national pastime. This would have been a surprise to Doubleday. The late Civil War hero \"never knew that he had invented baseball\". [But] 15 years [after his death], he was anointed as the father of the game\", writes baseball historian John Thorn. The myth about Doubleday inventing the game of baseball actually came from a Colorado mining engineer. Another early reference reports that \"base ball\" was regularly played on Saturdays in 1823 on the outskirts of New York City in an area that today is Greenwich Village.\n\nIn 1828, an article published in a Hagerstown, Maryland, newspaper briefly describes a young girl who's drawn away from her daily chores to play a familiar game with her friends. In \"A Village Sketch\", author Miss Mitford wrote: \"\"Then comes a sun-burnt gipsy of six, beginning to grow tall and thin and to find the cares of the world gathering about her; with a pitcher in one hand, a mop in the other, an old straw bonnet of ambiguous shape, half hiding her tangled hair; a tattered stuff petticoat once green, hanging below an equally tattered cotton frock, once purple; her longing eyes fixed on a game of baseball at the corner of the green till she reaches the cottage door, flings down the mop and pitcher and darts off to her companions quite regardless of the storm of scolding with which the mother follows her runaway steps.\"\"\n\nThe first team to play baseball under modern rules were the New York Knickerbockers. The club was founded on September 23, 1845, as a social club for the upper middle classes of New York City, and was strictly amateur until it disbanded. The club members, which included its president Doc Adams and Alexander Cartwright, formulated the \"Knickerbocker Rules\", which in large part dealt with organizational matters but which also laid out rules for playing the game. One of the significant rules prohibited \"soaking\" or \"plugging\" the runner; under older rules, a fielder could put a runner out by hitting the runner with the thrown ball, similarly to the common schoolyard game of kickball. The Knickerbocker Rules required fielders to tag or force the runner, as is done today, and avoided a lot of the arguments and fistfights that resulted from the earlier practice.\n\nWriting the rules did not help the Knickerbockers in the first known competitive game between two clubs under the new rules, played at Elysian Fields in Hoboken, New Jersey on June 19, 1846. The self-styled \"New York Nine\" humbled the Knickerbockers by a score of 23 to 1. Nevertheless, the Knickerbocker Rules were rapidly adopted by teams in the New York area and their version of baseball became known as the \"New York Game\" (as opposed to the \"Massachusetts Game\", played by clubs in the Boston area).\n\nAs late as 1855, the New York press was still devoting more space to coverage of cricket than to baseball.\n\nIn 1857, sixteen New York area clubs, including the Knickerbockers, formed the National Association of Base Ball Players (NABBP). The NABBP was the first organization to govern the sport and to establish a championship. Aided by the Civil War, membership grew to almost 100 clubs by 1865 and to over 400 by 1867, including clubs from as far away as California. During the Civil War, soldiers from different parts of the United States played baseball together, leading to a more unified national version of the sport. Beginning in 1869, the NABBP permitted professional play, addressing a growing practice that had not been permitted under its rules to that point. The first and most prominent professional club of the NABBP era was the Cincinnati Red Stockings in Ohio, which lasted only two years. Businessman Ivers Whitney Adams then courted manager Harry Wright and founded the \"Boston Red Stockings\" and the Boston Base Ball Club on January 20, 1871.\n\nIn 1858, in the Corona neighborhood of Queens (now part of New York City), at the Fashion Race Course, the first games of baseball to charge admission took place. The games, which took place between the all stars of Brooklyn, including players from the Brooklyn Atlantics, Excelsior of Brooklyn, Putnams and Eckford of Brooklyn, and the All Stars of New York (Manhattan), including players from the New York Knickerbockers, Gothams (predecessors of the San Francisco Giants), Eagles and Empire, are commonly believed to the first all star baseball games.\n\nBefore the Civil War, baseball competed for public interest with cricket and regional variants of baseball, notably town ball played in Philadelphia and the Massachusetts Game played in New England. In the 1860s, aided by the war, \"New York\" style baseball expanded into a national game, as its first governing body. The National Association of Base Ball Players was formed. The NABBP soon expanded into a true national organization, although most of the strongest clubs remained those based in the northeastern part of the country. In its 12-year history as an amateur league, the Brooklyn Atlantics won seven championships, establishing themselves as the first true dynasty in the sport, although, the New York Mutuals were widely considered to be one of the best teams of the era as well. By the end of 1865, almost 100 clubs were members of the NABBP. By 1867, it ballooned to over 400 members, including some clubs from as far away as San Francisco and Louisiana. One of these clubs, the Chicago White Stockings, won the championship in 1870. Today known as the Chicago Cubs, they are the oldest team in American organized sports. Because of this growth, regional and state organizations began to assume a more prominent role in the governance of the sport.\n\nThe NABBP of America was initially established upon principles of amateurism. However, even early in its history some star players, such as James Creighton of Excelsior, received compensation, either secretly or indirectly. In 1866, the NABBP investigated Athletic of Philadelphia for paying three players including Lip Pike, but ultimately took no action against either the club or the players. To address this growing practice, and to restore integrity to the game, at its December 1868 meeting the NABBP established a professional category for the 1869 season. Clubs desiring to pay players were now free to declare themselves professional.\n\nThe Cincinnati Red Stockings were the first to so declare themselves as openly professional, and were easily the most aggressive in recruiting the best available players. Twelve clubs, including most of the strongest clubs in the NABBP, ultimately declared themselves professional for the 1869 season.\n\nThe first attempt at forming a \"major league\" produced the National Association of Professional Base Ball Players, which lasted from 1871 to 1875. The now all professional Chicago White Stockings, financed by businessman William Hulbert, became a charter member of the league along with the Red Stockings, who had dissolved and moved to Boston. The White Stockings were close contenders all season, despite the fact that the Great Chicago Fire had destroyed the team's home field and most of their equipment. The White Stockings finished the season in second place, but ultimately were forced to drop out of the league during the city's recovery period, finally returning to National Association play in 1874. Over the next couple seasons, The Boston Red Stockings dominated the league and hoarded many of the game's best players, even those who were under contract with other teams. After Davy Force signed with Chicago, and then breached his contract to play in Boston, Hulbert became discouraged by the \"contract jumping\" as well as the overall disorganization of the N.A., and thus spearheaded the movement to form a stronger organization. The end result of his efforts was the formation a much more \"ethical\" league, which became known as the National Base Ball League. After a series of rival leagues were organized but failed (most notably the American Base Ball Association, which spawned the clubs which would ultimately become the St. Louis Cardinals and Brooklyn Dodgers), the current American League, evolving from the minor Western League of 1893, was established in 1901.\n\nIn 1870, a schism developed between professional and amateur ballplayers. The NABBP split into two groups. The National Association of \"Professional\" Base Ball Players operated from 1871 through 1875, and is considered by some to have been the first major league. Its amateur counterpart disappeared after only a few years.\n\nWilliam Hulbert's National League, which was formed after the National Association proved ineffective, put its emphasis on \"clubs\" rather than \"players\". Clubs now had the ability to enforce player contracts, preventing players from jumping to higher-paying clubs. Clubs in turn were required to play their full schedule of games, rather than forfeiting scheduled games once out of the running for the league championship, as happened frequently under the National Association. A concerted effort was made to reduce the amount of gambling on games which was leaving the validity of results in doubt.\n\nAt the same time, a \"gentlemen's agreement\" was struck between the clubs to exclude non-white players from professional baseball, a bar that remained until 1947. It is a common misconception that Jackie Robinson was the first African-American major-league ballplayer; he was actually only the first after a long gap (and the first in the modern era). Moses Fleetwood Walker and his brother Welday Walker were unceremoniously dropped from major and minor-league rosters in the 1880s, as were other African-Americans in baseball. An unknown number of African-Americans played in the major leagues by representing themselves as Indians, or South or Central Americans, and a still larger number played in the minor leagues and on amateur teams as well. In the majors, however, it was not until the signing of Robinson (in the National League) and Larry Doby (in the American League) that baseball began to remove its color bar.\n\nThe early years of the National League were tumultuous, with threats from rival leagues and a rebellion by players against the hated \"reserve clause\", which restricted the free movement of players between clubs. Competitive leagues formed regularly, and also disbanded regularly. The most successful was the American Association (1881–1891), sometimes called the \"beer and whiskey league\" for its tolerance of the sale of alcoholic beverages to spectators. For several years, the National League and American Association champions met in a postseason championship series—the first attempt at a World Series.\n\nThe Union Association survived for only one season (1884), as did the Players' League (1890), an attempt to return to the National Association structure of a league controlled by the players themselves. Both leagues are considered major leagues by many baseball researchers because of the perceived high caliber of play and the number of star players featured. However, some researchers have disputed the major league status of the Union Association, pointing out that franchises came and went and contending that the St. Louis club, which was deliberately \"stacked\" by the league's president (who owned that club), was the only club that was anywhere close to major league caliber.\n\nIn fact, there were dozens of leagues, large and small, at this time. What made the National League \"major\" was its dominant position in the major cities, particularly New York City, the edgy, emotional nerve center of baseball. The large cities offered baseball teams national media distribution systems and fan bases that could generate revenues enabling teams to hire the best players in the country.\n\nA number of other leagues, including the venerable Eastern League, threatened the dominance of the National League. The Western League, founded in 1893, became particularly aggressive. Its fiery leader Ban Johnson railed against the National League and promised to build a new league that would grab the best players and field the best teams. The Western League began play in April 1894 with teams in Detroit (the only league team that has not moved since), Grand Rapids, Indianapolis, Kansas City, Milwaukee, Minneapolis, Sioux City and Toledo. Prior to the 1900 season, the league changed its name to the American League and moved several franchises to larger, strategic locations. In 1901 the American League declared its intent to operate as a major league.\n\nThe resulting bidding war for players led to widespread contract-breaking and legal disputes. One of the most famous involved star second baseman Napoleon Lajoie, who in 1901 went across town in Philadelphia from the National League Phillies to the American League Athletics. Barred by a court injunction from playing baseball in the state of Pennsylvania the next year, Lajoie was traded to the Cleveland team, where he played and managed for many years.\n\nThe war between the American and National caused shock waves throughout the baseball world. At a meeting in 1901, the other baseball leagues negotiated a plan to maintain their independence. On September 5, 1901 Patrick T. Powers, president of the Eastern League announced the formation of the second National Association of Professional Baseball Leagues, the NABPL or \"NA\" for short.\n\nThese leagues did not consider themselves \"minor\" – a term that did not come into vogue until St. Louis Cardinals GM Branch Rickey pioneered the farm system in the 1930s. Nevertheless, these financially troubled leagues, by beginning the practice of selling players to the more affluent National and American leagues, embarked on a path that eventually led to the loss of their independent status.\n\nBan Johnson had other designs for the NA. While the NA continues to this day, he saw it as a tool to end threats from smaller rivals who might some day want to expand in other territories and threaten his league's dominance.\n\nAfter 1902 both leagues and the NABPL signed a new National Agreement which achieved three things:\n\nThe new agreement tied independent contracts to the reserve-clause national league contracts. Baseball players were a commodity, like cars. $5,000 bought a player's skill set. It set up a rough classification system for independent leagues that regulated the dollar value of contracts, the forerunner of the system refined by Rickey and used today.\n\nIt also gave the NA great power. Many independents walked away from the 1901 meeting. The deal with the NA punished those other indies who had not joined the NA and submitted to the will of the 'majors.' The NA also agreed to the deal to prevent more pilfering of players with little or no compensation for the players' development. Several leagues, seeing the writing on the wall, eventually joined the NA, which grew in size over the next several years.\n\nIn the very early part of the 20th century, known as the \"dead-ball era\", baseball rules and equipment favored the \"inside game\" and the game was played more violently and aggressively than it is today. This period ended in the 1920s with several changes that gave advantages to hitters. In the largest parks, the outfield fences were brought closer to the infield. In addition, the strict enforcement of new rules governing the size, shape and construction of the ball caused it to travel farther.\n\nThe first professional black baseball club, the Cuban Giants, was organized in 1885. Subsequent professional black baseball clubs played each other independently, without an official league to organize the sport. Rube Foster, a former ballplayer, founded the Negro National League in 1920. A second league, the Eastern Colored League, was established in 1923. These became known as the Negro Leagues, though these leagues never had any formal overall structure comparable to the Major Leagues. The Negro National League did well until 1930, but folded during the Great Depression.\n\nFrom 1942 to 1948, the Negro League World Series was revived. This was the golden era of Negro League baseball, a time when it produced some of its greatest stars. In 1947, Jackie Robinson signed a contract with the Brooklyn Dodgers, breaking the color barrier that had prevented talented African American players from entering the white-only major leagues. Although the transformation was not instantaneous, baseball has since become fully integrated. In 1948, the Negro Leagues faced financial difficulties that effectively ended their existence.\n\nPitchers dominated the game in the 1960s and early 1970s. In 1973, the designated hitter (DH) rule was adopted by the American League, while in the National League pitchers still bat for themselves to this day. The DH rule now constitutes the primary difference between the two leagues.\n\nDuring the late 1960s, the Baseball Players Union became much stronger and conflicts between owners and the players' union led to major work stoppages in 1972, 1981, and 1994. The 1994 baseball strike led to the cancellation of the World Series, and was not settled until the spring of 1995. In the late 1990s, functions that had been administered separately by the two major leagues' administrations were united under the rubric of Major League Baseball.\n\nAt this time the games tended to be low scoring, dominated by such pitchers as Walter Johnson, Cy Young, Christy Mathewson, and Grover Cleveland Alexander to the extent that the period 1900–1919 is commonly called the \"Dead-ball era\". The term also accurately describes the condition of the baseball itself. Baseballs cost three dollars apiece, a hefty sum at the time, which in 1900 would be equal to $ today; club owners were therefore reluctant to spend much money on new balls if not necessary. It was not unusual for a single baseball to last an entire game. By the end of the game, the ball would be dark with grass, mud, and tobacco juice, and it would be misshapen and lumpy from contact with the bat. Balls were only replaced if they were hit into the crowd and lost, and many clubs employed security guards expressly for the purpose of retrieving balls hit into the stands—a practice unthinkable today.\n\nAs a consequence, home runs were rare, and the \"inside game\" dominated—singles, bunts, stolen bases, the hit-and-run play, and other tactics dominated the strategies of the time.\n\nDespite this, there were also several superstar hitters, the most famous being Honus Wagner, held to be one of the greatest shortstops to ever play the game, and Detroit's Ty Cobb, the \"Georgia Peach.\" Cobb was a mean-spirited man, fiercely competitive and loathed by many of his fellow professionals, but his career batting average of .366 has yet to be bested.\n\nThe 1908 pennant races in both the AL and NL were among the most exciting ever witnessed. The conclusion of the National League season, in particular, involved a bizarre chain of events, often referred to as the Merkle Boner. On September 23, 1908, the New York Giants and Chicago Cubs played a game in the Polo Grounds. Nineteen-year-old rookie first baseman Fred Merkle, later to become one of the best players at his position in the league, was on first base, with teammate Moose McCormick on third with two outs and the game tied. Giants shortstop Al Bridwell socked a single, scoring McCormick and apparently winning the game. However, Merkle, instead of advancing to second base, ran toward the clubhouse to avoid the spectators mobbing the field, which at that time was a common, acceptable practice. The Cubs' second baseman, Johnny Evers, noticed this. In the confusion that followed, Evers claimed to have retrieved the ball and touched second base, forcing Merkle out and nullifying the run scored. Evers brought this to the attention of the umpire that day, Hank O'Day, who after some deliberation called the runner out. Because of the state of the field O'Day thereby called the game. Despite the arguments by the Giants, the league upheld O'Day's decision and ordered the game replayed at the end of the season, if necessary. It turned out that the Cubs and Giants ended the season tied for first place, so the game was indeed replayed, and the Cubs won the game, the pennant, and subsequently the World Series (the last Cubs Series victory until 2016).\n\nFor his part, Merkle was doomed to endless criticism and vilification throughout his career for this lapse, which went down in history as \"Merkle's Boner\". In his defense, some baseball historians have suggested that it was not customary for game-ending hits to be fully \"run out\", it was only Evers's insistence on following the rules strictly that resulted in this unusual play. In fact, earlier in the 1908 season, the identical situation had been brought to the umpires' attention by Evers; the umpire that day was the same Hank O'Day. While the winning run was allowed to stand on that occasion, the dispute raised O'Day's awareness of the rule, and directly set up the Merkle controversy.\n\nTurn of the century baseball attendances were modest by later standards. The average for the 1,110 games in the 1901 season was 3,247. However the first 20 years of the 20th century saw an unprecedented rise in the popularity of baseball. Large stadiums dedicated to the game were built for many of the larger clubs or existing grounds enlarged, including Tiger Stadium in Detroit, Shibe Park, home of the Philadelphia Athletics, Ebbets Field in Brooklyn, the Polo Grounds in Manhattan, Boston's Fenway Park along with Wrigley Field and Comiskey Park in Chicago. Likewise from the Eastern League to the small developing leagues in the West, and the rising Negro Leagues professional baseball was being played all across the country. Average major league attendances reached a pre World War I peak of 5,836 in 1909, before falling back during the war. Where there weren't professional teams, there were semi-pro teams, traveling teams barnstorming, company clubs and amateur men's leagues. In the days before television, if you wanted to see a game, you had to go to the ballpark.\n\nThe fix of baseball games by gamblers and players working together had been suspected as early as the 1850s. Hal Chase was particularly notorious for throwing games, but played for a decade after gaining this reputation; he even managed to parlay these accusations into a promotion to manager. Even baseball stars such as Ty Cobb and Tris Speaker have been credibly alleged to have fixed game outcomes. When MLB's complacency during this \"Golden Age\" was eventually exposed after the 1919 World Series, it became known as the Black Sox scandal.\n\nAfter an excellent regular season (88–52, .629 W%), the Chicago White Sox were heavy favorites to win the 1919 World Series. Arguably the best team in baseball, The White Sox had a deep lineup, a strong pitching staff, and a good defense. Even though the National League champion Cincinnati Reds had a superior regular season record (96–44, .689 W%,) no one, including gamblers and bookmakers, anticipated the Reds having a chance. When the Reds triumphed 5–3, many pundits cried foul.\n\nAt the time of the scandal, the White Sox were arguably the most successful franchise in baseball, with excellent gate receipts and record attendance. At the time, most baseball players were not paid especially well and had to work other jobs during the winter to survive. Some elite players on the big-city clubs made very good salaries, but Chicago was a notable exception.\n\nFor many years, the White Sox were owned and operated by Charles Comiskey, who paid the lowest player salaries, on average, in the American League. The White Sox players all intensely disliked Comiskey and his penurious ways, but were powerless to do anything, thanks to baseball's so-called \"reserve clause\", that prevented players from switching teams without their team owner's consent.\n\nBy late 1919, Comiskey's tyrannical reign over the Sox had sown deep bitterness among the players, and White Sox first baseman Arnold \"Chick\" Gandil decided to conspire to throw the 1919 World Series. He persuaded gambler Joseph \"Sport\" Sullivan, with whom he had had previous dealings, that the fix could be pulled off for $100,000 total (which would be equal to $ today), paid to the players involved. New York gangster Arnold Rothstein supplied the $100,000 that Gandil had requested through his lieutenant Abe Attell, a former featherweight boxing champion.\n\nAfter the 1919 series, and through the beginning of the 1920 baseball season, rumors swirled that some of the players had conspired to purposefully lose. At last, in 1920, a grand jury was convened to investigate these and other allegations of fixed baseball games. Eight players (Charles \"Swede\" Risberg, Arnold \"Chick\" Gandil, \"Shoeless\" Joe Jackson, Oscar \"Happy\" Felsch, Eddie Cicotte, George \"Buck\" Weaver, Fred McMullin, and Claude \"Lefty\" Williams) were indicted and tried for conspiracy. The players were ultimately acquitted.\n\nHowever, the damage to the reputation of the sport of baseball led the team owners to appoint Federal judge Kenesaw Mountain Landis to be the first Commissioner of Baseball. His first act as commissioner was to ban the \"Black Sox\" from professional baseball for life. The White Sox, meanwhile would not return to the World Series until 1959 and it was not until their next appearance in 2005 they won the World Series.\n\nUntil July 5, 1947, baseball had two histories. One fills libraries, while baseball historians are only just beginning to chronicle the other fully. African Americans have played baseball as long as white Americans. Players of color, both African-American and Hispanic, played for white baseball clubs throughout the early days of the organizing amateur sport. Moses Fleetwood Walker is considered the first African-American to play at the major league level, in 1884.\n\nThe Negro Leagues were American professional baseball leagues comprising predominantly African-American teams. The term may be used broadly to include professional black teams outside the leagues and it may be used narrowly for the seven relatively successful leagues beginning 1920 that are sometimes termed \"Negro Major Leagues\".\n\nThe first professional team, established in 1885, achieved great and lasting success as the Cuban Giants, while the first league, the National Colored Base Ball League, failed in 1887 after only two weeks due to low attendance. The Negro American League of 1951 is considered the last major league season and the last professional club, the Indianapolis Clowns, operated amusingly rather than competitively from the mid-1960s to 1980s.\n\nWhile many of the players that made up the black baseball teams were African-Americans, many more were Latin Americans from nations that deliver some of the greatest talents that make up the major league rosters of today. Black players moved freely through the rest of baseball, playing in Canadian Baseball, Mexican Baseball, Caribbean Baseball, and Central America and South America where more than a few found that level of fame that they were unable to attain in the country of their birth.\n\nIt was not the Black Sox scandal by which an end was put to the dead-ball era, but by a rule change and a single player.\n\nSome of the increased offensive output can be explained by the 1920 rule change outlawing tampering with the ball, which pitchers had often done to produce \"spitballs\", \"shine balls\" and other trick pitches which had 'unnatural' flight through the air. Umpires were also required to put new balls into play whenever the current ball became scuffed or discolored. This rule change was enforced all the more stringently following the death of Ray Chapman, who was struck in the temple by a pitched ball from Carl Mays in a game on August 16, 1920 (he died the next day). Discolored balls, harder for batters to see and therefore harder for batters to dodge, have been rigorously removed from play ever since. There are two side effects. One, of course, is that if the batter can see the ball more easily, the batter can hit the ball more easily. The second is that without scuffs and other damage, pitchers are limited in their ability to control spin and so to cause altered trajectories.\n\nAt the end of the 1919 season Harry Frazee, then owner of the Boston Red Sox, sold a group of his star players to the New York Yankees. Amongst them was George Herman Ruth, known affectionately as \"Babe\". The story that Frazee did so in order to fund theatrical shows on Broadway for his actress lady friend is unfounded. No, No, Nanette was indeed first produced in 1925 by Harry Frazee, though the sale of baseball superstar Babe Ruth to the New York Yankees had occurred five years earlier. In the lore of the Curse of the Bambino, Frazee supposedly needed to sell Ruth among other reasons to make up for his lack of financial success up to that point in making Broadway shows, and it was hitting it big with \"No, No, Nanette\" in 1925 that paid off the loan.\n\nRuth's career mirrors the shift in dominance from pitching to hitting at this time. He started his career as a pitcher in 1914, and by 1916 was considered one of the dominant left-handed pitchers in the game. When Edward Barrow, managing the Red Sox, converted him to an outfielder, ballplayers and sportswriters were shocked. It was apparent, however, that Ruth's bat in the lineup every day was far more valuable than Ruth's arm on the mound every fourth day. Ruth swatted 29 home runs in his last season in Boston. The next year, as a Yankee, he would hit 54 and in 1921 he hit 59. His 1927 mark of 60 home runs would last until 1961.\nRuth's power hitting ability demonstrated a new way to play the game, and one that was extremely popular with the crowds. Accordingly, the ballparks were expanded, sometimes by building outfield seating which shrunk the size of the outfield and made home run hitting more practical. In addition to Ruth, hitters such as Rogers Hornsby also took advantage, with Hornsby compiling extraordinary figures for both power and average in the early 1920s. By the late 1920s and 1930s all the good teams had their home run hitting \"sluggers\": the Yankees' Lou Gehrig, Jimmie Foxx in Philadelphia, Hank Greenberg in Detroit and Chicago's Hack Wilson were the most storied. While the American League championship, and to a lesser extent the World Series, would be dominated by the Yankees, there were many other excellent teams in the inter-war years. Also, the National League's St. Louis Cardinals would win three titles themselves in nine years, the last with a group of players known as the \"Gashouse Gang\".\n\nThe first radio broadcast of a baseball game was on August 5, 1921 over Westinghouse station KDKA from Forbes Field in Pittsburgh. Harold Arlin announced the Pirates-Phillies game. Attendances in the 1920s were consistently better than they had been before the war. The interwar peak average attendance was 8,211 in 1930, but baseball was hit hard by the Great Depression and in 1933 the average fell below five thousand for the only time between the wars.\n\n1933 also saw the introduction of the All-Star game, a mid-season break in which the greatest players in each league play against one another in a hard fought but officially meaningless demonstration game. In 1936 the Baseball Hall of Fame was instituted and five players elected: Ty Cobb, Walter Johnson, Christy Mathewson, Babe Ruth and Honus Wagner. The Hall formally opened in 1939.\n\nThe beginning of US involvement in World War II necessitated depriving the game of many players who joined the armed forces, but the major leagues continued play throughout the duration. In 1941, a year which saw the premature death of Lou Gehrig, Boston's great left fielder Ted Williams had a batting average over .400 – the last time anyone has achieved that feat. During the same season Joe DiMaggio hit successfully in 56 consecutive games, an accomplishment both unprecedented and unequaled. Both Williams and DiMaggio would miss playing time in the services, with Williams also flying later in the Korean War. During this period Stan Musial led the St. Louis Cardinals to the 1942, 1944 and 1946 World Series titles. The war years also saw the founding of the All-American Girls Professional Baseball League.\n\nBaseball boomed after World War II. 1945 saw a new attendance record and the following year average crowds leapt nearly 70% to 14,914. Further records followed in 1948 and 1949, when the average reached 16,913. While average attendances slipped to somewhat lower levels through the 1950s, 1960s and the first half of the 1970s, they remained well above pre-war levels, and total seasonal attendance regularly hit new highs from 1962 onwards as the number of major league games increased.\n\nThe post-War years in baseball also witnessed the racial integration of the sport. Participation by African Americans in organized baseball had been precluded since the 1890s by formal and informal agreements, with only a few players surreptitiously being included in lineups on a sporadic basis.\n\nAmerican society as a whole moved toward integration in the post-War years, partially as a result of the distinguished service by African American military units such as the Tuskegee Airmen, 366th Infantry Regiment, and others. During the baseball winter meetings in 1943, noted African American athlete and actor Paul Robeson campaigned for integration of the sport. After World War II ended, several team managers considered recruiting members of the Negro Leagues for entry into organized baseball. In the early 1920s, New York Giants' manager John McGraw slipped a black player, Charlie Grant, into his lineup (reportedly by passing him off to the front office as an Indian), and McGraw's wife reported finding names of dozens of Negro players that McGraw fantasized about signing, after his death. Pittsburgh Pirates owner Bill Bensawanger reportedly signed Josh Gibson to a contract in 1943, and the Washington Senators were also said to be interested in his services. But those efforts (and others) were opposed by Kenesaw Mountain Landis, baseball's powerful commissioner and a staunch segregationist. Bill Veeck claimed that Landis blocked his purchase of the Philadelphia Phillies because he planned to integrate the team. While this is disputed, Landis was opposed to integration, and his death in 1944 (and subsequent replacement as Commissioner by Happy Chandler) removed a major obstacle for black players in the major leagues.\n\nThe general manager who would be eventually successful in breaking the color barrier was Branch Rickey of the Brooklyn Dodgers. Rickey himself had experienced the issue of segregation. While playing and coaching for his college team at Ohio Wesleyan University, Rickey had a black teammate named Charles Thomas. On one particular road trip through southern Ohio his fellow player was refused a room in a hotel. Although Rickey was able to get the player into his room for that night, he was taken aback when he reached his room to find Thomas upset and crying about this injustice. Rickey related this incident as an example of why he wanted a full de-segregation of the nation, not only in baseball.\n\nIn the mid-1940s, Rickey had compiled a list of Negro League ballplayers for a potential major league contract. Realizing that the first African American signee would be a magnet for prejudicial sentiment, however, Rickey was intent on finding a player with a distinguished personality and character that would allow him to tolerate the inevitable abuse. Rickey's sights eventually settled on Jackie Robinson, a shortstop with the Kansas City Monarchs. Although probably not the best player in the Negro Leagues at the time, Robinson was an exceptional talent, was college-educated, and had the marketable distinction of serving as an officer during World War II. More importantly, Robinson possessed the inner strength to handle the inevitable abuse to come. To prepare him for the task, Robinson first played in 1946 for the Dodgers' minor league team, the Montreal Royals, which proved an arduous emotional challenge, but he also enjoyed fervently enthusiastic support from the Montreal fans. On April 15, 1947, Robinson broke the color barrier, which had been tacitly recognized for over 50 years, with his appearance for the Brooklyn Dodgers at Ebbets Field.\n\nEleven weeks later, on July 5, 1947, the American League was integrated by the signing of Larry Doby to the Cleveland Indians. Over the next few years a handful of black baseball players made appearances in the majors, including Roy Campanella (teammate to Robinson in Brooklyn) and Satchel Paige (teammate to Doby in Cleveland). Paige, who had pitched more than 2400 innings in the Negro Leagues, sometimes two and three games a day, was still effective at 42, and still playing at 59. His ERA in the Major Leagues was 3.29.\n\nHowever, the initial pace of integration was slow. By 1953, only six of the sixteen major league teams had a black player on the roster. The Boston Red Sox became the last major league team to integrate its roster with the addition of Pumpsie Green on July 21, 1959. While limited in numbers, the on-field performance of early black major league players was outstanding. In the fourteen years from 1947–1960, black players won one or more of the Rookie of the Year awards nine times.\n\nWhile never prohibited in the same fashion as African Americans, Latin American players also benefitted greatly from the integration era. In 1951, two Chicago White Sox, Venezuelan-born Chico Carrasquel and Cuban-born (and black) Minnie Miñoso, became the first Hispanic All-Stars.\n\nAccording to some baseball historians, Robinson and the other African American players helped reestablish the importance of baserunning and similar elements of play that were previously de-emphasized by the predominance of power hitting.\n\nFrom 1947 to the 1970s, African American participation in baseball rose steadily. By 1974, 27% of baseball players were African American. As a result of this on-field experience, minorities began to experience long-delayed gains in managerial positions within baseball. In 1975, Frank Robinson (who had been the 1956 Rookie of the Year with the Cincinnati Reds) was named player-manager of the Cleveland Indians, making him the first African American manager in the major leagues.\n\nAlthough these front-office gains continued, Major League Baseball saw a lengthy slow decline in the percentage of black players after the mid-1970s. By 2007, black players made up less than 9% of the major leagues. While this trend is largely attributed to an increased emphasis on the recruitment of players from Latin America (with the number of Hispanic players in the major leagues rising to 29% by 2007), other factors have been cited as well. Hall of Fame player Dave Winfield, for instance, has cited the fact that urban America places less emphasis and provides less resources for youth baseball than in the past. Despite this continued prevalence of Hispanic players, the percentage of black players rose again in 2008 to 10.2%.\n\nArturo Moreno became the first Hispanic owner of a MLB franchise when he purchased the Anaheim Angels in 2004.\n\nIn 2005, a Racial and Gender Report Card on Major League Baseball was issued, which generally found positive results on the inclusion of African Americans and Latinos in baseball, and gave Major League Baseball a grade of \"A\" or better for opportunities for players, managers and coaches as well as for MLB's central office. At that time, 37% of major league players were people of color: Latino (26 percent), African-American (9 percent) or Asian (2 percent). Also by 2004, 29% of the professional staff in MLB's central office were people of color, 11% of team vice presidents were people of color, and seven of the league's managers were of color (four African-Americans and three Latinos).\n\nBaseball had been in the West for almost as long as the National League and the American League had been around. It evolved into the Pacific Coast League, which included the Hollywood Stars, Los Angeles Angels, Oakland Oaks, Portland Beavers, Sacramento Solons, San Francisco Seals, San Diego Padres, Seattle Rainiers.\n\nThe PCL was huge in the West. A member of the National Association of Professional Baseball Leagues, it kept losing great players to the National and the American leagues for less than $8,000 a player.\n\nThe PCL was far more independent than the other \"minor\" leagues, and rebelled continuously against their Eastern masters. Clarence Pants Rowland, the President of the PCL, took on baseball commissioners Kenesaw Mountain Landis and Happy Chandler at first to get better equity from the major leagues, then to form a third major league. His efforts were rebuffed by both commissioners. Chandler and several of the owners, who saw the value of the markets in the West, started to plot the extermination of the PCL. They had one thing that Rowland did not: The financial power of the Eastern major league baseball establishment.\n\nNo one was going to back a PCL club building a major-league size stadium if the National or the American League was going to build one too, and potentially put the investment in the PCL ballpark into jeopardy.\n\nUntil the 1950s, major league baseball franchises had been largely confined to the northeastern United States, with the teams and their locations having remained unchanged from 1903 to 1952. The first team to relocate in fifty years was the Boston Braves, who moved in 1953 to Milwaukee, where the club set attendance records. In 1954, the St. Louis Browns moved to Baltimore and were renamed the Baltimore Orioles. In 1955, the Philadelphia Athletics moved to Kansas City.\n\nIn 1958 the New York market ripped apart. The Yankees were becoming the dominant draw, and the cities of the West offered generations of new fans in much more sheltered markets for the other venerable New York clubs, the Brooklyn Dodgers and the New York Giants. Placing these storied, powerhouse clubs in the two biggest cities in the West had the specific design of crushing any attempt by the PCL to form a third major league. Eager to bring these big names to the West, Los Angeles gave Walter O'Malley, owner of the Dodgers, a helicopter tour of the city and asked him to pick his spot. The Giants were given the lease to the PCL San Francisco Seals digs while Candlestick Park was built for them.\n\nThe logical first candidates for major league \"expansion\" were the same metropolitan areas that had just attracted the Dodgers and Giants. It is said that the Dodgers and Giants—National League rivals in New York City—chose their new cities because Los Angeles (in southern California) and San Francisco (in northern California) already had a fierce rivalry (geographical, economic, cultural and political), dating back to the state's founding. The only California expansion team—and also the first in Major League Baseball in over 70 years—was the |Los Angeles Angels (later the California Angels, the Anaheim Angels, and, as of 2005, the Los Angeles Angels of Anaheim), who brought the American League to southern California in 1961. Northern California, however, would later gain its own American League team, in 1968, when the Athletics would move again, settling in Oakland, across San Francisco Bay from the Giants.\n\nAlong with the Angels, the other 1961 expansion team was the Washington Senators, who joined the American League and took over the nation's capital when the moved to Minnesota and became the Twins. 1961 is also noted as being the year in which Roger Maris surpassed Babe Ruth's single season home run record, hitting 61 for the New York Yankees, albeit in a slightly longer season than Ruth's. To keep pace with the American League—which now had ten teams—the National League likewise expanded to ten teams, in 1962, with the addition of the Houston Colt .45s and New York Mets.\n\nIn 1969, the American League expanded when the Kansas City Royals and Seattle Pilots, the latter in a longtime PCL stronghold, were admitted to the league. The Pilots stayed just one season in Seattle before moving to Milwaukee and becoming today's Milwaukee Brewers. The National League also added two teams that year, the Montreal Expos and San Diego Padres. The Padres were the last of the core PCL teams to be absorbed. The Coast League did not die, though. It reformed, and moved into other markets, and endures to this day as a Class AAA league.\n\nIn 1972, the second Washington Senators moved to the Dallas-Fort Worth area and became the Texas Rangers.\n\nIn 1977, the American League expanded to fourteen teams, with the newly formed Seattle Mariners and Toronto Blue Jays. Sixteen years later, in 1993, the National League likewise expanded to fourteen teams, with the newly formed Colorado Rockies and Florida Marlins (now Miami Marlins).\n\nBeginning with the 1994 season, both the AL and the NL were divided into three divisions (East, West, and Central), with the addition of a wild card team (the team with the best record among those finishing in second place) to enable four teams in each league to advance to the preliminary division series. However, due to the 1994–95 Major League Baseball strike (which canceled the 1994 World Series), the new rules did not go into effect until the 1995 World Series.\n\nIn 1998, the AL and the NL each added a fifteenth team, for a total of thirty teams in Major League Baseball. The Arizona Diamondbacks joined the National League, and the Tampa Bay Devil Rays—now called simply the Rays—joined the American League. In order to keep the number of teams in each league at an even number (14 – AL, 16 – NL), Milwaukee changed leagues and became a member of the National League.\n\nBy the late 1960s, the balance between pitching and hitting had swung in favor of the pitchers. In 1968 Carl Yastrzemski won the American League batting title with an average of just .301, the lowest in history. That same year, Detroit Tigers pitcher Denny McLain won 31 games – making him the last pitcher to win 30 games in a season. St. Louis Cardinals starting pitcher Bob Gibson achieved an equally remarkable feat by allowing an ERA of just 1.12.\n\nIn response to these events, major league baseball implemented certain rules changes in 1969 to benefit the batters. The pitcher's mound was lowered, and the strike zone was reduced.\n\nIn 1973 the American League, which had been suffering from much lower attendance than the National League, made a move to increase scoring even further by initiating the designated hitter rule.\n\nFrom the time of the formation of the Major Leagues to the 1960s, when it came to the control of the game of baseball the team owners held the whip hand. After the so-called \"Brotherhood Strike\" of 1890 and the failure of the Brotherhood of Professional Base Ball Players and its Players National League, the owners control of the game seemed absolute. It lasted over 70 years despite a number of short-lived players organizations. In 1966, however, the players enlisted the help of labor union activist Marvin Miller to form the Major League Baseball Players Association (MLBPA). The same year, Sandy Koufax and Don Drysdale – both Cy Young Award winners for the Los Angeles Dodgers – refused to re-sign their contracts, and the era of the reserve clause, which held players to one team, was coming toward an end.\n\nThe first legal challenge came in 1970. Backed by the MLBPA, St. Louis Cardinals outfielder Curt Flood took the leagues to court to negate a player trade, citing the 13th Amendment and antitrust legislation. In 1972 he finally lost his case in the United States Supreme Court by a vote of 5 to 3, but gained large-scale public sympathy, and the damage had been done. The reserve clause survived, but it had been irrevocably weakened. In 1975 Andy Messersmith of the Dodgers and Dave McNally of the Montreal Expos played without contracts, and then declared themselves free agents in response to an arbitrator's ruling. Handcuffed by concessions made in the Flood case, the owners had no choice but to accept the collective bargaining package offered by the MLBPA, and the reserve clause was effectively ended, to be replaced by the current system of free-agency and arbitration.\n\nWhile the legal challenges were going on, the game continued. In 1969 the \"Miracle Mets\", just 7 years after their formation, recorded their first winning season, won the National League East and finally the World Series.\n\nOn the field, the 1970s saw some of the longest standing records fall and the rise of two powerhouse dynasties. In Oakland, the Swinging A's were overpowering, winning the Series in '72, '73 and '74, and five straight division titles. The strained relationships between teammates, who included Catfish Hunter, Vida Blue and Reggie Jackson, gave the lie to the need for \"chemistry\" between players. The National League, on the other hand, belonged to the Big Red Machine in Cincinnati, where Sparky Anderson's team, which included Pete Rose as well as Hall of Famers Tony Pérez, Johnny Bench and Joe Morgan, succeeded the A's run in 1975.\n\nThe decade also contained great individual achievements as well. On April 8, 1974, Hank Aaron of the Atlanta Braves hit his 715th career home run, surpassing Babe Ruth's all-time record. He would retire in 1976 with 755 and that was just one of numerous records he achieved, many of which, including Total bases scored, still stand today. There was great pitching too: between 1973 and 1975, Nolan Ryan threw 4 \"no-hit\" games. He would add a record-breaking fifth in 1981 and two more before his retirement in 1993, by which time he had also accumulated 5,714 strikeouts, another record, in a 27-year career.\n\nFrom the 1980s onward, the major league game has changed dramatically from a combination of effects brought about by free agency, improvements in the science of sports conditioning, changes in the marketing and television broadcasting of sporting events, and the push by brand-name products for greater visibility. These events lead to greater labor difficulties, fan disaffection, skyrocketing prices, changes in the way that the game is played, and problems with the use of performance-enhancing substances like steroids tainting the race for records. Through this period crowds generally rose. Average attendances first broke 20,000 in 1979 and 30,000 in 1993. That year total attendance hit 70 million, but baseball was hit hard by a strike in 1994, and as of 2005 it has only marginally improved on those 1993 records.\n\nDuring the 1980s, the science of conditioning and workouts greatly improved. Weight rooms and training equipment were improved. Trainers and doctors developed better diets and regimens to make athletes bigger, healthier, and stronger than they had ever been.\n\nAnother major change that had been occurring during this time was the adoption of the pitch count. Starting pitchers playing complete games had not been an unusual thing in baseball's history. Now pitching coaches watched to see how many pitches a player had thrown over the game. At anywhere from 100 to 125, pitchers increasingly would be pulled out to preserve their arms. Bullpens began to specialize more, with more pitchers being trained as middle relievers, and a few hurlers, usually possessing high velocity but not much durability, as closers.\n\nAlong with the expansion of teams, the addition of more pitchers needed to play a complete game stressed the total number of quality players available in a system that restricted its talent searches at that time to America, Canada, Latin America, and the Caribbean.\n\nBaseball had been watched live since the mid 20th century. Television sports' arrival in the 1950s increased attention and revenue for all major league clubs at first. The television programming was extremely regional. It hurt the minor and independent leagues most. People stayed home to watch Maury Wills rather than watch unknowns at their local baseball park. Major League Baseball, as it always did, made sure that it controlled rights and fees charged for the broadcasts of all games, just as it did on radio. It brought additional revenues and attention both from the broadcast itself, and from the increases in attendance and merchandise sales that expanded audiences allowed.\n\nThe national networks began televising national games of the week, opening the door for a national audience to see particular clubs. While most teams were broadcast, emphasis was always on the league leaders and the major market franchises that could draw the largest audience.\n\nIn the 1970s the cable revolution began. The Atlanta Braves became a power contender with greater revenues generated by WTBS, Ted Turner's Atlanta-based Super-Station, that broadcast \"America's Team\" to cable households nationwide. The roll out of ESPN, then regional sports networks (now mostly under the umbrella of Fox Sports Net) changed sports news and particularly impacted baseball. Boiled down to the thirty-second game highlight, and now under the microscope of news organizations that needed to fill 24 hours of time, the amount of attention paid to major league players magnified to staggering levels from where it had been just 20 years prior.\n\nIt brought with it increased attention for individual players, who reached super-star status nationwide on careers that often were not as compelling as those who had come before them in a less media intense time.\n\nAs player contract values soared, and the number of broadcasters, commentators, columnists, and sports writers also soared. The competition for a fresh angle on any story became fierce. Media pundits began questioning the high salaries that the players received. Coverage began to become intensely negative. Players personal lives, which had always been off-limits unless something extreme happened, became the fodder of editorials, insider stories on television, and features in magazines. When the use of performance-enhancing drugs became an issue, the gap between the sports media and the players whom they covered widened further.\n\nWith the development of satellite television and digital cable, Major League Baseball launched baseball channels with season subscription fees, making it possible for fans to watch virtually every game played as they played.\n\nThe next round became the single-team cable networks. YES Network & NESN, the New York Yankees & Boston Red Sox cable television networks, respectively, took in millions to broadcast games not only in New York and Boston but around the country. These networks generated as much revenue or more annually for large market teams like the Yankees and Red Sox as their entire baseball operations did. By making these separate companies, these owners were able to exclude the money from consideration of deals.\n\nTelevision and greater media coverage in magazines and newspapers trying to attract a new generation of non-readers also brought in the sponsors, and even more money, that would attract players to new financial opportunities and bring in other elements to the business of baseball that would impact the game.\n\nBaseball memorabilia and souvenirs, including baseball cards, exploded in price as networks of adults became more sophisticated in their trading. This would explode yet again in the late 1990s, as the Internet, and the website eBay provided venues for collectors of all things baseball to trade with each other. Regionalized pricing was wiped away, and many objects, baseballs, bats, and the like began selling for high dollar values. This in turn brought in new businessmen whose sole means of making a living was acquiring autographs and memorabilia from the athletes. Memorabilia hounds fought with fans to get signatures worth $20, $60, or even $100 or more in their stores.\n\nBeyond the staple billboards, large corporations like NIKE and Champion fought to make sure that their logos were seen on the clothing and shoes worn by athletes on the field. This kind of association branding became a new revenue stream. In the late 1990s and into the dawn of the 21st century, the dugout, the backstops behind home plate, and anywhere else that might be seen by a camera all became fair game for inserting advertising.\n\nPlayers who had been dramatically underpaid for generations were now replaced by players who were paid extremely well for their services.\nBy the 1970s a new generation of sports agents were hawking the talents of players who knew baseball but didn't know how the business end of the game was played. The agents broke down what the teams were generating in revenue off of the players' performances. They calculated what their player might be worth to energize a television contract, or provide more merchandise revenue, or put more fans into seats.\nThe athletes signed shoe deals, baseball card sponsorships, and commercial endorsements for products of every size and shape.\nSky high salaries also changed many of the strategies of the game. Players rarely were \"sent\" down to the minors if they failed to perform. Who could justify paying a slumping player millions to sit in Toledo where the major league fans couldn't pay their way? Other players in the Triple-A level of the minor leagues, who used to rise on merit, became trapped under these overpaid \"stars.\" Also, in order to make the media happy, trades, rather than call-ups, became the order of the day. It was much better to buy someone else's shortstop who was a known quantity to the national sports media than to take a chance on a player with no name value and no visibility if you were in a major market ballclub.\n\nTactics on the field changed too. Risky moves that could get players hurt, and sideline millions of dollars in payroll on the disabled list, became less common. Stealing home, a popular tactic of great stars of the day like Ty Cobb or Pete Rose, became infrequent occurrences.\n\nThe perception of players by the general public changed from larger-than-life heroes to a more cynical view of many of them as spoiled and overpaid. This was fed by the growing legions of television reporters, commentators, and print sports writers who also started asking questions about what justified the kind of money being paid to these players.\n\nWith players seeking greener pastures when their contracts came up, fewer players became career members of one ballclub. In the modern era, it is unusual to see a player stay with any one club for more than a few years if they are good enough to command a better salary.\n\nPlayers with any ability increasingly gravitated towards the money. Large market clubs like the New York Yankees, the Boston Red Sox, and the Chicago Cubs given big revenues from their cable television operations signed more and more of the big name players away from mid-sized and smaller market baseball clubs that could not afford to compete with them for salaries.\n\nAll was not well with the game. The many contractual disputes between players and owners came to a head in 1981. Previous players' strikes (in 1972, 1973 and 1980) had been held in preseason, with only the 1972 stoppage – over benefits – causing disruption to the regular season from April 1 to April 13. Also, in 1976 the owners had locked the players out of Spring training in a dispute over free agency.\n\nThe crux of the 1981 dispute was about compensation for the loss of players to free agency. After losing a top-rank player in such a way the owners wanted a mid-rank player in return, the so-called \"sixteenth player\" (each club was allowed to protect 15 players from this rule). Losing lower rated free agents would have correspondingly smaller compensation. The players, only recently freed from the bondage of the reserve clause, found this unacceptable, and withdrew their labor, striking on June 12. Immediately, the U.S. Government National Labor Relations Board ruled that the owners had not been negotiating in good faith, and installed a federal mediator to reach a solution. Seven weeks and 713 games were lost in the middle of the season, before the owners backed down on July 31, settling for much lower ranked players as compensation. By then much of the season had been lost, and the season was continued as distinct halves starting August 9, with the playoffs reorganized to reflect this.\n\nThroughout the 1980s then, baseball seemed to prosper. The competitive balance between franchises saw fifteen different teams make the World Series, and nine different champions during the decade. Also, every season from 1978 through 1987 saw a different World Series winner, a streak unprecedented in baseball history. Turmoil was, however, just around the corner. In 1986 Pete Rose retired from playing for the Cincinnati Reds, having broken Ty Cobb's record by accumulating 4,256 hits during his career. He continued as Reds manager until, in 1989 it was revealed that he was being investigated for sports gambling, including the possibility that he had bet on teams with which he was involved. While Rose admitted a gambling problem, he denied having bet on baseball. Federal prosecutor John Dowd investigated and, on his recommendation, Rose was banned from organised baseball, a move which precluded his possible inclusion in the Hall of Fame. In a meeting with Commissioner Giamatti, Rose, having failed in a legal action to prevent it, accepted his punishment. It was, essentially, the same fate that had befallen the Black Sox seventy years previously. (Rose, however, would continue to deny that he bet on baseball until he finally confessed to it in his 2004 autobiography.)\n\nLabor relations were still strained. There had been a two-day strike in 1985 (over the division of television revenue money), and a 32-day spring training lockout in 1990 (again over salary structure and benefits). By far the worst action would come in 1994. The seeds were sown earlier: in 1992 the owners sought to renegotiate salary and free-agency terms, but little progress was made. The standoff continued until the beginning of 1994 when the existing agreement expired, with no agreement on what was to replace it. Adding to the problems was the perception that \"small market\" teams, such as the struggling Seattle Mariners could not compete with high spending teams such as those in New York or Los Angeles. Their plan was to institute TV revenue sharing to increase equity amongst the teams and impose a salary cap to keep expenditures down. Players felt that such a cap would reduce their potential earnings.\n\nThe players officially went on strike on August 12, 1994. In September 1994 Major League Baseball announced the cancellation of the World Series for the first time since 1904.\n\nThe cancellation of the 1994 World Series was a severe embarrassment for Major League Baseball. Americans were cursed, outraged, frightened, angered, frustrated, and plagued to their core as a result of the strike. Fans had declared the strike as an act of war. Although there were few signs of the predicted \"outrage\" on the part of the fans, attendance figures and broadcast ratings were lower in 1995 than before the strike. However, it would be a decade until baseball would recover from the strike.\n\nOn September 6, 1995, Baltimore Orioles shortstop Cal Ripken, Jr. played his 2,131st consecutive game, breaking Lou Gehrig's 56-year-old record. This was the first high-profile moment in baseball after the strike. Ripken continued his streak for another three years, voluntarily ending it at 2,632 consecutive games played on September 20, 1998.\n\nIn 1997, the Florida Marlins won the World Series in just their fifth season. This made them the youngest expansion team to win the Fall Classic (with the exception of the 1903 Boston Red Sox and later the 2001 Arizona Diamondbacks, who won in their fourth season). Virtually all the key players on the 1997 Marlins team were soon traded or let go to save payroll costs (although the 2003 Marlins did win a second world championship).\n\nIn 1998, St. Louis Cardinals first baseman Mark McGwire and Chicago Cubs outfielder Sammy Sosa engaged in a home run race for the ages. With both rapidly approaching Roger Maris's record of 61 home runs (set in 1961), seemingly the entire nation watched as the two power hitters raced to be the first to break the record. McGwire reached 62 first on September 8, 1998, with Sosa also eclipsing it later. Sosa finished with 66 home runs, just behind McGwire's unheard-of 70. However, recent steroid allegations have marred the season in the minds of many fans.\n\nThat same year, the New York Yankees won a record 125 games, including going 11–2 in the postseason, to win the World Series as what many consider to be one of the greatest teams of all time.\n\nMcGwire's record of 70 would last a mere three years following the meteoric rise of veteran San Francisco Giants left fielder Barry Bonds in 2001. In 2001 Bonds knocked out 73 home runs, breaking the record set by McGwire by hitting his 71st on October 5, 2001. In addition to the home run record, Bonds also set single-season marks for base on balls with 177 (breaking the previous record of 170, set by Babe Ruth in 1923) and slugging percentage with .863 (breaking the mark of .847 set by Ruth in 1920). Bonds continued his torrid home run hitting in the next few seasons, hitting his 660th career home run on April 12, 2004, tying him with his godfather Willie Mays for third place on the all-time career home runs list. He hit his 661st home run the next day, April 13, to take sole possession of third place. Only three years later Bonds surpassed the great Hank Aaron to become baseball's most prolific home run hitter.\n\nHowever, both Bonds' accomplishments in the 2000s have not been without controversy. During his run, journalists questioned McGwire about his use of the steroid-precursor androstenedione, and in March 2005 was unforthcoming when questioned as part of a Congressional inquiry into steroids. Bonds has also has been dogged by allegations of steroid use and his involvement in the BALCO drugs scandal, as his personal trainer Greg Anderson pleaded guilty to supplying steroids (without naming Bonds as a recipient). Neither Bonds nor McGwire has failed a drug test at any time since there was no steroid-testing until 2003 after the new August 7, 2002 agreement between owners and players was reached. McGwire retired after the 2001 season; in 2010, he admitted to having used steroids throughout his MLB career.\n\nThe 1990s also saw Major League Baseball expand into new markets as four new teams joined the league. In 1993, the Colorado Rockies and Florida Marlins began play, and in just their fifth year of existence, the Marlins became the first wild card team to win the championship. \n\nThe year 1998 brought two more teams into the mix, the Tampa Bay Devil Rays and the Arizona Diamondbacks, the latter of which become the youngest expansion franchise to win the championship. \n\nFor the most part, the late 1990s were dominated by the New York Yankees, who won four out of five World Series championships from 1996–2000.\n\nThe lure of big money pushed players harder and harder to perform at their peaks. There is only so much conditioning that one can do to obtain an edge without inducing injury. The wearying travel schedule and 162-game season meant that amphetamines, usually in the form of pep pills known as \"greenies\", had been widespread in baseball since at least the 1960s. Baseball's drug scene was no particular secret, having been discussed in \"Sports Illustrated\" and in Jim Bouton's groundbreaking book \"Ball Four\", but there was virtually no public backlash. Two decades later, however, some Major League players turned to newer performance-enhancing drugs, including ephedra and improved steroids.\n\nA memo circulated in 1991 by baseball commissioner Fay Vincent said, \"The possession, sale or use of any illegal drug or controlled substance by Major League players and personnel is strictly prohibited ... [and those players involved] are subject to discipline by the Commissioner and risk permanent expulsion from the game... This prohibition applies to all illegal drugs and controlled substances, including steroids…\" Some general managers of the time do not remember this memo, and it was not emphasized or enforced.\n\nEphedra, a Chinese herb used to cure cold symptoms, and also used in some allergy medications, sped up the heart and was considered by some to be a weight-loss short-cut. Overweight pitcher Steve Bechler, who wanted to stay on the Baltimore Orioles roster, took just such a shortcut. He collapsed on February 17, 2003 while pitching, and was soon pronounced dead. Bechler's death raised concerns over the use of performance-enhancing drugs in baseball. Ephedra was banned, and soon the furor died down.\n\nThe 1998 home run race had generated nearly unbroken positive publicity, but Barry Bonds' run for the all-time home run record provoked a backlash over steroids, which increase a person's testosterone level and subsequently enable that person to bodybuild with much more ease. Some athletes have said that the main advantage to steroids is not so much the additional power or endurance that they can provide, but that they can drastically shorten rehab time from injury.\n\nCommissioner Bud Selig imposed a very strict anti-drug policy upon its minor league players, who are not part of the Major League Baseball Players Association (the PA). Random drug testing, education and treatment, and strict penalties for those caught were the rule of law. Anyone on a Major League team's forty man roster, including 15 minor leaguers that are on that list, were exempt from that program. Some called Selig's move a public relations stunt, or window dressing.\n\nIn a \"Sports Illustrated\" cover story in 2002, a year after his retirement, Ken Caminiti admitted that he had used steroids during his National League MVP-winning 1996 season, and for several seasons afterwards. Caminiti died unexpectedly of an apparent heart attack in The Bronx at the age of 41; he was pronounced dead on October 10, 2004 at New York's Lincoln Memorial Hospital. On November 1, the New York City Medical Examiners Office announced that Caminiti died from \"acute intoxication due to the combined effects of cocaine and opiates\", but coronary artery disease and cardiac hypertrophy (an enlarged heart) were also contributing factors.\n\nIn 2005, Jose Canseco published \"\", admitting steroid usage and claiming that it was prevalent throughout major league baseball. When the United States Congress decided to investigate the use of steroids in the sport, some of the game's most prominent players came under scrutiny for possibly using steroids. These include Barry Bonds, Jason Giambi, and Mark McGwire. Other players, such as Canseco and Gary Sheffield, have admitted to have either knowingly (in Canseco's case) or not (Sheffield's) using steroids. In confidential testimony to the BALCO Grand Jury (that was later leaked to the \"San Francisco Chronicle\"), Giambi also admitted steroid use. He later held a press conference in which he appeared to affirm this admission, without actually saying the words. And after an appearance before Congress where he (unlike McGwire) emphatically denied using steroids, \"period\", slugger Rafael Palmeiro became the first major star to be suspended (10 days) on August 1, 2005 for violating Major League Baseball's newly strengthened ban on controlled substances, including steroids, adopted on August 7, 2002, starting in the 2003 season. Many lesser players (mostly from the minor leagues) have tested positive for use, as well.\n\nIn 2006, the Commissioner of Baseball tasked former United States Senator George J. Mitchell to lead an investigation into the use of performance-enhancing drugs in Major League Baseball (MLB) and on December 13, 2007, the 409-page Mitchell Report was released ('Report to the Commissioner of Baseball of an Independent Investigation into the Illegal Use of Steroids and Other Performance Enhancing Substances by Players in Major League Baseball'). The report described the use of anabolic steroids and human growth hormone (HGH) in MLB and assessed the effectiveness of the MLB Joint Drug Prevention and Treatment Program. Mitchell also advanced certain recommendations regarding the handling of past illegal drug use and future prevention practices. The report names 89 MLB players who are alleged to have used steroids or drugs.\n\nBaseball has been taken to task for turning a blind eye to its drug problems. It benefited from these drugs in the ever-increasingly competitive fight for airtime and media attention. MLB and its Players Association finally announced tougher measures, but many felt that they did not go far enough. \n\nIn December 2009, Sports Illustrated named Baseball's Steroid Scandal as the number one sports story of the decade of the 2000s. In 2013, no player from the first \"steroid class\" of players eligible for the Baseball Hall of Fame was elected. Bonds and Clemens received less than half the number of votes needed, and some voters stated that they would not vote for any first-time candidate who played during the steroid era—whether accused of using banned substances or not—because of the effect the substances had on baseball.\n\nIn 2002, a major scandal arose when it was discovered that a company called BALCO (Bay Area Laboratory Co-operative), owned by Victor Conte, had been producing so-called \"designer steroids\", (specifically \"the clear\" and \"the cream\") which are steroids that could not be detected through drug tests at that time. In addition, the company had connections to several San Francisco Bay Area sports trainers and athletes, including the trainers of Jason Giambi and Barry Bonds. This revelation lead to a vast criminal investigation into BALCO's connections with athletes from baseball and many other sports. Among the many athletes who have been linked to BALCO are Olympic sprinters Tim Montgomery and Marion Jones, Olympic shot-putter C. J. Hunter, and Major League Baseball players Jason Giambi and Barry Bonds.\n\nDuring grand jury testimony in December 2003 – which was illegally leaked to the San Francisco Chronicle and published in December 2004 – Giambi allegedly admitted to using many different steroids, including fertility drugs (which could account for his declining health in the past few years). The reports that came from the San Francisco Chronicle were done by Mark Fainaru-Wada and Lance Williams, who revealed that the Bay Area Laboratory Cooperative did not merely manufacture nutritional supplements, but also distributed exotic steroids. Williams and Fairanu-Wada also provided compelling evidence that Bonds, arguably the greatest player of his generation, was one of BALCO's steroid clients. The paper reported that these substances were probably designer steroids. Bonds said that Greg Anderson gave him a rubbing balm and a liquid substance that at the time he did not believe them to be steroids and thought they were flaxseed oil and other health supplements. Based on the testimony from many of the athletes, Conte and Anderson accepted plea agreements from the government in 2005 on charges they distributed steroids and laundered money to avoid significant time in jail. Conte received a sentence of four months in jail, Anderson received a sentence of three months. Also that year, James Valente, the vice president of Balco, and Remi Korchemny, a track coach affiliated with BALCO, pleaded guilty to distributing banned substances and received probation.\n\nVarious baseball pundits, fans, and even players have taken this as confirmation that Bonds used illegal steroids. Bonds never tested positive in tests performed in 2003, 2004, and 2005, which may be attributable to successful obfuscation of continued use as documented in the 2006 book \"Game of Shadows\".\n\nWhile the introduction of steroids certainly increased the power production of greats there were other factors that drastically increased the power surge after 1994. The factors cited are: smaller sized ballparks than in the past, the \"juiced balls\" theory claiming that the balls are wound tighter thus travel further following contact with the bat, \"watered down pitching\" implying that lesser quality pitchers are up in the Major Leagues due to too many teams. Albeit that these factors did play a large role in increasing home run thus scoring totals during this time, others that directly impact ballplayers have an equally important role. As noted earlier one of those factors is anabolic steroids which have the capability of increasing muscle mass, which enables hitters to not only hit \"mistake\" pitches farther, but it also enables hitters to adjust to \"good\" pitches such as a well-placed fastball, slider, changeup, or curveball, and hit them for home runs. Another such factor is better nutrition, as well as training and training facilities/equipment which can work with (or without) steroids to produce a more potent ballplayer and further enhance his skills.\n\nRoutinely in today's baseball age we see players reach 40 and 50 home runs in a season, a feat that even in the 1980s was considered rare. Many modern baseball theorists believe that a new pitch will swing the balance of power back to the pitcher. A pitching revolution would not be unprecedented—several pitches have changed the game of baseball in the past, including the slider in the 1950s and 1960s and the split-fingered fastball in the 1970s to 1990s. Since the 1990s, the changeup has made a resurgence, being thrown masterfully by pitchers such as Tim Lincecum, Pedro Martínez, Trevor Hoffman, Greg Maddux, Matt Cain, Tom Glavine, Johan Santana, Justin Verlander and Cole Hamels.\n\n\n\n\n", "id": "3856", "title": "History of baseball in the United States"}
{"url": "https://en.wikipedia.org/wiki?curid=3858", "text": "Major League Baseball Most Valuable Player Award\n\nThe Major League Baseball Most Valuable Player Award (MVP) is an annual Major League Baseball (MLB) award, given to one outstanding player in the American League and one in the National League. Since 1931, it has been awarded by the Baseball Writers' Association of America (BBWAA). The winners receive the Kenesaw Mountain Landis Memorial Baseball Award, which became the official name of the award in 1944, in honor of the first MLB commissioner, who served from 1920 until his death on November 25, 1944.\n\nMVP voting takes place before the postseason, but the results are not announced until after the World Series. The BBWAA began by polling three writers in each league city in 1938, reducing that number to two per league city in 1961. The BBWAA does not offer a clear-cut definition of what \"most valuable\" means, instead leaving the judgment to the individual voters.\n\nFirst basemen, with 34 winners, have won the most MVPs among infielders, followed by second basemen (16), third basemen (15), and shortstops (15). Of the 24 pitchers who have won the award, 15 are right-handed while 9 are left-handed. Walter Johnson, Carl Hubbell, and Hal Newhouser are the only pitchers who have won multiple times, Newhouser winning consecutively in 1944 and 1945.\n\nHank Greenberg, Stan Musial, Alex Rodriguez, and Robin Yount have won at different positions, while Rodriguez is the only player who has won the award with two different teams at two different positions. Barry Bonds has won the most often (seven times) and the most consecutively (2001–04). Jimmie Foxx was the first player to win multiple times; 9 players have won three times, and 19 have won twice. Frank Robinson is the only player to win the award in both the American and National Leagues.\n\nThe award's only tie occurred in the National League in 1979, when Keith Hernandez and Willie Stargell received an equal number of points. There have been 18 unanimous winners, who received all the first-place votes. The New York Yankees have the most winning players with 22, followed by the St. Louis Cardinals with 17 winners. The award has never been presented to a member of the following four teams: Arizona Diamondbacks, Miami Marlins, New York Mets, and Tampa Bay Rays. The most recent recipients are Mike Trout in the American League and Kris Bryant in the National League.\n\nIn recent decades, pitchers have rarely won the award. When Justin Verlander won the AL award in 2011, he became the first pitcher in either league to be named the MVP since Dennis Eckersley in 1992. Verlander also became the first starting pitcher to win this award since Roger Clemens had accomplished the feat in 1986. The National League went even longer without an MVP award to a pitcher—after Bob Gibson won in 1968, no pitcher in that league was named MVP until Clayton Kershaw in 2014.\n\nBefore the 1910 season, Hugh Chalmers of Chalmers Automobile announced he would present a Chalmers Model 30 automobile to the player with the highest batting average in Major League Baseball at the end of the season. The 1910 race for best average in the American League was between the Detroit Tigers' widely disliked Ty Cobb and Nap Lajoie of the Cleveland Indians. On the last day of the season, Lajoie overtook Cobb's batting average with seven bunt hits against the St. Louis Browns. American League President Ban Johnson said a recalculation showed that Cobb had won the race anyway, and Chalmers ended up awarding cars to both players.\n\nThe following season, Chalmers created the Chalmers Award. A committee of baseball writers were to convene after the season to determine the \"most important and useful player to the club and to the league\". Since the award was not as effective at advertising as Chalmers had hoped, it was discontinued after 1914.\n\nIn 1922 the American League created a new award to honor \"the baseball player who is of the greatest all-around service to his club\". Winners, voted on by a committee of eight baseball writers chaired by James Crusinberry, received a bronze medal and a cash prize. Voters were required to select one player from each team and player-coaches and prior award winners were ineligible. These incredible flaws resulted in Babe Ruth only winning 1 MVP as the award was dropped after 1928. Under the All-Star Formula, Ruth would have won the MVP in 1920, 1921, 1923, 1925, 1928 and 1930 [citation needed]. The National League award, without these restrictions, lasted from 1924 to 1929.\n\nThe BBWAA first awarded the modern MVP after the 1931 season, adopting the format the National League used to distribute its league award. One writer in each city with a team filled out a ten-place ballot, with ten points for the recipient of a first-place vote, nine for a second-place vote, and so on. In 1938, the BBWAA raised the number of voters to three per city and gave 14 points for a first-place vote. The only significant change since then occurred in 1961, when the number of voters was reduced to two per league city.\n\n\n\n", "id": "3858", "title": "Major League Baseball Most Valuable Player Award"}
{"url": "https://en.wikipedia.org/wiki?curid=3859", "text": "Major League Baseball Rookie of the Year Award\n\nIn Major League Baseball, the Rookie of the Year Award is annually given to one player from each league as voted on by the Baseball Writers' Association of America (BBWAA). The award was established in 1940 by the Chicago chapter of the BBWAA, which selected an annual winner from 1940 through 1946. The award became national in 1947; Jackie Robinson, the Brooklyn Dodgers' second baseman, won the inaugural award. One award was presented for both leagues in 1947 and 1948; since 1949, the honor has been given to one player each in the National and American League. Originally, the award was known as the J. Louis Comiskey Memorial Award, named after the Chicago White Sox owner of the 1930s. The award was renamed the Jackie Robinson Award in July 1987, 40 years after Jackie Robinson broke the baseball color line.\n\nOf the 140 players named Rookie of the Year (as of 2016), 16 have been elected to the National Baseball Hall of Fame—Jackie Robinson, five American League players, and ten others from the National League. The award has been shared twice: once by Butch Metzger and Pat Zachry of the National League in 1976; and once by John Castino and Alfredo Griffin of the American League in 1979. Members of the Brooklyn and Los Angeles Dodgers have won the most awards of any franchise (with 17), twice the total of the New York Yankees, and members of the Philadelphia and Oakland Athletics (eight), who have produced the most in the American League. Fred Lynn and Ichiro Suzuki are the only two players who have been named Rookie of the Year and Most Valuable Player in the same year, and Fernando Valenzuela is the only player to have won Rookie of the Year and the Cy Young Award in the same year. Sam Jethroe is the oldest player to have won the award, at age 32, 33 days older than 2000 winner Kazuhiro Sasaki (also 32). Michael Fulmer of the Detroit Tigers and Corey Seager of the Los Angeles Dodgers are the most recent winners.\n\nFrom 1947 through 1956, each BBWAA voter used discretion as to who qualified as a rookie. In 1957, the term was first defined as someone with fewer than 75 at bats or 45 innings pitched in any previous Major League season. This guideline was later amended to 90 at bats, 45 innings pitched, or 45 days on a Major League roster before September 1 of the previous year. The current standard of 130 at bats, 50 innings pitched or 45 days on the active roster of a Major League club (excluding time in military service or on the disabled list) before September 1 was adopted in 1971.\n\nSince 1980, each voter names three rookies: a first-place choice is given five points, a second-place choice three points, and a third-place choice one point. The award goes to the player who receives the most overall points. Edinson Vólquez received three second-place votes in 2008 balloting despite no longer being a rookie under the award's definition.\n\nThe award has drawn criticism in recent years because several players with experience in Nippon Professional Baseball (NPB) have won the award, such as Hideo Nomo in 1995, Kazuhiro Sasaki in 2000, and Ichiro Suzuki in 2001. The current definition of rookie status for the award is based only on Major League experience, but some feel that past NPB players are not true rookies because of their past professional experience. Others, however, believe it should make no difference since the first recipient and the award's namesake played for the Negro Leagues prior to his MLB career and thus could also not be considered a \"true rookie\". This issue arose in 2003 when Hideki Matsui narrowly lost the AL award to Ángel Berroa. Jim Souhan of the \"Minneapolis Star Tribune\" said he did not see Matsui as a rookie in 2003 because \"it would be an insult to the Japanese league to pretend that experience didn't count.\" \"The Japan Times\" ran a story in 2007 on the labeling of Daisuke Matsuzaka, Kei Igawa, and Hideki Okajima as rookies, saying \"[t]hese guys aren't rookies.\" Past winners such as Jackie Robinson, Don Newcombe, and Sam Jethroe had professional experience in the Negro Leagues.\n\n\n\n", "id": "3859", "title": "Major League Baseball Rookie of the Year Award"}
{"url": "https://en.wikipedia.org/wiki?curid=3860", "text": "National League Championship Series\n\nThe National League Championship Series (NLCS) is a best-of-seven series played in October in the Major League Baseball postseason that determines the winner of the National League (NL) pennant. The winner of the series advances to play the winner of the American League (AL) Championship Series (ALCS) in the World Series, Major League Baseball's championship series.\n\nPrior to 1969, the National League champion (the \"pennant winner\") was determined by the best win-loss record at the end of the regular season. There were four \"ad hoc\" three-game playoff series due to ties under this formulation (in 1946, 1951, 1959, and 1962). (The American League had to resolve a tie in 1948, but used a single-game playoff.)\n\nA structured postseason series began in 1969, when both the National and American Leagues were reorganized into two divisions each, East and West. The two division winners within each league played each other in a best-of-five series to determine who would advance to the World Series. In 1985, the format changed to best-of-seven.\n\nThe NLCS and ALCS, since the expansion to seven games, are always played in a 2–3–2 format: games 1, 2, 6, and 7 are played in the stadium of the team that has home field advantage, and games 3, 4, and 5 are played in the stadium of the team that does not. Home field advantage is given to the team that has the better record, with the exception that the team that made the postseason as the Wild Card team cannot get home field advantage. From 1969 to 1993, home field advantage was alternated between divisions each year regardless of regular season record and from 1995 to 1997 home field advantage was predetermined before the season.\n\nIn 1981, a divisional series was held due to a split season caused by a players' strike.\n\nIn 1994, the league was restructured into three divisions, with the three division winners and a wild-card team advancing to a best-of-five postseason round, the National League Division Series (NLDS). The winners of that round advance to the best-of-seven NLCS.\n\nEvery current National League franchise has appeared in the NLCS at least once. The Houston Astros made four NLCS appearances before moving to the AL in 2013.\n\nAs of the 2015 season, the Milwaukee Brewers are the only franchise to play in both the NLCS (in 2011) and the ALCS (in 1982). No franchise has won both the National and American League Championship Series.\n\nThe Warren C. Giles Trophy, named for the president of the NL from 1951 to 1969, is awarded to the NLCS winner.\n\nA Most Valuable Player (MVP) award is given to the outstanding player in each series, though voters can consider performances made during the divisional series. The MVP award has been given to a player on the losing team twice, in 1986 to Mike Scott of the Houston Astros and in 1987 to Jeff Leonard of the San Francisco Giants.\n\nAlthough the National League began its LCS MVP award in 1977, the American League did not begin its LCS MVP award until 1980.\n\n\n", "id": "3860", "title": "National League Championship Series"}
{"url": "https://en.wikipedia.org/wiki?curid=3861", "text": "American League Championship Series\n\nThe American League Championship Series (ALCS) is a best-of-seven series played in October in the Major League Baseball postseason that determines the winner of the American League (AL) pennant. The winner of the series advances to play the winner of the National League (NL) Championship Series (NLCS) in the World Series, Major League Baseball's championship series.\n\nIt started in 1969, when the AL reorganized into two divisions, East and West. The winners of each division played each other in a best-of-five series to determine who would advance to the World Series. In 1985, the format changed to best-of-seven. In 1994, the league was restructured into three divisions, with the three division winners and a wild-card team advancing to a best-of-five postseason round, known as the American League Division Series (ALDS). The winners of that round then advanced to the best-of-seven ALCS. In 2012, the playoffs were expanded again so that two wild card teams face off in a one-game wild card round to determine which team advances to the division series, with the playoffs then continuing as it had before 2012 (though with the possibility of a fifth seed being in the playoffs and a fourth seed being out) after the end of the wild card round. This is the system currently in use.\n\nThe ALCS and NLCS, since the expansion to best-of-seven, are always played in a 2–3–2 format: Games 1, 2, 6, and 7 are played in the stadium of the team that has home field advantage, and Games 3, 4, and 5 are played in the stadium of the team that does not. The series concludes when one team records its fourth win. Since 1998, home field advantage has been given to the team that has the better regular season record, unless that team happens to be the Wild Card team. In that case, the other team gets home field advantage, because by rule the Wild Card team is never allowed home field advantage in a Division Series or LCS. In the event that both teams have identical records in the regular season, home field advantage goes to the team that has the winning head-to-head record. From 1969 to 1993, home field advantage alternated between the two divisions, and from 1995 to 1997 home field advantage was determined before the season.\n\nEvery current American League team except for the Houston Astros has played in the ALCS at least once. The Milwaukee Brewers, an American League team between 1969 and 1997, is the only franchise to play in the ALCS (1982) and NLCS (2011). No franchise has won both the National League and American League Championship Series.\n\nThe William Harridge Trophy is awarded to the ALCS champion. The trophy's namesake comes from the American League president from 1931 to 1959.\n\nThe Lee MacPhail Most Valuable Player (MVP) award is given to the outstanding player in the ALCS. No MVP award is given for Division Series play.\n\nAlthough the National League began its LCS MVP award in 1977, the American League did not begin its LCS MVP award till 1980. The winners are listed (1) below in the section on \"ALCS results (1969–present)\", in the \"Series MVP\" column, (2) at League Championship Series Most Valuable Player Award, and (3) on the MLB website.\n\nClick the link on the far left for detailed information on that series.\n\n\n\n", "id": "3861", "title": "American League Championship Series"}
{"url": "https://en.wikipedia.org/wiki?curid=3862", "text": "American League Division Series\n\nIn Major League Baseball, the American League Division Series (ALDS) determines which teams will play in the American League Championship Series (ALCS). It is played in a best-of-five format.\n\nThe Division Series was implemented in 1981 as a result of a midseason strike, with the first place teams before the strike taking on the teams in first place after the strike. After 1993, it was implemented for good when Major League Baseball restructured each league into three divisions. In 1981, a split-season format forced the first ever divisional playoff series, in which the New York Yankees won the Eastern Division series over the Milwaukee Brewers (who were in the American League until 1998) in five games while in the Western Division, the Oakland Athletics swept the Kansas City Royals (the only team with an overall losing record to ever make the postseason). The Yankees have currently played in the most division series in history, with eighteen appearances. In 2015 the Toronto Blue Jays and Houston Astros became the last teams to make their first appearance in the ALDS. The Astros had been in the National League until switching to the American League in 2013 and had previously made the NLDS 7 times.\n\nFrom 1998 to 2011, the wild-card team was assigned to play in the division winner with the best winning percentage (outside of their own division) in one series, and the other two division winners met in the other series. However, if the wild-card team and the division winner with the best record were from the same division, the wild-card team played the division winner with the second-best record, and the remaining two division leaders played each other. Beginning with the 2012 season, the wild card team that advances to the Division Series was to face the number 1 seed, regardless of whether or not they are in the same division. The two series winners move on to the best-of-seven ALCS. Home field advantage goes to the team with the better regular season record (or head-to-head record if there is a tie between two or more teams), except for the wild card team, which never receives the home field advantage.\n\nBeginning in 2007, MLB has implemented a new rule to give the team from the league that wins the All-Star Game with the best regular season record a slightly greater advantage. In order to spread out the Division Series games for broadcast purposes, the two ALDS series follow one of two off-day schedules. Starting in 2007, after consulting the MLBPA, MLB has decided to allow the team with the best record in the league that wins the All-Star Game to choose whether to use the seven-day schedule (1-2-off-3-4-off-5) or the eight-day schedule (1-off-2-off-3-4-off-5). The team only gets to choose the schedule; the opponent is still determined by win-loss records.\n\nInitially, the best-of-5 series played in a 2-3 format, with the first two games set at home for the lower seed team and the last three for the higher seed. Since 1998, the series has followed a 2-2-1 format, where the higher seed team plays at home in Games 1 and 2, the lower seed plays at home in Game 3 and Game 4 (if necessary), and if a Game 5 is needed, the teams return to the higher seed's field. When MLB added a second wild card team in 2012, the Division Series re-adopted the 2-3 format due to scheduling conflicts. It reverted to the 2-2-1 format in 2013.\n\n\n\n", "id": "3862", "title": "American League Division Series"}
{"url": "https://en.wikipedia.org/wiki?curid=3863", "text": "National League Division Series\n\nIn Major League Baseball, the National League Division Series (NLDS) determines which two teams from the National League will advance to the National League Championship Series. The Division Series consists of two best-of-five series, featuring the three division winners and the winner of the wild-card play-off.\n\nThe Division Series was implemented in 1981 – for only one year – as a result of a midseason strike with first-place teams before the strike taking on the first-place teams after. After 1993, Major League Baseball decided to implement the Division Series permanently, because it was restructuring each league into three divisions. (The implementation of this decision, however, was delayed a year, until 1995, due to the 1994–1995 players' strike.) Previously, because of a players' strike in 1981, a split-season format forced a divisional playoff series, in which the Montreal Expos won the Eastern Division series over the Philadelphia Phillies three games to two while the Los Angeles Dodgers beat the Houston Astros three games to two in the Western Division. The team with the best overall record in the major leagues, the Cincinnati Reds, failed to win their division in either half of that season and were controversially excluded, as were the St. Louis Cardinals, who finished with the NL's second-best record. The Atlanta Braves have currently played in the most NL division series with thirteen appearances. The St. Louis Cardinals have currently won the most NL division series, winning ten of the twelve series in which they have played. The Pittsburgh Pirates (who finished with a losing record from 1993 to 2012) were the last team to make their first appearance in the NL division series, making their debut in 2013 after winning the 2013 National League Wild Card Game. In 2008, the Milwaukee Brewers became the first team to play in division series in both leagues when they won the National League wild card, their first postseason berth since winning the American League East Division title in 1982 before switching leagues in 1998. Milwaukee had competed in an American League Division Series in the strike-shortened 1981 season.\n\nThe NLDS is a five-game series where the wild-card team is assigned to play the divisional winner with the best winning percentage in the regular season. The two remaining divisional winners meet in the other (NLDS) series with the team with the second best winning percentage, hosting that series. (From 1998 to 2011, if the wild-card team and the division winner with the best record were from the same division, the wild-card team played the division winner with the second-best record, and the remaining two division leaders played each other.) The two series winners move on to the best-of-seven NLCS. The winner of the wild card has won the first round seven out of the 11 years since the re-alignment and creation of the NLDS. According to Nate Silver, the advent of this playoff series, and especially of the wild card, has caused teams to focus more on \"getting to the playoffs\" rather than \"winning the pennant\" as the primary goal of the regular season.\n\nInitially, the best-of-5 series played in a 2-3 format, with the first two games set at home for the lower-seed team and the last three for the higher seed. Since 1998, the series has followed a 2-2-1 format, where the higher seed team plays at home in Games 1 and 2, the lower seed plays at home in Game 3 and Game 4 (if necessary), and if a Game 5 is needed, the teams return to the higher seed's field. When MLB added a second wild card team in 2012, the Division Series re-adopted the 2-3 format due to scheduling conflicts. It reverted to the 2-2-1 format from 2013 onwards.\n\nNOTE: With the Houston Astros move to the American League at the conclusion of the 2012 season, the Braves vs Astros series is no longer possible.\n\n\n\n\n", "id": "3863", "title": "National League Division Series"}
{"url": "https://en.wikipedia.org/wiki?curid=3864", "text": "2001 World Series\n\nThe 2001 World Series, the 97th edition of Major League Baseball's championship series, took place between the Arizona Diamondbacks of the National League and the three-time defending champions New York Yankees of the American League. The Diamondbacks won the best-of-seven series four games to three. Considered one of the greatest World Series of all time, memorable aspects included two extra-inning games and three late-inning comebacks. It ended on a Game 7 walk-off hit in the form of a bases-loaded blooper single off the bat of Luis Gonzalez. This was the third World Series to end in this way, following and . This was also the Yankees' fourth consecutive World Series appearance, after winning it in , , and (the previous year).\n\nThis was the first World Series ever played in the state of Arizona and the Mountain Time Zone. With the All-Star Game format change in 2003, the World Series would not open in the city of the National League champion again until . This was the last World Series not to feature a wild card team until . This was also the first World Series to end in November, due to the delay in the regular season after the September 11 attacks.\n\nWith the win by the Diamondbacks, the franchise became the first World Series champions from a Far West state other than California and the first major professional sports team from the state of Arizona to win a title.\n\nRandy Johnson and Curt Schilling were named the series co-MVPs.\n\nThe Arizona Diamondbacks reached the Series in just their fourth season, breaking a record previously held by the Florida Marlins, and took on the three-time defending champion New York Yankees, who had won the World Series in four of the last five years and tried to become the first team to win four straight titles since the Yankees' five consecutive titles from to . Arizona captured the best of seven games Series, four games to three, thereby dethroning the defending World Champions and earning their first title.\n\nArizona won the first two games at home handily, but New York won the next three in close contests at Yankee Stadium, including two dramatic ninth-inning comebacks against Arizona closer Byung-Hyun Kim. Arizona won the sixth game handily with Randy Johnson pitching a masterful game. Johnson also pitched in relief of Curt Schilling in Game 7. The Diamondbacks won that game by the score of 3–2, ending when Jay Bell scored the winning run on a bloop single by Luis Gonzalez, in the bottom of the ninth inning off the Yankees' ace closer, Mariano Rivera. Johnson, credited with the Game 7 win, became the first pitcher to win three games in the same World Series since the Detroit Tigers' Mickey Lolich in .\n\nThe home team won every game in the Series, and as of 2016 is the most recent time this has happened. This had only happened twice before, in and also in domed ballparks; in the two earlier championships, the Minnesota Twins won the Series. This Series was the subject of an HBO documentary \"Nine Innings from Ground Zero\" in 2004.\n\nThough the series was played to the maximum seven games, the Diamondbacks outscored the Yankees 37–14 as a result of large margins of victory achieved by Arizona in Bank One Ballpark relative to the one run margins the Yankees achieved at Yankee Stadium. Arizona held powerhouse New York to an .183 batting average, the lowest ever in a seven-game World Series. The previous record was .185 by the St. Louis Cardinals in the 1985 World Series when they lost to the Kansas City Royals.\n\nDue to the postponement of MLB games as a result of the September 11 attacks, the World Series began Saturday, October 27, 2001, the latest start date ever for a World Series until the 2009 World Series, which started on October 28. The last three games were the first major-league games (other than exhibitions) played in the month of November. This was just the fourth time that no World Series champion was decided within the traditional month of October. The previous three occurrences were in (no series), (series held in September due to World War I), and (no series due to work stoppage). Additionally, the Series took place in New York City only seven weeks after the attacks, representing a remarkable boost in morale for the fatigued city.\n\nThe Yankees struck first in Game 1 when Derek Jeter was hit by a pitch with one out in the first and scored on Bernie Williams's double two batters later. However, Arizona's Curt Schilling and two relievers held the Yankees scoreless afterward. They managed to get only two walks and two hits for the rest of the game, Scott Brosius's double in the second and Jorge Posada's single in the fourth, both with two outs.\n\nMeanwhile, the Diamondbacks tied the game on Craig Counsell's one-out home run in the first off of Mike Mussina. After a scoreless second, Mussina led off the third by hitting Tony Womack with a pitch. He moved to second on Counsell's sacrifice bunt before Luis Gonzalez's home run put the Diamondbacks up 3-1. A single and right fielder David Justice's error put runners on second and third before Matt Williams's sacrifice fly put Arizona up 4-1. After Mark Grace was intentionally walked, Damian Miller's RBI double gave Arizona a 5-1 lead.\n\nNext inning, Gonzalez hit a two-out double off of Randy Choate. Reggie Sanders was intentionally walked before Gonzalez scored on Steve Finley's single. An error by third baseman Brosius scored Sanders, put Finley at third, and Williams at second. Both men scored on Mark Grace's double, putting Arizona up 9-1. Though the Diamondbacks got just one more hit for the rest of the game off of Sterling Hitchcock and Mike Stanton (Williams's leadoff single in the seventh), they went up 1-0 in the series.\n\nArizona continued to take control of the Series with the strong pitching performance of Randy Johnson. The Big Unit pitched a complete game shutout, allowing only four baserunners and three hits while striking out eleven Yankees. Andy Pettitte meanwhile nearly matched him, retiring Arizona in order in five of the seven innings he pitched. In the second, he allowed a leadoff single to Reggie Sanders, who scored on Danny Bautista's double. Bautista was the only Arizona runner stranded for the entire game. In the seventh, Pettitte hit Luis Gonzalez with a pitch before Sanders grounded into a forceout. After Bautista singled, Matt Williams's three-run home run put Arizona up 4-0. They won the game with that score and led the series two games to none as it moved to New York City.\n\nThe game was opened in New York by United States President George W. Bush, who threw the ceremonial first pitch, a strike to Yankees backup catcher Todd Greene. Bush became the first sitting US President to throw a World Series first pitch since Dwight D. Eisenhower in . He also threw the baseball from the mound where the pitcher would be set (unlike most ceremonial first pitches which are from in front of the mound) and threw it for a strike. Chants of \"\"U-S-A, U-S-A\"\" rang throughout Yankee Stadium. Yankees starter Roger Clemens allowed only three hits and struck out nine in seven innings of work. Yankees closer Mariano Rivera pitched two innings for the save.\n\nJorge Posada's leadoff home run off of Brian Anderson in the second put the Yankees up 1-0. The Diamondbacks loaded the bases in the fourth on two walks and one hit before Matt Williams's sacrifice fly tied the game. Bernie Williams hit a leadoff single in the sixth and moved to second on a wild pitch one out later before Posada walked. Mike Morgan relieved Anderson and struck out David Justice before Scott Brosius broke the tie with an RBI single. The Yankees cut Arizona's series lead to 2-1 with the win.\n\nGame 4 saw the Yankees send Orlando Hernandez to the mound while the Diamondbacks elected to bring back Curt Schilling on three days' rest. Both pitchers gave up solo home runs, with Schilling doing so to Shane Spencer in the third inning and Hernandez doing so to Mark Grace in the fourth. Hernandez pitched solid innings, giving up four hits while Schilling went seven innings and gave up three.\n\nWith the game still tied entering the eighth, Arizona struck. After Mike Stanton recorded the first out of the inning, Luis Gonzalez singled and Erubiel Durazo hit a double to bring him in. Matt Williams followed by grounding into a fielder's choice off of Ramiro Mendoza, which scored pinch runner Midre Cummings and gave the team a 3-1 lead.\n\nWith his team on the verge of taking a commanding 3-1 series lead, Diamondbacks manager Bob Brenly elected to bring in closer Byung-Hyun Kim in the bottom of the eighth for a two-inning save. Kim, at 22, became the first Korean-born player ever to play in the MLB World Series. Kim struck out the side in the eighth, but ran into trouble in the ninth.\n\nDerek Jeter led off by trying to bunt for a hit but was thrown out by Williams. Paul O'Neill then lined a single in front of Gonzalez. After Bernie Williams struck out, Kim seemed to be out of trouble with Tino Martinez coming to the plate. However, Martinez drove the first pitch he saw from Kim into the right-center field bleachers, tying the score at 3-3. The Yankees were not done, as Jorge Posada walked and David Justice moved him into scoring position with a single. Kim struck Spencer out to end the threat.\n\nWhen the scoreboard clock in Yankee Stadium passed midnight, World Series play in November began, with the message on the scoreboard \"Welcome to November Baseball\".\n\nMariano Rivera took the hill for the Yankees in the tenth and retired the Diamondbacks in order.\nKim went out for a third inning of work and retired Scott Brosius and Alfonso Soriano, but Jeter hit an opposite field walk-off home run on a 3–2 pitch count from Kim. This walk-off home run gave the Yankees a 4–3 victory and tied the Series at two games apiece, making Jeter the first player to hit a November home run and earning him the tongue-in-cheek nickname of \"Mr. November\". \n\nGame 5 saw the Yankees return to Mike Mussina for the start while the Diamondbacks sent Miguel Batista, who had not pitched in twelve days, to the mound. Batista pitched a strong scoreless innings, striking out six. Mussina bounced back from his poor Game 1 start, recording ten strikeouts, but allowed solo home runs to Steve Finley and Rod Barajas in the fifth.\n\nWith the Diamondbacks leading 2–0 in the ninth, Byung-Hyun Kim was called upon for the save despite having thrown three innings the night before. Jorge Posada doubled to open the inning, but Kim got Shane Spencer to ground out and then struck out Chuck Knoblauch. Unfortunately, as had happened the previous night, Kim could not hold the lead as Scott Brosius hit a 1–0 pitch over the left field wall, the second straight game tying home run in the bottom of the ninth for the Yankees. Kim was pulled from the game in favor of Mike Morgan who recorded the final out.\n\nMorgan retired the Yankees in order in the tenth and eleventh innings, while the Diamondbacks got to Mariano Rivera in the eleventh. Danny Bautista and Erubiel Durazo opened the inning with hits and Matt Williams advanced them into scoring position with a sacrifice bunt. Rivera then intentionally walked Steve Finley to load the bases, then got Reggie Sanders to line out and Mark Grace grounded out to end the inning.\n\nArizona went to Albie Lopez in the twelfth, and in his first at bat he gave up a single to Knoblauch. Brosius moved him over with a bunt, and then Alfonso Soriano ended the game with an RBI single to give the Yankees a 3-2 victory and a 3-2 lead in the series.\n\nWith Arizona in a must-win situation, Johnson pitched seven innings and struck out seven, giving up just two runs. The Diamondbacks struck first when Tony Womack hit a leadoff double off of Andy Pettitte and scored on Danny Bautista's single in the first. Next inning, Womack's bases-loaded single scored two and Bautista's single scored another. The Yankees loaded the bases in the third on a single and two walks, but Johnson struck out Jorge Posada to end the inning. The Diamondbacks broke the game open in the bottom half. Pettitte allowed a leadoff walk to Greg Colbrunn and subsequent double to Matt Williams before being relieved by Jay Witasick, who allowed four straight singles to Reggie Sanders, Jay Bell, Damian Miller, and Johnson that scored three runs. After Womack struck out, Bautista's single scored two more runs and Luis Gonzalez's double scored another, with Bautista being thrown out at home. Colbrunn's single and Williams's double scored a run each before Sanders struck out to end the inning. In the fourth, Bell reached first on a strike-three wild pitch before scoring on Miller's double. Johnson struck out before Womack singled to knock Witasick out of the game. With Randy Choate pitching, Yankees second baseman Alfonso Soriano's error on Bautista's ground ball allowed Miller to score and put runners on first and second before Gonzalez's single scored the Diamondbacks' final run. Choate and Mike Stanton kept them scoreless for the rest of the game. Pettitte was charged with six runs in two innings while Witasick was charged with nine runs in 1 1/3 innings. The Yankees scored their only runs in the sixth on back-to-back one-out singles by Shane Spencer and Luis Sojo with runners on second and third. The Diamondbacks hit six doubles and Danny Bautista batted 3-for-4 with five RBIs. The team set a World Series record with 22 hits and defeated the New York Yankees in its most lopsided postseason loss in 293 postseason games. The 15–2 win evened the series at three games apiece and set up a Game 7 for the ages between Roger Clemens and Curt Schilling, again pitching on three days' rest.\n\nIt was a matchup of two twenty-game winners in the Series finale that would crown a new champion. Roger Clemens at 39 years old became the oldest Game 7 starter ever. Curt Schilling had already started two games of the Series and pitched his 300th inning of the season on just three days' rest. The two aces matched each other inning by inning and after seven full innings, the game was tied at 1–1. The Diamondbacks scored first in the sixth inning with a Steve Finley single and a Danny Bautista double (Bautista would be called out at third base). The Yankees responded with an RBI single from Tino Martinez, which drove in Derek Jeter. Brenly stayed with Schilling into the eighth, and the move backfired as Alfonso Soriano hit a solo home run on an 0–2 pitch. After Schilling struck out Scott Brosius, he gave up a single to David Justice, and he left the game trailing 2–1. Brenly brought in Miguel Batista to get out Derek Jeter and then in an unconventional move, brought in the previous night's starter Randy Johnson, who had thrown 104 pitches, in relief to keep it a one-run game. It proved to be a smart move, as Johnson retired pinch hitter Chuck Knoblauch (who batted for the left handed Paul O'Neill) on a fly out to Danny Bautista in right field, then Johnson returned to the mound for the top of the ninth where he got Bernie Williams to fly out to Steve Finley in center field, Tino Martinez to ground out to Tony Womack at shortstop and he then struck out catcher Jorge Posada to send the game to the bottom of the ninth inning.\n\nWith the Yankees ahead 2–1 in the bottom of the eighth, manager Joe Torre turned the game over to his ace closer Mariano Rivera for a two-inning save. Rivera struck out the side in the eighth, including Arizona's Luis Gonzalez, Matt Williams, and Danny Bautista, which lowered his ERA in the postseason to a major league-best of 0.70. Although he was effective in the eighth, this game would end in the third ninth-inning comeback of the Series.\n\nMark Grace led off the inning with a single to center on a 1–0 pitch. Rivera's errant throw to second base on a bunt attempt by Damian Miller on an 0–1 pitch put runners on first and second. Derek Jeter tried to reach for the ball, but got tangled in the legs of pinch-runner David Dellucci, who was sliding in an attempt to break up the double play. Rivera appeared to regain control when he fielded Jay Bell's bunt and threw out Dellucci at third base, but third baseman Scott Brosius decided to hold onto the baseball instead of throwing to first to complete the double play. Midre Cummings was sent in to pinch-run for Damian Miller. With Cummings at second and Bell at first, the next batter, Tony Womack, hit a double down the right-field line on a 2–2 pitch that tied the game and earned Rivera a blown save. Bell advanced to third and the Yankees pulled the infield and outfield in as the potential winning run stood at third with fewer than two outs. After Rivera hit Craig Counsell with an 0–1 pitch, the bases were loaded. On an 0–1 pitch, Luis Gonzalez lofted a soft single over the drawn-in Derek Jeter that barely reached the outfield grass, plating Jay Bell with the winning run. This ended New York's bid for a fourth consecutive title and brought Arizona its first championship within its fourth year of existence, making the Diamondbacks the fastest expansion team to win a World Series, as well as the first major professional sports championship for the state of Arizona.\n\nIn 2009, Game 7 was chosen by \"Sports Illustrated\" as the Best Postseason Game of the Decade (2000–2009).\n\n2001 World Series (4–3): Arizona Diamondbacks (N.L.) over New York Yankees (A.L.)\nFor the second consecutive year, Fox carried the World Series over its network with its top broadcast team, Joe Buck and Tim McCarver (himself a Yankees broadcaster). This was the first year of Fox's exclusive rights to the World Series (in the previous contract, Fox only broadcast the World Series in even numbered years while NBC broadcast it in odd numbered years), which it has held ever since (this particular contract also had given Fox exclusive rights to the entire baseball postseason, which aired over its family of networks; the contract was modified following Disney's purchase of Fox Family Channel shortly after the World Series ended, as ESPN regained their postseason rights following a year of postseason games on ABC Family, Fox Family's successor). ESPN Radio provided national radio coverage for the fourth consecutive year, with Jon Miller and Joe Morgan calling the action.\n\nLocally, the Series was broadcast by KTAR-AM in Phoenix with Thom Brennaman, Greg Schulte, Rod Allen and Jim Traber, and by WABC-AM in New York City with John Sterling and Michael Kay. This would be Sterling and Kay's last World Series working together, and Game 7 would be the last Yankee broadcast on WABC. Kay moved to television and the new YES Network the following season and WCBS picked up radio rights to the Yankees. It was Kay who announced Derek Jeter's game-winning home run in Game 4 of the series and subsequently anointed him as \"Mr. November\".\n\nAfter the Yankees lost the World Series, several players moved onto other teams or retired, the most notable changes being the signing of Jason Giambi to replace Martinez, and the retirements of Brosius and O'Neill. Martinez would later finish his career with the Yankees in 2005 after spending the previous three years in St. Louis and Tampa Bay. The Yankees would lose the 2003 World Series to the Florida Marlins and wouldn't win another World Series until 2009, when they defeated the defending champions, the Philadelphia Phillies, in six games.\n\nAfter winning the NL West again in 2002 the Diamondbacks were swept 3–0 by St. Louis in the NLDS. From here they declined, losing 111 games in 2004 as Bob Brenly was fired during that season. Arizona would not win another NL West title until 2007. Schilling was traded to the Boston Red Sox after the 2003 season and in 2004 helped lead them to their first world championship since 1918. He helped them win another championship in 2007 and retired after four years with Boston, missing the entire 2008 season with a shoulder injury. Johnson was traded to the Yankees after the 2004 season, a season that saw him throw a perfect game against the Atlanta Braves, though he would be traded back to the Diamondbacks two years later and finish his career with the San Francisco Giants in 2009. The last player from the 2001 Diamondbacks roster, Lyle Overbay, retired following the 2014 season with the Milwaukee Brewers while the last player from the 2001 Yankees, Randy Choate, announced his retirement on February 16, 2017. \n\nFrom 2002 through 2007, the Yankees' misfortune in the postseason continued, with the team losing the ALDS to the Anaheim Angels in 2002, the World Series to the Florida Marlins in 2003, the ALCS to the Boston Red Sox (in the process becoming the first ever team in postseason history to blow a 3-0 series lead) in 2004, the ALDS again to the Angels in 2005, the ALDS to Detroit in 2006, and the ALDS to Cleveland in 2007. Joe Torre's contract was allowed to expire and he was replaced by Joe Girardi in 2008, a season in which the Yankees would miss the playoffs for the first time since 1993.\n\nBuster Olney, who covered the Yankees for the \"New York Times\" before joining ESPN, would write a book titled \"The Last Night of the Yankee Dynasty\". The book is a play by play account of Game 7 in addition to stories about key players, executives, and moments from the 1996–2001 dynasty. In a 2005 reprinting, Olney included a new epilogue covering the aftermath of the 2001 World Series up to the Boston Red Sox epic comeback from down 3–0 in the 2004 ALCS.\n\n, this is the state of Arizona's only world championship among the four major professional sports. The NFL's Arizona Cardinals came close, but lost to the Pittsburgh Steelers in Super Bowl XLIII. The Phoenix Suns have been to two NBA Finals, and lost both times, to the Boston Celtics in 1976, and again to the Chicago Bulls in 1993. The Arizona Coyotes have never advanced to the Stanley Cup Final. Their closest call came in 2012, when they were defeated in five games by the Los Angeles Kings in the Western Conference Finals.\n\nOn October 11, 2005, A&E Home Video released the \"New York Yankees Fall Classic Collectors Edition (1996–2001)\" DVD set. Game 4 of the 2001 World Series is included in the set. On April 29, 2008, \"The Arizona Diamondbacks 2001 World Series\" DVD set was released. All seven games are included on this set.\n\n", "id": "3864", "title": "2001 World Series"}
{"url": "https://en.wikipedia.org/wiki?curid=3865", "text": "1903 World Series\n\nThe 1903 World Series was the first modern World Series to be played in Major League Baseball. It matched the Boston Americans of the American League against the Pittsburgh Pirates of the National League in a best-of-nine series, with Boston prevailing five games to three, winning the last four.\n\nPittsburg pitcher Sam Leever injured his shoulder while trap-shooting, so his teammate Deacon Phillippe pitched five complete games. Phillippe won three of his games, but it was not enough to overcome the club from the new American League. Boston pitchers Bill Dinneen and Cy Young led Boston to victory. In Game 1, Phillippe struck out ten Boston batters. The next day, Dinneen bettered that mark, striking out eleven Pittsburg batters in Game 2.\n\nHonus Wagner, bothered by injuries, batted only 6 for 27 (.222) in the Series and committed six errors. The shortstop was deeply distraught by his performance. The following spring, Wagner (who in 1903 led the league in batting average) refused to send his portrait to a \"Hall of Fame\" for batting champions. \"I was too bum last year\", he wrote. \"I was a joke in that Boston-Pittsburg Series. What does it profit a man to hammer along and make a few hits when they are not needed only to fall down when it comes to a pinch? I would be ashamed to have my picture up now.\"\n\nDue to overflow crowds at the Exposition Park games in Allegheny City, if a batted ball rolled under a rope in the outfield that held spectators back, a \"ground-rule triple\" would be scored. Seventeen ground-rule triples were hit in the four games played at the stadium.\n\nIn the series, Boston came back from a three games to one deficit, winning the final four games to capture the title. Such a large comeback would not happen again until the Pirates came back to defeat the Washington Senators in the 1925 World Series, and has happened only eleven times in baseball history. (The Pirates themselves repeated this feat in against the Baltimore Orioles.) Much was made of the influence of Boston's \"Royal Rooters\", who traveled to Exposition Park and sang their theme song \"Tessie\" to distract the opposing players (especially Wagner). Boston wound up winning three out of four games in Allegheny City.\n\nPirates owner Barney Dreyfuss added his share of the gate receipts to the players' share, so the losing team's players actually finished with a larger individual share than the winning team's.\n\nThe Series brought the new American League prestige and proved its best could beat the best of the National League, thus strengthening the demand for future World Series competitions.\n\nIn 1901, Ban Johnson, president of the Western League, a minor league organization, formed the American League to take advantage of the National League's 1900 contraction from twelve teams to eight. Johnson and fellow owners raided the National League and signed away many star players, including Cy Young and Jimmy Collins. Johnson had a list of 46 National Leaguers he targeted for the American League; by 1902, all but one had made the jump. The constant raiding, however, scotched the idea of a championship between the two leagues. Pirates owner Barney Dreyfuss, whose team ran away with the 1902 National League pennant, was open to a post-season contest and even said he would allow the American League champion to stock its roster with all-stars. However, Johnson had spoken of putting a team in Pittsburg and even attempted to raid the Pirates' roster in August 1902, which soured Dreyfuss. At the end of the season, however, the Pirates played a group of American League All-Stars in a four-game exhibition series, winning two games to one, with one tie.\n\nThe leagues finally called a truce in the winter of 1902–03 and formed the National Commission to preside over organized baseball. The following season, the Boston Americans and Pittsburg Pirates had secured their respective championship pennants by September. That August, Dreyfuss challenged the American League to an eleven-game championship series. Encouraged by Johnson and National League President Harry Pulliam, Americans owner Henry J. Killilea met with Dreyfuss in Pittsburg in September and instead agreed to a best-of-nine championship, with the first three games played in Boston, the next four in Allegheny City, and the remaining two (if necessary) in Boston.\n\nOne significant point about this agreement was that it was an arrangement primarily between the two clubs rather than a formal arrangement between the leagues. In short, it was a voluntary event, a fact which would result in no Series at all for . The formal establishment of the Series as a compulsory event started in .\n\nThe Pirates won their third straight pennant in 1903 thanks to a powerful line-up that included legendary shortstop Honus Wagner, who hit .355 and drove in 101 runs, player-manager Fred Clarke, who hit .351, and Ginger Beaumont, who hit .341 and led the league in hits and runs. The Pirates' pitching was weaker than it had been in previous years but boasted 24-game winner Deacon Phillippe and 25-game winner Sam Leever.\n\nThe Americans had a strong pitching staff, led by Cy Young, who went 28–9 in 1903 and became the all-time wins leader that year. Bill Dinneen and Long Tom Hughes, right-handers like Young, had won 21 games and 20 games each. The Boston outfield, featuring Chick Stahl (.274), Buck Freeman (.287, 104 RBIs) and Patsy Dougherty (.331, 101 runs scored) was considered excellent.\nAlthough the Pirates had dominated their league for the previous three years, they went into the series riddled with injuries and plagued by bizarre misfortunes. Otto Krueger, the team's only utility player, was beaned on September 19 and never fully played in the series. 16-game winner Ed Doheny left the team three days later, exhibiting signs of paranoia; he was committed to an insane asylum the following month. Leever had been battling an injury to his pitching arm (which he made worse by entering a trapshooting competition). Worst of all, Wagner, who had a sore thumb throughout the season, injured his right leg in September and was never 100 percent for the post-season.\n\nSome sources say Boston were heavy underdogs. Boston bookies actually gave even odds to the teams (and only because Dreyfuss and other \"sports\" were alleged to have bet on Pittsburg to bring down the odds). The teams were generally thought to be evenly matched, with the Americans credited with stronger pitching and the Pirates with superior offense and fielding. The outcome, many believed, hinged on Wagner's health. \"If Wagner does not play, bet your money at two to one on Boston\", said the \"Sporting News\", \"but if he does play, place your money at two to one on Pittsburg.\"\n\nThursday, October 1, 1903, at Huntington Avenue Baseball Grounds in Boston, Massachusetts\n\nThe Pirates started Game 1 strong, scoring six runs in the first four innings. They extended their lead to 7–0 on a solo home run by Jimmy Sebring in the seventh, the first home run in World Series history. Boston tried to mount a comeback in the last three innings, but it was too little too late, and they ended up losing by a score of 7–3 in the first ever World Series game. Both Phillippe and Young threw complete games, with Phillippe striking out ten and Young fanning five, but Young also gave up twice as many hits and allowed three earned runs to Phillippe's two.\n\nFriday, October 2, 1903, at Huntington Avenue Baseball Grounds in Boston, Massachusetts\n\nAfter starting out strong in Game 1, the Pirates simply shut down offensively, eking out a mere three hits, all singles. Pittsburg starter Sam Leever went 1 inning and gave up three hits and two runs, before his ailing arm forced him to leave in favor of Bucky Veil, who finished the game. Bill Dinneen struck out eleven and pitched a complete game for the Americans, while Patsy Dougherty hit home runs in the first and sixth innings for two of the Boston's three runs. The Americans' Patsy Dougherty led off the Boston scoring with an inside-the-park home run, the first time a leadoff batter did just that until Alcides Escobar of the Kansas City Royals duplicated the feat in the 2015 World Series, 112 years later. Dougherty's second home run was the first in World Series history to actually sail over the fence, an incredibly rare feat at the time. \n\nSaturday, October 3, 1903, at Huntington Avenue Baseball Grounds in Boston, Massachusetts\n\nPhillippe, pitching after only a single day of rest, started Game 3 for the Pirates and didn't let them down, hurling his second complete game victory of the Series to put Pittsburg up two games to one.\n\nTuesday, October 6, 1903, at Exposition Park (III) in Allegheny, Pennsylvania\n\nAfter two days of rest, Phillippe was ready to pitch a second straight game. He threw his third complete game victory of the series against Bill Dinneen, who was making his second start of the series. But Phillippe's second straight win was almost not to be, as the Americans, down 5–1 in the top of the ninth, rallied to narrow the deficit to one run. The comeback attempt failed, as Phillippe managed to put an end to it and give the Pirates a commanding 3–1 Series lead.\n\nWednesday, October 7, 1903, at Exposition Park (III) in Allegheny, Pennsylvania\n\nGame 5 was a pitcher's duel for the first five innings, with Boston's Cy Young and Pittsburg's Brickyard Kennedy giving up no runs. That changed in the top of the sixth, however, when the Americans scored a then-record six runs before being retired. Young, on the other hand, managed to keep his shutout intact before finally giving up a pair of runs in the bottom of the eighth. He went the distance and struck out four for his first World Series win.\n\nThursday, October 8, 1903, at Exposition Park (III) in Allegheny, Pennsylvania\n\nGame 6 was a rematch between the starters of Game 2, Boston's Dinneen and Pittsburg's Leever. Leever pitched a complete game this time but so did Dinneen, who outmatched him to earn his second complete game victory of the series. After losing three of the first four games of the World Series, the underdog Americans had tied the series at three games apiece.\n\nSaturday, October 10, 1903, at Exposition Park (III) in Allegheny, Pennsylvania\n\nThe fourth and final game in Allegheny saw Phillippe start his fourth game of the Series for the Pirates. This time, however, he wouldn't fare as well as he did in his first three starts. Cy Young, in his third start of the Series, held the Pirates to three runs and put the Americans ahead for the first time as the Series moved back to Boston.\n\nTuesday, October 13, 1903, at Huntington Avenue Baseball Grounds in Boston, Massachusetts\n\nThe final game of this inaugural World Series started out as an intense pitcher's duel, scoreless until the bottom of the fourth when Hobe Ferris hit a two-run single. Phillippe started his fifth and final game of the series and Dinneen his fourth. As he did in Game 2, Dinneen threw a complete game shutout, striking out seven and leading his Americans to victory, while Phillippe pitched respectably but just couldn't match Dinneen because his arm had been worn out with five starts in the eight games, giving up three runs to give the first 20th-century World Championship to the Boston Americans, Honus Wagner striking out to end the Series.\n\n1903 World Series (5–3): Boston Americans (A.L.) over Pittsburg Pirates (N.L.)\n\n\"Note: GP=Games played; AB=At Bats; H=Hits; Avg.=Batting Average; HR=Home Runs; RBI=Runs Batted In\"\n", "id": "3865", "title": "1903 World Series"}
{"url": "https://en.wikipedia.org/wiki?curid=3866", "text": "Bluetongue disease\n\nBluetongue disease is a non-contagious, insect-borne, viral disease of ruminants, mainly sheep and less frequently cattle, goats, buffalo, deer, dromedaries, and antelope. It is caused by the Bluetongue virus (BTV). The virus is transmitted by the midge \"Culicoides imicola\", \"Culicoides variipennis\", and other culicoids.\n\nIn sheep, BTV causes an acute disease with high morbidity and mortality. BTV also infects goats, cattle and other domestic animals as well as wild ruminants (for example, blesbuck, white-tailed deer, elk, and pronghorn antelope).\n\nMajor signs are high fever, excessive salivation, swelling of the face and tongue and cyanosis of the tongue. Swelling of the lips and tongue gives the tongue its typical blue appearance, though this sign is confined to a minority of the animals. Nasal signs may be prominent, with nasal discharge and stertorous respiration.\n\nSome animals also develop foot lesions, beginning with coronitis, with consequent lameness. In sheep, this can lead to knee-walking. In cattle, constant changing of position of the feet gives bluetongue the nickname The Dancing Disease. Torsion of the neck (opisthotonos or torticollis) is observed in severely affected animals.\n\nNot all animals develop signs, but all those that do lose condition rapidly, and the sickest die within a week. For affected animals which do not die, recovery is very slow, lasting several months.\n\nThe incubation period is 5–20 days, and all signs usually develop within a month. The mortality rate is normally low, but it is high in susceptible breeds of sheep. In Africa, local breeds of sheep may have no mortality, but in imported breeds it may be up to 90 percent.\n\nIn cattle, goats and wild ruminants infection is usually asymptomatic despite high virus levels in blood. Red deer are an exception, and in them the disease may be as acute as in sheep.\n\nBluetongue is caused by the pathogenic virus, Bluetongue virus (BTV), of the genus \"Orbivirus\", of the Reoviridae family. Twenty-six serotypes are now recognised for this virus.\n\nThe virus particle consists of ten strands of double-stranded RNA surrounded by two protein shells. Unlike other arboviruses, BTV lacks a lipid envelope. The particle has a diameter of 86 nm. The structure of the 70 nm core was determined in 1998 and was at the time the largest atomic structure to be solved.\n\nThe two outer capsid proteins, VP2 and VP5, mediate attachment and penetration of BTV into the target cell. The virus makes initial contact with the cell with VP2, triggering receptor-mediated endocytosis of the virus. The low pH within the endosome then triggers BTV's membrane penetration protein VP5 to undergo a conformational change that disrupts the endosomal membrane. Uncoating yields a transcriptionally active 470S core particle which is composed of two major proteins VP7 and VP3, and the three minor proteins VP1, VP4 and VP6 in addition to the dsRNA genome. There is no evidence that any trace of the outer capsid remains associated with these cores, as has been described for reovirus. The cores may be further uncoated to form 390S subcore particles that lack VP7, also in contrast to reovirus. Subviral particles are probably akin to cores derived \"in vitro\" from virions by physical or proteolytic treatments that remove the outer capsid and causes activation of the BTV transcriptase. In addition to the seven structural proteins, three non-structural (NS) proteins, NS1, NS2, NS3 (and a related NS3A) are synthesised in BTV-infected cells. Of these, NS3/NS3A is involved in the egress of the progeny virus. The two remaining non-structural proteins, NS1 and NS2, are produced at high levels in the cytoplasm and are believed to be involved in virus replication, assembly and morphogenesis.\n\nBluetongue has been observed in Australia, the USA, Africa, the Middle East, Asia and Europe.\n\nIts occurrence is seasonal in the affected Mediterranean countries, subsiding when temperatures drop and hard frosts kill the adult midge vectors. \nViral survival and vector longevity is seen during milder winters. \nA significant contribution to the northward spread of Bluetongue disease has been the ability of \"Culicoides obsoletus\" and \"C.pulicaris\" to acquire and transmit the pathogen, both of which are spread widely throughout Europe. This is in contrast to the original \"C.imicola\" vector which is limited to North Africa and the Mediterranean. The relatively recent novel vector has facilitated a far more rapid spread than the simple expansion of habitats North through global warming.\n\nIn August 2006, cases of bluetongue were found in the Netherlands, then Belgium, Germany, and Luxembourg. \nIn 2007, the first case of bluetongue in the Czech Republic was detected in one bull near Cheb at the Czech-German border. \nIn September 2007, the UK reported its first ever suspected case of the disease, in a Highland cow on a rare breeds farm near Ipswich, Suffolk. \nSince then the virus has spread from cattle to sheep in Britain. \nBy October 2007 bluetongue had become a serious threat in Scandinavia and Switzerland \nand the first outbreak in Denmark was reported. In autumn 2008, several cases were reported in the southern Swedish provinces of Småland, Halland, and Skåne,\nas well as in areas of the Netherlands bordering Germany, prompting veterinary authorities in Germany to intensify controls.\nNorway saw its first finding in February 2009, when cows at two farms in Vest-Agder in the south of Norway showed an immune response to bluetongue. Norway have since been declared free of the disease in 2011.\n\nAlthough the disease is not a threat to humans the most vulnerable common domestic ruminants in the UK are cattle, goats and, especially, sheep.\n\nA puzzling aspect of BTV is its survival between midge seasons in temperate regions. Adult \"Culicoides\" are killed by cold winter temperatures, and BTV infections typically do not last for more than 60 days, which is not long enough for BTV to last until the next spring. It is believed that the virus somehow survives in overwintering midges or animals. Multiple mechanisms have been proposed. A few adult \"Culicoides\" midges infected with BTV may survive the mild winters of the temperate zone. Some midges may even move indoors to avoid the cold temperature of the winter. Additionally, BTV could cause a chronic or latent infection in some animals, providing another means for BTV to survive the winter. BTV can also be transmitted from mother to fetus. The outcome is abortion or stillbirth if fetal infection occurs early in gestation and survival if infection occurs late. However infection at an intermediate stage, before the fetal immune system is fully developed, may result in a chronic infection that lingers until the first months after birth of the lamb. Midges will then spread the pathogen from the calves to other animals, starting a new season of infection.\n\nPrevention is effected via quarantine, inoculation with live modified virus vaccine and control of the midge vector, including inspection of aircraft.\n\nHowever, simple husbandry changes and practical midge control measures may help break the livestock infection cycle. Housing livestock during times of maximum midge activity (from dusk to dawn) may lead to significantly reduced biting rates. Similarly, protecting livestock shelters with fine mesh netting or coarser material impregnated with insecticide will reduce contact with the midges. The \"Culicoides\" midges that carry the virus usually breed on animal dung and moist soils, either bare or covered in short grass. Identifying breeding grounds and breaking the breeding cycle will significantly reduce the local midge population. Turning off taps, mending leaks and filling in or draining damp areas will also help dry up breeding sites. Control by trapping midges and removing their breeding grounds may reduce vector numbers. Dung heaps or slurry pits should be covered or removed, and their perimeters (where most larvae are found) regularly scraped.\n\nOutbreaks in southern Europe have been caused by serotypes 2 and 4, and vaccines are available against these serotypes (ATCvet codes: for sheep, for cattle). However, the disease found in northern Europe (including the UK) in 2006 and 2007 has been caused by serotype 8. Vaccine companies Fort Dodge Animal Health (Wyeth), Merial and Intervet were developing vaccines against serotype 8 (Fort Dodge Animal Health has serotype 4 for sheep, serotype 1 for sheep and cattle and serotype 8 for sheep and cattle) and the associated production facilities. A vaccine for this is now available in the UK, produced by Intervet. Fort Dodge Animal Health has their vaccines available for multiple European Countries (vaccination will start in 2008 in Germany, Belgium, Switzerland, Spain and Italy). However, immunization with any of the available vaccines preclude later serological monitoring of affected cattle populations, a problem which could be resolved using next-generation subunit vaccines currently in development.\n\nIn January 2015, Indian researchers launched its vaccine. Named 'Raksha Blu', it will protect the animals against five strains of the ‘bluetongue’ virus prevalent in the country.\n\nAlthough bluetongue disease was already recognized in South Africa in the early 19th century, a comprehensive description of the disease was not published until the first decade of the 20th century. In 1906 Arnold Theiler showed that bluetongue was caused by a filterable agent. He also created the first bluetongue vaccine, which was developed from an attenuated BTV strain. For many decades bluetongue was thought to be confined to Africa. The first confirmed outbreak outside of Africa occurred in Cyprus in 1943.\n\nAfrican horse sickness is related to Bluetongue and is spread by the same midges (\"Culicoides\" species). It can kill the horses it infects and mortality may go as high as 90% of the infected horses during an epidemic.\n\nThe Epizootic Hemorrhagic Disease (EHD) virus is closely related and crossreacts with Bluetongue virus on many blood tests.\n\n", "id": "3866", "title": "Bluetongue disease"}
{"url": "https://en.wikipedia.org/wiki?curid=3869", "text": "Bruce Perens\n\nBruce Perens (born around 1958) is an American computer programmer and advocate in the free software movement. He created The Open Source Definition and published the first formal announcement and manifesto of open source. He co-founded the Open Source Initiative (OSI) with Eric S. Raymond.\n\nIn 2005, Perens represented Open Source at the United Nations World Summit on the Information Society, at the invitation of the United Nations Development Programme. He has appeared before national legislatures and is often quoted in the press, advocating for open source and the reform of national and international technology policy.\n\nPerens is also an amateur radio operator, with call sign K6BP. He promotes open radio communications standards and open source hardware.\n\nPerens is currently CEO of two companies: Algoram is a start-up which is creating a 50-1000 MHz software-defined radio transceiver. Legal Engineering is a legal-technical consultancy which specializes in resolving copyright infringement of Open Source software.\n\nPerens grew up in Long Island, New York. He was born with cerebral palsy, which caused him to have slurred speech as a child, a condition that led to a misdiagnosis of him as developmentally disabled in school and led the school to fail to teach him to read. He developed an interest in technology at an early age: besides his interest in amateur radio, he ran a pirate radio station in the town of Lido Beach and briefly engaged in phone phreaking.\n\nPerens worked for seven years at the New York Institute of Technology Computer Graphics Lab. After that, he worked at Pixar for 12 years, from 1987 to 1999. He is credited as a studio tools engineer on the Pixar films \"A Bug's Life\" (1998) and \"Toy Story 2\" (1999).\n\nPerens founded No-Code International in 1998 with the goal of ending the Morse Code test then required for an amateur radio license. His rationale was that amateur radio should be a tool for young people to learn advanced technology and networking, rather than something that preserved antiquity and required new hams to master outmoded technology before they were allowed on the air.\n\nTo achieve the end of Morse Code requirements, it was necessary for the International Telecommunications Union, a department of the United Nations, to strike section S25.5, requiring Morse Code proficiency of radio amateurs in all nations, from the International Telecommunications Treaty. Only then would it be possible for individual nations to strike all Morse Code requirements from their laws regarding radio amateur licensing.\n\nPerens lobbied intensively on the internet, at amateur radio events in the United States, and during visits to other nations. One of his visits was to Iceland, where he had half of that nation's radio amateurs in the room, and their vote in the International Amateur Radio Union was equivalent to that of the entire United States. The No-Code effort was successful. With the possible exception of Russia, all nations have struck their Morse Code requirements.\n\nIn 1995, Perens created BusyBox, a package of UNIX-style utilities for operating systems including Linux and FreeBSD. He stopped working on it in 1996, after which it was taken over by other developers.\n\nStarting in 2007, several lawsuits were filed for infringement of BusyBox copyright and licensing. These lawsuits were filed by the Software Freedom Law Center (SFLC), and some of the later managing developers of BusyBox.\n\nIn 2009, Bruce Perens released a statement about the lawsuits and those filing them. In it, he claims that he maintains a significant or even majority ownership of the software in the litigation, but was not contacted nor represented by the plaintiffs; and that some of the plaintiffs had themselves modified BusyBox and its distribution package in such a way as to violate applicable licensing terms and copyright owned by Perens and additional BusyBox developers. Perens supports enforcement of the GPL license used on Busybox. Because he was denied participation in the Busybox cases on the side of the prosecution, Perens started a consulting business to assist the defendants in coming into compliance with the GPL and arriving at an amicable settlement with the Software Freedom Law Center.\n\nFrom April 1996 to December 1997, while still working at Pixar, Perens served as Debian Project Leader, the person who coordinates development of the Debian open source operating system. He replaced Ian Murdock, the creator of Debian, who had been the first project leader.\n\nIn 1997, Perens was a co-founder of Software in the Public Interest (SPI), a nonprofit organization intended to serve as an umbrella organization to aid open-source software and hardware projects. It was originally created to allow the Debian Project to accept donations.\n\nIn 1997, Perens was carbon-copied an email conversation between Donnie Barnes of Red Hat and Ean Schuessler, who was then working on Debian. Schuessler bemoaned that Red Hat had never stated its social contract with the Open Source community. Perens took this as inspiration to create a formal social contract for Debian. In a blog posting, Perens claims not to have made use of the Three Freedoms (later the Four Freedoms) published by the Free Software Foundation in composing his document. Perens proposed a draft of the Debian Social Contract to the Debian developers on the debian-private mailing list early in June 1997. Debian developers contributed discussion and changes for the rest of the month while Perens edited, and the completed document was then announced as Debian project policy. Part of the Debian Social Contract was the Debian Free Software Guidelines, a set of 10 guidelines for determining whether a set of software can be described as \"free software\", and thus whether it could be included in Debian.\n\nOn February 3, 1998, a group of people (not including Perens) met at VA Linux Systems to discuss the promotion of Free Software to business in pragmatic terms, rather than the moral terms preferred by Richard Stallman. Christine Petersen of the nanotechnology organization Foresight Institute, who was present because Foresight took an early interest in Free Software, suggested the term \"Open Source\". The next day, Eric S. Raymond recruited Perens to work with him on the formation of Open Source. Perens modified the Debian Free Software Guidelines into the Open Source Definition by removing Debian references and replacing them with \"Open Source\".\n\nThe original announcement of The Open Source Definition was made on February 9, 1998 on Slashdot and elsewhere; the definition was given in Linux Gazette on February 10, 1998.\n\nConcurrently, Perens and Raymond established the Open Source Initiative, an organization intended to promote open source software.\n\nPerens left OSI in 1999, a year after co-founding it. In February 1999 in an email to the Debian developers mailing list he explained his decision and stated that, though \"most hackers know that Free Software and Open Source are just two words for the same thing\", the success of \"open source\" as a marketing term had \"de-emphasized the importance of the freedoms involved in Free Software\"; he added, \"It's time for us to fix that.\" He stated his regret that OSI co-founder Eric Raymond \"seems to be losing his free software focus.\" But in the following 2000s he spoke about Open source again.\n\nIn 1999, Perens left Pixar and became the president of Linux Capital Group, a business incubator and venture capital firm focusing on Linux-based businesses. Their major investment was in Progeny Linux Systems, a company headed by Debian founder Ian Murdock. In 2000, as a result of the economic downturn, Perens shut down Linux Capital Group. (Progeny Linux Systems would end operations in 2007.)\n\nFrom December 2000 to September 2002, Perens served as \"Senior Global Strategist for Linux and Open Source\" at Hewlett-Packard, internally evangelizing for the use of Linux and other open-source software. He was fired as a result of his anti-Microsoft statements, which especially became an issue after HP acquired Compaq, a major manufacturer of Microsoft Windows-based PCs, in 2002.\n\nIn 2001, Perens founded, and became the first project leader, of the Linux Standard Base project, a joint project by several Linux distributions under the organizational structure of the Linux Foundation to standardize the Linux software system structure.\n\nIn 2003 Perens created UserLinux, a Debian-based distribution whose stated goal was, \"Provide businesses with freely available, high quality Linux operating systems accompanied by certifications, service, and support options designed to encourage productivity and security while reducing overall costs.\" UserLinux was eventually overtaken in popularity by Ubuntu, another Debian-based distribution, which was started in 2004, and UserLinux became unmaintained in 2006.\n\nPerens was an employee of SourceLabs, a Seattle-based open source software and services company, from June 2005 until December 2007. He produced a video commercial, \"Impending Security Breach\", for SourceLabs in 2007. (SourceLabs went out of business in 2009.)\n\nBetween 1981 and 1986, Perens was on the staff of the New York Institute of Technology Computer Graphics Lab as a Unix kernel programmer.\n\nIn 2002, Perens was a remote Senior Scientist for Open Source with the Cyber Security Policy Laboratory of George Washington University under the direction of Tony Stanco. Stanco was director of the laboratory for a year, while its regular director was on sabbatical.\n\nBetween 2006 and 2007, Perens was a visiting lecturer and researcher for the University of Agder under a three-year grant from the Competence Fund of Southern Norway. During this time he consulted the Norwegian Government and other entities on government policy issues related to computers and software. After this time Perens worked remotely on Agder programs, mainly concerning the European Internet Accessibility Observatory.\n\nIn 2007, some of Perens government advisory roles included a meeting with the President of the Chamber of Deputies (the lower house of parliament) in Italy and testimony to the Culture Committee of the Chamber of Deputies; a keynote speech at the foundation of Norway's Open Source Center, following Norway's Minister of Governmental Reform (Perens is on the advisory board of the center); he provided input on the revision of the European Interoperability Framework; and he was keynote speaker at a European Commission conference on \"Digital Business Ecosystems at the Centre Borschette, Brussels, on November 7\".\n\nIn 2009, Perens acted as an expert witness on open source in the Jacobsen v. Katzer U.S. Federal lawsuit. His report, which was made publicly available by Jacobsen, presented the culture and impact of open-source software development to the federal courts.\n\nPerens delivered one of the keynote addresses at the 2012 linux.conf.au conference in Ballarat, Australia. He discussed the need for open source software to market itself better to non-technical users. He also discussed some of the latest developments in open source hardware, such as Papilio and Bus Pirate.\n\nIn 2013, Perens spoke in South America, as the closing keynote at Latinoware 2013. He was the keynote of CISL - Conferencia Internacional de Software Libre, in Buenos Aires, Argentina, and keynoted a special event along with the Minister of software and innovation of Chubut Province, in Puerto Madrin, Patagonia, Argentina. He keynoted the Festival de Software Libre 2013, in Puerto Vallarta, Mexico.\n\nIn 2014-2015, Perens took a break from Open Source conferences, having spoken at them often since 1996. In 2016, he returned to the conference circuit, keynoting the Open Source Insight conference in Seoul, sponsored by the Copyright Commission of South Korea. Perens web site presently advertises his availability to keynote conferences as long as travel and lodging expenses are compensated.\n\nPerens poses \"Open Source\" as a means of marketing the free and open-source software idea to business people and mainstream who might be more interested in the practical benefits of an open source development model and ecosystem than abstract ethics. He states that open source and free software are only two ways of talking about the same phenomenon, a point of view not shared by Stallman and his Free software movement. Perens postulated in 2004 an economic theory for business use of Open Source in his paper \"The Emerging Economic Paradigm of Open Source\" and his speech \"Innovation Goes Public\". This differs from Raymond's theory in \"The Cathedral and the Bazaar\", which having been written before there was much business involvement in open source, explains open source as a consequence of programmer motivation and leisure.\n\nIn February 2008, for the 10th anniversary of the phrase \"open source\", Perens published a message to the community called \"State of Open Source Message: A New Decade For Open Source\". Around the same time the ezine RegDeveloper published an interview with Perens where he spoke of the successes of open source, but also warned of dangers, including a proliferation of OSI-approved licenses which had not undergone legal scrutiny. He advocated the use of the GPLv3 license, especially noting Linus Torvalds' refusal to switch away from GPLv2 for the Linux kernel.\n\nBruce Perens supported Bernie Sanders for President and he claims that his experience with the open source movement influenced that decision. On July 13, 2016, following Sanders endorsement of Clinton for president, Perens endorsed Clinton.\n\nPerens is an avid amateur radio enthusiast (call sign K6BP) and maintained technocrat.net, which he closed in late 2008, because its revenues did not cover its costs.\n\nPerens is featured in the 2001 documentary film \"Revolution OS\" and the 2006 BBC television documentary \"The Code-Breakers\".\n\nFrom 2002 to 2006, Prentice Hall PTR published the Bruce Perens' Open Source Series, a set of 24 books covering various open source software tools, for which Perens served as the series editor. It was the first book series to be published under an open license.\n\nPerens lives in Berkeley, California with his wife, Valerie, and son, Stanley, born in 2000.\n\n", "id": "3869", "title": "Bruce Perens"}
{"url": "https://en.wikipedia.org/wiki?curid=3870", "text": "Bundle theory\n\nBundle theory, originated by the 18th century Scottish philosopher David Hume, is the ontological theory about objecthood in which an object consists only of a collection (\"bundle\") of properties, relations or tropes.\n\nAccording to bundle theory, an object consists of its properties and nothing more: thus neither can there be an object without properties nor can one even \"conceive\" of such an object; for example, bundle theory claims that thinking of an apple compels one also to think of its color, its shape, the fact that it is a kind of fruit, its cells, its taste, or at least one other of its properties. Thus, the theory asserts that the apple is no more than the collection of its properties. In particular, there is no \"substance\" in which the properties are \"inherent\".\n\nThe difficulty in conceiving of or describing an object without also conceiving of or describing its properties is a common justification for bundle theory, especially among current philosophers in the Anglo-American tradition.\n\nThe inability to comprehend any aspect of the thing other than its properties implies, this argument maintains, that one cannot conceive of a \"bare particular\" (a \"substance\" without properties), an implication that directly opposes substance theory. The conceptual difficulty of \"bare particulars\" was illustrated by John Locke when he described a \"substance\" by itself, apart from its properties, as \"\"something, I know not what. [...] The idea then we have, to which we give the general name substance, being nothing but the supposed, but unknown, support of those qualities we find existing, which we imagine cannot subsist sine re substante, without something to support them, we call that support substantia; which, according to the true import of the word, is, in plain English, standing under or upholding.\"\"\n\nWhether a \"relation\" of an object is one of its properties may complicate such an argument. However, the argument concludes that the conceptual challenge of \"bare particulars\" leaves a bundle of properties and nothing more as the only possible conception of an object, thus justifying bundle theory.\n\nObjections to bundle theory concern the nature of the \"bundle of properties\", the properties' \"compresence\" relation (the \"togetherness\" relation between those constituent properties), and the impact of language on understanding reality.\n\nBundle theory maintains that properties are \"bundled\" together in a collection without describing how they are tied together. For example, bundle theory regards an apple as red, four inches (100 mm) wide, and juicy but lacking an underlying \"substance\". The apple is said to be a \"bundle of properties\" including redness, being four inches (100 mm) wide, and juiciness.\n\nCritics question how bundle theory accounts for the properties' \"compresence\" (the \"togetherness\" relation between those properties) without an underlying \"substance\". Critics also question how any two given properties are determined to be properties of the same object if there is no \"substance\" in which they both \"inhere\".\n\nTraditional bundle theory explains the \"compresence\" of properties by defining an object as a collection of properties \"bound\" together. Thus, different combinations of properties and relations produce different objects. Redness and juiciness, for example, may be found together on top of the table because they are part of a bundle of properties located on the table, one of which is the \"looks like an apple\" property.\n\nBy contrast, substance theory explains the \"compresence\" of properties by asserting that the properties are found together because it is the \"substance\" that has those properties. In substance theory, a \"substance\" is the thing in which properties \"inhere\". For example, redness and juiciness are found on top of the table because redness and juiciness \"inhere\" in an apple, making the apple red and juicy.\n\nThe \"bundle theory of substance\" explains \"compresence\". Specifically, it maintains that properties' compresence itself engenders a \"substance\". Thus, it determines \"substancehood\" empirically by the \"togetherness\" of properties rather than by a \"bare particular\" or by any other non-empirical underlying strata. The \"bundle theory of substance\" thus rejects the substance theories of Aristotle, Descartes, and more recently, J.P. Moreland, Jia Hou, Joseph Bridgman, Quentin Smith, and others.\n\nThe \"language-reality\" objection to bundle theory relates to the impact language has on understanding reality. The objection maintains that language causes confusion that supports bundle theory.\n\nPer the objection, properties are synthetic constructions of language and thinking alone provides reality to the properties of any object. An apple, it claims, does not have the properties \"red\" or \"juicy\", but rather observers who already believe in a concept called \"Red\" use that concept to experience an apple as red. Further, the objection maintains that \"Red\" cannot be distilled from an apple because \"Red\" is an abstraction from other experiences and not an innate property an apple might contain. Per the objection, expressions such as, \"An apple is red and juicy,\" includes at least six concepts and would best be left as dead-end logical propositions. Since the objection regards the words \"Red\" and \"Juicy\" as simply abstractions of previous experiences, it contends that they contain only a personal summary concept of one individual. Thus, the experience of an apple is as close to the \"Apple\" concept that one can get. The objection regards any additional analytic work of the mind as a synthesis of other experiences that is incapable of logically revealing any true essence of \"Apple\".\n\nThe \"language-reality\" objection asserts that language encourages the belief that \"synthetic exercises\" distill experiences, yet it rejects the results of such exercises by maintaining that observers actually combine experiences to create each concept of any particular property. It holds that language is a complicated belief system whose only connection to reality is an abstraction of experience. The \"language-reality\" objection may even suggest that \"reality/non-reality\" or \"objective/subjective\" distinctions themselves are merely artifacts of language and therefore are also solely abstractions of experience.\n\nThe Buddhist Madhyamaka philosopher, Chandrakirti, used the aggregate nature of objects to demonstrate the lack of essence in what is known as the sevenfold reasoning. In his work, \"Guide to the Middle Way\" (Sanskrit: \"Madhyamakāvatāra\"), he says:\n\nHe goes on to explain what is meant by each of these seven assertions, but briefly in a subsequent commentary he explains that the conventions of the world do not exist essentially when closely analyzed, but exist only through being taken for granted, without being subject to scrutiny that searches for an essence within them.\n\nAnother view of the Buddhist theory of the self, especially in early Buddhism, is that the Buddhist theory is essentially an eliminativist theory. According to this understanding, the self can not be reduced to a bundle because there is nothing that answers to the concept of a self. Consequently, the idea of a self must be eliminated.\n\n\n", "id": "3870", "title": "Bundle theory"}
{"url": "https://en.wikipedia.org/wiki?curid=3873", "text": "Bernard Montgomery\n\nField Marshal Bernard Law Montgomery, 1st Viscount Montgomery of Alamein, (; 17 November 1887 – 24 March 1976), nicknamed \"Monty\" and the \"Spartan General\", was a senior British Army officer who fought in both the First World War and the Second World War.\n\nHe saw action in the First World War as a junior officer of the Royal Warwickshire Regiment. At Méteren, near the Belgian border at Bailleul, he was shot through the right lung by a sniper, during the First Battle of Ypres. He returned to the Western Front as a general staff officer and took part in the Battle of Arras in April/May 1917. He also took part in the Battle of Passchendaele in late 1917 before finishing the war as chief of staff of the 47th (2nd London) Division.\n\nIn the inter-war years he commanded the 17th (Service) Battalion, Royal Fusiliers and, later, the 1st Battalion, Royal Warwickshire Regiment before becoming commander of 9th Infantry Brigade and then General Officer Commanding (GOC) 8th Infantry Division.\n\nDuring the Second World War he commanded the British Eighth Army from August 1942 in the Western Desert until the final Allied victory in Tunisia in May 1943. This command included the Second Battle of El Alamein, a turning point in the Western Desert Campaign. He subsequently commanded the British Eighth Army during the Allied invasion of Sicily and the Allied invasion of Italy. He was in command of all Allied ground forces during Operation \"Overlord\" from the initial landings until after the Battle of Normandy. He then continued in command of the 21st Army Group for the rest of the campaign in North West Europe. As such he was the principal field commander for the failed airborne attempt to bridge the Rhine at Arnhem, and the Allied Rhine crossing. On 4 May 1945 he took the German surrender at Lüneburg Heath in Northern Germany.\n\nAfter the war he became Commander-in-Chief of the British Army of the Rhine (BAOR) in Germany and then Chief of the Imperial General Staff (1946-8). He then served as Deputy Supreme Commander of NATO in Europe until his retirement in 1958.\n\nMontgomery was born in Kennington, Surrey, in 1887, the fourth child of nine, to an Ulster-Scots Church of Ireland minister, The Reverend Henry Montgomery, and his wife, Maud (\"née\" Farrar). The Montgomerys, an 'Ascendancy' gentry family, were the County Donegal branch of the Clan Montgomery. Henry Montgomery, at that time Vicar of St Mark's Church, Kennington, was the second son of General Sir Robert Montgomery, a native of Inishowen in County Donegal in Ulster, the noted soldier and proconsul in British India, who died a month after his grandson's birth. He was probably a descendant of Colonel Alexander Montgomery (1686–1729). Bernard's mother, Maud, was the daughter of The V. Rev. Frederic William Canon Farrar, the famous preacher, and was eighteen years younger than her husband. After the death of Sir Robert Montgomery, Henry inherited the Montgomery ancestral estate of New Park in Moville in Inishowen in Ulster. There was still £13,000 to pay on a mortgage, a large debt in the 1880s, and Henry was at the time still only an Anglican vicar. Despite selling off all the farms that were at Ballynally, \"there was barely enough to keep up New Park and pay for the blasted summer holiday\" (i.e., at New Park).\n\nIt was a financial relief of some magnitude when, in 1889, Henry was made Bishop of Tasmania, then still a British colony and Bernard spent his formative years there. Bishop Montgomery considered it his duty to spend as much time as possible in the rural areas of Tasmania and was away for up to six months at a time. While he was away, his wife, still in her mid-twenties, gave her children \"constant\" beatings, then ignored them most of the time as she performed the public duties of the bishop's wife. Of Bernard's siblings, Sibyl died prematurely in Tasmania, and Harold, Donald and Una all emigrated. Maud Montgomery took little active interest in the education of her young children other than to have them taught by tutors brought from Britain. The loveless environment made Bernard something of a bully, as he himself recalled, \"I was a dreadful little boy. I don't suppose anybody would put up with my sort of behaviour these days.\" Later in life Montgomery refused to allow his son David to have anything to do with his grandmother, and refused to attend her funeral in 1949.\n\nThe family returned to England once for a Lambeth Conference in 1897, and Bernard and his brother Harold were educated for a term at The King's School, Canterbury. In 1901, Bishop Montgomery became secretary of the Society for the Propagation of the Gospel, and the family returned to London. Montgomery attended St Paul's School and then the Royal Military College, Sandhurst, from which he was almost expelled for rowdiness and violence. On graduation in September 1908 he was commissioned into the 1st Battalion the Royal Warwickshire Regiment as a second lieutenant, and first saw overseas service later that year in India. He was promoted to lieutenant in 1910, and in 1912 became adjutant of the 1st Battalion of his regiment at Shorncliffe Army Camp.\n\nThe Great War began in August 1914 and Montgomery moved to France with his battalion that month, which was at the time part of the 10th Brigade of the 4th Division. He saw action at the Battle of Le Cateau that month and during the retreat from Mons. At Méteren, near the Belgian border at Bailleul on 13 October 1914, during an Allied counter-offensive, he was shot through the right lung by a sniper. Montgomery was hit once more, in the knee. He was awarded the Distinguished Service Order for gallant leadership: the citation for this award, published in the \"London Gazette\" in December 1914 reads:\n\nAfter recovering in early 1915, he was appointed to be brigade major first of 112th Brigade and then with 104th Brigade under training in Lancashire. He returned to the Western Front in early 1916 as a general staff officer in the 33rd Division and took part in the Battle of Arras in April/May 1917. He became a general staff officer with IX Corps, part of General Sir Herbert Plumer's Second Army, in July 1917.\n\nMontgomery served at the Battle of Passchendaele in late 1917 before finishing the war as General Staff Officer Grade 1 and effectively Chief of Staff of the 47th (2nd London) Division, with the temporary rank of lieutenant colonel. A photograph from October 1918, reproduced in many biographies, shows the then unknown Lieutenant Colonel Montgomery standing in front of Winston Churchill (then the Minister of Munitions) at the parade following the liberation of Lille.\n\nAfter the First World War Montgomery commanded the 17th (Service) Battalion of the Royal Fusiliers, a battalion in the British Army of the Rhine, before reverting to his substantive rank of captain (brevet major) in November 1919. He had not at first been selected for the Staff College in Camberley, Surrey (his only hope of ever achieving high command). But at a tennis party in Cologne, he was able to persuade the Commander-in-Chief (C-in-C) of the British Army of Occupation, Field Marshal Sir William Robertson, to add his name to the list.\n\nAfter graduating from the Staff College, he was appointed brigade major in the 17th Infantry Brigade in January 1921. The brigade was stationed in County Cork, Ireland, carrying out counter-insurgency operations during the final stages of the Irish War of Independence.\n\nMontgomery came to the conclusion that the conflict could not be won without harsh measures, and that self-government for Ireland was the only feasible solution; in 1923, after the establishment of the Irish Free State and during the Irish Civil War, Montgomery wrote to Colonel Arthur Ernest Percival of the Essex Regiment: \n\nIn May 1923, Montgomery was posted to the 49th (West Riding) Infantry Division, a Territorial Army (TA) formation. He returned to the 1st Battalion, Royal Warwickshire Regiment in 1925 as a company commander and was promoted to major in July 1925. From January 1926 to January 1929 he served as Deputy Assistant Adjutant General at the Staff College, Camberley, in the temporary rank of lieutenant colonel.\n\nIn 1925, in his first known love affair, Montgomery, then in his late thirties, courted a seventeen-year-old girl, Miss Betty Anderson. His method of courtship apparently included drawing diagrams in the sand of how he would deploy his tanks and infantry in a future war, a contingency which seemed very remote at that time. She respected his ambition and single-mindedness, but declined his proposal of marriage.\n\nIn 1927, he met and married Elizabeth (Betty) Carver, Hobart, widow of Oswald Carver, Olympic rowing medallist who had been killed in the First World War. Betty Carver was the sister of the future Second World War commander, Major General Sir Percy Hobart. Betty Carver had two sons in their early teens, John and Dick, from her first marriage. Dick Carver later wrote that it had been \"a very brave thing\" for Montgomery to take on a widow with two children. Montgomery's son, David, was born in August 1928.\n\nWhile on holiday in Burnham-on-Sea in 1937, Betty suffered an insect bite which became infected, and she died in her husband's arms from septicaemia following amputation of her leg. The loss devastated Montgomery, who was then serving as a brigadier, but he insisted on throwing himself back into his work immediately after the funeral. Montgomery's marriage had been extremely happy. Much of his correspondence with his wife was destroyed when his quarters at Portsmouth were bombed during the Second World War. After Montgomery's death, John Carver wrote that his mother had arguably done the country a favour by keeping his personal oddities – his extreme single-mindedness, and his intolerance of and suspicion of the motives of others – within reasonable bounds long enough for him to have a chance of attaining high command.\n\nBoth of Montgomery's stepsons became army officers in the 1930s (both were serving in India at the time of their mother's death), and both served in the Second World War, each eventually attaining the rank of Colonel. While serving as a GSO2 with Eighth Army, Dick Carver was sent forward during the pursuit after El Alamein to help identify a new site for Eighth Army HQ. He was taken prisoner at Mersa Matruh on 7 November 1942. Montgomery wrote to his contacts in England asking that inquiries be made via the Red Cross as to where his stepson was being held, and that parcels be sent to him. Like many British POWs, the most famous being General Richard O'Connor, Dick Carver escaped in September 1943 during the brief hiatus between Italy's departure from the war and the German seizure of the country. He eventually reached British lines on 5 December 1943, to the delight of his stepfather, who sent him home to Britain to recuperate.\n\nIn January 1929 Montgomery was promoted to brevet lieutenant colonel. That month he returned to the 1st Battalion, Royal Warwickshire Regiment again, as Commander of Headquarters Company; he went to the War Office to help write the Infantry Training Manual in mid-1929. In 1931 Montgomery was promoted to substantive lieutenant colonel and became the Commanding Officer (CO) of the 1st Battalion, Royal Warwickshire Regiment and saw service in Palestine and British India. He was promoted to colonel in June 1934 (seniority from January 1932). He attended and was then recommended to become an instructor at the Indian Army Staff College (now the Pakistan Army Staff College) in Quetta, British India.\n\nOn completion of his tour of duty in India, Montgomery returned to Britain in June 1937 where he took command of the 9th Infantry Brigade with the temporary rank of brigadier. His wife died that year.\n\nIn 1938, he organised an amphibious combined operations landing exercise that impressed the new C-in-C of Southern Command, General Sir Archibald Percival Wavell. He was promoted to major general on 14 October 1938 and took command of the 8th Infantry Division in Palestine. There he quashed an Arab revolt before returning in July 1939 to Britain, suffering a serious illness on the way, to command the 3rd (Iron) Infantry Division. On hearing of the rebel defeat in April 1939, Montgomery said, \"I shall be sorry to leave Palestine in many ways, as I have enjoyed the war out here\".\n\nBritain declared war on Germany on 3 September 1939. The 3rd Division was deployed to Belgium as part of the British Expeditionary Force (BEF). During this time, Montgomery faced serious trouble from his military superiors and the clergy for his frank attitude regarding the sexual health of his soldiers, but was defended from dismissal by his superior Alan Brooke, commander of II Corps. Montgomery's training paid off when the Germans began their invasion of the Low Countries on 10 May 1940 and the 3rd Division advanced to the River Dijle and then withdrew to Dunkirk with great professionalism, entering the Dunkirk perimeter in a famous night-time march that placed his forces on the left flank, which had been left exposed by the Belgian surrender. The 3rd Division returned to Britain intact with minimal casualties. During Operation \"Dynamo\" — the evacuation of 330,000 BEF and French troops to Britain — Montgomery assumed command of the II Corps.\n\nOn his return Montgomery antagonised the War Office with trenchant criticisms of the command of the BEF and was briefly relegated back to divisional command of 3rd Division. He was made a Companion of the Order of the Bath. 3rd Division was at that time the only fully equipped division in Britain.\n\nMontgomery was ordered to make ready his 3rd division to invade the neutral Portuguese Azores. Models of the islands were prepared and detailed plans worked out for the invasion. The invasion plans did not go ahead and plans switched to invading Cape Verde island also belonging to neutral Portugal. These invasion plans also did not go ahead. Montgomery was then ordered to prepare plans for the invasion of neutral Ireland and to seize Cork, Cobh and Cork harbour. These invasion plans like those of the Portuguese islands also did not go ahead and in July 1940, Montgomery was appointed acting lieutenant-general, placed in command of V Corps, responsible for the defence of Hampshire and Dorset, and started a long-running feud with the new commander-in-chief, Southern Command, Claude Auchinleck.\n\nIn April 1941, he became commander of XII Corps responsible for the defence of Kent. During this period he instituted a regime of continuous training and insisted on high levels of physical fitness for both officers and other ranks. He was ruthless in sacking officers he considered would be unfit for command in action. Promoted to temporary lieutenant-general in July, in December Montgomery was given command of South-Eastern Command overseeing the defence of Kent, Sussex and Surrey.\n\nHe renamed his command the South-Eastern Army to promote offensive spirit. During this time he further developed and rehearsed his ideas and trained his soldiers, culminating in Exercise Tiger in May 1942, a combined forces exercise involving 100,000 troops.\n\nIn 1942, a new field commander was required in the Middle East, where Auchinleck was fulfilling both the role of Commander-in-chief Middle East Command and commander Eighth Army. He had stabilised the Allied position at the First Battle of El Alamein, but after a visit in August 1942, the Prime Minister, Winston Churchill, replaced him as Commander-in-chief with General Sir Harold Alexander and William Gott as commander of the Eighth Army in the Western Desert. After Gott was killed flying back to Cairo Churchill was persuaded by Brooke, who by this time was Chief of the Imperial General Staff (CIGS), to appoint Montgomery, who had only just been nominated to replace Alexander as commander of the British ground forces for Operation \"Torch\".\n\nA story, probably apocryphal but popular at the time, is that the appointment caused Montgomery to remark that \"After having an easy war, things have now got much more difficult.\" A colleague is supposed to have told him to cheer up – at which point Montgomery said \"I'm not talking about me, I'm talking about Rommel!\"\n\nMontgomery's assumption of command transformed the fighting spirit and abilities of the Eighth Army. Taking command on 13 August 1942, he immediately became a whirlwind of activity. He ordered the creation of the X Corps, which contained all armoured divisions to fight alongside his XXX Corps which was all infantry divisions. This was in no way similar to a German Panzer Corps. One of Rommel's Panzer Corps combined infantry, armour and artillery units under one corps commander. The only common commander for Montgomery's all infantry and all armour corps was the Eighth Army Commander himself. Correlli Barnett commented that Montgomery's solution \"... was in every way opposite to Auchinleck's and in every way wrong, for it carried the existing dangerous separatism still further.\" Montgomery reinforced the long front line at El Alamein, something that would take two months to accomplish. He asked Alexander to send him two new British divisions (51st Highland and 44th Home Counties) that were then arriving in Egypt and were scheduled to be deployed in defence of the Nile Delta. He moved his field HQ to Burg al Arab, close to the Air Force command post in order better to coordinate combined operations.\n\nMontgomery was determined that the Army, Navy and Air Forces should fight their battles in a unified, focused manner according to a detailed plan. He ordered immediate reinforcement of the vital heights of Alam Halfa, just behind his own lines, expecting the German commander, Erwin Rommel, to attack with the heights as his objective, something that Rommel soon did. Montgomery ordered all contingency plans for retreat to be destroyed. \"I have cancelled the plan for withdrawal. If we are attacked, then there will be no retreat. If we cannot stay here alive, then we will stay here dead\", he told his officers at the first meeting he held with them in the desert, though, in fact, Auchinleck had no plans to withdraw from the strong defensive position he had chosen and established at El Alamein.\n\nMontgomery made a great effort to appear before troops as often as possible, frequently visiting various units and making himself known to the men, often arranging for cigarettes to be distributed. Although he still wore a standard British officer's cap on arrival in the desert, he briefly wore an Australian broad-brimmed hat before switching to wearing the black beret (with the badge of the Royal Tank Regiment next to the British General Officer's badge) for which he became notable. The black beret was offered to him by Jim Fraser while the latter was driving him on an inspection tour. Both Brooke and Alexander were astonished by the transformation in atmosphere when they visited on 19 August, less than a week after Montgomery had taken command.\n\nRommel attempted to turn the left flank of the Eighth Army at the Battle of Alam el Halfa from 31 August 1942. The German/Italian armoured Corps infantry attack was stopped in very heavy fighting. Rommel's forces had to withdraw urgently lest their retreat through the British minefields be cut off. Montgomery was criticised for not counter-attacking the retreating forces immediately, but he felt strongly that his methodical build-up of British forces was not yet ready. A hasty counter-attack risked ruining his strategy for an offensive on his own terms in late October, planning for which had begun soon after he took command. He was confirmed in the permanent rank of lieutenant-general in mid October.\n\nThe conquest of Libya was essential for airfields to support Malta and to threaten the rear of Axis forces opposing Operation \"Torch\". Montgomery prepared meticulously for the new offensive after convincing Churchill that the time was not being wasted. (Churchill sent a telegram to Alexander on 23 September 1942 which began, \"We are in your hands and of course a victorious battle makes amends for much delay.\") He was determined not to fight until he thought there had been sufficient preparation for a decisive victory, and put into action his beliefs with the gathering of resources, detailed planning, the training of troops—especially in clearing minefields and fighting at night—and in the use of 252 of the latest American-built Sherman tanks, 90 M7 Priest self-propelled howitzers, and making a personal visit to every unit involved in the offensive. By the time the offensive was ready in late October, Eighth Army had 231,000 men on its ration strength.\n\nThe Second Battle of El Alamein began on 23 October 1942, and ended 12 days later with one of the first large-scale, decisive Allied land victories of the war. Montgomery correctly predicted both the length of the battle and the number of casualties (13,500). Soon after Allied armoured units and infantry broke through the German and Italian lines and were pursuing the enemy forces at speed along the coast road, a violent rainstorm burst over the region, bogging down the tanks and support trucks in the desert mud. Montgomery, standing before his officers at headquarters and close to tears, announced that he was forced to call off the pursuit. Historian Corelli Barnett has pointed out that the rain also fell on the Germans, and that the weather is therefore an inadequate explanation for the failure to exploit the breakthrough, but nevertheless the Battle of El Alamein had been a great success. Over 30,000 prisoners of war were taken, including the German second-in-command, General von Thoma, as well as eight other general officers. Rommel, having been in a hospital in Germany at the start of the battle, was forced to return on 25 October 1942 after Stumme – his replacement as German commander – died of a heart attack in the early hours of the battle.\n\nMontgomery was advanced to KCB and promoted to full general. He kept the initiative, applying superior strength when it suited him, forcing Rommel out of each successive defensive position. On 6 March 1943, Rommel's attack on the over-extended Eighth Army at Medenine (Operation \"Capri\") with the largest concentration of German armour in North Africa was successfully repulsed. At the Mareth Line, 20 to 27 March, when Montgomery encountered fiercer frontal opposition than he had anticipated, he switched his major effort into an outflanking inland pincer, backed by low-flying RAF fighter-bomber support. For his role in North Africa he was awarded the Legion of Merit by the United States government in the rank of Chief Commander.\n\nThe next major Allied attack was the Allied invasion of Sicily (Operation \"Husky\"). Montgomery considered the initial plans for the Allied invasion, which had been agreed in principle by General Dwight D. Eisenhower, the Supreme Allied Commander Mediterranean, and General Alexander, the 15th Army Group commander, to be unworkable because of the dispersion of effort. He managed to have the plans recast to concentrate the Allied forces, having Lieutenant General George Patton's US Seventh Army land in the Gulf of Gela (on the Eighth Army's left flank, which landed around Syracuse in the south-east of Sicily) rather than near Palermo in the west and north of Sicily. Inter-Allied tensions grew as the American commanders, Patton and Omar Bradley (then commanding US II Corps under Patton), took umbrage at what they saw as Montgomery's attitudes and boastfulness.\n\nDuring late 1943, Montgomery continued to command the Eighth Army during the landings on the mainland of Italy itself, beginning with Operation Baytown. In conjunction with the Anglo-American landings at Salerno (near Naples) by Lieutenant General Mark Clark's US Fifth Army and seaborne landings by British paratroops in the heel of Italy (including the key port of Taranto, where they disembarked without resistance directly into the port), Montgomery led the Eighth Army up the toe of Italy. Montgomery abhorred the lack of coordination, the dispersion of effort, the strategic muddle and lack of opportunism he saw in the Allied effort in Italy and was glad to leave the \"dog's breakfast\" on 23 December 1943.\n\nMontgomery returned to Britain in January 1944. He was assigned to command the 21st Army Group consisting of all Allied ground forces participating in Operation \"Overlord\", the invasion of Normandy. Overall direction was assigned to the Supreme Allied Commander of the Allied Expeditionary Forces, American General Dwight D. Eisenhower. Both Churchill and Eisenhower had found Montgomery difficult to work with in the past and wanted the position to go to the more affable General Sir Harold Alexander. However Montgomery's patron, Alan Brooke, firmly argued that Montgomery was a much superior general to Alexander and ensured his appointment. Without Brooke's support, Montgomery would never have been tasked with leading the Allies into France, remaining in Italy. At St Paul's School on 7 April and 15 May Montgomery presented his strategy for the invasion. He envisaged a ninety-day battle, with all forces reaching the Seine. The campaign would pivot on an Allied-held Caen in the east of the Normandy bridgehead, with relatively static British and Canadian armies forming a shoulder to attract and defeat the main German counter-attacks, relieving the US armies who would move and seize the Cotentin Peninsula and Brittany, wheeling south and then east on the right forming a pincer.\nDuring the hard fought two and a half month Battle of Normandy that followed, the impact of a series of unfavourable autumnal weather conditions disrupted the Normandy landing areas. Montgomery's initial plan was for the Anglo-Canadian troops under his command to break out immediately from their beachheads on the Calvados coast towards Caen with the aim of taking the city on either D Day or two days later. Depending on the historical interpretation, he was unable or unwilling to do so. Montgomery's attempt to follow up the D-Day landings with an attempt to take Caen with the British 3rd Division, British 50th Northumbrian Division and Canadian 3rd Division, was stopped on 7–8 June by 21st Panzer Division and 12th Waffen SS Panzer Division \"Hitlerjugend\" who hit the advancing Anglo-Canadian troops very hard. To follow up this victory Rommel ordered the 2nd Panzer Division to Caen while Field Marshal Gerd von Rundstedt asked for and received permission from Hitler to have the elite 1st Waffen SS Division \"Leibstandarte Adolf Hitler\" and 2nd Waffen SS Division \"Das Reich\" to be sent to Caen with the aim of driving the Anglo-Canadian forces back into the sea. Montgomery thus had to face what the British historian Stephen Badsey called the \"most formidable\" of all the German divisions in France. The 12th Waffen SS Division \"Hitlerjugend\" as its name implies was drawn entirely from the more fanatical elements of the Hitler Youth and commanded by the very ruthless SS-\"Brigadeführer\" Kurt Meyer, aka \"Panzer Meyer\", who was in the words of the Canadian historian Colonel John English: \"the scourge of Canadian arms throughout the Normandy campaign\" as time after time Meyer foiled Montgomery's attempts to have his Canadian soldiers take Caen. The National Socialist fanatic Meyer, considered to be tough and ferocious even by the standards of the SS was known as \"Panzer Meyer\" because it was said he would crush anyone who got in his way just like a tank, and was in the words of Carlo D'Este a \"fanatical Nazi who would fight to the death for his beloved \"Führer\"\".\n\nThe failure to take Caen immediately has been the source of an immense historiographical dispute with bitter nationalist overtones. Broadly, there has been a \"British school\" which accepts Montgomery's post-war claim that he never intended to take Caen at once, and instead the Anglo-Canadian operations around Caen were a \"holding operation\" intended to attract the bulk of the German forces towards the Caen sector to allow the Americans to stage the \"break out operation\" on the left flank of the German positions, which was all part of Montgomery's \"Master Plan\" that he had conceived long before the Normandy campaign. By contrast, the \"American school\" argued that Montgomery's initial \"master plan\" was for the 21st Army Group to take Caen at once, move his tank divisions into the plains south of Caen to stage a breakout that would lead the 21st Army Group into the plains of northern France and hence into Antwerp and finally the Ruhr. Letters written by Eisenhower at the time of the battle make it clear that Eisenhower was expecting from Montgomery \"the early capture of the important focal point of Caen\". Later, when this plan had clearly failed, Eisenhower wrote that Montgomery had \"evolved\" the plan to have the US forces achieve the break-out instead.\n\nAs the campaign progressed, Montgomery altered his initial plan for the invasion and switched to a strategy of attracting and holding German counter-attacks in the area north of Caen, which was designed to allow the United States Army in the west to take Cherbourg. A memo summarising Montgomery's operations written by Eisenhower's chief of staff, General Walter Bedell Smith who met with Montgomery in late June 1944 says nothing about Montgomery conducting a \"holding operation\" in the Caen sector, and instead speaks of him seeking a \"breakout\" into the plains south of the Seine. On 12 June, Montgomery ordered the 7th Armoured Division into an offensive against the Panzer Lehr Division that made good progress at first, but ended when the Panzer Lehr was joined by the 2nd SS Division. At the celebrated tank action at Villers Bocage on 14 June, the British lost twenty Cromwell tanks to five Tiger tanks led by SS \"Obersturmführer\" Michael Wittmann in about five minutes. Despite the setback at Villers Bocage, Montgomery was still optimistic as the Allies were landing more troops and supplies than they were losing in battle, and though the German lines were holding, the \"Wehrmacht\" and \"Waffen SS\" were taking terrible casualties to hold their positions. A furious Air Marshal Sir Arthur Tedder complained that it was impossible to move fighter squadrons to France until Montgomery had captured some air fields, something he asserted that Montgomery appeared incapable of doing. The first V-1 attacks on London, which started on 13 June further increased the pressure on Montgomery from Whitehall to speed up his advance.\n\nOn 18 June, Montgomery ordered Bradley to take Cherbourg while the British were to take Caen by 23 June. In \"Operation Epsom\", the British VII Corps commanded by Sir Richard O'Connor attempted to outflank Caen from the west by breaking through the dividing line between the Panzer Lehr and the 12th SS to take the strategic Hill 112. Epsom began well with O'Connor's assault force, the British 15th Scottish Division breaking through, and joined by the 11th Armoured Division stopping the counter-attacks of the 12th SS Division. General Friedrich Dollmann of the 7th Army had to commit the newly arrived II SS Corps to stop the British offensive. Dollmann, fearing that Epsom would be a success, committed suicide and was replaced by SS \"Oberstegruppenführer\" Paul Hausser. O'Connor, at the cost of about 4,000 men, had won a salient deep and wide, but placed the Germans into an unviable long-term position. But at the time, there was a strong sense of crisis in the Allied command as the Allies had advanced only about inland at a time when their plans called for them to have already taken Rennes, Alençon and St. Malo. After Epsom, Montgomery had to tell General Harry Crerar that the activation of the First Canadian Army would have to wait as there was room at present in the Caen sector only for the newly arrived XII Corps under General Neil Ritchie, which caused some tension with Crerar who was anxious to get into the field. Epsom had forced further German forces into Caen, but all through June and the first half of July Rommel, Rundstedt and Hitler were engaged in planning for a great offensive to drive the British into the sea; it was never launched and would have required the commitment of a large number of German forces to the Caen sector).\n\nIt was only after several failed attempts to break out in the Caen sector that Montgomery devised what he later called his \"master plan\" of having the 21st Army Group hold the bulk of the German forces, thus allowing the Americans to break out. The Canadian historians Terry Copp and Robert Vogel wrote about the dispute between the \"American school\" and \"British school\" after having suffered several setbacks in June 1944:\n\nHampered by stormy weather and the bocage terrain, Montgomery had to ensure Rommel focused on the British in the east rather than the Americans in the west, who had to take the Cotentin Peninsula and Brittany before the Germans could be trapped by a general swing east. Montgomery told General Sir Miles Dempsey, the commander of the 2nd British Army: \"Go on hitting, drawing the German strength, especially some of the armour, on to yourself – so as to ease the way for Brad [Bradley].\" The Germans had deployed 12 divisions, of which six were Panzer divisions against the British while deploying 8 divisions, of which 3 were Panzer divisions against the Americans. By the middle of July Caen had not been taken, as Rommel continued to prioritise prevention of the break-out by British forces rather than the western territories being taken by the Americans. This was broadly as Montgomery had planned, albeit not with the same speed as he outlined at St Paul's, although as the American historian Carlo D'Este pointed out the actual situation in Normandy was \"vastly different\" from what was envisioned at the St. Paul's conference as only one of four goals outlined in May had been achieved by 10 July.\n\nOn 7 July, Montgomery began \"Operation Charnwood\" with a carpet bombing offensive that turned much of the French countryside and the city of Caen into a wasteland. The German resistance was extremely fierce with Kurt \"Panzer\" Meyer at one point appearing in the streets of Caen with a \"Panzerfaust\" (anti-tank rocket projector) to inspire his men to keep fighting, while shouting such slogans as \"Hunde, wollt ihr ewig leben!\" (\"Dogs, do you want to live forever!). The British and Canadians succeeded in advancing into northern Caen before the Germans who used the ruins to their advantage and stopped the offensive. On 10 July, Montgomery ordered Bradley to take Avranches, after which the 3rd US Army would be activated to drive towards Le Mans and Alençon. On 14 July 1944, Montgomery wrote to his patron Brooke, saying he had chosen on a \"real show down on the eastern flanks, and to loose a Corps of three armoured divisions in the open country about the Caen-Falaise road...The possibilities are immense; with seven hundred tanks loosed to the South-east of Caen, and the armoured cars operating far ahead, anything can happen.\" The French Resistance had launched Plan Violet in June 1944 to systematically destroy the telephone system of France, which forced the Germans to use their radios more and more to communicate, and as the code-breakers of Bletchley Park had broken many of the German codes, Montgomery had - via Ultra intelligence - a good idea of the German situation. Montgomery thus knew German Army Group B had lost 96,400 men while receiving 5,200 replacements and the Panzer Lehr Division now based at St. Lô was down to only 40 tanks. Montgomery later wrote that he knew he had the Normandy campaign won at this point as the Germans had almost no reserves while he had three armoured divisions in reserve.\n\nAn American break-out was achieved with Operation \"Cobra\" and the encirclement of German forces in the Falaise pocket at the cost of British losses with the diversionary Operation \"Goodwood\". On the early morning of 18 July 1944, Operation Goodwood began with British heavy bombers beginning carpet bombing attacks that further devastated what was left of Caen and the surrounding countryside. A British tank crewman from the Guards Armoured Division later recalled: \"At 0500 hours a distant thunder in the air brought all the sleepy-eyed tank crews out of their blankets. 1,000 Lancasters were flying from the sea in groups of three or four at . Ahead of them the pathfinders were scattering their flares and before long the first bombs were dropping\". A German tankman from the 21st Panzer Division at the receiving end of this bombardment remembered: \"We saw little dots detach themselves from the planes, so many of them that the crazy thought occurred to us: are those leaflets?...Among the thunder of the explosions, we could hear the wounded scream and the insane howling of men who had [been] driven mad\". The British bombing had badly smashed the German front-line units; e.g. tanks were thrown up on the roofs of French farmhouses. Initially, the three British armoured divisions assigned to lead the offensive, the 7th, 11th and the Guards, made rapid progress and were soon approaching the Borguebus ridge, which dominated the landscape south of Caen by noon.\n\nIf the British could take the Borguebus ridge, the way to the plains of northern France would be wide open, and potentially Paris could be taken, which explains the ferocity with which the Germans defended the Borguebus ridge. One German officer, Lieutenant Baron von Rosen recalled that to motivate a Luftwaffe officer commanding a battery of four 88mm guns to fight against the British tanks that he had to hold his handgun to his head \"...and asked him whether he would like to be killed immediately or get a high decoration. He decided for the latter\". The well dug-in 88mm guns around the Borguebus ridge began taking a toll of the British Sherman tanks and the countryside was soon dotted with dozens of burning Shermans. One British officer reported with worry: \"I see palls of smoke and tanks brewing up with flames belching forth from their turrets. I see men climbing out, on fire like torches, rolling on the ground to try and douse the flames\". Despite Montgomery's orders to try to press on, fierce German counter-attacks stopped the British offensive.\n\nThe objectives of Operation Goodwood were all achieved except the complete capture of the Bourgebus Ridge, which was partially taken. The operation was a strategic Allied success in drawing in the last German reserves in Normandy towards the Caen sector away from the American sector, greatly assisting the American break out in Operation Cobra. By the end of Goodwood on 25 July 1944, the Canadians had finally taken Caen while the British tanks had reached the plains south of Caen, giving Montgomery the \"hinge\" he had been seeking, while forcing the Germans to commit the last of their reserves to stop the Anglo-Canadian offensive. Ultra decrypts indicated that the Germans now facing Bradley were seriously understrength with Operation Cobra about to commence. During Operation Goodwood, the British had 400 tanks knocked out with many recovered returning to service. The casualties were 5,500 with of ground gained. Bradley recognised Montgomery's plan to pin down German armour and allow US forces to break out:\n\"The British and Canadian armies were to decoy the enemy reserves and draw them to their front on the extreme eastern edge of the Allied beachhead. Thus, while Monty taunted the enemy at Caen, we [the Americans] were to make our break on the long roundabout road to Paris. When reckoned in terms of national pride, this British decoy mission became a sacrificial one, for while we tramped around the outside flank, the British were to sit in place and pin down the Germans. Yet strategically it fitted into a logical division of labors, for it was towards Caen that the enemy reserves would race once the alarm was sounded.\"\n\nThe long running dispute over what Montgomery's \"master plan\" in Normandy was, led historians to differ greatly about the purpose of Goodwood. The British journalist Mark Urban, wrote that the purpose of Goodwood was to draw German troops to their left flank to allow the Americans to breakout on the right flank, arguing that Montgomery had to lie to his soldiers about the purpose of Goodwood as the average British soldier would not have understood why they were being asked to create a diversion to allow the Americans to have the glory of staging the breakout with Operation Cobra. By contrast, the American historian Stephen Power argued that Goodwood was intended to be the \"breakout\" offensive and not a \"holding operation\", writing: \"It is unrealistic to assert that an operation which called for the use of 4,500 Allied aircraft, 700 artillery pieces and over 8,000 armored vehicles and trucks and that cost the British over 5,500 casualties was conceived and executed for so limited an objective\". Power noted that Goodwood and Cobra were supposed to take effect on the same day, 18 July 1944, but Cobra was cancelled owing to heavy rain in the American sector, and argued that both operations were meant to be breakout operations to trap the German armies in Normandy. American military writer Drew Middleton wrote that there is no doubt that Montgomery wanted Goodwood to provide a \"shield\" for Bradley, but at the same time Montgomery was clearly hoping for more than merely diverting German attention away from the American sector. British historian John Keegan pointed out that Montgomery made differing statements before Goodwood about the purpose of the operation. Keegan wrote that Montgomery engaged in what he called a \"hedging of his bets\" when drafting his plans for Goodwood, with a plan for a \"break out if the front collapsed, if not, sound documentary evidence that all he had intended in the first place was a battle of attrition\". With Goodwood drawing the Wehrmacht towards the British sector, the 1st American Army enjoyed a two to one numerical superiority and General Omar Bradley had accepted Montgomery's advice to begin the offensive by concentrating at one point instead of a \"broad front\" as Eisenhower would have preferred.\n\nOperation Goodwood almost cost Montgomery his job, as Eisenhower seriously considered sacking him and only chose not to do so because to sack the popular \"Monty\" would have caused such a political backlash in Britain against the Americans at a critical moment in the war that the resulting strains in the Atlantic alliance were not considered worth it. Montgomery expressed his satisfaction at the results of Goodwood when calling the operation off. Eisenhower was under the impression that Goodwood was to be a break out operation. There was a miscommunication between the two men or Eisenhower did not understand the strategy. Alan Brooke chief of the British Imperial General Staff wrote: \"Ike knows nothing about strategy and is quite unsuited to the post of Supreme Commander. It is no wonder that Monty's real high ability is not always realised\" Bradley fully understood Montgomery's intentions. Both men would not give away to the press the true intentions of their strategy.\n\nMany American officers had found Montgomery a difficult man to work with, and after Goodwood, pressured Eisenhower to fire Montgomery. Although the Eisenhower-Montgomery dispute is sometimes depicted in nationalist terms as being an Anglo-American struggle, it was the British Air Marshal Arthur Tedder who was pressing Eisenhower most strongly after Goodwood to fire Montgomery. An American officer wrote in his diary that Tedder had come to see Eisenhower to \"pursue his current favourite subject, the sacking of Monty\". With Tedder leading the \"sack Monty\" campaign, it encouraged Montgomery's American enemies to press Eisenhower to fire Montgomery. Brooke was sufficiently worried about the \"sack Monty\" campaign to visit Montgomery at his Tactical Headquarters (TAC) in France and as he wrote in his diary; \"warned [Montgomery] of a tendency in the PM [Churchill] to listen to suggestions that Monty played for safety and was not prepared to take risks\". Brooke advised Montgomery to invite Churchill to Normandy, arguing that if the \"sack Monty\" campaign had won the Prime Minister over, then his career would be over as having Churchill's backing would give Eisenhower the political \"cover\" to fire Montgomery. On 20 July, Montgomery met Eisenhower and on 21 July Churchill at the TAC in France. One of Montgomery's staff officers wrote afterwards that it was \"common knowledge at Tac that Churchill had come to sack Monty\". No notes were taken at the Eisenhower-Montgomery and Churchill-Montgomery meetings, but Montgomery was able to persuade both men not to fire him.\n\nWith the success of Cobra, which was soon followed by unleashing the 3rd American Army under the General George S. Patton, Eisenhower wrote to Montgomery: \"Am delighted that your basic plan has begun brilliantly to unfold with Bradley's initial success\". The success of Cobra was aided by \"Operation Spring\" when the II Canadian Corps under General Guy Simonds (the only Canadian general whose skill Montgomery respected) began an offensive south of Caen that made little headway, but which the Germans regarded as the main offensive. Once the 3rd American Army arrived, Bradley was promoted to take command of the newly created 12th Army Group consisting of 1st and 3rd American Armies. Following the American breakout, there followed the Battle of Falaise Gap as the British, Canadian and Polish soldiers of 21st Army Group commanded by Montgomery advanced south while the American and French soldiers of Bradley's 12th Army Group advanced north to encircle the German Army Group B at Falaise as Montgomery waged what Urban called \"a huge battle of annihilation\" in August 1944. Montgomery began his offensive into the \"Suisse Normande\" region with Operation Bluecoat with Richard O'Connor's VIII Corps and Gerard Bucknall's XXX Corps heading south. A dissatisfied Montgomery sacked Bucknall for being insufficiently aggressive and replaced him with General Brian Horrocks. At the same time, Montgomery ordered that Patton whose Third Army was supposed to advance into Brittany to send only minimal forces and instead ordered that Patton was to capture Nantes, which was soon taken.\n\nHitler waited too long to order his soldiers to retreat from Normandy, leading Montgomery to write: \"He [Hitler] refused to face the only sound military course. As a result the Allies caused the enemy staggering losses in men and materials\". Knowing via Ultra that Hitler was not planning to retreat from Normandy, Montgomery, on 6 August 1944, ordered that an envelopment operation against Army Group B with the First Canadian Army under Harry Crerar, which was to advance towards Falaise, the Second British Army under Sir Miles Dempsey was to advance towards Argentan and the Third American Army under George S. Patton was to advance to Alençon. On 11 August, Montgomery changed his plan with the Canadians to take Falaise and to meet the Americans at Argentan. The First Canadian Army launched two operations, \"Operation Totalize\" on 7 August which advanced only in four days in the face of fierce German resistance and \"Operation Tractable\" on 14 August which finally took Falaise on 17 August. In view of the slow Canadian advance, Patton requested permission to take Falaise, but was refused by Bradley on 13 August, which prompted much controversy with many historians arguing that Bradley lacked aggression and that Montgomery should have overruled Bradley. The so-called \"Falaise Gap\" was closed on 22 August 1944, but several American generals, most notably Patton, accused Montgomery of being insufficiently aggressive in closing it, about 60,000 German soldiers were trapped in Normandy, but before 22 August, about 20,000 Germans had escaped through the \"Falaise Gap\" to fight another day. About 10,000 Germans had been killed in the Battle of the Falaise Gap, which led to a stunned Eisenhower, who viewed the battlefield on 24 August, to comment with horror that it was impossible to walk without stepping on corpses.\nThe successful conclusion of the Normandy campaign saw the beginning of the debate between the \"American school\" and \"British school\" as both American and British generals started to advance claims about who was most responsible for this victory. Brooke wrote in defence of his protégé Montgomery: \"Ike knows nothing about strategy and is \"quite\" unsuited to the post of Supreme Commander. It is no wonder that Monty's real high ability is not always realised. Especially so when 'national' spectacles pervert the perspective of the strategic landscape\". About Montgomery's conduct of the Normandy campaign, Badsey wrote: \"Too much discussion on Normandy has centered on the controversial decisions of the Allied commanders. It was not good enough, apparently, to win such a complete and spectacular victory over an enemy that had conquered most of Europe unless it was done perfectly. Most of the blame for this lies with Montgomery, who was foolish enough to insist that it \"had\" been done perfectly, that Normandy – and all his other battles – had been fought accordingly to a precise master plan drawn up beforehand, from which he never deviated. It says much for his personality that Montgomery found others to agree with him, despite overwhelmingly evidence to the contrary. His handling of the Battle of Normandy was of a very high order, and as the person who would certainly have been blamed for losing the battle, he deserves the credit for winning it\".\n\nGeneral Eisenhower took over Ground Forces Command on 1 September, while continuing as Supreme Commander, with Montgomery continuing to command the 21st Army Group, now consisting mainly of British and Canadian units. Montgomery bitterly resented this change, although it had been agreed before the D-Day invasion. The British journalist Mark Urban writes that Montgomery seemed unable to grasp that as the majority of the 2.2 million Allied soldiers fighting against Germany on the Western Front were now American (the ratio was 3:1) that it was politically unacceptable to American public opinion to have Montgomery remain as Land Forces Commander as: \"Politics would not allow him to carry on giving orders to great armies of Americans simply because, in his view, he was better than their generals\".\n\nWinston Churchill had Montgomery promoted to field marshal by way of compensation. In September 1944, Montgomery ordered Crerar and his First Canadian Army to take the French ports on the English Channel, namely Calais, Boulogne and Dunkirk. On 4 September, Antwerp, the third largest port in Europe was captured by Horrocks, with its harbour mostly intact. The \"Witte Brigade\" (White Brigade) of the Belgian resistance had captured the Port of Antwerp, before the Germans could destroy the port. Antwerp was a deep water inland port connected to the North Sea via the river Scheldt. The Scheldt was wide enough and dredged deep enough to allow the passage of ocean-going ships.\n\nOn 3 September 1944 Hitler ordered the 15th German Army, which had been stationed in the Pas de Calais region and was withdrawing north into the Low Countries to hold the mouth of the river Scheldt to deprive the Allies of the use of Antwerp. Thanks to ULTRA, Montgomery was aware of Hitler's order by 5 September. Starting that same day, SHAEF's naval commander, Admiral Sir Bertram Ramsay had urged Montgomery to make clearing the mouth of the Schedlt his number one priority, arguing that as long as the mouth of the Scheldt was in German hands, it was impossible for the Royal Navy to clear the mines in the river, and as the Scheldt was mined, the port of Antwerp was useless. Alone among the senior commanders, only Ramsay saw opening Antwerp as crucial.\n\nOn 6 September 1944, Montgomery told Crerar that \"I want Boulogne badly\" and that city should be taken no matter what the cost. By this point, ports like Cherbourg were too far away from the front line, causing the Allies great logistical problems. The importance of ports closer to Germany was highlighted with the liberation of the city of Le Havre, which was assigned to John Crocker's I Corps. To take Le Havre, two infantry divisions, two tank brigades, most of the artillery of the 2nd British Army, the specialized armored \"gadgets\" of Percy Hobart's 79th Armoured Division, the battleship HMS \"Warspite\" and the monitor HMS \"Erebus\" were all committed. On 10 September 1944, Bomber Command dropped 4,719 tons of bombs on Le Havre, which was the prelude to \"Operation Astonia\", the assault on Le Havre by Crocker's men, which was taken two days later. The Canadian historian Terry Copp wrote that the commitment of this much firepower and men to take only one French city might \"seem excessive\", but by this point, the Allies desperately needed ports closer to the front line to sustain their advance.\n\nOn 9 September, Montgomery wrote to Brooke that \"one good Pas de Calais port\" would be sufficient to meet all the logistical needs of the 21st Army Group, but only the supply needs of the same formation. At the same time, Montgomery noted that \"one good Pas de Calais port\" would be insufficient for the American armies in France, which thus forced Eisenhower, if for no other reasons than logistics, to favour Montgomery's plans for an invasion of northern Germany by the 21st Army Group, whereas if Antwerp were opened up, then all of the Allied armies could be supplied. Montgomery ordered that Crerar take Calais, Boulogne and Dunkirk and clear the Scheldt, a task that Crerar stated was impossible as he lacked enough troops to perform both operations at once. Montgomery refused Crerar's request to have 12th British Corps under Neil Ritchie assigned to help clear the Scheldt as Montgomery stated he needed 12th Corps for Operation Market Garden. Montgomery was able to insist that Eisenhower adopt his strategy of a single thrust to the Ruhr with Operation \"Market Garden\" in September 1944. The offensive was strategically bold.\n\nOn 22 September 1944, General Guy Simonds's 2nd Canadian Corps took Boulogne, followed up by taking Calais on 1 October 1944. Montgomery was highly impatient with Simonds, complaining that it had taken Crocker's 1st British Corps only two days to take Le Havre while it took Simonds two weeks to take Boulogne and Calais, but Simonds noted that at Le Havre, three divisions and two brigades had been employed whereas as at both Boulogne and Calais, only two brigades were sent in to take both cities. After an attempt to storm the Leopold Canal by the 4th Canadian Division had been badly smashed by the German defenders, Simonds ordered a stop to further attempts to clear the river Scheldt until his mission of capturing the French ports on the English Channel had been accomplished; this allowed the German 15th Army ample time to dig into its new home on the Scheldt. The only port that was not captured by the Canadians was Dunkirk, as Montgomery ordered the 2nd Canadian Division on 15 September to hold his flank at Antwerp as a prelude for an advance up the Scheldt.\n\nMontgomery's plan for Operation Market Garden (17–25 September 1944) was to outflank the Siegfried Line and cross the Rhine, setting the stage for later offensives into the Ruhr region. The 21st Army Group would attack north from Belgium, 60 miles (97 km) through the Netherlands, across the Rhine and consolidate north of Arnhem on the far side of the Rhine. The risky plan required three Airborne Divisions to capture numerous intact bridges along a single-lane road, on which an entire Corps had to attack and use as its main supply route. The offensive failed to achieve its objectives.\n\nIn the aftermath of Market Garden, Montgomery made holding the Arnhem salient his first priority, arguing that the 2nd British Army might still be able to break through and reach the wide open plains of northern Germany, and that he might be able to take the Ruhr by the end of October. In the meantime, the 1st Canadian Army, which been given the task of clearing the mouth of the river Scheldt, despite the fact that in the words of Copp and Vogel \"...that Montgomery's Directive required the Canadians to continue to fight alone for almost two weeks in a battle which everyone agreed could only be won with the aid of additional divisions\". For his part, Field Marshal Gerd von Rundstedt, the German commander of the Western Front ordered General Gustav-Adolf von Zangen, the commander of 15th Army that: \"The attempt of the enemy to occupy the West Scheldt in order to obtain the free use of the harbor of Antwerp must be \"resisted to the utmost\"\" (emphasis in the original). Rundstedt argued with Hitler that as long as the Allies could not use the port of Antwerp, the Allies would lack the logistical capacity for an invasion of Germany.\n\nMontgomery pulled away from the 1st Canadian Army (temporarily commanded now by Simonds as Crerar was ill), the British 51st Highland Division, 1st Polish Division, British 49th (West Riding) British Division and 2nd Canadian Armored Brigade and sent all of these formations to help the 2nd British Army hold the Arnhem salient. However, Simonds seems to have regarded the Scheldt campaign as a test of his ability, and he felt he could clear the Scheldt with only 3 Canadian divisions, namely the 2nd, the 3rd, and the 4th, despite having to take on the entire 15th Army, which held strongly fortified positions in a landscape that favoured the defence. Simonds never complained about the lack of air support (made worse by the cloudy October weather), shortages of ammunition or having insufficient troops, regarding these problems as challenges for him to overcome, rather than a cause for complaint. As it was, Simonds made only slow progress in October 1944 during the fighting in the Battle of the Scheldt, although he was praised by Copp for imaginative and aggressive leadership who managed to achieve much, despite all of the odds against him. Montgomery had little respect for the Canadian generals, whom he dismissed as mediocre, except for Simonds, whom he consistently praised as Canada's only \"first-rate\" general in the entire war.\n\nAdmiral Ramsay, who proved to be a far more articulate and forceful champion of the Canadians than their own generals, starting on 9 October demanded of Eisenhower in a meeting that he either order Montgomery to make supporting the 1st Canadian Army in the Scheldt fighting his number one priority or sack him. Ramsay in very strong language argued to Eisenhower that the Allies could only invade Germany if Antwerp was opened, and that as long as the three Canadian divisions fighting in the Scheldt had shortages of ammunition and artillery shells because Montgomery made the Arnhem salient his first priority, then Antwerp would not be opened anytime soon. Even Brooke wrote in his diary: \"I feel that Monty's strategy for once is at fault. Instead of carrying out the advance to Arnhem he ought to have made certain of Antwerp\". Prompted by Ramsay, Eisenhower sent Montgomery later on 9 October 1944, a cable that emphasizing the \"supreme importance of Antwerp\", stating that \"the Canadian Army will not, repeat not, be able to attack until November unless immediately supplied with adequate ammunition\", and finally warning that the Allied advance into Germany would totally stop by mid-November unless Antwerp was opened in October. Montgomery replied by accusing Ramsay of making \"wild statements\" unsupported by the facts, he denied that the Canadians were having to ration ammunition, and claimed that he would soon take the Ruhr, making the Scheldt campaign a sideshow. Montgomery further issued a memo entitled \"Notes on Command in Western Europe\" demanding that he once again be made Land Forces Commander, which led to an exasperated Eisenhower telling Montgomery that the question was not the command arrangement, but rather his ability and willingness to obey orders, and that he either obey orders to clear the mouth of the Scheldt at once, or he would be sacked.\n\nA chastised Montgomery told Eisenhower on 15 October 1944 that he was now making clearing the Scheldt his \"top priority\", and the ammunition shortages in the 1st Canadian Army, a problem which he denied even existed five days earlier, were now over as supplying the Canadians was henceforth his first concern. Simonds, now reinforced with British troops and Royal Marines, cleared the Scheldt by taking Walcheren island, the last of the German \"fortresses\" on the Scheldt on 8 November 1944. With the Scheldt in Allied hands, Royal Navy minesweepers removed the German mines in the river, and Antwerp was finally opened to shipping on 28 November 1944. Reflecting Antwerp's importance, the Germans spent the winter of 1944–45 firing V-1 flying bombs and V-2 rockets at it in an attempt to shut down the port, and the German offensive in December 1944 in the Ardennes had as its ultimate objective the capture of Antwerp. Urban wrote that Montgomery's most \"serious failure\" in the entire war was not the well publicised Battle of Arnhem, but rather his lack of interest in opening up Antwerp, as without it the entire Allied advance from the North Sea to the Swiss Alps stalled in the autumn of 1944 for logistical reasons.\n\nWhen the surprise attack on the Ardennes took place on 16 December 1944, starting the Battle of the Bulge, the front of the US 12th Army Group was split, with the bulk of the US First Army being on the northern shoulder of the German 'bulge'. The 12th Army Group commander, General Omar Bradley, was located south of the penetration at Luxembourg and command of the US First Army became problematic. Montgomery was the nearest commander on the ground and on 20 December, Eisenhower (who was in Versailles in France) temporarily transferred Courtney Hodges' US First Army and William Simpson's US Ninth Army to Montgomery's 21st Army Group until the \"bulge\" could be reduced and a simpler line of communications restored, despite Bradley's vehement objections on national grounds. When Bradley learned that Montgomery had been given command of two American armies totalling some 200,000 men, he phoned Eisenhower to say: \"I cannot be responsible to the American people if you do this. I resign!\" Eisenhower sharply responded that Bradley was in fact responsible to him and \"Your resignation means absolutely nothing...Well, Brad, these are my orders.\" Montgomery grasped the situation quickly, visiting all divisional, corps, and field army commanders himself and instituting his 'Phantom' network of liaison officers. He grouped the British XXX Corps as a strategic reserve behind the Meuse and reorganised the US defence of the northern shoulder, shortening and strengthening the line and ordering the evacuation of St Vith. The German commander of the 5th Panzer Army, Hasso von Manteuffel said:\n\nThe Wehrmacht's objectives for Betrieb Wacht am Rhein (Operation Watch on the Rhine) was to split the Allied Armies in two by attacking the center of the allied armies through the Ardennes Forest in Belgium (during one of the worst storms in history) and then turning north to recapture the port at Antwerp. On the north-western side of the battle area was Montgomery's 21st Army Group which anchored the northern flank of the allied lines, with Bradley's army group on Montgomery's right flank and Patton's 3rd Army on the far right of Bradley's flank.\n\nSince SHAEF believed the Wehrmacht was no longer capable of launching a major offensive, nor that any offensive could be launched through such rugged terrain as the Ardennes Forest — particularly during winter — the Ardennes was used as an area to send US divisions, which had recently fought and sustained severe casualties, in the Battle of Hürtgen Forest to regroup and refit. It was also used as a place where new units recently from the US were sent to get some field experience in a \"safe place\".\n\nGeneral Patton's 3rd Army, which was 90 miles (145 km) to the south, switched from its mission in order to turn north and fought its way through the severe weather and German opposition and broke through to Bastogne, the bad weather cleared so that the USAAF and the RAF could resume air assault operations against NAZI armored divisions and the Wehrmacht ran out of petrol.\n\nIt is known that the Battle of the Bulge was the largest land battle fought by the western allies during all of World War II. This battle is possibly best summed up by Winston Churchill in his speech to the House of Commons:\n\nMontgomery's 21st Army Group (later) advanced to the Rhine with operations \"Veritable\" and \"Grenade\" in February 1945. A meticulously planned Rhine crossing occurred on 24 March. While successful, it was two weeks after the Americans had unexpectedly (sans meticulous planning) captured the Ludendorff Bridge at Remagen and crossed the river on 7 March — \"with less than a battalion\". Montgomery's river crossing was followed by the encirclement of German Army Group B in the Ruhr. Initially Montgomery's role was to guard the flank of the American advance. This was altered to forestall any chance of a Red Army advance into Denmark, and the 21st Army Group occupied Hamburg and Rostock and sealed off the Danish peninsula. On 4 May 1945, on Lüneburg Heath, Montgomery accepted the surrender of German forces in north-west Germany, Denmark and the Netherlands.\n\nMontgomery was notorious for his lack of tact and diplomacy. Even his \"patron,\" the Chief of the Imperial General Staff Lord Alanbrooke, frequently mentions it in his war diaries: \"he is liable to commit untold errors in lack of tact\" and \"I had to haul him over the coals for his usual lack of tact and egotistical outlook which prevented him from appreciating other people's feelings\". One incident that illustrated this occurred during the North African campaign when Montgomery bet Walter Bedell Smith that he could capture Sfax by the middle of April 1943. Smith jokingly replied that if Montgomery could do it he would give him a Flying Fortress complete with crew. Smith promptly forgot all about it, but Montgomery did not, and when Sfax was taken on 10 April he sent a message to Smith \"claiming his winnings\". Smith tried to laugh it off, but Montgomery was having none of it and insisted on his aircraft. It got as high as Eisenhower who, with his renowned skill in diplomacy, ensured Montgomery did get his Flying Fortress, though at a great cost in ill feeling. Even Alanbrooke thought it \"crass stupidity\".\n\nIn August 1945, whilst Alanbrooke, Sir Andrew Cunningham and Sir Charles Portal were discussing their possible successors as \"Chiefs Of Staff\", they concluded that Montgomery would be very efficient as CIGS from the Army's point of view but that he was also very unpopular with a large proportion of the Army. Despite this, Cunningham and Portal were strongly in favour of Montgomery succeeding Alanbrooke after his retirement. Prime Minister Winston Churchill, by all accounts a faithful friend, is quoted as saying of Montgomery, \"In defeat, unbeatable; in victory, unbearable.\"\n\nAfter the war Lord Montgomery became the Commander in Chief of the British Army of the Rhine (BAOR), the name given to the British Occupation Forces, and was the British member of the Allied Control Council. He was created 1st Viscount Montgomery of Alamein in 1946. He was Chief of the Imperial General Staff from 1946 to 1948, succeeding Alanbrooke. As CIGS, Montgomery toured Africa in 1947 and in a secret 1948 report to Attlee's government proposed a \"master plan\" to exploit the raw materials of Africa, thereby counteracting the loss of British influence in Asia. Montgomery held racist views towards Africans, describing them as \"complete savages\" incapable of developing their own countries. He was barely on speaking terms with his fellow chiefs, sending his VCIGS to attend their meetings and he clashed particularly with Sir Arthur Tedder, who was by now Chief of the Air Staff. When Montgomery's term of office expired, Prime Minister Clement Attlee appointed Sir William Slim from retirement with the rank of Field Marshal as his successor; when Montgomery protested that he had told his protégé John Crocker, a former corps commander from the 1944–45 campaign, that the job was to be his, Attlee is said to have given the memorable retort \"Untell him\".\n\nHe was then appointed Chairman of the Western European Union's commanders-in-chief committee. Volume 3 of Nigel Hamilton's \"Life of Montgomery of Alamein\" gives an account of the bickering between Montgomery and his land forces chief, a French general, which created splits through the Union headquarters. He was thus pleased to become Eisenhower's deputy in creating the North Atlantic Treaty Organisation's European forces in 1951. He would continue to serve under Eisenhower's successors, Matthew Ridgway and Al Gruenther, until his retirement, aged nearly 71, in 1958. His mother Maud, Lady Montgomery, died at New Park in Moville in Inishowen in 1949; she was buried alongside her husband in the 'kirkyard' behind St. Columb's Church, the small Church of Ireland church beside New Park, overlooking Lough Foyle. Lord Montgomery did not attend the funeral, claiming he was \"too busy\".\n\nHe was chairman of the governing body of St. John's School in Leatherhead, Surrey, from 1951 to 1966, and a generous supporter. Lord Montgomery was an Honorary Member of the Winkle Club, a noted charity in Hastings, East Sussex, and introduced Sir Winston Churchill to the club in 1955.\n\nMontgomery's memoirs (1958) criticised many of his wartime comrades in harsh terms, including Eisenhower. He was threatened with legal action by Field Marshal Auchinleck for suggesting that Auchinleck had intended to retreat from the Alamein position if attacked again, and had to give a radio broadcast (20 November 1958) expressing his gratitude to Auchinleck for having stabilised the front at the First Battle of Alamein. The 1960 paperback edition of his memoirs contains a publisher's note drawing attention to that broadcast, and stating that in the publisher's view the reader might reasonably assume from Montgomery's text that Auchinleck had been planning to retreat \"into the Nile Delta or beyond\" and pointing out that it had been Auchinleck's intention to launch an offensive as soon as the Eighth Army was \"rested and regrouped\". Montgomery was stripped of his honorary citizenship of Montgomery, Alabama, and was challenged to a duel by an Italian officer.\n\nIn retirement he publicly supported \"apartheid\" after a visit to South Africa in 1962, outraging much British liberal opinion, and after a visit to China declared himself impressed by the Chinese leadership. He spoke out against the legalisation of homosexuality in the United Kingdom, arguing that the Sexual Offences Act 1967 was a \"charter for buggery\" and that \"this sort of thing may be tolerated by the French, but we're British – thank God\". Biographer Nigel Hamilton has suggested Montgomery may have been a repressed homosexual; in the late 1940s Montgomery maintained an affectionate friendship with a 12-year-old Swiss boy. One biographer called the friendship \"bizarre\", although not \"improper\", and a sign of \"pitiful loneliness\".\n\nHe twice met with Israeli general Moshe Dayan. After an initial meeting in the early 1950s, Montgomery met Dayan again in the 1960s to discuss the Vietnam War, which Dayan was studying. Montgomery was harshly critical of US strategy in Vietnam, which involved deploying large numbers of combat troops, aggressive bombing attacks, and uprooting entire village populations and forcing them into strategic hamlets. Montgomery said that the Americans' most important problem was that they had no clear-cut objective, and allowed local commanders to set military policy. At the end of their meeting, Montgomery asked Dayan to tell the Americans, in his name, that they were \"insane\".\n\nDuring a visit to the Alamein battlefields in May 1967, he bluntly told high-ranking Egyptian Army officers that they would lose any war with Israel, a warning they ignored to their cost only a few weeks later.\n\nMontgomery died from unspecified causes in 1976 at his home Isington Mill in Isington, near Alton in Hampshire, aged 88. After his funeral at St George's Chapel, Windsor, Montgomery was interred in Holy Cross churchyard, Binsted. His Garter banner, which had hung in St. George's Chapel in Windsor during his lifetime, is now on display in St Mary's, Warwick.\n\n\n\nViscount Montgomery's ribbons as they would appear today, not including campaign or other awards.\n\n\n\n\n", "id": "3873", "title": "Bernard Montgomery"}
{"url": "https://en.wikipedia.org/wiki?curid=3874", "text": "Herman Boerhaave\n\nHerman Boerhaave (, 31 December 1668 – 23 September 1738) was a Dutch botanist, chemist, Christian humanist, and physician of European fame. He is regarded as the founder of clinical teaching and of the modern academic hospital and is sometimes referred to as \"the father of physiology,\" along with Venetian physician Santorio Santorio (1561–1636). He introduced the quantitative approach into medicine, along with his pupil Albrecht von Haller (1708–1777). He is best known for demonstrating the relation of symptoms to lesions, and he was the first to isolate the chemical urea from urine. He was the first physician to put thermometer measurements to clinical practice. His motto was Simplex sigillum veri: The simple is the sign of the true. He is often hailed as the \"Dutch Hippocrates\".\n\nBoerhaave was born at Voorhout near Leiden. The son of a Protestant pastor, in his youth Boerhaave studied for a divinity degree and wanted to become a preacher. After the death of his father, however, he was offered a scholarship and he entered the University of Leiden, where he took his degree in philosophy in 1689, with a dissertation \"De distinctione mentis a corpore\" (on the difference of the mind from the body). There he attacked the doctrines of Epicurus, Thomas Hobbes and Spinoza. He then turned to the study of medicine, in which he graduated in 1693 at Harderwijk in present-day Gelderland.\n\nIn 1701 he was appointed lecturer on the institutes of medicine at Leiden; in his inaugural discourse, \"De commendando Hippocratis studio\", he recommended to his pupils that great physician as their model. In 1709 he became professor of botany and medicine, and in that capacity he did good service, not only to his own university, but also to botanical science, by his improvements and additions to the botanic garden of Leiden, and by the publication of numerous works descriptive of new species of plants.\n\nOn 14 September 1710, Boerhaave married Maria Drolenvaux, the daughter of the rich merchant, Alderman Abraham Drolenvaux. They had four children, of whom one daughter, Maria Joanna, lived to adulthood. In 1722, he began to suffer from an extreme case of gout, recovering the next year.\n\nIn 1714, when he was appointed rector of the university, he succeeded Govert Bidloo in the chair of practical medicine, and in this capacity he introduced the modern system of clinical instruction. Four years later he was appointed to the chair of chemistry as well. In 1728 he was elected into the French Academy of Sciences, and two years later into the Royal Society of London. In 1729 declining health obliged him to resign the chairs of chemistry and botany; and he died, after a lingering and painful illness, at Leiden.\n\nHis reputation so increased the fame of the University of Leiden, especially as a school of medicine, that it became popular with visitors from every part of Europe. All the princes of Europe sent him pupils, who found in this skillful professor not only an indefatigable teacher, but an affectionate guardian. When Peter the Great went to Holland in 1716 (he was in Holland before in 1697 to instruct himself in maritime affairs), he also took lessons from Boerhaave. Voltaire traveled to see him, as did Carl Linnaeus, who became a close friend. His reputation was not confined to Europe; a Chinese mandarin sent him a letter addressed to \"the illustrious Boerhaave, physician in Europe,\" and it reached him in due course.\n\nThe operating theatre of the University of Leiden in which he once worked as an anatomist is now at the center of a museum named after him; the Boerhaave Museum. Asteroid 8175 Boerhaave is named after Boerhaave. From 1955 to 1961 Boerhaave's image was printed on Dutch 20-guilder banknotes. The Leiden University Medical Centre organises medical trainings called \"Boerhaave-courses\".\n\nHe had a prodigious influence on the development of medicine and chemistry in Scotland. British medical schools credit Boerhaave for developing the system of medical education upon which their current institutions are based. Every founding member of the Edinburgh Medical School had studied at Leyden and attended Boerhaave's lectures on chemistry including John Rutherford and Francis Home. Boerhaave's \"Elementa Chemiae (1732)\" is recognized as the first text on chemistry. \nBoerhaave first described Boerhaave syndrome, which involves tearing of the esophagus, usually a consequence of vigorous vomiting. He notoriously described in 1724 the case of Baron Jan von Wassenaer, a Dutch admiral who died of this condition following a gluttonous feast and subsequent regurgitation. This condition was uniformly fatal prior to modern surgical techniques allowing repair of the esophagus.\n\nBoerhaave was critical of his Dutch contemporary, Baruch Spinoza, attacking him in his dissertation in 1689. At the same time, he admired Isaac Newton and was a devout Christian who often wrote about God in his works. A collection of his religious thoughts on medicine, translated from Latin to English, has been compiled by the \"Sir Thomas Browne Instituut Leiden\" under the name \"Boerhaaveìs Orations\" (meaning \"Boerhaavian Prayers\"). Among other things, he considered nature as God's Creation and he used to say that the poor were his best patients because God was their paymaster.\n\nAs a credible chemist and physician of European, the human body was a very inquisitive and compelling component in agreed with other physicians of his time – like Borelli –’s mind and thus he devoted a diligent focus towards this subject matter.\n\nBoerhaave's ideas about the human body were heavily influenced by French mathematician and philosopher Rene Descartes. Descartes contributed much to iatromechanical theories. Another influencer of Boerhaave’s reasoning was Giovanni Borelli. He was a distinguished astronomer and mathematician in the 1600s and published writings on animal motions mirroring machinery principles. Inspired by this and Cartesianism, Boerhaave proposed that the human body's operations are based on a hydraulic model. In his work, he alluded to simple machines, such as levers and pulleys, as well as other mechanisms to explain parts of the body. Boerhaave described the body as made up of pipe-like structures such as are found in a machine. For example, pipes in a machine parallel the physiology of veins in the human body. Boerhaave took this mechanistic notion even further by noting the importance of a proper balance of fluid pressure. Fluids within the body should be able to move around freely, without obstacles. Similar to a machine, a well-being was self-regulating. The body must maintain these conditions to be a healthy equilibrium state. Boerhaave’s view on medicine accepted this apparatus-like body philosophy and thus focused attention on materialistic problems rather than mystical explanations of illness.\n\nBoerhaave stressed the atomically research conceived on sense experiences and scientific experiments. Boerhaave attracted many students to Leiden University, where he taught his perception and philosophy. Boerhaave notion of the systematic body burgeoned throughout Europe and consequently aiding in the transformation of medical educations in European schools\nBoerhaave intuition struck up a great deal of interest among other critical medical thinkers – one being Friedrich Hoffmann who emphasized using physico-mechanical principles to preserve and, if needed, restore health. As a professor at Leiden, Boerhaave influenced many of his students. Some of his followers conducted experiments to bolster Boerhaave’s philosophy, while other physicians rejected and proposed alternative notions about the human body. Boerhaave also contributed a great number of textbooks and writings that circulated Europe. The enrichment extracted from his lectures at Leiden. In 1708, his publication of the Institutes of Medicine was issued in over five languages with approximately ten editions. Elementa Chemia, a world-renounced chemistry textbook was published in 1732.\n\nViewing the body of a mechanism deviated from the path that Galen and Aristotle previously laid down. Instead of solely relying on past finding, Boerhaave understood the importance of seeking fixed results on his own and experiencing the findings first hand. This new thinking augmented Renaissance anatomy and opened doors of novel reasoning leading up to the medical enlightenment germane to iatrochemistry.\n\n\n\n", "id": "3874", "title": "Herman Boerhaave"}
{"url": "https://en.wikipedia.org/wiki?curid=3875", "text": "Benjamin Disraeli\n\nBenjamin Disraeli, 1st Earl of Beaconsfield, (21 December 1804 – 19 April 1881) was a British politician and writer who twice served as Prime Minister of the United Kingdom. He played a central role in the creation of the modern Conservative Party, defining its policies and its broad outreach. Disraeli is remembered for his influential voice in world affairs, his political battles with the Liberal Party leader William Ewart Gladstone, and his one-nation conservatism or \"Tory democracy\". He made the Conservatives the party most identified with the glory and power of the British Empire. He is the only British Prime Minister of Jewish birth.\n\nDisraeli was born in Bloomsbury, then part of Middlesex. His father left Judaism after a dispute at his synagogue; young Benjamin became an Anglican at the age of 12. After several unsuccessful attempts, Disraeli entered the House of Commons in 1837. In 1846 the Prime Minister, Sir Robert Peel split the party over his proposal to repeal the Corn Laws, which involved ending the tariff on imported grain. Disraeli clashed with Peel in the Commons. Disraeli became a major figure in the party. When Lord Derby, the party leader, thrice formed governments in the 1850s and 1860s, Disraeli served as Chancellor of the Exchequer and Leader of the House of Commons. He also forged a bitter rivalry with Gladstone of the Liberal Party.\n\nUpon Derby's retirement in 1868, Disraeli became Prime Minister briefly before losing that year's election. He returned to opposition, before leading the party to a majority in the 1874 election. He maintained a close friendship with Queen Victoria, who in 1876 created him Earl of Beaconsfield. Disraeli's second term was dominated by the Eastern Question—the slow decay of the Ottoman Empire and the desire of other European powers, such as Russia, to gain at its expense. Disraeli arranged for the British to purchase a major interest in the Suez Canal Company (in Ottoman-controlled Egypt). In 1878, faced with Russian victories against the Ottomans, he worked at the Congress of Berlin to obtain peace in the Balkans at terms favourable to Britain and unfavourable to Russia, its longstanding enemy. This diplomatic victory over Russia established Disraeli as one of Europe's leading statesmen.\n\nWorld events thereafter moved against the Conservatives. Controversial wars in Afghanistan and South Africa undermined his public support. He angered British farmers by refusing to reinstitute the Corn Laws in response to poor harvests and cheap imported grain. With Gladstone conducting a massive speaking campaign, his Liberals bested Disraeli's Conservatives in the 1880 election. In his final months, Disraeli led the Conservatives in opposition. He had throughout his career written novels, beginning in 1826, and he published his last completed novel, \"Endymion\", shortly before he died at the age of 76.\n\nDisraeli was born on 21 December 1804 at 6 King's Road, Bedford Row, Bloomsbury, London, the second child and eldest son of Isaac D'Israeli, a literary critic and historian, and Maria (Miriam), \"née\" Basevi. The family was of Sephardic Jewish Italian mercantile background. All Disraeli's grandparents and great grandparents were born in Italy; Isaac's father, Benjamin, moved to England from Venice in 1748. Disraeli later romanticised his origins, claiming that his father's family was of grand Spanish and Venetian descent; in fact Isaac's family was of no great distinction, but on Disraeli's mother's side, in which he took no interest, there were some distinguished forebears. Historians differ on Disraeli's motives for rewriting his family history: Bernard Glassman argues that it was intended to give him status comparable to that of England's ruling élite; Sarah Bradford believes \"his dislike of the commonplace would not allow him to accept the facts of his birth as being as middle class and undramatic as they really were\".\nDisraeli's siblings were Sarah (1802–1859), Naphtali (born and died 1807), Ralph (1809–1898), and James (\"Jem\") (1813–1868). He was close to his sister, and on affectionate but more distant terms with his surviving brothers. Details of his schooling are sketchy. From the age of about six he was a day boy at a dame school in Islington that one of his biographers later described as \"for those days a very high-class establishment\". Two years later or so—the exact date has not been ascertained—he was sent as a boarder to Rev John Potticary's St Piran's school at Blackheath. While he was there events at the family home changed the course of Disraeli's education and of his whole life: his father renounced Judaism and had the four children baptised into the Church of England in July and August 1817.\n\nIsaac D'Israeli had never taken religion very seriously, but had remained a conforming member of the Bevis Marks Synagogue. His father, the elder Benjamin, was a prominent and devout member; it was probably from respect for him that Isaac did not leave when he fell out with the synagogue authorities in 1813. After Benjamin senior died in 1816 Isaac felt free to leave the congregation following a second dispute. Isaac's friend Sharon Turner, a solicitor, convinced him that although he could comfortably remain unattached to any formal religion it would be disadvantageous to the children if they did so. Turner stood as godfather when Benjamin was baptised, aged twelve, on 31 July 1817.\n\nConversion to Christianity enabled Disraeli to contemplate a career in politics. Britain in the early 19th century was not a greatly anti-Semitic society, and there had been Members of Parliament (MPs) from Jewish families since Samson Gideon in 1770. But until 1858 MPs were required to take the oath of allegiance \"on the true faith of a Christian\", necessitating at least nominal conversion. It is not known whether Disraeli formed any ambition for a parliamentary career at the time of his baptism, but there is no doubt that he bitterly regretted his parents' decision not to send him to Winchester College. As one of the great public schools of England, Winchester consistently provided recruits to the political élite. His two younger brothers were sent there, and it is not clear why Isaac D'Israeli chose to send his eldest son to a much less prestigious school. The boy evidently held his mother responsible for the decision; Bradford speculates that \"Benjamin's delicate health and his obviously Jewish appearance may have had something to do with it.\" The school chosen for him was run by Eliezer Cogan at Higham Hill in Walthamstow. He began there in the autumn term of 1817; he later recalled his education:\nIn November 1821, shortly before his seventeenth birthday, Disraeli was articled as a clerk to a firm of solicitors—Swain, Stevens, Maples, Pearse and Hunt—in the City of London. T F Maples was not only the young Disraeli's employer and friend of his father's, but also his prospective father-in-law: Isaac and Maples entertained the possibility that the latter's only daughter might be a suitable match for Benjamin. A friendship developed, but there was no romance. The firm had a large and profitable business, and as the biographer R W Davis observes, the clerkship was \"the kind of secure, respectable position that many fathers dream of for their children\". Although biographers including Robert Blake and Bradford comment that such a post was incompatible with Disraeli's romantic and ambitious nature, he reportedly gave his employers satisfactory service, and later professed to have learned a good deal from his time with the firm. He recalled, \"I had some scruples, for even then I dreamed of Parliament. My father's refrain always was 'Philip Carteret Webb', who was the most eminent solicitor of his boyhood and who was an MP. It would be a mistake to suppose that the two years and more that I was in the office of our friend were wasted. I have often thought, though I have often regretted the University, that it was much the reverse.\"\nThe year after joining Maples's firm, Benjamin changed his surname from D'Israeli to Disraeli. His reasons for doing so are unknown, but the biographer Bernard Glassman surmises that it was to avoid being confused with his father. Disraeli's sister and brothers adopted the new version of the name; Isaac and his wife retained the older form.\n\nDisraeli toured Belgium and the Rhine Valley with his father in the summer of 1824; he later wrote that it was while travelling on the Rhine that he decided to abandon his position: \"I determined when descending those magical waters that I would not be a lawyer.\" On their return to England he left the solicitors, at the suggestion of Maples, with the aim of qualifying as a barrister. He enrolled as a student at Lincoln's Inn and joined the chambers of his uncle, Nathaniel Basevy, and then those of Benjamin Austen, who persuaded Isaac that Disraeli would never make a barrister and should be allowed to pursue a literary career. He had made a tentative start: in May 1824 he submitted a manuscript to his father's friend, the publisher John Murray, but withdrew it before Murray could decide whether to publish it. Released from the law, Disraeli did some work for Murray, but turned most of his attention not to literature but to speculative dealing on the stock exchange.\n\nThere was at the time a boom in shares in South American mining companies. Spain was losing its South American colonies in the face of rebellions. At the urging of George Canning the British government recognised the new independent governments of Argentina (1824), Colombia and Mexico (both 1825). With no money of his own, Disraeli borrowed money to invest. He became involved with the financier J. D. Powles, who was prominent among those encouraging the mining boom. In the course of 1825, Disraeli wrote three anonymous pamphlets for Powles, promoting the companies. The pamphlets were published by John Murray, who invested heavily in the boom.\n\nMurray had for some time had ambitions to establish a new morning paper to compete with \"The Times\". In 1825 Disraeli convinced him that he should proceed. The new paper, \"The Representative\", promoted the mines and those politicians who supported them, particularly Canning. Disraeli impressed Murray with his energy and commitment to the project, but he failed in his key task of persuading the eminent writer John Gibson Lockhart to edit the paper. After that, Disraeli's influence on Murray waned, and to his resentment he was sidelined in\nthe affairs of \"The Representative\". The paper survived for only six months, partly because the mining bubble burst in late 1825, and partly because, according to Blake, the paper was \"atrociously edited\", and would have failed regardless.\n\nThe bursting of the mining bubble was ruinous for Disraeli. By June 1825 he and his business partners had lost £7,000. Disraeli could not pay off the last of his debts from this debacle until 1849. He turned to writing, motivated partly by his desperate need for money, and partly by a wish for revenge on Murray and others by whom he felt slighted. There was a vogue for what was called \"silver-fork fiction\"—novels depicting aristocratic life, usually by anonymous authors, read avidly by the aspirational middle classes. Disraeli's first novel, \"Vivian Grey\", published anonymously in four volumes in 1826–27, was a thinly veiled re-telling of the affair of \"The Representative\". It sold well, but caused much offence in influential circles when the authorship was discovered. Disraeli, then just twenty-three, did not move in high society, as the numerous solecisms in his book made obvious. Reviewers were sharply critical on these grounds of both the author and the book. Furthermore, Murray and Lockhart, men of great influence in literary circles, believed that Disraeli had caricatured them and abused their confidence—an accusation denied by the author but repeated by many of his biographers. In later editions Disraeli made many changes, softening his satire, but the damage to his reputation proved long-lasting.\n\nDisraeli's biographer Jonathan Parry writes that the financial failure and personal criticism that Disraeli suffered in 1825 and 1826 were probably the trigger for a serious nervous crisis affecting him over the next four years: \"He had always been moody, sensitive, and solitary by nature, but now became seriously depressed and lethargic.\" He was still living with his parents in London, but in search of the \"change of air\" recommended by the family's doctors Isaac took a succession of houses in the country and on the coast, before Disraeli sought wider horizons.\n\nTogether with his sister's fiancé, William Meredith, Disraeli travelled widely in southern Europe and beyond in 1830–31. The trip was financed partly by another high society novel, \"The Young Duke\", written in 1829–30. The tour was cut short suddenly by Meredith's death from smallpox in Cairo in July 1831. Despite this tragedy, and the need for treatment for a sexually transmitted disease on his return, Disraeli felt enriched by his experiences. He became, in Parry's words, \"aware of values that seemed denied to his insular countrymen. The journey encouraged his self-consciousness, his moral relativism, and his interest in Eastern racial and religious attitudes.\" Blake regards the tour as one of the formative experiences of Disraeli's whole career: \"[T]he impressions that it made on him were life-lasting. They conditioned his attitude toward some of the most important political problems which faced him in his later years—especially the Eastern Question; they also coloured many of his novels.\"\n\nDisraeli wrote two novels in the aftermath of the tour. \"Contarini Fleming\" (1832) was avowedly a self-portrait. It is subtitled \"a psychological autobiography\", and depicts the conflicting elements of its hero's character: the duality of northern and Mediterranean ancestry, the dreaming artist and the bold man of action. As Parry observes, the book ends on a political note, setting out Europe's progress \"from feudal to federal principles\". \"The Wondrous Tale of Alroy\" the following year portrayed the problems of a medieval Jew in deciding between a small, exclusively Jewish state and a large empire embracing all.\nAfter the two novels were published, Disraeli declared that he would \"write no more about myself\". He had already turned his attention to politics in 1832, during the great crisis over the Reform Bill. He contributed to an anti-Whig pamphlet edited by John Wilson Croker and published by Murray entitled \"England and France: or a cure for Ministerial Gallomania\". The choice of a Tory publication was regarded as strange by Disraeli's friends and relatives, who thought him more of a Radical. Indeed, he had objected to Murray about Croker's inserting \"high Tory\" sentiment: Disraeli remarked, \"it is quite impossible that anything adverse to the general measure of Reform can issue from my pen.\" Moreover, at the time \"Gallomania\" was published, Disraeli was electioneering in High Wycombe in the Radical interest.\n\nDisraeli's politics at the time were influenced both by his rebellious streak and by his desire to make his mark. At that time, the politics of the nation were dominated by members of the aristocracy, together with a few powerful commoners. The Whigs derived from the coalition of lords who had forced through the Bill of Rights in 1689 and in some cases were their actual descendants, not merely spiritual. The Tories tended to support King and Church, and sought to thwart political change. A small number of Radicals, generally from northern constituencies, were the strongest advocates of continuing reform. In the early 1830s the Tories and the interests they represented appeared to be a lost cause. The other great party, the Whigs, were anathema to Disraeli: \"Toryism is worn out & I cannot condescend to be a Whig.\" There were two general elections in 1832; Disraeli unsuccessfully stood as a Radical at High Wycombe in each.\n\nDisraeli's political views embraced certain Radical policies, particularly democratic reform of the electoral system, and also some Tory ones, including protectionism. He began to move in Tory circles. In 1834 he was introduced to the former Lord Chancellor, Lord Lyndhurst, by Henrietta Sykes, wife of Sir Francis Sykes. She was having an affair with Lyndhurst, and began another with Disraeli. Disraeli and Lyndhurst took an immediate liking to each other. Lyndhurst was an indiscreet gossip with a fondness for intrigue; this appealed greatly to Disraeli, who became his secretary and go-between. In 1835 Disraeli stood for the last time as a Radical, unsuccessfully contesting High Wycombe once again.\nIn April 1835 Disraeli fought a by-election at Taunton as a Tory. The Irish MP Daniel O'Connell, misled by inaccurate press reports, thought Disraeli had slandered him while electioneering at Taunton; he launched an outspoken attack, referring to Disraeli as:\nDisraeli's public exchanges with O'Connell, extensively reproduced in \"The Times\", included a demand for a duel with the 60-year-old O'Connell's son (which resulted in Disraeli's temporary detention by the authorities), a reference to \"the inextinguishable hatred with which [he] shall pursue [O'Connell's] existence\", and the accusation that O'Connell's supporters had a \"princely revenue wrung from a starving race of fanatical slaves\". Disraeli was highly gratified by the dispute, which propelled him to general public notice for the first time. He did not defeat the incumbent Whig member, Henry Labouchere, but the Taunton constituency was regarded as unwinnable by the Tories. Disraeli kept Labouchere's majority down to 170, a good showing that put him in line for a winnable seat in the near future.\n\nWith Lyndhurst's encouragement Disraeli turned to writing propaganda for his newly adopted party. His \"Vindication of the English Constitution\", was published in December 1835. It was couched in the form of an open letter to Lyndhurst, and in Bradford's view encapsulates a political philosophy that Disraeli adhered to for the rest of his life. Its themes were the value of benevolent aristocratic government, a loathing of political dogma, and the modernisation of Tory policies. The following year he wrote a series of satires on politicians of the day, which he published in \"The Times\" under the pen-name \"Runnymede\". His targets included the Whigs, collectively and individually, Irish nationalists, and political corruption. One essay ended:\n\nDisraeli was now firmly in the Tory camp. He was elected to the exclusively Tory Carlton Club in 1836, and was also taken up by the party's leading hostess, Lady Londonderry. In June 1837 WilliamIV died, the young Queen Victoria, his niece, succeeded him, and parliament was dissolved. On the recommendation of the Carlton Club, Disraeli was adopted as a Tory parliamentary candidate at the ensuing General Election.\n\nIn the election in July 1837 Disraeli won a seat in the House of Commons as one of two members, both Tory, for the constituency of Maidstone. The other was Wyndham Lewis, who helped finance Disraeli's election campaign, and who died the following year. In the same year Disraeli published a novel, \"Henrietta Temple\", which was a love story and social comedy, drawing on his affair with Henrietta Sykes. He had broken off the relationship in late 1836, distraught that she had taken yet another lover. His other novel of this period is \"Venetia\", a romance based on the characters of Shelley and Byron, written quickly to raise much-needed money.\n\nDisraeli made his maiden speech in Parliament on 7 December 1837. He followed O'Connell, whom he sharply criticised for the latter's \"long, rambling, jumbling, speech\". He was shouted down by O'Connell's supporters. After this unpromising start Disraeli kept a low profile for the rest of the parliamentary session. He was a loyal supporter of the party leader Sir Robert Peel and his policies, with the exception of a personal sympathy for the Chartist movement that most Tories did not share.\nIn 1839 Disraeli married Mary Anne Lewis, the widow of Wyndham Lewis. Twelve years Disraeli's senior, Mary Lewis had a substantial income of £5,000 a year. His motives were generally assumed to be mercenary, but the couple came to cherish one another, remaining close until she died more than three decades later. \"Dizzy married me for my money\", his wife said later, \"But, if he had the chance again, he would marry me for love.\"\n\nFinding the financial demands of his Maidstone seat too much, Disraeli secured a Tory nomination for Shrewsbury, winning one of the constituency's two seats at the 1841 general election, despite serious opposition, and heavy debts which opponents seized on. The election was a massive defeat for the Whigs across the country, and Peel became Prime Minister. Disraeli hoped, unrealistically, for ministerial office. Though disappointed at being left on the back benches, he continued his support for Peel in 1842 and 1843, seeking to establish himself as an expert on foreign affairs and international trade.\n\nAlthough a Tory (or Conservative, as some in the party now called themselves) Disraeli was sympathetic to some of the aims of Chartism, and argued for an alliance between the landed aristocracy and the working class against the increasing power of the merchants and new industrialists in the middle class. After Disraeli won widespread acclaim in March 1842 for worsting the formidable Lord Palmerston in debate, he was taken up by a small group of idealistic new Tory MPs, with whom he formed the Young England group. They held that the landed interests should use their power to protect the poor from exploitation by middle-class businessmen.\n\nFor many years in his parliamentary career Disraeli hoped to forge a paternalistic Tory-Radical alliance, but he was unsuccessful. Before the Reform Act 1867, the working class did not possess the vote and therefore had little political power. Although Disraeli forged a personal friendship with John Bright, a Lancashire manufacturer and leading Radical, Disraeli was unable to persuade Bright to sacrifice his distinct position for parliamentary advancement. When Disraeli attempted to secure a Tory-Radical cabinet in 1852, Bright refused.\n\nDisraeli gradually became a sharp critic of Peel's government, often deliberately taking positions contrary to those of his nominal chief. The best known of these stances were over the Maynooth Grant in 1845 and the repeal of the Corn Laws in 1846. But the young MP had attacked his leader as early as 1843 on Ireland and then on foreign policy interventions. In a letter of February 1844, he slighted the Prime Minister for failing to send him a Policy Circular. He laid into the Whigs as freebooters, swindlers and conmen but Peel's own Free Trade policies were directly in the firing line.\n\nThe President of the Board of Trade, William Gladstone, resigned from the cabinet over the Maynooth Grant. The Corn Laws imposed a tariff on imported wheat, protecting British farmers from foreign competition, but making the cost of bread artificially high. Peel hoped that the repeal of the Corn Laws and the resultant influx of cheaper wheat into Britain would relieve the condition of the poor, and in particular the suffering caused by successive failure of potato crops in Ireland—the Great Famine.\n\nThe first months of 1846 were dominated by a battle in Parliament between the free traders and the protectionists over the repeal of the Corn Laws, with the latter rallying around Disraeli and Lord George Bentinck. The landowning interest in the Party, under its leaderWilliam Miles MP for East Somerset, had called upon Disraeli to lead the Party. Disraeli had declined, though pledged support to the Country Gentlemen's Interes, as Bentink had offered to lead if he had Disraeli's support. Disraeli stated, in a letter to Sir William Miles of 11 June 1860, that he wished to help \"because, from my earliest years, my sympathies had been with the landed interest of England\".\n\nAn alliance of free-trade Conservatives (the \"Peelites\"), Radicals, and Whigs carried repeal, and the Conservative Party split: the Peelites moved towards the Whigs, while a \"new\" Conservative Party formed around the protectionists, led by Disraeli, Bentinck, and Lord Stanley (later Lord Derby).\n\nThe split in the Tory party over the repeal of the Corn Laws had profound implications for Disraeli's political career: almost every Tory politician with experience of office followed Peel, leaving the rump bereft of leadership. In Blake's words, \"[Disraeli] found himself almost the only figure on his side capable of putting up the oratorical display essential for a parliamentary leader.\" Looking on from the House of Lords, the Duke of Argyll wrote that Disraeli \"was like a subaltern in a great battle where every superior officer was killed or wounded\". If the Tory Party could muster the electoral support necessary to form a government, then Disraeli now seemed to be guaranteed high office. However, he would take office with a group of men who possessed little or no official experience, who had rarely felt moved to speak in the House of Commons, and who, as a group, remained hostile to Disraeli on a personal level. In the event the matter was not put to the test, as the Tory split soon had the party out of office, not regaining power until 1852. The Conservatives would not again have a majority in the House of Commons until 1874.\n\nPeel successfully steered the repeal of the Corn Laws through Parliament, and was then defeated by an alliance of all his enemies on the issue of Irish law and order; he resigned in June 1846. The Tories remained split and the Queen sent for Lord John Russell, the Whig leader. In the 1847 general election, Disraeli stood, successfully, for the Buckinghamshire constituency. The new House of Commons had more Conservative than Whig members, but the depth of the Tory schism enabled Russell to continue to govern. The Conservatives were led by Bentinck in the Commons and Stanley in the Lords.\nIn 1847 a small political crisis occurred which removed Bentinck from the leadership and highlighted Disraeli's differences with his own party. In that year's general election, Lionel de Rothschild had been returned for the City of London. As a practising Jew he could not take the oath of allegiance in the prescribed Christian form, and therefore could not take his seat. Lord John Russell, the Whig leader who had succeeded Peel as Prime Minister and like Rothschild was a member for the City of London, proposed in the Commons that the oath should be amended to permit Jews to enter Parliament.\n\nDisraeli spoke in favour of the measure, arguing that Christianity was \"completed Judaism\", and asking the House of Commons \"Where is your Christianity if you do not believe in their Judaism?\" Russell and Disraeli's future rival Gladstone thought it brave of him to speak as he did; the speech was badly received by his own party. The Tories and the Anglican establishment were hostile to the bill. Samuel Wilberforce, Bishop of Oxford, spoke strongly against the measure and implied that Russell was paying off the Jews for helping elect him. With the exception of Disraeli, every member of the future protectionist cabinet then in Parliament voted against the measure. One who was not yet an MP, Lord John Manners, stood against Rothschild when the latter re-submitted himself for election in 1849. Disraeli who had attended the Protectionists dinner at the Merchant Taylors Hall, joined Bentinck in speaking and voting for the bill, although his own speech was a standard one of toleration. The measure was voted down.\n\nIn the aftermath of the debate Bentinck resigned the leadership and was succeeded by Lord Granby; Disraeli's own speech, thought by many of his own party to be blasphemous, ruled him out for the time being. While these intrigues played out, Disraeli was working with the Bentinck family to secure the necessary financing to purchase Hughenden Manor, in Buckinghamshire. The possession of a country house, and incumbency of a county constituency were regarded as essential for a Tory with ambitions to lead the party. Disraeli and his wife alternated between Hughenden and several homes in London for the rest of their marriage. The negotiations were complicated by Bentinck's sudden death on 21 September 1848, but Disraeli obtained a loan of £25,000 from Bentinck's brothers Lord Henry Bentinck and Lord Titchfield.\n\nWithin a month of his appointment Granby resigned the leadership in the Commons, feeling himself inadequate to the post, and the party functioned without a leader in the Commons for the rest of the parliamentary session. At the start of the next session, affairs were handled by a triumvirate of Granby, Disraeli, and John Charles Herries—indicative of the tension between Disraeli and the rest of the party, who needed his talents but mistrusted him. This confused arrangement ended with Granby's resignation in 1851; Disraeli effectively ignored the two men regardless.\n\nIn March 1851, Lord John Russell's government was defeated over a bill to equalise the county and borough franchises, mostly because of divisions among his supporters. He resigned, and the Queen sent for Stanley, who felt that a minority government could do little and would not last long, so Russell remained in office. Disraeli regretted this, hoping for an opportunity, however brief, to show himself capable in office. Stanley, on the other hand, deprecated his inexperienced followers as a reason for not assuming office, \"These are not names I can put before the Queen.\"\n\nAt the end of June 1851, Stanley's father died, and he succeeded to his title as Earl of Derby. The Whigs were wracked by internal dissensions during the second half of 1851, much of which Parliament spent in recess. Russell dismissed Lord Palmerston from the cabinet, leaving the latter determined to deprive the Prime Minister of office as well. Palmerston did so within weeks of Parliament's reassembly on 4 February 1852, his followers combining with Disraeli's Tories to defeat the government on a Militia Bill, and Russell resigned. Derby had either to take office or risk damage to his reputation and he accepted the Queen's commission as Prime Minister. Palmerston declined any office; Derby had hoped to have him as Chancellor of the Exchequer. Disraeli, his closest ally, was his second choice and accepted, though disclaiming any great knowledge in the financial field. Gladstone refused to join the government. Disraeli may have been attracted to the office by the £5,000 per year salary, which would help pay his debts. Few of the new cabinet had held office before; when Derby tried to inform the Duke of Wellington of the names of the Queen's new ministers, the old Duke, who was somewhat deaf, inadvertently branded the new government by incredulously repeating \"Who? Who?\"\n\nIn the following weeks, Disraeli served as Leader of the House (with Derby as Prime Minister in the Lords) and as Chancellor. He wrote regular reports on proceedings in the Commons to Victoria, who described them as \"very curious\" and \"much in the style of his books\". Parliament was prorogued on 1 July 1852 as the Tories could not govern for long as a minority; Disraeli hoped that they would gain a majority of about 40. Instead, the election later that month had no clear winner, and the Derby government held to power pending the meeting of Parliament.\n\nDisraeli's task as Chancellor was to devise a budget which would satisfy the protectionist elements who supported the Tories, without uniting the free-traders against it. His proposed budget, which he presented to the Commons on 3 December, lowered the taxes on malt and tea, provisions designed to appeal to the working class. To make his budget revenue-neutral, as funds were needed to provide defences against the French, he doubled the house tax and continued the income tax. Disraeli's overall purpose was to enact policies which would benefit the working classes, making his party more attractive to them. Although the budget did not contain protectionist features, the opposition was prepared to destroy it—and Disraeli's career as Chancellor—in part out of revenge for his actions against Peel in 1846. MP Sidney Herbert predicted that the budget would fail because \"Jews make no converts\".\nDisraeli delivered the budget on 3 December 1852, and prepared to wind up the debate for the government on 16 December—it was customary for the Chancellor to have the last word. A massive defeat for the government was predicted. Disraeli attacked his opponents individually, and then as a force, \"I face a Coalition ... This, too, I know, that England does not love coalitions.\" His speech of three hours was quickly seen as a parliamentary masterpiece. As MPs prepared to divide, Gladstone rose to his feet and began an angry speech, despite the efforts of Tory MPs to shout him down. The interruptions were fewer, as Gladstone gained control of the House, and in the next two hours painted a picture of Disraeli as frivolous and his budget as subversive. The government was defeated by 19 votes, and Derby resigned four days later. He was replaced by the Peelite Earl of Aberdeen, with Gladstone as his Chancellor. Because of Disraeli's unpopularity among the Peelites, no party reconciliation was possible while he remained Tory leader in the House of Commons.\n\nWith the fall of the government, Disraeli and the Conservatives returned to the opposition benches. Disraeli would spend three-quarters of his 44-year parliamentary career in opposition. Derby was reluctant to seek to unseat the government, fearing a repetition of the Who? Who? Ministry and knowing that despite his lieutenant's strengths, shared dislike of Disraeli was part of what had formed the governing coalition. Disraeli, on the other hand, was anxious to return to office. In the interim, Disraeli, as Conservative leader in the Commons, opposed the government on all major measures.\n\nIn June 1853 Disraeli was awarded an honorary degree by Oxford University. He had been recommended for it by Lord Derby, the university's Chancellor. The start of the Crimean War in 1854 caused a lull in party politics; Disraeli spoke patriotically in support. The British military efforts were marked by bungling, and in 1855 a restive Parliament considered a resolution to establish a committee on the conduct of the war. The Aberdeen government chose to make this a motion of confidence; Disraeli led the opposition to defeat the government, 305 to 148. Aberdeen resigned, and the Queen sent for Derby, who to Disraeli's frustration refused to take office. Palmerston was deemed essential to any Whig ministry, and he would not join any he did not head. The Queen reluctantly asked Palmerston to form a government. Under Palmerston, the war went better, and was ended by the Treaty of Paris in early 1856. Disraeli was early to call for peace, but had little influence on events.\n\nWhen a rebellion broke out in India in 1857, Disraeli took a keen interest in affairs, having been a member of a select committee in 1852 which considered how best to rule the subcontinent, and had proposed eliminating the governing role of the British East India Company. After peace was restored, and Palmerston in early 1858 brought in legislation for direct rule of India by the Crown, Disraeli opposed it. Many Conservative MPs refused to follow him and the bill passed the Commons easily.\n\nPalmerston's grip on the premiership was weakened by his response to the Orsini affair, in which an attempt was made to assassinate the French Emperor Napoleon III by an Italian revolutionary with a bomb made in Birmingham. At the request of the French ambassador, Palmerston put forward amendments to the conspiracy to murder statute, proposing to make creating an infernal device a felony rather than a misdemeanour. He was defeated by 19 votes on the second reading, with many Liberals crossing the aisle against him. He immediately resigned, and Lord Derby returned to office.\n\nDerby took office at the head of a purely \"Conservative\" administration, not in coalition with any other faction. He again offered a place to Gladstone, who declined. Disraeli was once more leader of the House of Commons and returned to the Exchequer. As in 1852, Derby led a minority government, dependent on the division of its opponents for survival. As Leader of the House, Disraeli resumed his regular reports to Queen Victoria, who had requested that he include what she \"could not meet in newspapers\".\n\nDuring its brief life of just over a year, the Derby government proved moderately progressive. The Government of India Act 1858 ended the role of the East India Company in governing the subcontinent. Disraeli had supported efforts to allow Jews to sit in Parliament—the oaths required of new members could only be made in good faith by a Christian. Disraeli had a bill passed through the Commons allowing each house of Parliament to determine what oaths its members should take. This was grudgingly agreed to by the House of Lords, with a minority of Conservatives joining with the opposition to pass it. In 1858, Baron Lionel de Rothschild became the first MP to profess the Jewish faith.\n\nFaced with a vacancy, Disraeli and Derby tried yet again to bring Gladstone, still nominally a Conservative MP, into the government, hoping to strengthen it. Disraeli wrote a personal letter to Gladstone, asking him to place the good of the party above personal animosity: \"Every man performs his office, and there is a Power, greater than ourselves, that disposes of all this.\" In responding to Disraeli, Gladstone denied that personal feelings played any role in his decisions then and previously whether to accept office, while acknowledging that there were differences between him and Derby \"broader than you may have supposed\".\n\nThe Tories pursued a Reform Bill in 1859, which would have resulted in a modest increase to the franchise. The Liberals were healing the breaches between those who favoured Russell and the Palmerston loyalists, and in late March 1859, the government was defeated on a Russell-sponsored amendment. Derby dissolved Parliament, and the ensuing general election resulted in modest Tory gains, but not enough to control the Commons. When Parliament assembled, Derby's government was defeated by 13 votes on an amendment to the Address from the Throne. He resigned, and the Queen reluctantly sent for Palmerston again.\n\nAfter Derby's second ejection from office, Disraeli faced dissension within Conservative ranks from those who blamed him for the defeat, or who felt he was disloyal to Derby—the former Prime Minister warned Disraeli of some MPs seeking his removal from the front bench. Among the conspirators were Lord Robert Cecil, a young Conservative MP who would a quarter century later become Prime Minister as Lord Salisbury; he wrote that having Disraeli as leader in the Commons decreased the Conservatives' chance of holding office. When Cecil's father objected, Lord Robert stated, \"I have merely put into print what all the country gentlemen were saying in private.\"\nDisraeli led a toothless opposition in the Commons—seeing no way of unseating Palmerston, Derby had privately agreed not to seek the government's defeat. Disraeli kept himself informed on foreign affairs, and on what was going on in cabinet, thanks to a source within it. When the American Civil War began in 1861, Disraeli said little publicly, but like most Englishmen expected the South to win. Less reticent were Palmerston, Gladstone (again Chancellor) and Russell, whose statements in support of the South contributed to years of hard feelings in the United States. In 1862, Disraeli met Prussian Count Otto von Bismarck for the first time and said of him, \"be careful about that man, he means what he says\".\n\nThe party truce ended in 1864, with Tories outraged over Palmerston's handling of the territorial dispute between the German Confederation and Denmark known as the Schleswig-Holstein Question. Disraeli had little help from Derby, who was ill, but he united the party enough on a no-confidence vote to limit the government to a majority of 18—Tory defections and absentees kept Palmerston in office. Despite rumours about Palmerston's health as he passed his eightieth birthday, he remained personally popular, and the Liberals increased their margin in the July 1865 general election. In the wake of the poor election results, Derby predicted to Disraeli that neither of them would ever hold office again.\n\nPolitical plans were thrown into disarray by Palmerston's death on 18 October 1865. Russell became Prime Minister again, with Gladstone clearly the Liberal Party's leader-in-waiting, and as Leader of the House Disraeli's direct opponent. One of Russell's early priorities was a Reform Bill, but the proposed legislation that Gladstone announced on 12 March 1866 divided his party. The Conservatives and the dissident Liberals repeatedly attacked Gladstone's bill, and in June finally defeated the government; Russell resigned on 26 June. The dissidents were unwilling to serve under Disraeli in the House of Commons, and Derby formed a third Conservative minority government, with Disraeli again as Chancellor. In 1867, the Conservatives introduced a Reform Bill. Without a majority in the Commons, the Conservatives had little choice but to accept amendments that considerably liberalised the legislation, though Disraeli refused to accept any from Gladstone.\n\nThe Reform Act 1867 passed that August, extending the franchise by 938,427—an increase of 88%—by giving the vote to male householders and male lodgers paying at least £10 for rooms. It eliminated rotten boroughs with fewer than 10,000 inhabitants, and granted constituencies to 15 unrepresented towns, with extra representation to large municipalities such as Liverpool and Manchester. This act was unpopular with the right wing of the Conservative Party, most notably Lord Cranborne (as Robert Cecil was by then known), who resigned from the government and spoke against the bill, accusing Disraeli of \"a political betrayal which has no parallel in our Parliamentary annals\". Cranborne, however, was unable to lead an effective rebellion against Derby and Disraeli. Disraeli gained wide acclaim and became a hero to his party for the \"marvellous parliamentary skill\" with which he secured the passage of Reform in the Commons.\n\nDerby had long suffered from attacks of gout which sent him to his bed, unable to deal with politics. As the new session of Parliament approached in February 1868, he was bedridden at his home, Knowsley Hall, near Liverpool. He was reluctant to resign, reasoning that he was only 68, much younger than either Palmerston or Russell at the end of their premierships. Derby knew that his \"attacks of illness would, at no distant period, incapacitate me from the discharge of my public duties\"; doctors had warned him that his health required his resignation from office. In late February, with Parliament in session and Derby absent, he wrote to Disraeli asking for confirmation that \"you will not shrink from the additional heavy responsibility\". Reassured, he wrote to the Queen, resigning and recommending Disraeli as \"only he could command the cordial support, en masse, of his present colleagues\". Disraeli went to Osborne House on the Isle of Wight, where the Queen asked him to form a government. The monarch wrote to her daughter, Prussian Crown Princess Victoria, \"Mr. Disraeli is Prime Minister! A proud thing for a man 'risen from the people' to have obtained!\" The new Prime Minister told those who came to congratulate him, \"I have climbed to the top of the greasy pole.\"\n\nThe Conservatives remained a minority in the House of Commons and the passage of the Reform Bill required the calling of a new election once the new voting register had been compiled. Disraeli's term as Prime Minister, which began in February 1868, would therefore be short unless the Conservatives won the general election. He made only two major changes in the cabinet: he replaced Lord Chelmsford as Lord Chancellor with Lord Cairns, and brought in George Ward Hunt as Chancellor of the Exchequer. Derby had intended to replace Chelmsford once a vacancy in a suitable sinecure developed. Disraeli was unwilling to wait, and Cairns, in his view, was a far stronger minister.\n\nDisraeli's first premiership was dominated by the heated debate over the Church of Ireland. Although Ireland was overwhelmingly Roman Catholic, the Protestant Church remained the established church and was funded by direct taxation, which was greatly resented by the Catholic majority. An initial attempt by Disraeli to negotiate with Archbishop Manning the establishment of a Roman Catholic university in Dublin foundered in March when Gladstone moved resolutions to disestablish the Irish Church altogether. The proposal united the Liberals under Gladstone's leadership, while causing divisions among the Conservatives.\n\nThe Conservatives remained in office because the new electoral register was not yet ready; neither party wished a poll under the old roll. Gladstone began using the Liberal majority in the House of Commons to push through resolutions and legislation. Disraeli's government survived until the December general election, at which the Liberals were returned to power with a majority of about 110.\n\nDespite its short life, the first Disraeli government succeeded in passing a number of pieces of legislation of a politically noncontentious sort. It ended public executions, and the Corrupt Practices Act did much to end electoral bribery. It authorised an early version of nationalisation, having the Post Office buy up the telegraph companies. Amendments to the school law, the Scottish legal system, and the railway laws were passed. Disraeli sent the successful expedition against Tewodros II of Ethiopia under Sir Robert Napier.\n\nWith Gladstone's Liberal majority dominant in the Commons, Disraeli could do little but protest as the government advanced legislation. Accordingly, he chose to await Liberal mistakes. Having leisure time as he was not in office, he wrote a new novel, \"Lothair\" (1870). A work of fiction by a former Prime Minister was a new thing for Britain, and the book became a best seller.\n\nBy 1872 there was dissent in the Conservative ranks over the failure to challenge Gladstone and his Liberals. This was quieted as Disraeli took steps to assert his leadership of the party, and as divisions among the Liberals became clear. Public support for Disraeli was shown by cheering at a thanksgiving service in 1872 on the recovery of the Prince of Wales from illness, while Gladstone was met with silence. Disraeli had supported the efforts of party manager John Eldon Gorst to put the administration of the Conservative Party on a modern basis. On Gorst's advice, Disraeli gave a speech to a mass meeting in Manchester that year. To roaring approval, he compared the Liberal front bench to \"a range of exhausted volcanoes. Not a flame flickers on a single pallid crest. But the situation is still dangerous. There are occasional earthquakes and ever and again the dark rumbling of the sea.\" Gladstone, Disraeli stated, dominated the scene and \"alternated between a menace and a sigh\".\n\nAt his first departure from 10 Downing Street in 1868, Disraeli had had Victoria create Mary Anne Viscountess of Beaconsfield in her own right in lieu of a peerage for himself. Through 1872 the eighty-year-old peeress was suffering from stomach cancer. She died on 15 December. Urged by a clergyman to turn her thoughts to Jesus Christ in her final days, she said she could not: \"You know Dizzy is my J.C.\" After she died, Gladstone, who always had a liking for Mary Anne, sent her widower a letter of condolence.\n\nIn 1873, Gladstone brought forward legislation to establish a Catholic university in Dublin. This divided the Liberals, and on 12 March an alliance of Conservatives and Irish Catholics defeated the government by three votes. Gladstone resigned, and the Queen sent for Disraeli, who refused to take office. Without a general election, a Conservative government would be another minority, dependent for survival on the division of its opponents. Disraeli wanted the power a majority would bring, and felt he could gain it later by leaving the Liberals in office now. Gladstone's government struggled on, beset by scandal and unimproved by a reshuffle. As part of that change, Gladstone took on the office of Chancellor, leading to questions as to whether he had to stand for re-election on taking on a second ministry—until the 1920s, MPs becoming ministers, thus taking an office of profit under the Crown, had to seek re-election.\n\nIn January 1874, Gladstone called a general election, convinced that if he waited longer, he would do worse at the polls. Balloting was spread over two weeks, beginning on 1 February. Disraeli devoted much of his campaign to decrying the Liberal programme of the past five years. As the constituencies voted, it became clear that the result would be a Conservative majority, the first since 1841. In Scotland, where the Conservatives were perennially weak, they increased from seven seats to nineteen. Overall, they won 350 seats to 245 for the Liberals and 57 for the Irish Home Rule League. The Queen sent for Disraeli, and he became Prime Minister for the second time.\n\nDisraeli's cabinet of twelve, with six peers and six commoners, was the smallest since Reform. Of the peers, five of them had been in Disraeli's 1868 cabinet; the sixth, Lord Salisbury, was reconciled to Disraeli after negotiation and became Secretary of State for India. Lord Stanley (who had succeeded his father, the former Prime Minister, as Earl of Derby) became Foreign Secretary and Sir Stafford Northcote the Chancellor.\n\nIn August 1876, Disraeli was elevated to the House of Lords as Earl of Beaconsfield and Viscount Hughenden. The Queen had offered to ennoble him as early as 1868; he had then declined. She did so again in 1874, when he fell ill at Balmoral, but he was reluctant to leave the Commons for a house in which he had no experience. Continued ill health during his second premiership caused him to contemplate resignation, but his lieutenant, Derby, was unwilling, feeling that he could not manage the Queen. For Disraeli, the Lords, where the debate was less intense, was the alternative to resignation from office. Five days before the end of the 1876 session of Parliament, on 11 August, Disraeli was seen to linger and look around the chamber before departing the Commons. Newspapers reported his ennoblement the following morning.\n\nIn addition to the viscounty bestowed on Mary Anne Disraeli; the earldom of Beaconsfield was to have been bestowed on Edmund Burke in 1797, but he had died before receiving it. The name Beaconsfield, a town near Hughenden, also was given to a minor character in \"Vivian Grey\". Disraeli made various statements about his elevation, writing to Selina, Lady Bradford on 8 August 1876, \"I am quite tired of that place [the Commons]\" but when asked by a friend how he liked the Lords, replied, \"I am dead; dead but in the Elysian fields.\"\n\nUnder the stewardship of Richard Assheton Cross, the Home Secretary, Disraeli's new government enacted many reforms, including the Artisans' and Labourers' Dwellings Improvement Act 1875, which made inexpensive loans available to towns and cities to construct working-class housing. Also enacted were the Public Health Act 1875, modernising sanitary codes through the nation, the Sale of Food and Drugs Act (1875), and the Education Act (1876).\n\nDisraeli's government also introduced a new Factory Act meant to protect workers, the Conspiracy, and Protection of Property Act 1875, which allowed peaceful picketing, and the Employers and Workmen Act (1875) to enable workers to sue employers in the civil courts if they broke legal contracts. As a result of these social reforms the Liberal-Labour MP Alexander Macdonald told his constituents in 1879, \"The Conservative party have done more for the working classes in five years than the Liberals have in fifty.\"\n\nGladstone in 1870 had sponsored an Order in Council, introducing competitive examination into the Civil Service, diminishing the political aspects of government hiring. Disraeli did not agree, and while he did not seek to reverse the order, his actions often frustrated its intent. For example, Disraeli made political appointments to positions previously given to career civil servants. In this, he was backed by his party, hungry for office and its emoluments after almost thirty years with only brief spells in government. Disraeli gave positions to hard-up Conservative leaders, even—to Gladstone's outrage—creating one office at £2,000 per year. Nevertheless, Disraeli made fewer peers (only 22, and one of those one of Victoria's sons) than had Gladstone—the Liberal leader had arranged for the bestowal of 37 peerages during his just over five years in office.\n\nAs he had in government posts, Disraeli rewarded old friends with clerical positions, making Sydney Turner, son of a good friend of Isaac D'Israeli, Dean of Ripon. He favoured Low church clergymen in promotion, disliking other movements in Anglicanism for political reasons. In this, he came into disagreement with the Queen, who out of loyalty to her late husband, Albert, Prince Consort, preferred Broad church teachings. One controversial appointment had occurred shortly before the 1868 election. When the position of Archbishop of Canterbury fell vacant, Disraeli reluctantly agreed to the Queen's preferred candidate, Archibald Tait, the Bishop of London. To fill Tait's vacant see, Disraeli was urged by many people to appoint Samuel Wilberforce, the former Bishop of Winchester and leading figure in London society. Disraeli disliked Wilberforce and instead appointed John Jackson, the Bishop of Lincoln. Blake suggested that, on balance, these appointments cost Disraeli more votes than they gained him.\n\nDisraeli always considered foreign affairs to be the most critical and most interesting part of statesmanship. Nevertheless, his biographer Robert Blake doubts that his subject had specific ideas about foreign policy when he took office in 1874. He had rarely travelled abroad; since his youthful tour of the Middle East in 1830–1831, he had left Britain only for his honeymoon and three visits to Paris, the last of which was in 1856. As he had criticised Gladstone for a do-nothing foreign policy, he most probably contemplated what actions would reassert Britain's place in Europe. His brief first premiership, and the first year of his second, gave him little opportunity to make his mark in foreign affairs.\n\nThe Suez Canal, opened in 1869, cut weeks and thousands of miles off the journey between Britain and India; in 1875, approximately 80% of the ships using the canal were British. In the event of another rebellion in India, or of a Russian invasion, the time saved at Suez might be crucial. Built by French interests, much of the ownership and bonds in the canal remained in their hands, though some of the stock belonged to Isma'il Pasha, the Khedive of Egypt, who was noted for his profligate spending. The canal was losing money, and an attempt by Ferdinand de Lesseps, builder of the canal, to raise the tolls had fallen through when the Khedive had threatened to use military force to prevent it, and had also attracted Disraeli's attention. The Khedive governed Egypt under the Ottoman Empire; as in the Crimea, the issue of the Canal raised the Eastern Question of what to do about the decaying empire governed from Constantinople. With much of the pre-canal trade and communications between Britain and India passing through the Ottoman Empire, Britain had done its best to prop up the Ottomans against the threat that Russia would take Constantinople, cutting those communications, and giving Russian ships unfettered access to the Mediterranean. The French might also threaten those lines from colonies in Syria. Britain had had the opportunity to purchase shares in the canal but had declined to do so.\n\nDisraeli had passed near Suez in his tour of the Middle East in his youth, and on taking office, recognising the British interest in the canal as a gateway to India, he sent the Liberal MP Nathan Rothschild to Paris to enquire about buying de Lesseps's shares. On 14 November 1875, the editor of the \"Pall Mall Gazette\", Frederick Greenwood, learned from London banker Henry Oppenheim that the Khedive was seeking to sell his shares in the Suez Canal Company to a French firm. Greenwood quickly told Lord Derby, the Foreign Secretary, who notified Disraeli. The Prime Minister moved immediately to secure the shares. On 23 November, the Khedive offered to sell the shares for 100,000,000 francs. Rather than seek the aid of the Bank of England, Disraeli asked Lionel de Rothschild to loan funds. Rothschild did so and controversially took a commission on the deal. The banker's capital was at risk as Parliament could have refused to ratify the transaction. The contract for purchase was signed at Cairo on 25 November and the shares deposited at the British consulate the following day.\n\nDisraeli told the Queen, \"it is settled; you have it, madam!\" The public saw the venture as a daring British statement of its dominance of the seas. Sir Ian Malcolm described the Suez Canal share purchase as \"the greatest romance of Mr. Disraeli's romantic career\". In the following decades, the security of the Suez Canal, as the pathway to India, became a major focus of British foreign policy. A later Foreign Secretary, Lord Curzon, described the canal in 1909 as \"the determining influence of every considerable movement of British power to the east and south of the Mediterranean\".\n\nAlthough initially curious about Disraeli when he entered Parliament in 1837, Victoria came to detest him over his treatment of Peel. Over time, her dislike softened, especially as Disraeli took pains to cultivate her. He told Matthew Arnold, \"Everybody likes flattery; and, when you come to royalty, you should lay it on with a trowel\". Disraeli's biographer, Adam Kirsch, suggests that Disraeli's obsequious treatment of his queen was part flattery, part belief that this was how a queen should be addressed by a loyal subject, and part awe that a middle-class man of Jewish birth should be the companion of a monarch. By the time of his second premiership, Disraeli had built a strong relationship with Victoria, probably closer to her than any of her Prime Ministers except her first, Lord Melbourne. When Disraeli returned as Prime Minister in 1874 and went to kiss hands, he did so literally, on one knee; and, according to Richard Aldous in his book on the rivalry between Disraeli and Gladstone, \"for the next six years Victoria and Disraeli would exploit their closeness for mutual advantage.\"\n\nVictoria had long wished to have an imperial title, reflecting Britain's expanding domain. She was irked when Czar Alexander II held a higher rank than her as an emperor, and was appalled that her daughter, the Prussian Crown Princess, would outrank her when her husband came to the throne. She also saw an imperial title as proclaiming Britain's increased stature in the world. The title \"Empress of India\" had been used informally with respect to Victoria for some time and she wished to have that title formally bestowed on her. The Queen prevailed upon Disraeli to introduce a Royal Titles Bill, and also told of her intent to open Parliament in person, which during this time she did only when she wanted something from legislators. Disraeli was cautious in response, as careful soundings of MPs brought a negative reaction, and declined to place such a proposal in the Queen's Speech.\n\nOnce the desired bill was prepared, Disraeli's handling of it was not adept. He neglected to notify either the Prince of Wales or the opposition, and was met by irritation from the prince and a full-scale attack from the Liberals. An old enemy of Disraeli, former Liberal Chancellor Robert Lowe, alleged during the debate in the Commons that two previous Prime Ministers had refused to introduce such legislation for the Queen. Gladstone immediately stated that he was not one of them, and the Queen gave Disraeli leave to quote her saying she had never approached a Prime Minister with such a proposal. According to Blake, Disraeli \"in a brilliant oration of withering invective proceeded to destroy Lowe\", who apologised and never held office again. Disraeli said of Lowe that he was the only person in London with whom he would not shake hands and, \"he is in the mud and there I leave him.\"\n\nFearful of losing, Disraeli was reluctant to bring the bill to a vote in the Commons, but when he eventually did, it passed with a majority of 75. Once the bill was formally enacted, Victoria began signing her letters \"Victoria R & I\" (\"Regina et Imperatrix\", that is, Queen and Empress). According to Aldous, \"the unpopular Royal Titles Act, however, shattered Disraeli's authority in the House of Commons\".\n\nIn July 1875 Serb populations in Bosnia and Herzegovina, then provinces of the Ottoman Empire, rose in revolt against their Turkish masters, alleging religious persecution and poor administration. The following January, Sultan Abdülaziz agreed to reforms proposed by Hungarian statesman Julius Andrássy, but the rebels, suspecting they might win their freedom, continued their uprising, joined by militants in Serbia and Bulgaria. The Turks suppressed the Bulgarian uprising harshly, and when reports of these actions escaped, Disraeli and Derby stated in Parliament that they did not believe them. Disraeli called them \"coffee-house babble\" and dismissed allegations of torture by the Ottomans since \"Oriental people usually terminate their connections with culprits in a more expeditious fashion\".\n\nGladstone, who had left the Liberal leadership and retired from public life, was appalled by reports of atrocities in Bulgaria, and in August 1876, penned a hastily written pamphlet arguing that the Turks should be deprived of Bulgaria because of what they had done there. He sent a copy to Disraeli, who called it \"vindictive and ill-written ... of all the Bulgarian horrors perhaps the greatest\". Gladstone's pamphlet became an immense best-seller and rallied the Liberals to urge that the Ottoman Empire should no longer be a British ally. Disraeli wrote to Lord Salisbury on 3 September, \"Had it not been for these unhappy 'atrocities', we should have settled a peace very honourable to England and satisfactory to Europe. Now we are obliged to work from a new point of departure, and dictate to Turkey, who has forfeited all sympathy.\" In spite of this, Disraeli's policy favoured Constantinople and the territorial integrity of its empire.\nDisraeli and the cabinet sent Salisbury as lead British representative to the Constantinople Conference, which met in December 1876 and January 1877. In advance of the conference, Disraeli sent Salisbury private word to seek British military occupation of Bulgaria and Bosnia, and British control of the Ottoman Army. Salisbury ignored these instructions, which his biographer, Andrew Roberts deemed \"ludicrous\". Nevertheless, the conference failed to reach agreement with the Turks.\n\nParliament opened in February 1877, with Disraeli now in the Lords as Earl of Beaconsfield. He spoke only once there in the 1877 session on the Eastern Question, stating on 20 February that there was a need for stability in the Balkans, and that forcing Turkey into territorial concessions would do nothing to secure it. The Prime Minister wanted a deal with the Ottomans whereby Britain would temporarily occupy strategic areas to deter the Russians from war, to be returned on the signing of a peace treaty, but found little support in his cabinet, which favoured partition of the Ottoman Empire. As Disraeli, by then in poor health, continued to battle within the cabinet, Russia invaded Turkey on 21 April, beginning the Russo-Turkish War.\n\nThe Russians pushed through Ottoman territory and by December 1877 had captured the strategic Bulgarian town of Plevna; their march on Constantinople seemed inevitable. The war divided the British, but the Russian success caused some to forget the atrocities and call for intervention on the Turkish side. Others hoped for further Russian successes. The fall of Plevna was a major story for weeks in the newspapers, and Disraeli's warnings that Russia was a threat to British interests in the eastern Mediterranean were deemed prophetic. The jingoistic attitude of many Britons increased Disraeli's political support, and the Queen acted to help him as well, showing her favour by visiting him at Hughenden—the first time she had visited the country home of her Prime Minister since the Melbourne ministry. At the end of January 1878, the Ottoman Sultan appealed to Britain to save Constantinople. Amid war fever in Britain, the government asked Parliament to vote £6,000,000 to prepare the Army and Navy for war. Gladstone, who had involved himself again in politics, opposed the measure, but less than half his party voted with him. Popular opinion was with Disraeli, though some thought him too soft for not immediately declaring war on Russia.\nWith the Russians close to Constantinople, the Turks yielded and in March 1878, signed the Treaty of San Stefano, conceding a Bulgarian state which would cover a large part of the Balkans. It would be initially Russian-occupied and many feared that it would give them a client state close to Constantinople. Other Ottoman possessions in Europe would become independent; additional territory was to be ceded directly to Russia. This was unacceptable to the British, who protested, hoping to get the Russians to agree to attend an international conference which German Chancellor Bismarck proposed to hold at Berlin. The cabinet discussed Disraeli's proposal to position Indian troops at Malta for possible transit to the Balkans and call out reserves. Derby resigned in protest, and Disraeli appointed Salisbury as Foreign Secretary. Amid British preparations for war, the Russians and Turks agreed to discussions at Berlin.\n\nIn advance of the meeting, confidential negotiations took place between Britain and Russia in April and May 1878. The Russians were willing to make changes to the big Bulgaria, but were determined to retain their new possessions, Bessarabia in Europe and Batum and Kars on the east coast of the Black Sea. To counterbalance this, Britain required a possession in the Eastern Mediterranean where it might base ships and troops, and negotiated with the Ottomans for the cession of Cyprus. Once this was secretly agreed, Disraeli was prepared to allow Russia's territorial gains.\nThe Congress of Berlin was held in June and July 1878, the central relationship in it that between Disraeli and Bismarck. In later years, the German chancellor would show visitors to his office three pictures on the wall: \"the portrait of my Sovereign, there on the right that of my wife, and on the left, there, that of Lord Beaconsfield\". Disraeli caused an uproar in the congress by making his opening address in English, rather than in French, hitherto accepted as the international language of diplomacy. By one account, the British ambassador in Berlin, Lord Odo Russell, hoping to spare the delegates Disraeli's awful French accent, told Disraeli that the congress was hoping to hear a speech in the English tongue by one of its masters.\n\nDisraeli left much of the detailed work to Salisbury, concentrating his efforts on making it as difficult as possible for the broken-up big Bulgaria to reunite. Disraeli did not have things all his own way: he intended that Batum be demilitarised, but the Russians obtained their preferred language, and in 1886, fortified the town. Nevertheless, the Cyprus Convention ceding the island to Britain was announced during the congress, and again made Disraeli a sensation.\n\nDisraeli gained agreement that Turkey should retain enough of its European possessions to safeguard the Dardanelles. By one account, when met with Russian intransigence, Disraeli told his secretary to order a special train to return them home to begin the war. Although Russia yielded, Czar Alexander II later described the congress as \"a European coalition against Russia, under Bismarck\".\n\nThe Treaty of Berlin was signed on 13 July 1878 at the Radziwill Palace in Berlin. Disraeli and Salisbury returned home to heroes' receptions at Dover and in London. At the door of 10 Downing Street, Disraeli received flowers sent by the Queen. There, he told the gathered crowd, \"Lord Salisbury and I have brought you back peace—but a peace I hope with honour.\" The Queen offered him a dukedom, which he declined, though accepting the Garter, as long as Salisbury also received it. In Berlin, word spread of Bismarck's admiring description of Disraeli, \"\"Der alte Jude, das ist der Mann!\" \"\n\nIn the weeks after Berlin, Disraeli and the cabinet considered calling a general election to capitalise on the public applause he and Salisbury had received. Parliaments were then for a seven-year term, and it was the custom not to go to the country until the sixth year unless forced to by events. Only four and a half years had passed since the last general election. Additionally, they did not see any clouds on the horizon that might forecast Conservative defeat if they waited. This decision not to seek re-election has often been cited as a great mistake by Disraeli. Blake, however, pointed out that results in local elections had been moving against the Conservatives, and doubted if Disraeli missed any great opportunity by waiting.\n\nAs successful invasions of India generally came through Afghanistan, the British had observed and sometimes intervened there since the 1830s, hoping to keep the Russians out. In 1878 the Russians sent a mission to Kabul; it was not rejected by the Afghans, as the British had hoped. The British then proposed to send their own mission, insisting that the Russians be sent away. The Viceroy, Lord Lytton, concealed his plans to issue this ultimatum from Disraeli, and when the Prime Minister insisted he take no action, went ahead anyway. When the Afghans made no answer, the British advanced against them in the Second Anglo-Afghan War, and under Lord Roberts easily defeated them. The British installed a new ruler, and left a mission and garrison in Kabul.\n\nBritish policy in South Africa was to encourage federation between the British-run Cape Colony and Natal, and the Boer republics, the Transvaal (annexed by Britain in 1877) and the Orange Free State. The governor of Cape Colony, Sir Bartle Frere, believing that the federation could not be accomplished until the native tribes acknowledged British rule, made demands on the Zulu and their king, Cetewayo, which they were certain to reject. As Zulu troops could not marry until they had washed their spears in blood, they were eager for combat. Frere did not send word to the cabinet of what he had done until the ultimatum was about to expire. Disraeli and the cabinet reluctantly backed him, and in early January 1879 resolved to send reinforcements. Before they could arrive, on 22 January, a Zulu \"impi\", or army, moving with great speed and stealth, ambushed and destroyed a British encampment in South Africa in the Battle of Isandlwana. Over a thousand British and colonial troops were killed. Word of the defeat did not reach London until 12 February. Disraeli wrote the next day, \"the terrible disaster has shaken me to the centre\". He reprimanded Frere, but left him in charge, attracting fire from all sides. Disraeli sent General Sir Garnet Wolseley as High Commissioner and Commander in Chief, and Cetewayo and the Zulus were crushed at the Battle of Ulundi on 4 July 1879.\n\nOn 8 September 1879 Sir Louis Cavagnari, in charge of the mission in Kabul, was killed with his entire staff by rebelling Afghan soldiers. Roberts undertook a successful punitive expedition against the Afghans over the next six weeks.\n\nGladstone, in the 1874 election, had been returned for Greenwich, finishing second behind a Conservative in the two-member constituency, a result he termed more like a defeat than a victory. In December 1878, he was offered the Liberal nomination at the next election for Edinburghshire, a constituency popularly known as Midlothian. The small Scottish electorate was dominated by two noblemen, the Conservative Duke of Buccleuch and the Liberal Earl of Rosebery. The Earl, a friend of both Disraeli and Gladstone who would succeed the latter after his final term as Prime Minister, had journeyed to the United States to view politics there, and was convinced that aspects of American electioneering could be translated to the United Kingdom. On his advice, Gladstone accepted the offer in January 1879, and later that year began his Midlothian campaign, speaking not only in Edinburgh, but across Britain, attacking Disraeli, to huge crowds.\n\nConservative chances of re-election were damaged by the poor weather, and consequent effects on agriculture. Four consecutive wet summers through 1879 had led to poor harvests in the United Kingdom. In the past, the farmer had the consolation of higher prices at such times, but with bumper crops cheaply transported from the United States, grain prices remained low. Other European nations, faced with similar circumstances, opted for protection, and Disraeli was urged to reinstitute the Corn Laws. He declined, stating that he regarded the matter as settled. Protection would have been highly unpopular among the newly enfranchised urban working classes, as it would raise their cost of living. Amid an economic slump generally, the Conservatives lost support among farmers.\n\nDisraeli's health continued to fail through 1879. Owing to his infirmities, Disraeli was three-quarters of an hour late for the Lord Mayor's Dinner at the Guildhall in November, at which it is customary that the Prime Minister speaks. Though many commented on how healthy he looked, it took him great effort to appear so, and when he told the audience he expected to speak to the dinner again the following year, attendees chuckled—Gladstone was then in the midst of his campaign. Despite his public confidence, Disraeli recognised that the Conservatives would probably lose the next election, and was already contemplating his Resignation Honours.\n\nDespite this pessimism, Conservatives hopes were buoyed in early 1880 with successes in by-elections the Liberals had expected to win, concluding with victory in Southwark, normally a Liberal stronghold. The cabinet had resolved to wait before dissolving Parliament; in early March they reconsidered, agreeing to go to the country as soon as possible. Parliament was dissolved on 24 March; the first borough constituencies began voting a week later.\n\nDisraeli took no public part in the electioneering, it being deemed improper for peers to make speeches to influence Commons elections. This meant that the chief Conservatives—Disraeli, Salisbury, and India Secretary Lord Cranbrook—would not be heard from. The election was thought likely to be close. Once returns began to be announced, it became clear that the Conservatives were being decisively beaten. The final result gave the Liberals an absolute majority of about 50.\n\nDisraeli refused to cast blame for the defeat, which he understood was likely to be final for him. He wrote to Lady Bradford that it was just as much work to end a government as to form one, without any of the fun. Queen Victoria was bitter at his departure as Prime Minister. Among the honours he arranged before resigning as Prime Minister on 21 April 1880 was one for his private secretary, Montagu Corry, who became Baron Rowton.\nReturning to Hughenden, Disraeli brooded over his electoral dismissal, but also resumed work on \"Endymion\", which he had begun in 1872 and laid aside before the 1874 election. The work was rapidly completed and published by November 1880. He carried on a correspondence with Victoria, with letters passed through intermediaries. When Parliament met in January 1881, he served as Conservative leader in the Lords, attempting to serve as a moderating influence on Gladstone's legislation.\n\nSuffering from asthma and gout, Disraeli went out as little as possible, fearing more serious episodes of illness. In March, he fell ill with bronchitis, and emerged from bed only for a meeting with Salisbury and other Conservative leaders on the 26th. As it became clear that this might be his final sickness, friends and opponents alike came to call. Disraeli declined a visit from the Queen, saying, \"She would only ask me to take a message to Albert.\" Almost blind, when he received the last letter from Victoria of which he was aware on 5 April, he held it momentarily, then had it read to him by Lord Barrington, a Privy Councillor. One card, signed \"A Workman\", delighted its recipient, \"Don't die yet, we can't do without you.\"\n\nDespite the gravity of Disraeli's condition, the doctors concocted optimistic bulletins, for public consumption. The Prime Minister, Gladstone, called several times to enquire about his rival's condition, and wrote in his diary, \"May the Almighty be near his pillow.\" There was intense public interest in the former Prime Minister's struggles for life. Disraeli had customarily taken the sacrament at Easter; when this day was observed on 17 April, there was discussion among his friends and family if he should be given the opportunity, but those against, fearing that he would lose hope, prevailed. On the morning of the following day, Easter Monday, he became incoherent, then comatose. Disraeli's last confirmed words before dying in the early morning of 19 April were \"I had rather live but I am not afraid to die\". The anniversary of Disraeli's death is now commemorated in the United Kingdom as Primrose Day.\n\nDisraeli's executors decided against a public procession and funeral, fearing that too large crowds would gather to do him honour. The chief mourners at the service at Hughenden on 26 April were his brother Ralph and nephew Coningsby, to whom Hughenden would eventually pass. Queen Victoria was prostrated with grief, and considered ennobling Ralph or Coningsby as a memorial to Disraeli (without children, his titles became extinct with his death) but decided against it on the ground that their means were too small for a peerage. Protocol forbade her attending Disraeli's funeral (this would not be changed until 1965, when Elizabeth II attended the rites for the former Prime Minister Sir Winston Churchill) but she sent primroses (\"his favourite flowers\") to the funeral, and visited the burial vault to place a wreath of china blooms four days later.\n\nDisraeli is buried with his wife in a vault beneath the Church of St Michael and All Angels which stands in the grounds of his home, Hughenden Manor, accessed from the churchyard. There is also a memorial to him in the chancel in the church, erected in his honour by Queen Victoria. His literary executor was his private secretary, Lord Rowton. The Disraeli vault also contains the body of Sarah Brydges Willyams, the wife of James Brydges Willyams of St Mawgan in Cornwall. Disraeli carried on a long correspondence with Mrs. Willyams, writing frankly about political affairs. At her death in 1865, she left him a large legacy, which helped clear up his debts. His will was proved at £84,000.\n\nDisraeli has a memorial in Westminster Abbey. This monument was erected by the nation on the motion of Gladstone in his memorial speech on Disraeli in the House of Commons. Gladstone had absented himself from the funeral, with his plea of the press of public business met with public mockery. His speech was widely anticipated, if only because his dislike for Disraeli was well known, and caused the Prime Minister much worry. In the event, the speech was a model of its kind, in which he avoided comment on Disraeli's politics, while praising his personal qualities.\n\nDisraeli's literary and political career interacted over his lifetime and fascinated Victorian Britain, making him \"one of the most eminent figures in Victorian public life\", and occasioned a large output of commentary. Critic Shane Leslie noted three decades after his death that \"Disraeli's career was a romance such as no Eastern vizier or Western plutocrat could tell. He began as a pioneer in dress and an aesthete of words... Disraeli actually made his novels come true.\"\n\nBlake comments that Disraeli \"produced an epic poem, unbelievably bad, and a five-act blank verse tragedy, if possible worse. Further he wrote a discourse on political theory and a political biography, the \"Life of Lord George Bentinck\", which is excellent ... remarkably fair and accurate.\" But it is on his novels that Disraeli's literary achievements are generally judged. They have from the outset divided critical opinion. The writer R. W. Stewart observed that there have always been two criteria for judging Disraeli's novels—one political and the other artistic. The critic Robert O'Kell, concurring, writes, \"It is after all, even if you are a Tory of the staunchest blue, impossible to make Disraeli into a first-rate novelist. And it is equally impossible, no matter how much you deplore the extravagances and improprieties of his works, to make him into an insignificant one.\"\n\nDisraeli's early \"silver fork\" novels \"Vivian Grey\" (1826) and \"The Young Duke\" (1831) featured romanticised depictions of aristocratic life (despite his ignorance of it) with character sketches of well-known public figures lightly disguised. In some of his early fiction Disraeli also portrayed himself and what he felt to be his Byronic dual nature: the poet and the man of action. His most autobiographical novel was \"Contarini Fleming\" (1832), an avowedly serious work that did not sell well. The critic William Kuhn suggests that Disraeli's fiction can be read as \"the memoirs he never wrote\", revealing the inner life of a politician for whom the norms of Victorian public life appeared to represent a social straitjacket—particularly with regard to what Kuhn sees as the author's \"ambiguous sexuality\".\n\nOf the other novels of the early 1830s, \"Alroy\" is described by Blake as \"profitable but unreadable\", and \"The Rise of Iskander\" (1833), \"The Infernal Marriage\" and \"Ixion in Heaven\" (1834) made little impact. \"Henrietta Temple\" (1837) was Disraeli's next major success. It draws on the events of his affair with Henrietta Sykes to tell the story of a debt-ridden young man torn between a mercenary loveless marriage and a passionate love-at-first-sight for the eponymous heroine. \"Venetia\" (1837) was a minor work, written to raise much-needed cash.\n\nIn the 1840s Disraeli wrote a trilogy of novels with political themes. With \"Coningsby; or, The New Generation\" (1844), Disraeli, in Blake's view, \"infused the novel genre with political sensibility, espousing the belief that England's future as a world power depended not on the complacent old guard, but on youthful, idealistic politicians.\" \"Coningsby\" was followed by \"Sybil; or, The Two Nations\" (1845), another political novel, which was less idealistic and more clear-eyed than \"Coningsby\"; the \"two nations\" of its sub-title referred to the huge economic and social gap between the privileged few and the deprived working classes. The last in Disraeli's political novel trilogy was \"Tancred; or, The New Crusade\" (1847), promoting the Church of England's role in reviving Britain's flagging spirituality.\n\nDisraeli's last completed novels were \"Lothair\" (1870) and \"Endymion\" (1880). The first, described by Daniel R Schwarz as \"Disraeli's ideological \"Pilgrim's Progress\"\", is a story of political life with particular regard to the roles of the Anglican and Roman Catholic churches. \"Endymion\", despite having a Whig as hero, is a last exposition of the author's economic policies and political beliefs. Disraeli continued to the last to pillory his enemies in barely disguised caricatures: the character St Barbe in \"Endymion\" is widely seen as a parody of Thackeray, who had offended Disraeli more than thirty years earlier by lampooning him in \"Punch\" as \"Codlingsby\". Disraeli left an unfinished novel in which the priggish central character, Falconet, is unmistakably a caricature of Gladstone.\n\nIn the years after Disraeli's death, as Salisbury began his reign of more than twenty years over the Conservatives, the party emphasised the late leader's \"One Nation\" views, that the Conservatives at root shared the beliefs of the working classes, with the Liberals the party of the urban élite. Disraeli had, for example, stressed the need to improve the lot of the urban labourer. The memory of Disraeli was used by the Conservatives to appeal to the working classes, with whom he was said to have had a rapport. This aspect of his policies has been re-evaluated by historians in the 20th and 21st centuries. In 1972 BHAbbott stressed that it was not Disraeli but Lord Randolph Churchill who invented the term \"Tory democracy\", though it was Disraeli who made it an essential part of Conservative policy and philosophy. In 2007 Parry wrote, \"The tory democrat myth did not survive detailed scrutiny by professional historical writing of the 1960s [which] demonstrated that Disraeli had very little interest in a programme of social legislation and was very flexible in handling parliamentary reform in 1867.\" Despite this, Parry sees Disraeli, rather than Peel, as the founder of the modern Conservative party. The Conservative politician and writer Douglas Hurd wrote in 2013, \"[Disraeli] was not a one-nation Conservative—and this was not simply because he never used the phrase. He rejected the concept in its entirety.\"\n\nDisraeli's enthusiastic propagation of the British Empire has also been seen as appealing to working class voters. Before his leadership of the Conservative Party, imperialism was the province of the Liberals, most notably Palmerston, with the Conservatives murmuring dissent across the aisle. Disraeli made the Conservatives the party that most loudly supported both the Empire and military action to assert its primacy. This came about in part because Disraeli's own views stemmed that way, in part because he saw advantage for the Conservatives, and partially in reaction against Gladstone, who disliked the expense of empire. Blake argued that Disraeli's imperialism \"decisively orientated the Conservative party for many years to come, and the tradition which he started was probably a bigger electoral asset in winning working-class support during the last quarter of the century than anything else\". Some historians have commented on a romantic impulse behind Disraeli's approach to Empire and foreign affairs: Abbott writes, \"To the mystical Tory concepts of Throne, Church, Aristocracy and People, Disraeli added Empire.\" Others have identified a strongly pragmatic aspect to his policies. Gladstone's biographer Philip Magnus contrasted Disraeli's grasp of foreign affairs with that of Gladstone, who \"never understood that high moral principles, in their application to foreign policy, are more often destructive of political stability than motives of national self-interest.\" In Parry's view, Disraeli's foreign policy \"can be seen as a gigantic castle in the air (as it was by Gladstone), or as an overdue attempt to force the British commercial classes to awaken to the realities of European politics.\"\n\nDuring his lifetime Disraeli's opponents, and sometimes even his friends and allies, questioned whether he sincerely held the views he propounded, or whether they were adopted by him as essential to one who sought to spend his life in politics, and were mouthed by him without conviction. Lord John Manners, in 1843 at the time of Young England, wrote, \"could I only satisfy myself that D'Israeli believed all that he said, I should be more happy: his historical views are quite mine, but does he believe them?\" Blake (writing in 1966) suggested that it is no more possible to answer that question now than it was then. Nevertheless, Paul Smith, in his journal article on Disraeli's politics, argues that Disraeli's ideas were coherently argued over a political career of nearly half a century, and \"it is impossible to sweep them aside as a mere bag of burglar's tools for effecting felonious entry to the British political pantheon.\"\nStanley Weintraub, in his biography of Disraeli, points out that his subject did much to advance Britain towards the 20th century, carrying one of the two great Reform Acts of the 19th despite the opposition of his Liberal rival, Gladstone. \"He helped preserve constitutional monarchy by drawing the Queen out of mourning into a new symbolic national role and created the climate for what became 'Tory democracy'. He articulated an imperial role for Britain that would last into World War II and brought an intermittently self-isolated Britain into the concert of Europe.\"\n\nFrances Walsh comments on Disraeli's multifaceted public life:\nThe debate about his place in the Conservative pantheon has continued since his death. Disraeli fascinated and divided contemporary opinion; he was seen by many, including some members of his own party, as an adventurer and a charlatan and by others as a far-sighted and patriotic statesman. As an actor on the political stage he played many roles: Byronic hero, man of letters, social critic, parliamentary virtuoso, squire of Hughenden, royal companion, European statesman. His singular and complex personality has provided historians and biographers with a particularly stiff challenge.\n\nHistorical writers have often played Disraeli and Gladstone against each other as great rivals. Roland Quinault, however, cautions us not to exaggerate the confrontation:they were not direct antagonists for most of their political careers. Indeed initially they were both loyal to the Tory party, the Church and the landed interest. Although their paths diverged over the repeal of the Corn Laws in 1846 and later over fiscal policy more generally, it was not until the later 1860s that their differences over parliamentary reform, Irish and Church policy assumed great partisan significance. Even then their personal relations remained fairly cordial until their dispute over the Eastern Question in the later 1870s.\n\n\n\n\n\nNotes\n\nReferences\n\n\n", "id": "3875", "title": "Benjamin Disraeli"}
{"url": "https://en.wikipedia.org/wiki?curid=3876", "text": "Binomial distribution\n\n</math>\n\nIn probability theory and statistics, the binomial distribution with parameters \"n\" and \"p\" is the discrete probability distribution of the number of successes in a sequence of \"n\" independent experiments, each asking a yes–no question, and each with its own boolean-valued outcome: a random variable containing single bit of information: success/yes/true/one (with probability \"p\") or failure/no/false/zero (with probability \"q\" = 1 − \"p\"). \nA single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., \"n\" = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance.\n\nThe binomial distribution is frequently used to model the number of successes in a sample of size \"n\" drawn with replacement from a population of size \"N.\" If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for \"N\" much larger than \"n\", the binomial distribution remains a good approximation, and is widely used.\n\nIn general, if the random variable \"X\" follows the binomial distribution with parameters \"n\" ∈ ℕ and \"p\" ∈ [0,1], we write \"X\" ~ B(\"n\", \"p\"). The probability of getting exactly \"k\" successes in \"n\" trials is given by the probability mass function:\n\nfor \"k\" = 0, 1, 2, ..., \"n\", where\n\nis the binomial coefficient, hence the name of the distribution. The formula can be understood as follows. \"k\" successes occur with probability \"p\" and \"n\" − \"k\" failures occur with probability (1 − \"p\"). However, the \"k\" successes can occur anywhere among the \"n\" trials, and there are formula_10 different ways of distributing \"k\" successes in a sequence of \"n\" trials.\n\nIn creating reference tables for binomial distribution probability, usually the table is filled in up to \"n\"/2 values. This is because for \"k\" > \"n\"/2, the probability can be calculated by its complement as\n\nThe probability mass function satisfies the following recurrence relation, for every formula_12:\n\nLooking at the expression \"ƒ\"(\"k\", \"n\", \"p\") as a function of \"k\", there is a \"k\" value that maximizes it. This \"k\" value can be found by calculating\nand comparing it to 1. There is always an integer \"M\" that satisfies\n\n\"ƒ\"(\"k\", \"n\", \"p\") is monotone increasing for \"k\" < \"M\" and monotone decreasing for \"k\" > \"M\", with the exception of the case where (\"n\" + 1)\"p\" is an integer. In this case, there are two values for which \"ƒ\" is maximal: (\"n\" + 1)\"p\" and (\"n\" + 1)\"p\" − 1. \"M\" is the \"most probable\" (\"most likely\") outcome of the Bernoulli trials and is called the mode. Note that the probability of it occurring can be fairly small.\n\nThe cumulative distribution function can be expressed as:\n\nwhere formula_17 is the \"floor\" under \"k\", i.e. the greatest integer less than or equal to \"k\".\n\nIt can also be represented in terms of the regularized incomplete beta function, as follows:\n\nSome closed-form bounds for the cumulative distribution function are given below.\n\nSuppose a biased coin comes up heads with probability 0.3 when tossed. What is the probability of achieving 0, 1..., 6 heads after six tosses?\n\nIf \"X\" ~ \"B\"(\"n\", \"p\"), that is, \"X\" is a binomially distributed random variable, n being the total number of experiments and p the probability of each experiment yielding a successful result, then the expected value of \"X\" is:\n\nFor example, if \"n\" = 100, and \"p\" =1/4, then the average number of successful results will be 25.\n\nProof: We calculate the mean, \"μ\", directly calculated from its definition \n\nand the binomial theorem:\n\nIt is also possible to deduce the mean from the equation formula_29 whereby all formula_30 are Bernoulli distributed random variables with formula_31. We get\n\nThe variance is:\n\nProof: Let formula_29 where all formula_30 are independently Bernoulli distributed random variables. Since formula_36, we get:\n\nUsually the mode of a binomial \"B\"(\"n\", \"p\") distribution is equal to formula_38, where formula_39 is the floor function. However, when (\"n\" + 1)\"p\" is an integer and \"p\" is neither 0 nor 1, then the distribution has two modes: (\"n\" + 1)\"p\" and (\"n\" + 1)\"p\" − 1. When \"p\" is equal to 0 or 1, the mode will be 0 and \"n\" correspondingly. These cases can be summarized as follows:\n\nProof: Let \n\nFor formula_42 only formula_43 has a nonzero value with formula_44. For formula_45 we find formula_46 and formula_47 for formula_48. This proves that the mode is 0 for formula_42 and formula_7 for formula_45.\n\nLet formula_52. We find \n\nFrom this follows\n\nSo when formula_55 is an integer, then formula_55 and formula_57 is a mode. In the case that formula_58, then only formula_59 is a mode.\n\nIn general, there is no single formula to find the median for a binomial distribution, and it may even be non-unique. However several special results have been established:\n\nIf two binomially distributed random variables \"X\" and \"Y\" are observed together, estimating their covariance can be useful. Using the definition of covariance, in the case \"n\" = 1 (thus being Bernoulli trials) we have\n\nThe first term is non-zero only when both \"X\" and \"Y\" are one, and \"μ\" and \"μ\" are equal to the two probabilities. Defining \"p\" as the probability of both happening at the same time, this gives\n\nand for \"n\" independent pairwise trials\n\nIf \"X\" and \"Y\" are the same variable, this reduces to the variance formula given above.\n\nIf \"X\" ~ B(\"n\", \"p\") and \"Y\" ~ B(\"m\", \"p\") are independent binomial variables with the same probability \"p\", then \"X\" + \"Y\" is again a binomial variable; its distribution is \"Z=X+Y\" ~ B(\"n+m\", \"p\"):\n\nHowever, if \"X\" and \"Y\" do not have the same probability \"p\", then the variance of the sum will be smaller than the variance of a binomial variable distributed as formula_64\n\nIf \"X\" ~ B(\"n\", \"p\") and, conditional on \"X\", \"Y\" ~ B(\"X\", \"q\"), then \"Y\" is a simple binomial variable with distribution.\n\nFor example, imagine throwing \"n\" balls to a basket \"U\" and taking the balls that hit and throwing them to another basket \"U\". If \"p\" is the probability to hit \"U\" then \"X\" ~ B(\"n\", \"p\") is the number of balls that hit \"U\". If \"q\" is the probability to hit \"U\" then the number of balls that hit \"U\" is \"Y\" ~ B(\"X\", \"q\") and therefore \"Y\" ~ B(\"n\", \"pq\").\n\n</div>\n\nThe Bernoulli distribution is a special case of the binomial distribution, where \"n\" = 1. Symbolically, \"X\" ~ B(1, \"p\") has the same meaning as \"X\" ~ B(\"p\"). Conversely, any binomial distribution, B(\"n\", \"p\"), is the distribution of the sum of \"n\" Bernoulli trials, B(\"p\"), each with the same probability \"p\".\n\nThe binomial distribution is a special case of the Poisson binomial distribution, or general binomial distribution, which is the distribution of a sum of \"n\" independent non-identical Bernoulli trials B(\"p\").\n\nIf \"n\" is large enough, then the skew of the distribution is not too great. In this case a reasonable approximation to B(\"n\", \"p\") is given by the normal distribution\n\nand this basic approximation can be improved in a simple way by using a suitable continuity correction.\nThe basic approximation generally improves as \"n\" increases (at least 20) and is better when \"p\" is not near to 0 or 1. Various rules of thumb may be used to decide whether \"n\" is large enough, and \"p\" is far enough from the extremes of zero or one:\n\n\nThe following is an example of applying a continuity correction. Suppose one wishes to calculate Pr(\"X\" ≤ 8) for a binomial random variable \"X\". If \"Y\" has a distribution given by the normal approximation, then Pr(\"X\" ≤ 8) is approximated by Pr(\"Y\" ≤ 8.5). The addition of 0.5 is the continuity correction; the uncorrected normal approximation gives considerably less accurate results.\n\nThis approximation, known as de Moivre–Laplace theorem, is a huge time-saver when undertaking calculations by hand (exact calculations with large \"n\" are very onerous); historically, it was the first use of the normal distribution, introduced in Abraham de Moivre's book \"The Doctrine of Chances\" in 1738. Nowadays, it can be seen as a consequence of the central limit theorem since B(\"n\", \"p\") is a sum of \"n\" independent, identically distributed Bernoulli variables with parameter \"p\". This fact is the basis of a hypothesis test, a \"proportion z-test\", for the value of \"p\" using \"x/n\", the sample proportion and estimator of \"p\", in a common test statistic.\n\nFor example, suppose one randomly samples \"n\" people out of a large population and ask them whether they agree with a certain statement. The proportion of people who agree will of course depend on the sample. If groups of \"n\" people were sampled repeatedly and truly randomly, the proportions would follow an approximate normal distribution with mean equal to the true proportion \"p\" of agreement in the population and with standard deviation formula_72\n\nThe binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while the product \"np\" remains fixed or at least \"p\" tends to zero. Therefore, the Poisson distribution with parameter \"λ\" = \"np\" can be used as an approximation to B(\"n\", \"p\") of the binomial distribution if \"n\" is sufficiently large and \"p\" is sufficiently small. According to two rules of thumb, this approximation is good if \"n\" ≥ 20 and \"p\" ≤ 0.05, or if \"n\" ≥ 100 and \"np\" ≤ 10.\n\nConcerning the accuracy of Poisson approximation, see Novak, ch. 4, and references therein.\n\n\nBeta distributions provide a family of prior probability distributions for binomial distributions in Bayesian inference:\n\nEven for quite large values of \"n\", the actual distribution of the mean is significantly nonnormal. Because of this problem several methods to estimate confidence intervals have been proposed.\n\nLet \"n\" be the number of successes out of \"n\", the total number of trials, and let\nbe the proportion of successes. Let \"z\" be the 100(1 − α/2)th percentile of the standard normal distribution.\n\n\n\n\n\nThe exact (Clopper-Pearson) method is the most conservative. The Wald method although commonly recommended in the text books is the most biased.\n\nMethods for random number generation where the marginal distribution is a binomial distribution are well-established.\n\nOne way to generate random samples from a binomial distribution is to use an inversion algorithm. To do so, one must calculate the probability that P(X=k) for all values \"k\" from 0 through \"n\". (These probabilities should sum to a value close to one, in order to encompass the entire sample space.) Then by using a pseudorandom number generator to generate samples uniformly between 0 and 1, one can transform the calculated samples U[0,1] into discrete numbers by using the probabilities calculated in step one.\n\nFor \"k\" ≤ \"np\", upper bounds for the lower tail of the distribution function can be derived. Recall that formula_81, the probability that there are at most \"k\" successes.\n\nHoeffding's inequality yields the bound\n\nand Chernoff's inequality can be used to derive the bound\n\nMoreover, these bounds are reasonably tight when \"p = 1/2\", since the following expression holds for all \"k\" ≥ \"3n/8\"\n\nHowever, the bounds do not work well for extreme values of \"p\". In particular, as \"p\" formula_85 \"1\", value \"F(k;n,p)\" goes to zero (for fixed \"k\", \"n\" with \"k<n\")\nwhile the upper bound above goes to a positive constant. In this case a better bound is given by\n\nAsymptotically, this bound is reasonably tight; see\n\nBoth these bounds are derived directly from the Chernoff bound.\nIt can also be shown that,\n\nThis is proved using the method of types (see for example chapter 12 of Elements of Information Theory by Cover and Thomas ).\n\nWe can also change the formula_89 in the denominator to formula_90, by approximating the binomial coefficient with Stirlings formula.\n\n\n", "id": "3876", "title": "Binomial distribution"}
{"url": "https://en.wikipedia.org/wiki?curid=3878", "text": "Biostatistics\n\nBiostatistics is the application of statistics to a wide range of topics in biology. The science of biostatistics encompasses the design of biological experiments, especially in medicine, pharmacy, agriculture and fishery; the collection, summarization, and analysis of data from those experiments; and the interpretation of, and inference from, the results. A major branch of this is medical biostatistics, which is exclusively concerned with medicine and health.\n\nBiostatistical modeling forms an important part of numerous modern biological theories. In the early 1900s, after the rediscovery of Gregor Mendel's Mendelian inheritance work, the gaps in understanding between genetics and evolutionary Darwinism led to vigorous debate among biometricians, such as Walter Weldon and Karl Pearson, and Mendelians, such as Charles Davenport, William Bateson and Wilhelm Johannsen. By the 1930s, statisticians and models built on statistical reasoning had helped to resolve these differences and to produce the neo-Darwinian modern evolutionary synthesis.\n\nThe leading figures in the establishment of population genetics and this synthesis all relied on statistics and developed its use in biology.\n\nThese individuals and the work of other biostatisticians, mathematical biologists, and statistically inclined geneticists helped bring together evolutionary biology and genetics into a consistent, coherent whole that could begin to be quantitatively modeled.\n\nIn parallel to this overall development, the pioneering work of D'Arcy Thompson in \"On Growth and Form\" also helped to add quantitative discipline to biological study.\n\nDespite the fundamental importance and frequent necessity of statistical reasoning, there may nonetheless have been a tendency among biologists to distrust or deprecate results which are not qualitatively apparent. One anecdote describes Thomas Hunt Morgan banning the Friden calculator from his department at Caltech, saying \"Well, I am like a guy who is prospecting for gold along the banks of the Sacramento River in 1849. With a little intelligence, I can reach down and pick up big nuggets of gold. And as long as I can do that, I'm not going to let any people in my department waste scarce resources in placer mining.\"\n\nRecent developments have made a large impact on biostatistics. Two important changes have been the ability to collect data on a high-throughput scale, and the ability to perform much more complex analysis using computational techniques.\n\nNew biomedical technologies like microarrays, next generation sequencers (for genomics) and mass spectrometry (for proteomics) generate enormous amounts of data, allowing many tests to be performed simultaneously. Careful analysis with biostatistical methods is required to separate the signal from the noise. For example, a microarray could be used to measure many thousands of genes simultaneously, determining which of them have different expression in diseased cells compared to normal cells. However, only a fraction of genes will be differentially expressed.\n\nMulticollinearity often occurs in high-throughput biostatistical settings. Due to high intercorrelation between the predictors (such as gene expression levels), the information of one predictor might be contained in another one. It could be that only 5% of the predictors are responsible for 90% of the variability of the response. In such a case, one could apply the biostatistical technique of dimension reduction (for example via principal component analysis). Classical statistical techniques like linear or logistic regression and linear discriminant analysis do not work well for high dimensional data (i.e. when the number of observations n is smaller than the number of features or predictors p: n < p). As a matter of fact, one can get quite high R-values despite very low predictive power of the statistical model. These classical statistical techniques (esp. least squares linear regression) were developed for low dimensional data (i.e. where the number of observations n is much larger than the number of predictors p: n » p). In cases of high dimensionality, one should always consider an independent validation test set and the corresponding residual sum of squares (RSS) and R of the validation test set, not those of the training set.\n\nOften, it is useful to pool information from multiple predictors together. For example, Gene Set Enrichment Analysis (GSEA) considers the perturbation of whole (functionally related) gene sets rather than of single genes. These gene sets might be known biochemical pathways or otherwise functionally related genes. The advantage of this approach is that it is more robust: It is more likely that a single gene is found to be falsely perturbed than it is that a whole pathway is falsely perturbed. Furthermore, one can integrate the accumulated knowledge about biochemical pathways (like the JAK-STAT signaling pathway) using this approach.\n\nOn the other hand, the advent of modern computer technology and relatively cheap computing resources have enabled computer-intensive biostatistical methods like bootstrapping and resampling methods.\n\nIn recent times, random forests have gained popularity as a method for performing statistical classification. Random forest techniques generate a panel of decision trees. Decision trees have the advantage that you can draw them and interpret them (even with a very basic understanding of mathematics and statistics). Random Forests have thus been used for clinical decision support systems.\n\n\nAlmost all educational programmes in biostatistics are at postgraduate level. They are most often found in schools of public health, affiliated with schools of medicine, forestry, or agriculture, or as a focus of application in departments of statistics.\n\nIn the United States, where several universities have dedicated biostatistics departments, many other top-tier universities integrate biostatistics faculty into statistics or other departments, such as epidemiology. Thus, departments carrying the name \"biostatistics\" may exist under quite different structures. For instance, relatively new biostatistics departments have been founded with a focus on bioinformatics and computational biology, whereas older departments, typically affiliated with schools of public health, will have more traditional lines of research involving epidemiological studies and clinical trials as well as bioinformatics. In larger universities where both a statistics and a biostatistics department exist, the degree of integration between the two departments may range from the bare minimum to very close collaboration. In general, the difference between a statistics program and a biostatistics program is twofold: (i) statistics departments will often host theoretical/methodological research which are less common in biostatistics programs and (ii) statistics departments have lines of research that may include biomedical applications but also other areas such as industry (quality control), business and economics and biological areas other than medicine.\n\n\n", "id": "3878", "title": "Biostatistics"}
{"url": "https://en.wikipedia.org/wiki?curid=3879", "text": "Business statistics\n\nBusiness statistics is the science of good decision making in the face of uncertainty and is used in many disciplines such as financial analysis, econometrics, auditing, production and operations including services improvement, and marketing research. \n\nThese sources feature regular repetitive publication of series of data. This makes the topic of time series especially important for business statistics. It is also a branch of applied statistics working mostly on data collected as a by-product of doing business or by government agencies. It provides knowledge and skills to interpret and use statistical techniques in a variety of business applications. \n\nA typical business statistics course is intended for business majors, and covers statistical study, descriptive statistics (collection, description, analysis, and summary of data), probability, and the binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. \n\n\n\nDarlene Basilio", "id": "3879", "title": "Business statistics"}
{"url": "https://en.wikipedia.org/wiki?curid=3883", "text": "Lists of people\n\nLists of people by characteristics include:\n\n\n\n\n\n\n\n\n\n\n", "id": "3883", "title": "Lists of people"}
{"url": "https://en.wikipedia.org/wiki?curid=3912", "text": "List of major biblical figures\n\nThe Bible is a canonical collection of texts considered sacred in Judaism or Christianity. Different religious groups include different books within their canons, in different orders, and sometimes divide or combine books, or incorporate additional material into canonical books. Christian Bibles range from the sixty-six books of the Protestant canon to the eighty-one books of the Ethiopian Orthodox Church canon.\n\n\nAccording to the Book of Genesis, the Israelites were descendants of the sons of Jacob, who was renamed Israel after wrestling with an angel. His twelve male children become the ancestors of the Twelve Tribes of Israel.\n\n\nJesus\n\nThe Twelve:\n\nOthers:\n\n\n\n\n\n", "id": "3912", "title": "List of major biblical figures"}
{"url": "https://en.wikipedia.org/wiki?curid=3914", "text": "British and Irish Lions\n\nThe British and Irish Lions is a rugby union team selected from players eligible for any of the Home Nations – the national sides of England, Ireland, Scotland and Wales. The Lions are a Test side, and generally select international players, but they can pick uncapped players available to any one of the four unions. The side tours every four years, with these rotating among Australia, New Zealand, and South Africa. The 2009 Test series was lost 2–1 to South Africa, while the 2013 Test series was won 2–1 over Australia.\n\nFrom 1888 onwards combined rugby sides from the United Kingdom of Great Britain and Ireland toured the Southern Hemisphere. The first tour was a commercial venture, and was undertaken without official backing. The six subsequent visits enjoyed a growing degree of support from the authorities, before the 1910 South Africa tour, which was the first tour representative of the four Home Unions. In 1949 the four Home Unions formally created a Tours Committee and for the first time, every player of the 1950 Lions squad had played internationally before the tour. The 1950s tours saw high win rates in provincial games, but the Test series were typically lost or drawn. The winning series in 1971 (New Zealand) and 1974 (South Africa) changed this pattern. The last tour of the amateur age took place in 1993.\n\nThe multi-nation team that is today named the British and Irish Lions first came into existence in 1888 as the Shaw & Shrewsbury Team. It was then primarily English in composition but also contained players from Scotland and Wales. Later the name British Isles became associated with the team. On their 1950 tour of New Zealand and Australia they officially adopted the name British Lions, the nickname first used by British and South African journalists on the 1924 South African tour after the lion emblem on their ties, the emblem on their jerseys having been dropped in favour of the four-quartered badge with the symbols of the four represented unions.\n\nWhen the team first emerged in the nineteenth century, it represented the United Kingdom of Great Britain and Ireland, then one single state. The team continued to exist after the Irish war of independence and civil war and the subsequent division of Ireland in 1922 into the Irish Free State (later the Republic of Ireland) and Northern Ireland. To avoid ambiguity and possible offence to Irish players, it was later decided to define the team's identity as representing the two sovereign states – the United Kingdom and Ireland – with the team members being either \"British\" or \"Irish\". From the 2001 tour of Australia, the official name British and Irish Lions has been used. The team is often referred to simply as the Lions.\n\nAs the Lions represent two sovereign states, they do not have a national anthem. For the 2005 tour to New Zealand, the Lions management commissioned a song, \"The Power of Four\", although it met was with little support among Lions fans at the matches and was not used on the 2009 or 2013 Tours.\n\nFor more than half a century, the Lions have been synonymous with the red jersey that sports the amalgamated crests of the four unions. However, prior to 1950 the strip went through a number of significantly different formats.\n\nIn 1888, the promoter of the first expedition to Australia and New Zealand, Arthur Shrewsbury, demanded \"something that would be good material and yet take them by storm out here\". The result was a jersey in thick red, white and blue hoops, worn above white shorts and dark socks. The tours to South Africa in 1891 and 1896 retained the red, white and blue theme but this time as red and white hooped jerseys and dark blue shorts and socks. The 1899 trip to Australia saw a reversion to red, white and blue jerseys, but with the blue used in thick hoops and the red and white in thin bands. The shorts remained blue, as did the socks although a white flash was added to the latter. The one-off test in 1999 between England and Australia that was played to commemorate Australia's first test against Reverend Matthew Mullineux's British side saw England wear an updated version of this jersey. In 1903, the South Africa tour followed on from the 1896 tour, with red and white hooped jerseys. The slight differences were that the red hoops were slightly thicker than the white (the opposite was true in 1896), and the white flash on the socks introduced in 1899 was partially retained. The Australia of 1904 saw exactly the same kit as in 1899, and it seemed that the British touring sides had settled on kits particular to the host destination. However, in 1908 with the Scottish and Irish unions refusing to be involved, the Anglo-Welsh side only sported red jerseys with a thick white band on their jerseys on tour to Australia and New Zealand. Blue shorts were retained, but the socks were for the first time red, with a white flash.\n\nThe Scots were once again involved in Dr Tom Smyth's 1910 team to South Africa. Thus, dark blue jerseys, were introduced with white shorts and the red socks of 1908. The jerseys also had a single lion-rampant crest. The 1924 tour returned to South Africa, retaining the blue jerseys but now with shorts to match. It is the 1924 tour that is credited as being the first in which the team were referred to as \"the Lions\", the irony being that it was on this tour that the single lion-rampant crest was replaced with the forerunner of the four-quartered badge with the symbols of the four represented unions, that is still worn today. Although the lion had been dropped from the jersey, the players had worn the lion motif on their ties as they arrived in South Africa, which led the press and public referring to them as \"the Lions\".\n\nThe unofficial 1927 Argentina tour used the same kit and badge. So powerful was the attribution of \"the Lions\" nickname that three heraldic versions of the animal returned as the jersey badge in 1930. This was the tour to New Zealand where the tourists now standard blue jerseys caused some controversy. The convention in rugby is for the home side to accommodate its guests when there is a clash of kit. The New Zealand side, by then already synonymous with the appellation \"All Blacks\", had an all black kit that clashed with the Lions' blue. After much reluctance and debate New Zealand agreed to change for the Tests and the All Blacks became the All Whites for the first time. On the 1930 tour a delegation led by the Irish lock George Beamish expressed their displeasure at the fact that whilst the blue of Scotland, white of England and red of Wales were represented in the strip there was no green for Ireland. A green flash was added to the socks, which from 1938 became a green turnover (although on blue socks thus eliminating red from the kit), and that has remained a feature of the strip ever since. In 1936, the four-quartered badge returned for the tour to Argentina and has remained on the kits ever since, but other than that the strip remained the same.\n\nThe adoption of the red jersey happened in the 1950 tour. A return to New Zealand was accompanied by a desire to avoid the controversy of 1930 and so red replaced blue for the jersey with the resultant kit being that which is still worn today, the combination of red jersey, white shorts and green and blue socks, representing the four unions. The only additions to the strip since 1950 began appearing in 1993, with the addition of kit suppliers logos in prominent positions. Umbro had in 1989 asked for \"maximum brand exposure whenever possible\" but this did not affect the kit's appearance. Since then, Nike then Adidas have had more overt branding on the shirts, with sponsors Scottish Provident (1997), (2001), Zurich (2005) and HSBC (2009 & 2013).\n\nThe earliest tours date back to 1888, when a 21-man squad visited Australia and New Zealand. The squad drew players from England, Scotland and Wales, though English players predominated. The 35-match tour of two host nations included no tests, but the side played provincial, city and academic sides, winning 27 matches. They played 19 games of Australian rules football, against prominent clubs in Victoria and South Australia, winning six and drawing one of these (see Australian rules football in England).\n\nThe first tour, although unsanctioned by rugby bodies, established the concept of Northern Hemisphere sporting sides touring to the Southern Hemisphere. Three years after the first tour, the Western Province union invited rugby bodies in Britain to tour South Africa. Some saw the 1891 team – the first sanctioned by the Rugby Football Union – as the English national team, though others referred to it as \"the British Isles\". The tourists played a total of twenty matches, three of them tests. The team also played the regional side of South Africa (South Africa did not exist as a political unit in 1891), winning all three matches. In a notable event of the tour, the touring side presented the Currie Cup to Griqualand West, the province they thought produced the best performance on the tour.\n\nFive years later a British Isles side returned to South Africa. They played one extra match on this tour, making the total of 21 games, including four tests against South Africa, with the British Isles winning three of them. The squad had a notable Irish orientation, with the Irish national team contributing six players to the 21-man squad.\n\nIn 1899 the British Isles touring side returned to Australia for the first time since the unofficial tour of 1888. The squad of 23 for the first time ever had players from each of the home nations. The team again participated in 21 matches, playing state teams as well as northern Queensland sides and Victorian teams. A four-test series took place against Australia, the tourists winning three out of the four. The team returned via Hawaii and Canada playing additional games on route.\n\nFour years later, in 1903, the British and Irish team returned to South Africa. The opening performance of the side proved disappointing from the tourists' point of view, with defeats in its opening three matches by Western Province sides in Cape Town. From then on the team experienced mixed results, though more wins than losses. The side lost the test series to South Africa, drawing twice, but with the South Africans winning the decider 8 to nil.\n\nNo more than twelve months passed before the British and Irish team ventured to Australia and New Zealand in 1904. The tourists devastated the Australian teams, winning every single game. Australia also lost all three tests to the visitors, even getting held to a standstill in two of the three games. Though the New Zealand leg of the tour did not take long in comparison to the number of Australian games, the British and Irish experienced considerable difficulty across the Tasman after whitewashing the Australians. The team managed two early wins before losing the test to New Zealand and only winning one more game as well as drawing once. Despite their difficulties in New Zealand, the tour proved a raging success on-field for the British and Irish.\n\nIn 1908, another tour took place to Australia and New Zealand. In a reversal of previous practice, the planners allocated more matches in New Zealand rather than in Australia: perhaps the strength of the New Zealand teams and the heavy defeats of all Australian teams on the previous tour influenced this decision. Some commentators thought that this tour hoped to reach out to rugby communities in Australia, as rugby league (infamously) started in Australia in 1908. The Anglo-Welsh side (Irish and Scottish unions did not participate) performed well in all the non-test matches, but drew a test against New Zealand and lost the other two.\n\nVisits that took place before the 1910 South Africa tour (the first selected by a committee from the four Home Unions) had enjoyed a growing degree of support from the authorities, although only one of these included representatives of all four nations. The 1910 tour to South Africa marked the official beginning of British and Irish rugby tours: the inaugural tour operating under all four unions. The team performed moderately against the non-test parties, claiming victories in just over half their matches. The test series, however, went to South Africa, who won two of the three games. A side managed by Oxford University — supposedly the England rugby team, but actually including three Scottish players — toured Argentina at the time: the people of Argentina termed it the \"Combined British\".\n\nThe next British Isles team tour did not take place until 1924, again in South Africa. The team, led by Ronald Cove-Smith, struggled with injuries and lost three of the four test matches, drawing the other 3–3. In total, 21 games were played, with the touring side winning 9, drawing 3 and losing 9. This tour may have marked the occasion when the team first became known as \"the Lions\".\n\nIn 1927 a short, nine-game series took place in Argentina, with the Lions winning all nine encounters; the tour did however become a financial success for Argentine rugby. After a seemingly long absence from New Zealand, the Lions returned in 1930 to some success. The Lions won all of their games that did not have test status except for the matches against Auckland, Wellington and Canterbury; they did however lose three of their four test matches against New Zealand, winning the first test 6–3. The side also visited Australia, losing a test but winning five out of the six non-test games.\n\nIn 1936 the Lions visited Argentina for the second time, winning all ten of their matches and only conceding nine points in the whole tour. Two years later the Lions toured in South Africa, winning more than half of their normal matches. Despite having lost the test series to South Africa by game three, the Lions won the final test.\n\nThe first post-war tour went to New Zealand and Australia in 1950. The Lions, sporting newly redesigned jerseys and displaying a fresh style of play, managed to win 22 and draw one of 29 matches over the two nations. The Lions won the opening four fixtures before losing to Otago and Southland, but succeeded in holding the All Blacks to a 9–9 draw. The Lions performed well in the remaining All Black tests though they lost all three, the team did not lose another non-test in the New Zealand leg of the tour. The Lions won all their games in Australia except for their final fixture against a New South Wales XV in Newcastle. They won both tests against Australia, in Brisbane, Queensland and in Sydney.\n\nIn 1955 the Lions toured South Africa and left with another imposing record, one draw and 19 wins from the 25 fixtures. The four-test series against South Africa, a thrilling affair, ended in a drawn series.\n\nThe 1959 tour to Australia and New Zealand marked once again a very successful tour for the Lions, who only lost six of their 35 fixtures. The Lions easily won both tests against Australia and lost the first three tests against the All Blacks, but did find victory (9–6) in the final test.\n\nAfter the glittering decade of the 1950s, the first tour of the 1960s proved not nearly as successful as previous ones. The 1962 tour to South Africa saw the Lions still win 16 of their 25 games, but did not fare well against the Springboks, losing three of the four tests. For the 1966 tour to Australia and New Zealand John Robins became the first Lions Coach, and the trip started off very well for the Lions, who stormed through Australia, winning five non-tests and drawing one; and most notably defeating Australia in two tests as well. The Lions however experienced mixed results during the New Zealand leg of the tour, as well as losing all of the tests against the All Blacks. The Lions also played a test against Canada on their way home, winning 19 to 8 in Toronto. The 1968 tour of South Africa saw the Lions win 15 of their 16 provincial matches, but the team actually lost three tests against the Springboks and drew one.\n\nThe 1970s saw a renaissance for the Lions. The 1971 team, centred around the skilled Welsh half-back pairing of Gareth Edwards and Barry John, secured a series win over the All Blacks. The tour started with a loss to Queensland but proceeded to storm through the next provincial fixtures, winning 11 games in a row. The Lions then went on to defeat the All Blacks in Dunedin. The Lions would only lose a single match on the rest of the tour, and won the test series against New Zealand, winning and drawing the last two games, to take the series two wins to one.\n\nOne of the best-known and most successful Lions team toured South Africa in 1974 under the esteemed Irish forward Willie John McBride. It went through 22 games unbeaten, and triumphed 3–0 (with one drawn) in the test open series. The test series featured a lot of violence. The management of the Lions concluded that the Springboks dominated their opponents with physical aggression. At that time, test match referees came from the home nation, substitutions took place only if a doctor found a player unable to continue and there were no video cameras or sideline officials to prevent violent play. The Lions decided \"to get their retaliation in first\" with the infamous \"99 call\". The Lions postulated that a South African referee would probably not send off all of the Lions if they all retaliated against \"blatant thuggery\". Famous video footage of the 'battle of Boet Erasmus Stadium' shows JPR Williams running over half of the pitch and launching himself at Van Heerden after such a call.\n\nThe 1977 tour to New Zealand saw the Lions drop only one non-test out of 21 games, a loss to a Universities side. The team did not win the test series though, winning one game but losing the other three.\n\nIn August 1977 the British Lions made a stopover in Fiji on the way home from their tour of New Zealand. Fiji beat them 25–21 at Buckhurst Park, Suva.\n\nThe Lions toured South Africa in 1980. The team completed a flawless non-test record, winning 14 out of 14 non-test matches on the tour. The Lions did however lose the first three tests to South Africa, winning the last one, though the series had already been won by the Springboks.\n\nThe 1983 tour to New Zealand saw the team successful on the non-test front, winning all but two games, but getting white-washed in the test-series against the All Blacks. A tour to South Africa by the Lions was anticipated in 1986. However, the invitation for the Lions to tour South Africa was never accepted because of controversy surrounding Apartheid. As a result, the anticipated tour never occurred in 1986. The Lions did not return to South Africa until 1997, after the Apartheid era. A Lions team was selected in 1986 for the International Rugby Board centenary match against a Rest of the World XV in April of that year; the team was organised by the Four Home Unions Committee and the players were given the status of official British Lions. The Lions tour to Australia in 1989 was a short affair, being only 12 matches in total. The tour was very successful for the Lions, who won all eight non-tests and won the test series against Australia, two to one.\n\nThe Lions tour to New Zealand in 1993 was the last of the amateur era. The tourists won six and lost four non-test matches, and lost the test series 2–1.\nThe tour to South Africa in 1997 was a success for the Lions, who completed the tour with only two losses. The Lions won the test series 2–1.\n\nIn 2001, the ten game tour to Australia, saw the Wallabies win the test series 2–1. This series saw the first award of the Tom Richards Trophy. The Lions' 2005 tour to New Zealand, coached by 2003 England world cup winning coach Clive Woodward, won all seven games against provincial teams however suffered heavy defeats in all three tests and were narrowly defeated by the New Zealand Maori team.\n\nThe Lions faced the World Cup winners South Africa, with Ian McGeechan leading a coaching team including Warren Gatland, Shaun Edwards and Rob Howley. The Lions were captained by Irish lock Paul O'Connell. The initial Lions selection consisted of fourteen Irish players, thirteen Welsh, eight English and two Scots in the 37-man squad.\n\nIn the first Test on 20 June, they lost 26–21, and lost the series in the second 28–25 in a tightly-fought game at Loftus Versfeld on 27 June. The Lions won the third Test 28–9 at Ellis Park, and the series finished 2–1 to South Africa.\n\nDuring June 2013 the British and Irish Lions toured Australia.\n\nFormer Scotland and Lions full-back Andy Irvine was appointed as tour manager in 2010. Wales head coach Warren Gatland was the Lions' head coach, and their tour captain was Sam Warburton.\n\nThe tour started in Hong Kong with a match against the Barbarians before moving on to Australia for the main tour featuring six provincial matches and three tests.\n\nThe Lions won all but one non-test matches, losing to the Brumbies 41–12 on 18 June. The first test was followed shortly after this, which saw the Lions go 1-up over Australia winning 23–21. Australia did have a chance to take the win in the final moments of the game, but a missed penalty by Kurtley Beale saw the Lions take the win. The Wallabies drew the series in the second test winning 16–15, though the Lions had a chance to steal the win had it not been because of a missed penalty by Leigh Halfpenny. With tour captain Warburton out of the final test due to injury, Alun Wyn Jones took over the captaincy in the final test in Sydney. The final test was won by the Lions in what was a record win, winning 41–16 to earn their first series win since 1997 and their first over Australia since 1989.\n\nThe next British and Irish Lions tour will be to New Zealand in 2017.\n\nOverall test matches\n\nUpdated after the 2013 Tour\n\nOverall tour results\n\nThe Lions tour three southern hemisphere nations; Australia, South Africa and New Zealand. They also routinely toured in Argentina before World War II. Tours currently take place every four years. The most recent tour visited Australia in June–July 2013, and before that the Lions toured South Africa in 2009.\n\nIn a break with tradition, a \"home\" fixture against Argentina took place at Millennium Stadium in Cardiff on 23 May 2005, before the Lions went to New Zealand. It finished in a draw, 25–25.\n\nOn tour, games take place against local provinces, clubs or representative sides as well as the full tests against the host's national team.\n\nThe Lions, and their predecessor teams, have often played games against other nearby countries on tour. For example, they played Rhodesia (the future Zimbabwe) in 1910, 1924, 1938, 1955, 1962, 1968 & 1974 during their tours to South Africa. They also were beaten by Fiji on their 1977 tour to New Zealand. In addition, they toured pre-independence Namibia (then South West Africa), in 1955, 1962, 1968, and 1974.\n\nThere have also been games in other countries on the way home. These include games in in 1959 and 1966, East Africa (then mostly Kenya, and held in Nairobi), and an unofficial game against Ceylon (future Sri Lanka) in 1950.\n\nPrevious tours have seen some non-Test players become demotivated, but more recently this issue has reduced because of high injury rates, increased use of replacements and greater selection flexibility.\n\nIn recent tours a common issue has been weak opposition in many non-Test games, partly as opposition countries have their top players in national training camps, partly as sides protect top players for domestic games which are seen as more commercially important than Lions games, and possibly partly to prevent the Lions having high standard preparatory games.\n\nThe Lions have played a number of \"home matches\" against international opposition. With the exception of the 2005 home match against Argentina (which was played as a warm-up to the 2005 British and Irish Lions tour to New Zealand), these matches have been one-offs to mark special occasions:\n\n\n\na. Names of the Lions in the languages of Britain and Ireland:\n\n", "id": "3914", "title": "British and Irish Lions"}
{"url": "https://en.wikipedia.org/wiki?curid=3916", "text": "Bass guitar\n\nThe bass guitar (also called electric bass, or simply bass) is a stringed instrument played primarily with the fingers or thumb, by plucking, slapping, popping, strumming, tapping, thumping, or picking with a plectrum, often known as a pick.\n\nThe bass guitar is similar in appearance and construction to an electric guitar, but with a longer neck and scale length, and four to six strings or courses. The four-string bass, by far the most common, is usually tuned the same as the double bass, which corresponds to pitches one octave lower than the four lowest pitched strings of a guitar (E, A, D, and G). The bass guitar is a transposing instrument, as it is notated in bass clef an octave higher than it sounds (as is the double bass) to avoid excessive ledger lines. Like the electric guitar, the bass guitar has pickups and it is plugged into an amplifier and speaker on stage, or into a larger PA system using a DI unit, for live performances.\n\nSince the 1960s, the bass guitar has largely replaced the double bass in popular music as the bass instrument in the rhythm section. While types of basslines vary widely from one style of music to another, the bassist usually plays a similar role: anchoring the harmonic framework and establishing the beat. Many styles of music include the bass guitar, including rock, heavy metal, pop, punk rock, country, reggae, gospel, blues, symphonic rock, and jazz. It is often a solo instrument in jazz, jazz fusion, Latin, funk, progressive rock and other rock and metal styles.\n\nIn the 1930s, musician and inventor Paul Tutmarc from Seattle, Washington, who was manufacturing lap steel guitars, developed the first electric string bass in its modern form, a fretted instrument designed to be played horizontally. The 1935 sales catalog for Tutmarc's electronic musical instrument company, Audiovox, featured his \"Model 736 Bass Fiddle\", a four-stringed, solid-bodied, fretted electric bass instrument with a scale length. The adoption of a \"guitar\" form made the instrument easier to hold and transport than any of the existing stringed bass instruments. The addition of frets enabled bassists to play in tune more easily than on acoustic or electric upright basses. Around 100 of these instruments were made during this period. .\n\nAround 1947, Tutmarc's son, Bud, began marketing a similar bass under the Serenader brand name, prominently advertised in the nationally distributed L. D. Heater Music Company wholesale jobber catalogue of 1948. However, the Tutmarc family inventions did not achieve market success.\n\nIn the 1950s, Leo Fender, with the help of his employee George Fullerton, developed the first mass-produced electric bass. Fender was the founder of Fender Electric Instrument Manufacturing Company, which made electric popular brands of electric guitars (Fender Stratocaster and Fender Telecaster, basses and widely-used guitar amps (Fender Bassman). Fender's Fender Precision Bass, which began production in October 1951, became a widely copied industry standard for the instrument. The Precision Bass (or \"P-bass\") evolved from a simple, un-contoured \"slab\" body design and a single coil pickup similar to that of a Telecaster to a contoured body design with beveled edges for comfort and a split single coil pickup with four magnetic poles on each half, two poles for each string. This \"split pickup\" approach, introduced in 1957, was developed by using two mandolin pickups (Fender was marketing a four string solid body electric mandolin at the time). The pole pieces and leads of the coils were reversed with respect to each other, producing a \"humbucking\" effect. Humbucking is a design that electrically cancels the effect of any unwanted AC hum.\n\nThe \"Fender Bass\" was a revolutionary new instrument for gigging musicians. In comparison with the upright bass, which had been the main bass instrument in popular music, folk and country from the early 1900s to the 1940s, the Fender bass could be easily transported to shows and which was less prone to feedback when amplified than acoustic bass instruments. As well, the use of metal frets on electric basses, in comparison with the unfretted fingerboard on upright basses, made the electric bass easier to play in tune. As well, whereas it can be challenging to mic or find a good pickup for an upright bass, an electric bass comes with a built-in pickup system. Monk Montgomery was the first bass player to tour with the Fender bass guitar, with Lionel Hampton's postwar big band in 1953. Roy Johnson, and Shifty Henry with Louis Jordan & His Tympany Five, were other early Fender bass pioneers. Bill Black, playing with Elvis Presley, switched from upright bass to the Fender Precision Bass around 1957. The bass guitar was intended to appeal to guitarists as well as upright bass players, and many early pioneers of the instrument, such as Carol Kaye and Joe Osborn, were originally guitarists.\n\nFollowing Fender's lead, in 1953, Gibson released the first short scale violin-shaped electric bass with extendable end pin, allowing it to be played upright or horizontally. Gibson renamed the Electric Bass in 1958 to the EB-1 (The EB-1 was reissued around 1970, but this time without the end pin.) Also in 1958 Gibson released the maple arched top EB-2 described in the Gibson catalogue as \"A hollow-body electric bass that features a Bass/Baritone pushbutton for two different tonal characteristics\". In 1959 these were followed by the more conventional-looking EB-0 Bass. The EB-0 was very similar to a Gibson SG in appearance (although the earliest examples have a slab-sided body shape closer to that of the double-cutaway Les Paul Special).\n\nWhereas Fender basses had pickups mounted in positions in between the base of the neck and the top of the bridge, many of Gibson's early basses featured one humbucking pickup mounted directly against the neck pocket. The EB-3, introduced in 1961, also had a \"mini-humbucker\" at the bridge position. Gibson basses also tended to be smaller, sleeker instruments; Gibson did not produce a scale bass until 1963 with the release of the Thunderbird, which was also the first Gibson bass to use dual-humbucking pickups in a more traditional position, about halfway between the neck and bridge. A small number of other companies also began manufacturing bass guitars during the 1950s: Kay in 1952, and Danelectro in 1956;\n\n1956 saw the appearance at the German trade fair \"Musikmesse Frankfurt\" of the distinctive Höfner 500/1 violin bass made using violin construction techniques by Walter Höfner, a second generation violin luthier. The instrument is often known as the \"Beatle Bass\", due to its endorsement and use by Beatles bassist Paul McCartney. In 1957 Rickenbacker introduced the model 4000 bass, the first bass to feature a neck-through-body design in which the neck is part of the body wood. The Fender and Gibson versions used bolt-on and glued-on necks.\n\nWith the explosion of the popularity of rock music in the 1960s, many more manufacturers began making electric basses. First introduced in 1960, the Fender Jazz Bass was known as the Deluxe Bass and was meant to accompany the Jazzmaster guitar. The Jazz Bass (often referred to as a \"J-bass\") featured two single-coil pickups, one close to the bridge and one in the Precision bass' split coil pickup position. The earliest production basses had a 'stacked' volume and tone control for each pickup. This was soon changed to the familiar configuration of a volume control for each pickup, and a single, passive tone control. The Jazz Bass' neck was narrower at the nut than the Precision bass — versus — allowing for easier access to the lower strings and an overall spacing and feel closer to that of an electric guitar, allowing trained guitarists to transition to the bass guitar more easily.\n\nAnother visual difference that set the Jazz Bass apart from the Precision is its \"offset-waist\" body. Pickup shapes on electric basses are often referred to as \"P\" or \"J\" pickups in reference to the visual and electrical differences between the Precision Bass and Jazz Bass pickups.\n\nFender also began production of the Mustang Bass; a scale length instrument used by bassists such as Tina Weymouth of Talking Heads and Bill Wyman of The Rolling Stones (\"P\" and \"J\" basses have a scale length of , a design echoed on most current production electric basses of all makes). In the 1950s and 1960s, the instrument was often called the \"Fender bass\", due to Fender's early dominance in the market. The Fender VI, a baritone guitar, was tuned one octave lower than standard guitar tuning. It was released in 1961, and was favored by Jack Bruce of Cream.\n\nGibson introduced the short-scale EB-3 in 1961, also used by Jack Bruce.\n\nThe 1970s saw the founding of Music Man Instruments by Tom Walker, Forrest White and Leo Fender, which produced the StingRay, the first widely produced bass with active (powered) electronics. This amounts to an impedance-buffering preamplifier onboard the instrument to lower the output impedance of the bass's pickup circuit. This increasing low-pitch output and increased the instrument's tonal flexibility through giving the player the ability to amplify as well as attenuate (turn down) certain frequency ranges while improving the overall frequency response (more low-register and high-register sounds).\n\nSpecific bass brands/models became identified with particular styles of music, such as the Rickenbacker 4001 series, which became identified with progressive rock bassists like Chris Squire of Yes, and Geddy Lee of Rush, while the StingRay was used by funk/disco players such Louis Johnson of the funk band The Brothers Johnson and Bernard Edwards of Chic. The 4001 stereo bass was introduced in the late 1960s; it can be heard on from the Beatles \"I Am The Walrus\".\n\nIn 1971, Alembic established the template for what became known as \"boutique\" or \"high-end\" electric bass guitars. These expensive, custom-tailored instruments, as used by Phil Lesh, Jack Casady, and Stanley Clarke, featured unique designs, premium hand-finished wood bodies, onboard electronics for preamplification and equalization, and innovative construction techniques such as multi-laminate neck-through-body construction and graphite necks. In the mid-1970s, Alembic and other boutique bass manufacturers, such as Tobias, produced four-string and five-string basses with a low \"B\" string. In 1975, bassist Anthony Jackson commissioned luthier Carl Thompson to build a six-string bass tuned (low to high) B0, E1, A1, D2, G2, C3. In comparison with a standard four-string bass, Jackson's six-string adds a low BB string and a high C string. These 5 and 6-string \"extended-range basses\" would become popular with session bassists as they reduced the need for re-tuning to alternate detuned configurations like \"drop D\", and also allowed the bassist to play more notes from the same position on the fretboard with fewer shifts up and down the fingerboard, a crucial benefit for a session player sightreading basslines at a recording session.\n\nIn the 1980s, bass designers continued to explore new approaches. Ned Steinberger introduced a headless bass in 1979 and continued his innovations in the 1980s, using graphite and other new materials and (in 1984) introducing the TransTrem tremolo bar. In 1982, Hans-Peter Wilfer founded Warwick, to make a European bass, as the market at the time was dominated by Asian and American basses. Their first bass was the Streamer Bass, which is similar to the Spector NS. In 1987, the Guild Guitar Corporation launched the fretless Ashbory bass, which used silicone rubber strings and a piezoelectric pickup to achieve a \"upright bass\" sound with a short scale length. In the late 1980s, MTV's \"Unplugged\" show, which featured bands performing with acoustic instruments, helped to popularize hollow-bodied acoustic bass guitars amplified with piezoelectric pickups built into the bridge of the instrument.\n\nDuring the 1990s, as five-string basses became more widely available and more affordable, an increasing number of bassists in genres ranging from metal to gospel began using five-string instruments for added lower range—a low \"B\" string. As well, onboard battery-powered electronics such as preamplifiers and equalizer circuits, which were previously only available on expensive \"boutique\" instruments, became increasingly available on mid-priced basses. From 2000 to the 2010s, some bass manufacturers included digital modelling circuits inside the instrument on more costly instruments to recreate tones and sounds from many models of basses (e.g., Line 6's Variax bass). A modelling bass can digitally emulate the tone and sound of many famous basses, ranging from a vintage Fender Precision to a Rickenbacer. However, as with the electric guitar, traditional \"passive\" bass designs, which include only pickups, tone and volume knobs (without a preamp or other electronics) remained popular. Reissued versions of vintage instruments such as the Fender Precision Bass and Fender Jazz Bass remained popular amongst new instrument buyers up to the 2010s. In 2011, a 60th Anniversary P-bass was introduced by Fender, along with the re-introduction of the short-scale Fender Jaguar Bass.\n\nBass bodies are typically made of wood, although other materials such as graphite (for example, some of the Steinberger designs) and other lightweight composite materials have also been used. While a wide variety of woods are suitable for use in the body, neck, and fretboard of the bass guitar, the most common types of wood used are similar to those used for electric guitars; alder, ash or mahogany for the body, maple for the neck, and rosewood or ebony for the fretboard. While these traditional standards are most common, for tonal or aesthetic reasons luthiers more commonly experiment with different tonewoods on basses than with electric guitars (though this is changing), and rarer woods like walnut and figured maple, as well as exotic woods like bubinga, wenge, koa, and purpleheart, are often used as accent woods in the neck or on the face of mid- to high-priced production basses and on custom-made and boutique instruments.\n\nOther design options include finishes, such as lacquer, wax and oil; flat and carved designs; luthier-produced custom-designed instruments; headless basses, which have tuning machines in the bridge of the instrument (e.g., Steinberger and Hohner designs) and several artificial materials such as luthite. The use of artificial materials (e.g., BassLab) allows for unique production techniques such as die-casting, to produce complex body shapes. While most basses have solid bodies, they can also include hollow chambers to increase the resonance or reduce the weight of the instrument. Some basses are built with entirely hollow bodies, which change the tone and resonance of the instrument. Acoustic bass guitars have a hollow wooden body constructed similarly to an acoustic guitar, and are typically equipped with piezoelectric or magnetic pickups and amplified.\n\nInstruments handmade by highly skilled luthiers are becoming increasingly available in the 2010s. Exotic materials used in high-end instruments include woods such as bubinga, wenge, ovangkol, ebony and goncalo alves. Graphite composite is used to make lightweight necks Exotic woods are used on more expensive instruments: for example, Alembic uses cocobolo as a body or top layer material because of its attractive grain. Warwick bass guitars are also well known for exotic hardwoods: most of the necks are made of ovangkol, and the fingerboards use wenge or ebony. Solid bubinga bodies are also used for their tonal and aesthetic qualities.\n\nA common feature of more expensive basses is \"neck-through\" construction. Instead of milling the body from a single piece of wood (or \"bookmatched\" halves) and then attaching the neck into a pocket (so-called \"bolt-on\" design), neck-through basses are constructed first by assembling the neck, which may comprise one, three, five or more layers of wood in vertical stripes, which are longer than the length of the fretboard. To this elongated neck, the body is attached as two \"wings,\" which may also be made up of several layers. The entire bass is then milled and shaped. \"Neck-through\" construction advertisements claim this approach provides better sustain and a mellower tone than \"bolt-on\" neck construction. While neck-through construction is most common in handmade \"boutique\" basses, some models of mass-produced basses such as Ibanez's BTB series also have neck-through construction. Bolt-on neck construction does not necessarily imply a cheaply made instrument; virtually all traditional Fender designs still use bolt-on necks, including its high-end instruments costing thousands of dollars, and many boutique luthiers such as Sadowsky build bolt-on basses as well as neck-through instruments.\n\nThe number of frets installed on a bass guitar neck may vary. The original Fender basses had 20 frets, and most bass guitars have between 20 and 24 frets or fret positions. Instruments with between 24 and 36 frets (2 and 3 octaves) also exist. Instruments with more frets are used by bassists who play bass solos, as more frets gives them additional upper range notes. When a bass has a large number of frets, such as a 36 fret instrument, the bass may have a deeper \"cutaway\" to enable the performer to reach the higher pitches. Like electric guitars, fretted basses typically have markers on the fingerboard and on the side of the neck to assist the player in determining where notes and important harmonic points are. The markers indicate the 3rd, 5th, 7th, 9th fret and 12th fret (the 12th fret being the octave of the open string) and on the octave-up equivalents of the 3rd fret and as many additional positions as an instrument has frets for. Typically, one marker is used for the 3rd, 5th, 7th and 9th fret positions and two markers are used for the 12th fret.\n\nThe long scale necks on Leo Fender's basses—with a scale length (distance between nut and bridge) of — set the standard for electric basses, although \"short scale\" instruments, such as the Höfner 500/1 \"violin bass\" played by Paul McCartney, and the Fender Mustang Bass are also common. Short scale instruments use the same E-A-D-G tuning as a regular long scale instrument. Short scale instruments are good choices for bassists with smaller hands, such as children or young teens who are just starting the instrument. While , , and scale lengths were once only available in \"boutique\" instruments, in the 2000s (decade), many manufacturers began offering these \"extra long\" scale lengths. This extra long scale provides a higher string tension, which may yield a more defined, deep tone on the low \"B\" string of five- and six-stringed instruments (or detuned four-string basses).\n\nAnother design consideration for the bass is whether to use frets on the fingerboard. On a fretted bass, the metal frets divide the fingerboard into semitone divisions (as on an electric guitar or acoustic guitar). Fretless basses have a distinct sound, because the absence of frets means that the string must be pressed down directly onto the wood of the fingerboard with the fingers, as with the double bass. The string buzzes against the wood and is somewhat muted because the sounding portion of the string is in direct contact with the flesh of the player's finger. The fretless bass allows players to use the expressive approaches such as glissando (sliding up or down in pitch, with all of the pitches in between sounding), and true vibrato (in which the player alternates adds expression to a note by rocking the finger which is stopping the note and raising or lowering the pitch slightly). Players may also play music utilising microtones, or temperaments other than equal temperament, such as just intonation. While fretless basses are often associated with jazz and jazz fusion, bassists from other genres have used fretless basses, such as Freebo (country), Rick Danko (rock/blues), Rod Clements (folk), Steve DiGiorgio (metal) and Colin Edwin (modern/progressive rock). Some bassists alternate between fretted and fretless basses in performances, according to the type of material or tunes they are performing, e.g., Pino Palladino or Tony Levin.\n\nThe first fretless bass guitar was made by Bill Wyman in 1961 when he converted an inexpensive Japanese fretted bass by simply removing the frets and filling in the slots cut into the neck with wood putty. The first production fretless bass was the Ampeg AUB-1 introduced in 1966, and Fender introduced a fretless Precision Bass in 1970. Around 1970, Rick Danko from The Band began to use an Ampeg fretless, which he modified with Fender pickups—as heard on the 1971 \"Cahoots\" studio album and the \"Rock of Ages\" album recorded live in 1971. Danko said, \"It's a challenge to play fretless because you have to really use your ear.\" In the early 1970s, fusion-jazz bassist Jaco Pastorius had the fingerboard of his de-fretted Fender Jazz Bass coated in epoxy resin, allowing him to use roundwound strings for a brighter sound. Some fretless basses have \"fret line\" markers inlaid in the fingerboard as a guide, while others only use guide marks on the side of the neck.\n\nTapewound (double bass type) and flatwound strings are sometimes used with the fretless bass so the metal string windings do not wear down the fingerboard. Tapewound and flatwound strings have a distinctive tone and sound. Some fretless basses have epoxy-coated fingerboards, or fingerboards made of an epoxy composite like micarta, to increase the fingerboard's durability, enhance sustain, and give a brighter tone.\n\nThe standard design for the electric bass guitar has four strings, tuned E, A, D and G, in fourths such that the open highest string, G, is an eleventh (an octave and a fourth) below middle C, making the tuning of all four strings the same as that of the double bass (E1, A1, D2, G2). This tuning is also the same as the standard tuning on the lower four strings on a six-string guitar, only an octave lower. \n\nThere are a range of different string types include all-metal strings (which are available in many varieties, including roundwound, flatwound, halfwound, ground wound, and pressure wound); as well as metal strings with different coverings, such as tapewound and wound with plastic coatings. The variety of materials used in the strings gives bass players a range of tonal options. In the 1950s and early 1960s, bassists mostly used flatwound strings with a smooth surface, which had a smooth, damped sound reminiscent of a double bass. In the late 1960s and 1970s, roundwound bass strings producing a brighter tone similar to steel guitar strings became popular, though flatwounds also remain in use by players seeking a vintage tone. Roundwounds have a brighter timbre with longer sustain than flatwounds.\n\nA variety of tuning options and number of string courses have been used to extend the range of the instrument, or facilitate different modes of playing. The most common are four, five, or six strings:\n\n\nSome bassists use other types of tuning to extend the range or get other benefits, such as providing multiple octaves of notes at any given position, or a larger tonal range. Instrument types or tunings used for this purpose include basses with fewer than four strings (one-string bass guitars, two-string bass guitars, three-string bass guitars [tuned to E-A-D]) and alternative tunings e.g., tenor bass.\n\nExtended range basses (ERBs) are basses with six to twelve strings—with the additional strings used for range rather than unison or octave pairs. A seven-string bass (B0-E1-A1-D2-G2-C3-F3) was built by luthier Michael Tobias in 1987. This instrument, commissioned by bassist Garry Goodman, was an early example of a bass with more than six single course strings. In 1999 South American ERB player Igor Saavedra designed one of the first 8 string ERBs known, and asked Luthier Alfonso Iturra to build it for him.\n\nIn 2011 Warwick released a new Thumb NT 7 bass for Jeroen Paul Thesseling, featuring a scale with sub-contra tuning F-B-E-A-D-G-C. Yves Carbonne developed 10 and 12 string fretless sub-bass guitars.\n\nPiccolo basses are cosmetically similar to a four-stringed electric bass guitar, but usually tuned one whole octave higher than a normal bass. The first electric piccolo bass was constructed by luthier Carl Thompson for Stanley Clarke. To allow for the raised tuning, the strings are thinner, and the length of the neck (the scale) may be shorter. Several companies manufacture piccolo sets that can be put on any regular bass, thereby converting any bass into a piccolo bass. Because of the thinner strings, a new nut may be required to hold the strings. Some people prefer a slightly shorter scale, such as or , as the higher tension required for longer scale lengths coupled with the thinner gauge of higher-pitched strings can make a long-scale piccolo bass difficult to play. The tuning varies with the personal tastes of the artist, as does the number of strings. Joey DeMaio from the heavy metal band Manowar plays with four strings on his piccolo bass. Jazz bassist John Patitucci used a six-string piccolo bass, unaccompanied, on his song \"Sachi's Eyes\" on his album \"One More Angel\". Michael Manring has used a five-string piccolo bass in several altered tunings. Michael uses D'Addario EXL 280 piccolo bass strings on his four-string hyperbass, made by Zon Guitars.\n\nMost electric bass guitars use magnetic pickups. The vibrations of the instrument's ferrous metal strings within the magnetic field of the permanent magnets in magnetic pickups produce small variations in the magnetic flux threading the coils of the pickups. This in turn produces small electrical voltages in the coils. These low-level signals are then amplified and played through a speaker. Most basses have a volume potentiometer, which can be turned up or down, and a tone potentiometer, which rolls off the high frequencies when it is turned to the player's right. Some basses may also have a pickup selector control or switch. Since the 1980s, basses are often available with battery-powered \"active\" electronics that boost the signal with a preamplifier, provide equalization controls to boost or cut bass and treble frequencies, or both.\n\n\n\n\nMany basses have just one pickup, typically a \"P\" or \"MM\" pickup, though single soapbars are not unheard of. Multiple pickups are also quite common, two of the most common configurations being two \"J\" pickups (as on the stock Fender Jazz), or a \"P\" near the neck and a \"J\" near the bridge (e.g., Fender Precision Bass Special, Fender Precision Bass Plus). A two-\"soapbar\" configuration is also very common, especially on basses by makes such as Ibanez and Yamaha. A combination of a J or other single-coil pickup at the neck and a Music Man-style humbucker in the bridge has become popular among boutique builders, giving a very bright, focused tone that is good for jazz, funk and thumbstyle.\n\nSome basses use more unusual pickup configurations, such as a soapbar and a \"P\" pickup (found on some Fenders), Stu Hamm's \"Urge\" basses, which have a \"P\" pickup sandwiched between two \"J\" pickups, and some of Bootsy Collins' custom basses, which had as many as 5 J pickups. Another unusual pickup configuration is found on some of the custom basses that Billy Sheehan uses, in which there is one humbucker at the neck and a split-coil pickup at the middle position.\n\nThe placement of the pickup greatly affects the sound. A pickup near the neck joint emphasizes the fundamental and low-order harmonics and thus produces a deeper, bassier sound, while a pickup near the bridge emphasizes higher-order harmonics and makes a \"tighter\" or \"sharper\" sound. Usually basses with multiple pickups allow blending of the output from the pickups, with electrical and acoustical interactions between the two pickups (such as partial phase cancellations) allowing a range of tonal effects.\n\nThe use of non-magnetic pickups allows bassists to use non-ferrous strings such as nylon, brass, polyurethane and silicone rubber. These materials produce different tones and, in the case of the polyurethane or silicone rubber strings, allow much shorter scale lengths.\n\nLike the electric guitar, the electric bass guitar is almost always connected to an amplifier and a speaker with a patch cord for live performances. Electric bassists use either a \"combo\" amplifier, which combines an amplifier and a speaker in a single cabinet, or an amplifier and one or more speaker cabinets (typically stacked, with the amplifier sitting on the speaker cabinets, leading to the term \"half-stack\" for one cabinet setups and \"full stack\" for two). In most genres, a \"clean\" bass tone (without any amplifier-induced \"overdrive\" or \"distortion\") is desirable, and so while guitarists often prefer the more desirable distorted tones of tube-transistor amplifiers, bassists commonly use solid-state amplifier circuitry to achieve the necessary high output wattages with less weight than tubes (though smaller tubes can often still be found in the low-power \"preamplifier\" sections of the system, where they provide a warmer, smoother character to the bass tone for relatively little additional weight). A few all-tube bass amplifiers are still available, notably from the Ampeg brand. In some cases, when the bass is used with large-scale PA amplification, it is plugged into a \"DI\" or \"direct box\", which routes the signal to the bass amp while also sending the signal directly into a mixing console, and thence to the main and monitor speakers. When a recording of bass is being made, engineers may use a microphone set up in front of the amplifier's speaker cabinet for the amplified signal, a direct box signal that feeds the recording console, or a mix of both.\n\nVarious electronic bass effects such as preamplifiers, \"stomp box\"-style pedals and signal processors and the configuration of the amplifier and speaker can be used to alter the basic sound of the instrument. In the 1990s and early 2000s (decade), signal processors such as equalizers, overdrive devices (sometimes referred to as \"fuzz bass\"), and compressors or limiters became increasingly popular. Modulation effects like chorus, flanging, phase shifting, and time effects such as delay and looping are less commonly used with bass than with electric guitar, but they are used in some styles of music.\n\nMost bass players stand while playing, using a strap over the shoulder to hold the instrument, although sitting is also accepted, particularly in large ensemble settings, such as jazz big bands or in acoustic genres such as folk music. Some bassists, such as Jah Wobble, alternate between standing or seated playing. It is a matter of the player's preference as to which position gives the greatest ease of playing and what a bandleader expects. When sitting, right-handed players can balance the instrument on the right thigh or like classical guitar players, the left. When sitting, no strap is required. Balancing the bass on the left thigh usually positions it in such a way that it mimics the standing position, allowing for less difference between the standing and sitting positions. Balancing the bass on the right thigh provides better access to the neck and fretboard in its entirety, especially the lower-pitched frets.\n\nIn contrast to the upright bass (or double bass), the electric bass guitar is played horizontally across the body, like an electric guitar. When the strings are plucked with the fingers (pizzicato), the index and middle fingers (and sometimes the thumb, ring, and little fingers as well) are used. James Jamerson, an influential bassist from the Motown era, played intricate bass lines using only his index finger, which he called \"The Hook.\" There are also variations in how a bassist chooses to rest the right-hand thumb (or left thumb in the case of left-handed players). A player may rest his or her thumb on the top edge of one of the pickups or on the side of the fretboard, which is especially common among bassists who have an upright bass influence. Some bassists anchor their thumbs on the lowest string and move it off to play on the low string. Alternatively, the thumb can be rested loosely on the strings to mute the unused strings.\n\nThe string can be plucked at any point between the bridge and the point where the fretting hand is holding down the string; different timbres are produced depending on where along the string it is plucked. When plucked closer to the bridge, the string's harmonics are more pronounced, giving a brighter tone. Closer to the middle of the string, these harmonics are less pronounced, giving a more mellow tone.\n\nBassists trying to emulate the sound of a double bass sometimes pluck the strings with their thumb and use palm-muting to create a short, \"thumpy\" tone. The late Monk Montgomery (who played in Lionel Hampton's band) and Bruce Palmer (who performed with Buffalo Springfield) use thumb downstrokes. The use of the thumb was acknowledged by early Fender models, which came with a \"thumbrest\" or \"Tug Bar\" attached to the pickguard below the strings. Contrary to its name, this was not used to rest the thumb, but to provide leverage while using the thumb to pluck the strings. The thumbrest was moved above the strings in 1970s models (as a true thumbrest) and eliminated in the 1980s.\n\nThe slap and pop method, or \"thumbstyle\", most associated with funk, uses tones and percussive sounds achieved by striking, thumping, or \"slapping\" a string with the thumb and snapping (or \"popping\") a string or strings with the index or middle fingers. Bassists often interpolate left hand-muted \"dead notes\" between the slaps and pops to achieve a rapid percussive effect, and after a note is slapped or popped, the fretting hand may cause other notes to sound by using \"hammer ons\", \"pull offs\", or a left-hand glissando (slide). Larry Graham of Sly and the Family Stone and Graham Central Station was an early innovator of the slap style, and Louis Johnson of The Brothers Johnson is also credited as an early slap bass player.\n\nSlap and pop style is also used by many bassists in other genres, such as rock (e.g., J J Burnel and Les Claypool), metal (e.g., Eric Langlois, Martin Mendez, Fieldy and Ryan Martinie), and fusion (e.g., Marcus Miller, Victor Wooten and Alain Caron). Slap style playing was popularized throughout the 1980s and early 1990s by pop bass players such as Mark King (from Level 42) and rock bassists such as with Pino Palladino (currently a member of the John Mayer Trio and bassist for The Who), Flea (from the Red Hot Chili Peppers) and Alex Katunich (from Incubus). Spank bass developed from the slap and pop style and treats the electric bass as a percussion instrument, striking the strings above the pickups with an open palmed hand. Wooten popularized the \"double thump,\" in which the string is slapped twice, on the upstroke and a downstroke (for more information, see Classical Thump). A rarely used playing technique related to slapping is the use of wooden dowel \"funk fingers\", an approach popularized by Tony Levin.\n\nThe pick (or plectrum) is used to obtain a more articulate attack, for speed, or just personal preference. Although the use of a pick is primarily associated with rock and punk rock, picks are also used in other styles. Jazz bassist Steve Swallow often plays with a pick, while Pink Floyd bassist Roger Waters uses one for a heavier tone. Mike Gordon of Phish uses a pick while also incorporating slapping techniques into his playing. Picks can be used with alternating downstrokes and upstrokes, or with all downstrokes for a more consistent attack. The pick is usually held with the index and thumb, with the up-and-down plucking motion supplied by the wrist.\n\nThere are many varieties of picks available, but due to the thicker, heavier strings of the electric bass, bassists tend to use heavier picks than those used for electric guitar, typically ranging from 1.14 mm–3.00 mm (3.00 is unusual). Different materials are used for picks, including plastic, nylon, rubber, and felt, all of which produce different tones. Felt and rubber picks are used to emulate a fingerstyle tone.\n\nPalm-muting is a widely used bass technique. The outer edge of the palm of the picking hand is rested on the bridge while picking, and \"mutes\" the strings, shortening the sustain time. The harder the palm presses, or the more string area that is contacted by the palm, the shorter the string's sustain. The sustain of the picked note can be varied for each note or phrase. The shorter sustain of a muted note on an electric bass can be used to imitate the shorter sustain and character of an upright bass. Palm-muting is commonly done while using a pick, but can also be done without a pick, as when doing down-strokes with the thumb.\n\nOne prominent example of the pick/palm-muting combination is Paul McCartney, who has consistently used this technique for decades. Sting also uses palm-muting; but often does so without a pick, using the thumb and first finger to pluck.\nThe fretting hand, the left hand for right-handed bass players and the right hand for left-handed bass players, is used to press down the strings to play different notes and shape the tone or timbre of a plucked or picked note. The fundamental technique used in the fretting hand is known as \"a finger per fret\", where each finger in the fretting hand plays one fret in a given position. Also, the double bass technique can be used for fretting. This technique involves the use of four fingers in the space of three frets, especially in the lower positions. When considering the spacing between notes, this is a comfortable distance for the average person's hand size. The main advantage of the \"four fingers in three frets\" technique is less tendon strain, leading to a diminished likelihood of Repetitive Strain Injury (RSI). The \"four-in-three\" technique is demonstrated in the image below (A bassist performing tapping).\n\nThe fretting hand can be used to change a sounded note, either by fully muting it after it is plucked or picked to shorten its duration or by partially muting it near the bridge to reduce the volume of the note, or make the note die away faster. The fretting hand is often used to mute strings that are not being played and stop the sympathetic vibrations, particularly when the player wants a \"dry\" or \"focused\" sound. On the other hand, the sympathetic resonance of harmonically related strings may be desired for some songs, such as ballads. In these cases, a bassist can fret harmonically related notes. For example, while fretting a sustained \"F\" (on the third fret of the \"D\" string), underneath an F major chord being played by a piano player, a bassist might hold down the \"C\" and low \"F\" below this note so their harmonics sound sympathetically.\n\nThe fretting hand can add vibrato to a plucked or picked note, either a gentle, narrow vibrato or a more exaggerated, wide vibrato with bigger pitch variations. For fretted basses, vibrato is always an alternation between the pitch of the note and a slightly higher pitch. For fretless basses, the player can use this style of vibrato, or they can alternate between the note and a slightly lower pitch, as is done with the double bass and on other unfretted stringed instruments. While vibrato is mostly done on \"stopped\" notes—that is, notes that are pressed down on the fingerboard—open strings can also be vibratoed by pressing down on the string behind the nut. As well, the fretting hand can be used to \"bend\" a plucked or picked note up in pitch, by pushing or pulling the string so that the note sounds at a higher pitch. To create the opposite effect, a \"bend down\", the string is pushed to a higher pitch before being plucked or picked and then allowed to fall to the lower, regular pitch after it is sounded. Though rare, some bassists may use a tremolo bar-equipped bass to produce the same effect.\n\nIn addition to pressing down one note at a time, bassists can also press down several notes at one time with their fretting hand to perform a double stop (two notes at once) or a chord. While double stops and chords are used less often by bassists than by electric guitarists playing rhythm guitar, a variety of double stops and chords can be performed on the electric bass. Some double stops used by bassists include octaves. Chords can be especially with effective on instruments with higher ranges such as six-string basses. Another variation to fully pressing down a string is to gently graze the string with the finger at the harmonic node points on the string, which creates chime-like upper partials (also called \"overtones\"). Glissando is an effect in which the fretting hand slides up or down the neck, which can be used to create a slide in pitch up or down. A subtle glissando can be performed by moving the fretting hand without plucking or picking the string; for a more pronounced effect, the string is plucked or picked first, or, in a metal or hardcore punk context, a pick may be scraped along the sides of the lower strings.\n\nThe fretting hand can also be used to sound notes, either by plucking an open string with the fretting hand, or, in the case of a string that has already been plucked or picked, by \"hammering on\" a higher pitch or \"pulling off\" a finger to pluck a lower fretted or open stringed note. Jazz bassists use a subtle form of fretting hand pizzicato by plucking a very brief open string grace note with the fretting hand right before playing the string with the plucking hand. When a string is rapidly hammered on, the note can be prolonged into a trill.\n\nIn the two-handed tapping styles, bassists use both hands to play notes on the fretboard by rapidly pressing and holding the string to the fret. Instead of plucking or picking the string to create a sound, in this technique, the action of striking the string against the fret or the fretboard creates the sound. Since two hands can be used to play on the fretboard, this makes it possible to play interweaving contrapuntal lines, to simultaneously play a bass line and a simple chord, or play chords and arpeggios. Bassist John Entwistle of The Who tapped percussively on the strings, causing them to strike the fretboard with a twangy sound to create drum-style fills. Players noted for this technique include Cliff Burton, Billy Sheehan, Stuart Hamm, John Myung, Victor Wooten, Les Claypool, Mark King, and Michael Manring. The Chapman Stick and Warr Guitars are string instruments specifically designed to be played using two-handed tapping.\n\nStrumming, usually with finger nails, is a common technique on acoustic guitar, but it is not a commonly used technique for bass. However, a notable example is Stanley Clarke's bass playing on the introduction to \"School Days\", on the album of the same name.\n\nPopular music bands and rock groups use the bass guitar as a member of the rhythm section, which provides the chord sequence or \"progression\" and sets out the \"beat\" for the song. The rhythm section typically consists of a rhythm guitarist or electric keyboard player, or both, a bass guitarist and a drummer; larger groups may add additional guitarists, keyboardists, or percussionists.\n\nBassists often play a bass line composed by an arranger, songwriter or composer of a song—or, in the case of a cover song, the bass line from the original. In other bands—e.g., jazz-rock bands that play from lead sheets and country bands using the Nashville number system—bassists are expected to improvise or prepare their own part to fit the song's chord progression and rhythmic style.\n\nTypes of bass lines vary widely, depending on musical style. However, the bass guitarist generally fulfills a similar role: anchoring the harmonic framework (often by emphasizing the roots of the chord progression) and laying down the beat in collaboration with the drummer and other rhythm section instruments. The importance of the bass guitarist and the bass line varies in different styles of music. In some pop styles, such as 1980s-era pop and musical theater, the bass sometimes plays a relatively simple part as the music emphasizes vocals and melody instruments. In contrast, in reggae, funk, or hip-hop, entire songs may center on the bass groove, and the bass line is usually prominent in the mix.\n\nIn traditional music such as country music, folk rock, and related styles, the bass often plays the roots and fifth (typically the fifth below the root) of each chord in alternation. In these styles, bassists often use scalar \"walkups\" or \"walkdowns\" when there is a chord change. In Chicago blues, the electric bass often performs a walking bassline made up of scales and arpeggios. In blues rock bands, the bassist often plays blues scale-based riffs and chugging boogie-style lines. In metal, the bass guitar may perform complex riffs along with the rhythm guitarist or play a low, rumbling pedal point to anchor the group's sound.\n\nThe bass guitarist sometimes breaks out of the strict rhythm section role to perform bass breaks or bass solos. The types of bass lines used for bass breaks or bass solos vary by style. In a rock band, a bass break may consist of the bassist playing a riff or lick during a pause in the song. In some styles of metal, a bass break may consist of \"shred guitar\"-style tapping on the bass. In a funk or funk rock band, a bass solo may showcase the bassist's percussive slap and pop playing. In genres such as progressive rock, art rock, or progressive metal, the bass guitar player may play melody lines along with the lead guitar (or vocalist) and perform extended guitar solos.\n\nChords are not used that often by electric bass players. However, in some styles, bassists may sound \"double stops\", such as octaves with open strings and powerchords. In Latin music, double stops with fifths are used. Robert Trujillo of Metallica is known for playing \"massive chords\" and \"chord-based harmonics\" on the bass. Lemmy of Motörhead often played power chords in his bass lines.\n\nWhile bass guitar solos are not common in popular music, some artists, particularly in the heavy metal, funk, and progressive rock genres, do utilize them. In a rock context, bass guitar solos are structured and performed in a similar fashion as rock guitar solos, often with the musical accompaniment from the verse or chorus sections.\n\nBass solos are performed using a range of different techniques, such as plucking or fingerpicking. In the 1960s, The Who's bassist, John Entwistle, performed a bass break on the song \"My Generation\" using a plectrum. He originally intended to use his fingers, but could not put his plectrum down quickly enough. This is considered as one of the first bass solos in rock music, and also one of the most recognizable. Led Zeppelin's \"Good Times Bad Times\", the first song on their first album, contains two brief bass solos, occurring after the song's first and third choruses. Queen's bassist, John Deacon, occasionally played bass solos, such as on the song \"Liar\". Metallica's 1983 debut \"Kill Em All\" includes the song \"(Anesthesia) Pulling Teeth,\" consisting entirely of a bass solo played by Cliff Burton. John McVie of Fleetwood Mac performed a bass solo on \"The Chain\" from the 1977 \"Rumours\" album.\n\nManowar's bassist Joey DeMaio uses special piccolo bass for his extremely fast bass solos like \"Sting of the Bumblebee\" and \"William's Tale\". Green Day bassist Mike Dirnt played a bass solo on the song \"Welcome To Paradise\" from the 1994 album \"Dookie\" and on the song \"Makeout Party\" from the 2012 album \"¡Dos!\". U2 includes a bass solo most notably on \"Gloria\", in which Adam Clayton utilizes several playing techniques. Matt Freeman of Rancid performs a very fast, guitar-like bass solo in the song \"Maxwell Murder\". Blink-182's \"Voyeur\" has a bass solo, which is featured on both their studio album \"Dude Ranch\" & their live album \"The Mark, Tom and Travis Show (The Enema Strikes Back!)\".\n\nHeavy metal bass players such as Geezer Butler (Black Sabbath), Alex Webster (Cannibal Corpse), Cliff Burton (Metallica), and Les Claypool (Primus, Blind Illusion) have used chime-like harmonics and rapid plucking techniques in their bass solos. Geddy Lee of Rush has made frequent use of bass solos, such as on the instrumental \"YYZ\". In both published Van Halen concert videos, Michael Anthony performs unique maneuvers and actions during his solos. Funk bassists such as Larry Graham began using slapping and popping techniques for their solos, which coupled a percussive thumb-slapping technique of the lower strings with an aggressive finger-snap of the higher strings, often in rhythmic alternation. The slapping and popping technique incorporates a large number of muted (or 'ghost' tones) to normal notes to add to the rhythmic effect. Slapping and popping solos were prominent in 1980s pop and R&B, and they are still used by some modern funk and Latin bands.\n\nWhen playing bass solos, rock and metal bassists sometimes use effects such as fuzz bass or a wah-wah pedal to produce a more pronounced sound. Notably, Cliff Burton of Metallica used both effects. Due to the lower range of the bass, bass guitar solos usually have a much lighter accompaniment than solos for other instruments. In some cases, the bass guitar solo is unaccompanied, or accompanied only by the drums.\n\nThe electric bass is a relative newcomer to the world of jazz. The big bands of the 1930s and 1940s Swing era and the small combos of the 1950s Bebop and Hard Bop movements all used the double bass. The electric bass was introduced in some bands in the 1950s and it became prominent during the late 1960s and early 1970s, when rock influences were blended with jazz to create jazz-rock fusion.\n\nIn a jazz setting, the electric bass tends to have a much more expansive solo role than in most popular styles. In most rock settings, the bass guitarist may only have a few short bass breaks or brief solos during a concert. During a jazz concert, a jazz bassist may have a number of lengthy improvised solos, which are called \"blowing\" in jazz parlance. Whether a jazz bassist is comping (accompanying) or soloing, they usually aim to create a rhythmic drive and \"timefeel\" that creates a sense of \"swing\" and \"groove\". For information on notable jazz bassists, see the List of jazz bassists article.\nContemporary classical music uses both the standard instruments of Western Art music (piano, violin, double bass, etc.) and newer instruments or sound producing devices, ranging from electrically amplified instruments to tape players and radios.\n\nThe electric bass guitar has occasionally been used in contemporary classical music (art music) since the late 1960s.\nContemporary composers often obtained unusual sounds or instrumental timbres through the use of non-traditional (or unconventional) instruments or playing techniques. As such, bass guitarists playing contemporary classical music may be instructed to pluck or strum the instrument in unusual ways.\n\nHowever, contemporary classical composers may also write for the bass guitar in order to utilise its unique sound, and in particular its precise and piercing attack and timbre. For example, Steve Reich, explaining his decision to score 2x5 for two bass guitars, stated that \"[with electric bass guitars] you can have interlocking bass lines, which on an acoustic bass, played pizzicato, would be mud\".\n\nAmerican composers using electric bass in the 1960s included experimental classical music composer Christian Wolff (born 1934) (\"Electric Spring 1\", 1966; \"Electric Spring 2\", 1966/70; \"Electric Spring 3\", 1967; and \"Untitled\", 1996); Francis Thorne, a student of Paul Hindemith at Yale University (born 1922), who wrote (\"Liebesrock\" 1968–69); and Krzysztof Penderecki (Cello Concerto no. 1, 1966/67, rev. 1971/72), \"The Devils of Loudun\", 1969; \"Kosmogonia\", 1970; and \"Partita\", 1971), Louis Andriessen (\"Spektakel\", 1970; \"De Staat\", 1972–76; \"Hoketus\", 1976; \"De Tijd\", 1980–81 and \"De Materie\", 1984–1988). European composers who began scoring for the bass guitar in the 1960s included Danish composer Pelle Gudmundsen-Holmgreen (born 1932) (\"Symfoni på Rygmarven\", 1966; \"Rerepriser\", 1967; and \"Piece by Piece\", 1968); Irwin Bazelon (\"Churchill Downs\", 1970).\n\nIn the 1970s, electric bass was used by the American conductor-composer Leonard Bernstein (1918–1990) for his \"MASS\" (1971). American jazz pianist Dave Brubeck used bass guitar for his 1971 piece \"Truth Has Fallen\". Russian and Soviet composer Alfred Schnittke used the instrument for his Symphony no. 1, 1972. In 1977, David Amram (born 1930) scored for electric bass in \"En memoria de Chano Pozo\". Amram is an American composer known for his eclectic use of jazz, ethnic and folk music.\n\nIn the 1980s (and in following decades), electric bass was used in works by Hans Werner Henze (\"El Rey de Harlem\", 1980; and \"Il ritorno d'Ulisse in patria\", 1981), Harold Shapero, \"On Green Mountain (Chaconne after Monteverdi)\", 1957, orchestrated 1981; Schnittke's Symphony no. 3 (1981); Steve Reich's \"Electric Counterpoint\" (1987) and 2x5 (2008), Wolfgang Rihm (\"Die Eroberung von Mexico\", 1987–91), Arvo Pärt (\"Miserere\", 1989/92), Steve Martland (\"Dance works\", 1993; and \"Horses of Instruction\", 1994), Sofia Gubaidulina (\"Aus dem Stundenbuch\", 1991), Giya Kancheli (\"Wingless\", 1993), John Adams (\"I Was Looking at the Ceiling and Then I Saw the Sky\", 1995; and \"Scratchband\", 1996/97), Michael Nyman (various works for the Michael Nyman Band), Mark-Anthony Turnage (\"Blood on the Floor\", 1993–1996), numerous works by Art Jarvinen.\n\nThe pedagogy and training for the electric bass varies widely by genre and country. Rock and pop bass has a history of pedagogy dating back to the 1950s and 1960s, when method books were developed to help students learn the instrument. One notable method book was Carol Kaye's \"How to Play the Electric Bass\".\n\nIn the jazz scene, since the bass guitar takes on much of the same role as the double bass—laying down the rhythm, and outlining the harmonic foundation—electric bass players have long used both bass guitar methods and jazz double bass method books. The use of jazz double bass method books by electric bass players in jazz is facilitated in that jazz methods tend to emphasize improvisation techniques (e.g., how to improvise walking basslines) and rhythmic exercises rather than specific ways of holding or plucking the instrument.\n\nOf all of the genres, jazz and the mainstream commercial genres (rock, R&B, etc.) have the most established and comprehensive systems of instruction and training for electric bass. In the jazz scene, teens can begin taking private lessons on the instrument and performing in amateur big bands at high schools or run by the community. Young adults who aspire to becoming professional jazz bassists or studio rock bassists can continue their studies in a variety of formal training settings, including colleges and some universities.\n\nSeveral colleges offer electric bass training in the US. The Bass Institute of Technology (BIT) in Los Angeles was founded in 1978, as part of the Musician's Institute. Chuck Rainey (electric bassist for Aretha Franklin and Marvin Gaye) was BIT's first director. BIT was one of the earliest professional training program for electric bassists. The program teaches a range of modern styles, including funk, rock, jazz, Latin, and R&B.\n\nThe Berklee College of Music in Boston offers training for electric bass players. Electric bass students get private lessons and there is a choice of over 270 ensembles to play in. Specific electric bass courses include funk/fusion styles for bass; slap techniques for electric bass; fingerstyle R&B; five- and six-string electric bass playing (including performing chords); and how to read bass sheet music. Berklee College alumni include Jeff Andrews, Victor Bailey, Jeff Berlin, Michael Manring, and Neil Stubenhaus. The Bass Department has two rooms with bass amps for classes and ten private lesson studios equipped with audio recording gear. Berklee offers instruction for the four-, five-, and six-string electric bass, the fretless bass, and double bass. \"Students learn concepts in Latin, funk, Motown, and hip-hop, ... jazz, rock, and fusion.\"\n\nIn Canada, the Humber College Institute of Technology & Advanced Learning offers an Advanced Diploma (a three-year program) in jazz and commercial music. The program accepts performers who play bass, guitar, keyboard, drums, melody instruments (e.g., saxophone, flute, violin) and who sing. Students get private lessons and perform in 40 student ensembles.\n\nAlthough there are far fewer university programs that offer electric bass instruction in jazz and popular music, some universities offer bachelor's degrees (B.Mus.) and Master of Music (M.Mus.) degrees in jazz performance or \"commercial music\", where electric bass can be the main instrument. In the US, the Manhattan School of Music has a jazz program leading to B.Mus. and M.Mus degrees that accepts students who play bass (double bass and electric bass), guitar, piano, drums, and melody instruments (e.g., saxophone, trumpet, etc.).\n\nIn the Australian state of Victoria, the Victorian Curriculum and Assessment Authority has set out minimum standards for its electric bass students doing their end-of-year Solo performance recital. To graduate, students must perform pieces and songs from a set list that includes Baroque suite movements that were originally written for cello, 1960s Motown tunes, 1970s fusion jazz solos, and 1980s slap bass tunes. A typical program may include a Prelude by J.S. Bach; \"Portrait of Tracy\" by Jaco Pastorius; \"Twisted\" by Wardell Gray and Annie Ross; \"What's Going On\" by James Jamerson; and the funky Disco hit \"Le Freak\" by Chic.\n\nIn addition to college and university diplomas and degrees, there are a variety of other training programs such as jazz or funk summer camps and festivals, which give students the opportunity to play a wide range of contemporary music, from 1970s-style jazz-rock fusion to 2000s-style R&B.\n\nIn other less mainstream genres, such as hardcore punk or metal, the pedagogical systems and training sequences are typically not formalized and institutionalized. As such, many players learn by ear, by copying bass lines from records and CDs, and by playing in a number of bands. Even in non-mainstream styles, though, students may be able to take lessons from experts in these or other styles, adapting learned techniques to their own style. As well, there are a range of books, playing methods, and, since the 1990s, instructional DVDs (e.g., how to play metal bass). In the 2010s, many instructional videos are available online on video-sharing websites such as YouTube.\n\n\n\n\n", "id": "3916", "title": "Bass guitar"}
{"url": "https://en.wikipedia.org/wiki?curid=3921", "text": "Basketball\n\nBasketball is a non-contact team sport played on a rectangular court by two teams of five players each. The objective is to shoot a ball through a hoop in diameter and high that is mounted to a backboard at each end of the court. The game was invented in 1891 by Dr. James Naismith, who would be the first basketball coach of the Kansas Jayhawks, one of the most successful programs in the game's history.\n\nA team can score a field goal by shooting the ball through the basket being defended by the opposition team during regular play. A field goal scores three points for the shooting team if the player shoots from behind the three-point line, and two points if shot from in front of the line. A team can also score via free throws, which are worth one point, after the other team is assessed with certain fouls. The team with the most points at the end of the game wins, but additional time (overtime) is mandated when the score is tied at the end of regulation. The ball can be advanced on the court by passing it to a teammate, or by bouncing it while walking or running (dribbling). It is a violation to lift, or drag, one's pivot foot without dribbling the ball, to carry it, or to hold the ball with both hands then resume dribbling.\n\nThe game has many individual techniques for displaying skill—ball-handling, shooting, passing, dribbling, dunking, shot-blocking, and rebounding. Basketball teams generally have player positions, the tallest and strongest members of a team are called a center or power forward, while slightly shorter and more agile players are called small forward, and the shortest players or those who possess the best ball handling skills are called a point guard or shooting guard. The point guard directs the on court action of the team, implementing the coach's game plan, and managing the execution of offensive and defensive plays (player positioning).\n\nBasketball is one of the world's most popular and widely viewed sports. The National Basketball Association (NBA) is the most popular and widely considered to be the highest level of professional basketball in the world and NBA players are the world's best paid athletes by average annual salary per player. Outside North America, the top clubs from national leagues qualify to continental championships such as the Euroleague and FIBA Americas League. The FIBA Basketball World Cup attracts the top national teams from around the world. Each continent hosts regional competitions for national teams, like EuroBasket and FIBA Americas Championship.\n\nThe FIBA Women's Basketball World Cup features the top national women's basketball teams from continental championships. The main North American league is the WNBA, whereas the EuroLeague Women has been dominated by teams from the Russian Women's Basketball Premier League.\n\nIn early December 1891, Canadian Dr. James Naismith, a physical education professor and instructor at the International Young Men's Christian Association Training School (YMCA) (today, Springfield College) in Springfield, Massachusetts was trying to keep his gym class active on a rainy day. He sought a vigorous indoor game to keep his students occupied and at proper levels of fitness during the long New England winters. After rejecting other ideas as either too rough or poorly suited to walled-in gymnasiums, he wrote the basic rules and nailed a peach basket onto a elevated track. In contrast with modern basketball nets, this peach basket retained its bottom, and balls had to be retrieved manually after each \"basket\" or point scored; this proved inefficient, however, so the bottom of the basket was removed, allowing the balls to be poked out with a long dowel each time.\n\nBasketball was originally played with a soccer ball. These round balls from \"association football\" were made, at the time, with a set of laces to close off the hole needed for inserting the inflatable bladder after the other sewn-together segments of the ball's cover had been flipped outside-in. These laces could cause bounce passes and dribbling to be unpredictable. Eventually a lace-free ball construction method was invented, and this change to the game was endorsed by Naismith. (Whereas in American football, the lace construction proved to be advantageous for gripping and remains to this day.) The first balls made specifically for basketball were brown, and it was only in the late 1950s that Tony Hinkle, searching for a ball that would be more visible to players and spectators alike, introduced the orange ball that is now in common use. Dribbling was not part of the original game except for the \"bounce pass\" to teammates. Passing the ball was the primary means of ball movement. Dribbling was eventually introduced but limited by the asymmetric shape of early balls. Dribbling only became a major part of the game around the 1950s, as manufacturing improved the ball shape.\n\nThe peach baskets were used until 1906 when they were finally replaced by metal hoops with backboards. A further change was soon made, so the ball merely passed through. Whenever a person got the ball in the basket, his team would gain a point. Whichever team got the most points won the game. The baskets were originally nailed to the mezzanine balcony of the playing court, but this proved impractical when spectators in the balcony began to interfere with shots. The backboard was introduced to prevent this interference; it had the additional effect of allowing rebound shots. Naismith's handwritten diaries, discovered by his granddaughter in early 2006, indicate that he was nervous about the new game he had invented, which incorporated rules from a children's game called \"Duck on a Rock\", as many had failed before it.\n\nFrank Mahan, one of the players from the original first game, approached Naismith after the Christmas break, in early 1892, asking him what he intended to call his new game. Naismith replied that he hadn't thought of it because he had been focused on just getting the game started. Mahan suggested that it be called \"Naismith ball\", at which he laughed, saying that a name like that would kill any game. Mahan then said, \"Why not call it basketball?\" Naismith replied, \"We have a basket and a ball, and it seems to me that would be a good name for it.\" The first official game was played in the YMCA gymnasium in Albany, New York, on January 20, 1892, with nine players. The game ended at 1–0; the shot was made from , on a court just half the size of a present-day Streetball or National Basketball Association (NBA) court.\n\nAt the time, football was being played with 10 to a team (which was increased to 11). When winter weather got too icy to play football, teams were taken indoors, and it was convenient to have them split in half and play basketball with five on each side. By 1897–1898 teams of five became standard.\n\nBasketball's early adherents were dispatched to YMCAs throughout the United States, and it quickly spread through the USA and Canada. By 1895, it was well established at several women's high schools. While the YMCA was responsible for initially developing and spreading the game, within a decade it discouraged the new sport, as rough play and rowdy crowds began to detract from the YMCA's primary mission. However, other amateur sports clubs, colleges, and professional clubs quickly filled the void. In the years before World War I, the Amateur Athletic Union and the Intercollegiate Athletic Association of the United States (forerunner of the NCAA) vied for control over the rules for the game. The first pro league, the National Basketball League, was formed in 1898 to protect players from exploitation and to promote a less rough game. This league only lasted five years.\n\nDr. James Naismith was instrumental in establishing college basketball. His colleague C.O. Beamis fielded the first college basketball team just a year after the Springfield YMCA game at the suburban Pittsburgh Geneva College. Naismith himself later coached at the University of Kansas for six years, before handing the reins to renowned coach Forrest \"Phog\" Allen. Naismith's disciple Amos Alonzo Stagg brought basketball to the University of Chicago, while Adolph Rupp, a student of Naismith's at Kansas, enjoyed great success as coach at the University of Kentucky. On February 9, 1895, the first intercollegiate 5-on-5 game was played at Hamline University between Hamline and the School of Agriculture, which was affiliated with the University of Minnesota. The School of Agriculture won in a 9–3 game.\n\nIn 1901, colleges, including the University of Chicago, Columbia University, Cornell University, Dartmouth College, the University of Minnesota, the U.S. Naval Academy, the University of Colorado and Yale University began sponsoring men's games. In 1905, frequent injuries on the football field prompted President Theodore Roosevelt to suggest that colleges form a governing body, resulting in the creation of the Intercollegiate Athletic Association of the United States (IAAUS). In 1910, that body would change its name to the National Collegiate Athletic Association (NCAA). The first Canadian interuniversity basketball game was played at the YMCA in Kingston, Ontario on February 6, 1904, when McGill University visited Queen's University. McGill won 9–7 in overtime; the score was 7–7 at the end of regulation play, and a ten-minute overtime period settled the outcome. A good turnout of spectators watched the game.\n\nThe first men's national championship tournament, the National Association of Intercollegiate Basketball tournament, which still exists as the National Association of Intercollegiate Athletics (NAIA) tournament, was organized in 1937. The first national championship for NCAA teams, the National Invitation Tournament (NIT) in New York, was organized in 1938; the NCAA national tournament would begin one year later. College basketball was rocked by gambling scandals from 1948 to 1951, when dozens of players from top teams were implicated in match fixing and point shaving. Partially spurred by an association with cheating, the NIT lost support to the NCAA tournament.\n\nBefore widespread school district consolidation, most American high schools were far smaller than their present-day counterparts. During the first decades of the 20th century, basketball quickly became the ideal interscholastic sport due to its modest equipment and personnel requirements. In the days before widespread television coverage of professional and college sports, the popularity of high school basketball was unrivaled in many parts of America. Perhaps the most legendary of high school teams was Indiana's Franklin Wonder Five, which took the nation by storm during the 1920s, dominating Indiana basketball and earning national recognition.\n\nToday virtually every high school in the United States fields a basketball team in varsity competition. Basketball's popularity remains high, both in rural areas where they carry the identification of the entire community, as well as at some larger schools known for their basketball teams where many players go on to participate at higher levels of competition after graduation. In the 2003–04 season, 1,002,797 boys and girls represented their schools in interscholastic basketball competition, according to the National Federation of State High School Associations. The states of Illinois, Indiana and Kentucky are particularly well known for their residents' devotion to high school basketball, commonly called Hoosier Hysteria in Indiana; the critically acclaimed film \"Hoosiers\" shows high school basketball's depth of meaning to these communities.\n\nThere is currently no tournament to determine a national high school champion. The most serious effort was the National Interscholastic Basketball Tournament at the University of Chicago from 1917 to 1930. The event was organized by Amos Alonzo Stagg and sent invitations to state champion teams. The tournament started out as a mostly Midwest affair but grew. In 1929 it had 29 state champions. Faced with opposition from the National Federation of State High School Associations and North Central Association of Colleges and Schools that bore a threat of the schools losing their accreditation the last tournament was in 1930. The organizations said they were concerned that the tournament was being used to recruit professional players from the prep ranks. The tournament did not invite minority schools or private/parochial schools.\n\nThe National Catholic Interscholastic Basketball Tournament ran from 1924 to 1941 at Loyola University. The National Catholic Invitational Basketball Tournament from 1954 to 1978 played at a series of venues, including Catholic University, Georgetown and George Mason. The National Interscholastic Basketball Tournament for Black High Schools was held from 1929 to 1942 at Hampton Institute. The National Invitational Interscholastic Basketball Tournament was held from 1941 to 1967 starting out at Tuskegee Institute. Following a pause during World War II it resumed at Tennessee State College in Nashville. The basis for the champion dwindled after 1954 when \"Brown v. Board of Education\" began an integration of schools. The last tournaments were held at Alabama State College from 1964 to 1967.\n\nTeams abounded throughout the 1920s. There were hundreds of men's professional basketball teams in towns and cities all over the United States, and little organization of the professional game. Players jumped from team to team and teams played in armories and smoky dance halls. Leagues came and went. Barnstorming squads such as the Original Celtics and two all-African American teams, the New York Renaissance Five (\"Rens\") and the (still existing) Harlem Globetrotters played up to two hundred games a year on their national tours.\n\nIn 1946, the Basketball Association of America (BAA) was formed. The first game was played in Toronto, Ontario, Canada between the Toronto Huskies and New York Knickerbockers on November 1, 1946. Three seasons later, in 1949, the BAA merged with the National Basketball League to form the National Basketball Association (NBA). By the 1950s, basketball had become a major college sport, thus paving the way for a growth of interest in professional basketball. In 1959, a basketball hall of fame was founded in Springfield, Massachusetts, site of the first game. Its rosters include the names of great players, coaches, referees and people who have contributed significantly to the development of the game. The hall of fame has people who have accomplished many goals in their career in basketball. An upstart organization, the American Basketball Association, emerged in 1967 and briefly threatened the NBA's dominance until the ABA-NBA merger in 1976. Today the NBA is the top professional basketball league in the world in terms of popularity, salaries, talent, and level of competition.\n\nThe NBA has featured many famous players, including George Mikan, the first dominating \"big man\"; ball-handling wizard Bob Cousy and defensive genius Bill Russell of the Boston Celtics; charismatic center Wilt Chamberlain, who originally played for the barnstorming Harlem Globetrotters; all-around stars Oscar Robertson and Jerry West; more recent big men Kareem Abdul-Jabbar, Shaquille O'Neal, Hakeem Olajuwon and Karl Malone; playmakers John Stockton, Isiah Thomas and Steve Nash; crowd-pleasing forwards Julius Erving and Charles Barkley; European stars Dirk Nowitzki, Pau Gasol and Tony Parker; more recent superstars LeBron James, Allen Iverson and Kobe Bryant; and the three players who many credit with ushering the professional game to its highest level of popularity during the 1980s and 1990s: Larry Bird, Earvin \"Magic\" Johnson, and Michael Jordan.\n\nIn 2001, the NBA formed a developmental league, the NBA Development League (also known as the D-League). As of the 2016–17 season, the D-League has 22 teams.\n\nFIBA (International Basketball Federation) was formed in 1932 by eight founding nations: Argentina, Czechoslovakia, Greece, Italy, Latvia, Portugal, Romania and Switzerland. At this time, the organization only oversaw amateur players. Its acronym, derived from the French \"Fédération Internationale de Basket-ball Amateur\", was thus \"FIBA\". Men's basketball was first included at the Berlin 1936 Summer Olympics, although a demonstration tournament was held in 1904. The United States defeated Canada in the first final, played outdoors. This competition has usually been dominated by the United States, whose team has won all but three titles. The first of these came in a controversial final game in Munich in 1972 against the Soviet Union, in which the ending of the game was replayed three times until the Soviet Union finally came out on top. In 1950 the first FIBA World Championship for men, now known as the FIBA Basketball World Cup, was held in Argentina. Three years later, the first FIBA World Championship for Women, now known as the FIBA Women's Basketball World Cup, was held in Chile. Women's basketball was added to the Olympics in 1976, which were held in Montreal, Quebec, Canada with teams such as the Soviet Union, Brazil and Australia rivaling the American squads.\n\nFIBA dropped the distinction between amateur and professional players in 1989, and in 1992, professional players played for the first time in the Olympic Games. The United States' dominance continued with the introduction of their Dream Team. In the 2004 Athens Olympics, the United States suffered its first Olympic loss while using professional players, falling to Puerto Rico (in a 19-point loss) and Lithuania in group games, and being eliminated in the semifinals by Argentina. It eventually won the bronze medal defeating Lithuania, finishing behind Argentina and Italy. The \"Redeem Team\", won gold at the 2008 Olympics, and the so-called \"B-Team\", won gold at the 2010 FIBA World Championship in Turkey despite featuring no players from the 2008 squad. The United States continued its dominance as they won gold at the 2012 Olympics, 2014 FIBA World Cup and the 2016 Olympics.\n\nWorldwide, basketball tournaments are held for boys and girls of all age levels. The global popularity of the sport is reflected in the nationalities represented in the NBA. Players from all six inhabited continents currently play in the NBA. Top international players began coming into the NBA in the mid-1990s, including Croatians Dražen Petrović and Toni Kukoč, Serbian Vlade Divac, Lithuanians Arvydas Sabonis and Šarūnas Marčiulionis, Dutchman Rik Smits and German Detlef Schrempf.\n\nIn the Philippines, the Philippine Basketball Association's first game was played on April 9, 1975 at the Araneta Coliseum in Cubao, Quezon City. Philippines. It was founded as a \"rebellion\" of several teams from the now-defunct Manila Industrial and Commercial Athletic Association, which was tightly controlled by the Basketball Association of the Philippines (now defunct), the then-FIBA recognized national association. Nine teams from the MICAA participated in the league's first season that opened on April 9, 1975. The NBL is Australia's pre-eminent men's professional basketball league. The league commenced in 1979, playing a winter season (April–September) and did so until the completion of the 20th season in 1998. The 1998–99 season, which commenced only months later, was the first season after the shift to the current summer season format (October–April). This shift was an attempt to avoid competing directly against Australia's various football codes. It features 8 teams from around Australia and one in New Zealand. A few players including Luc Longley, Andrew Gaze, Shane Heal, Chris Anstey and Andrew Bogut made it big internationally, becoming poster figures for the sport in Australia. The Women's National Basketball League began in 1981.\n\nWomen's basketball began in 1892 at Smith College when Senda Berenson, a physical education teacher, modified Naismith's rules for women. Shortly after she was hired at Smith, she went to Naismith to learn more about the game. Fascinated by the new sport and the values it could teach, she organized the first women's collegiate basketball game on March 21, 1893, when her Smith freshmen and sophomores played against one another. However, the first women's interinstitutional game was played in 1892 between the University of California and Miss Head's School. Berenson's rules were first published in 1899, and two years later she became the editor of A. G. Spalding's first Women's Basketball Guide. Berenson's freshmen played the sophomore class in the first women's intercollegiate basketball game at Smith College, March 21, 1893. The same year, Mount Holyoke and Sophie Newcomb College (coached by Clara Gregory Baer) women began playing basketball. By 1895, the game had spread to colleges across the country, including Wellesley, Vassar, and Bryn Mawr. The first intercollegiate women's game was on April 4, 1896. Stanford women played Berkeley, 9-on-9, ending in a 2–1 Stanford victory.\n\nWomen's basketball development was more structured than that for men in the early years. In 1905, the Executive Committee on Basket Ball Rules (National Women's Basketball Committee) was created by the American Physical Education Association. These rules called for six to nine players per team and 11 officials. The International Women's Sports Federation (1924) included a women's basketball competition. 37 women's high school varsity basketball or state tournaments were held by 1925. And in 1926, the Amateur Athletic Union backed the first national women's basketball championship, complete with men's rules. The Edmonton Grads, a touring Canadian women's team based in Edmonton, Alberta, operated between 1915 and 1940. The Grads toured all over North America, and were exceptionally successful. They posted a record of 522 wins and only 20 losses over that span, as they met any team that wanted to challenge them, funding their tours from gate receipts. The Grads also shone on several exhibition trips to Europe, and won four consecutive exhibition Olympics tournaments, in 1924, 1928, 1932, and 1936; however, women's basketball was not an official Olympic sport until 1976. The Grads' players were unpaid, and had to remain single. The Grads' style focused on team play, without overly emphasizing skills of individual players. The first women's AAU All-America team was chosen in 1929. Women's industrial leagues sprang up throughout the United States, producing famous athletes, including Babe Didrikson of the Golden Cyclones, and the All American Red Heads Team, which competed against men's teams, using men's rules. By 1938, the women's national championship changed from a three-court game to two-court game with six players per team.\n\nThe NBA-backed Women's National Basketball Association (WNBA) began in 1997. Though it had shaky attendance figures, several marquee players (Lisa Leslie, Diana Taurasi, and Candace Parker among others) have helped the league's popularity and level of competition. Other professional women's basketball leagues in the United States, such as the American Basketball League (1996–98), have folded in part because of the popularity of the WNBA. The WNBA has been looked at by many as a niche league. However, the league has recently taken steps forward. In June 2007, the WNBA signed a contract extension with ESPN. The new television deal ran from 2009 to 2016. Along with this deal, came the first ever rights fees to be paid to a women's professional sports league. Over the eight years of the contract, \"millions and millions of dollars\" were \"dispersed to the league's teams.\" In a March 12, 2009 article, NBA commissioner David Stern said that in the bad economy, \"the NBA is far less profitable than the WNBA. We're losing a lot of money among a large number of teams. We're budgeting the WNBA to break even this year.\"\n\nMeasurements and time limits discussed in this section often vary among tournaments and organizations; international and NBA rules are used in this section.\n\nThe object of the game is to outscore one's opponents by throwing the ball through the opponents' basket from above while preventing the opponents from doing so on their own. An attempt to score in this way is called a shot. A successful shot is worth two points, or three points if it is taken from beyond the three-point arc from the basket in international games and in NBA games. A one-point shot can be earned when shooting from the foul line after a foul is made.\n\nGames are played in four quarters of 10 (FIBA) or 12 minutes (NBA). College men's games use two 20-minute halves, college women's games use 10-minute quarters, and United States high school varsity games use 8 minute quarters. 15 minutes are allowed for a half-time break under FIBA, NBA, and NCAA rules and 10 minutes in United States high schools. Overtime periods are five minutes in length except for high school, which is four minutes in length. Teams exchange baskets for the second half. The time allowed is actual playing time; the clock is stopped while the play is not active. Therefore, games generally take much longer to complete than the allotted game time, typically about two hours.\n\nFive players from each team may be on the court at one time. Substitutions are unlimited but can only be done when play is stopped. Teams also have a coach, who oversees the development and strategies of the team, and other team personnel such as assistant coaches, managers, statisticians, doctors and trainers.\n\nFor both men's and women's teams, a standard uniform consists of a pair of shorts and a jersey with a clearly visible number, unique within the team, printed on both the front and back. Players wear high-top sneakers that provide extra ankle support. Typically, team names, players' names and, outside of North America, sponsors are printed on the uniforms.\n\nA limited number of time-outs, clock stoppages requested by a coach (or sometimes mandated in the NBA) for a short meeting with the players, are allowed. They generally last no longer than one minute (100 seconds in the NBA) unless, for televised games, a commercial break is needed.\n\nThe game is controlled by the officials consisting of the referee (referred to as crew chief in the NBA), one or two umpires (referred to as referees in the NBA) and the table officials. For college, the NBA, and many high schools, there are a total of three referees on the court. The table officials are responsible for keeping track of each teams scoring, timekeeping, individual and team fouls, player substitutions, team possession arrow, and the shot clock.\n\nThe only essential equipment in a basketball game is the ball and the court: a flat, rectangular surface with baskets at opposite ends. Competitive levels require the use of more equipment such as clocks, score sheets, scoreboard(s), alternating possession arrows, and whistle-operated stop-clock systems.\nA regulation basketball court in international games is long and 49.2 feet (15 meters) wide. In the NBA and NCAA the court is . Most courts have wood flooring, usually constructed from maple planks running in the same direction as the longer court dimension. The name and logo of the home team is usually painted on or around the center circle.\n\nThe basket is a steel rim diameter with an attached net affixed to a backboard that measures and one basket is at each end of the court. The white outlined box on the backboard is high and wide. At almost all levels of competition, the top of the rim is exactly above the court and inside the baseline. While variation is possible in the dimensions of the court and backboard, it is considered important for the basket to be of the correct height – a rim that is off by just a few inches can have an adverse effect on shooting.\n\nThe size of the basketball is also regulated. For men, the official ball is in circumference (size 7, or a \"295 ball\") and weighs 22 oz (623.69 grams). If women are playing, the official basketball size is in circumference (size 6, or a \"285 ball\") with a weight of 20 oz (567 grams). In 3x3, a formalized version of the halfcourt 3-on-3 game, a dedicated ball with the circumference of a size 6 ball but the weight of a size 7 ball is used in all competitions (men's, women's, and mixed teams).\n\nThe ball may be advanced toward the basket by being shot, passed between players, thrown, tapped, rolled or dribbled (bouncing the ball while running).\n\nThe ball must stay within the court; the last team to touch the ball before it travels out of bounds forfeits possession. The ball is out of bounds if it touches a boundary line, or touches any player or object that is out of bounds.\n\nThere are limits placed on the steps a player may take without dribbling, which commonly results in an infraction known as traveling. Nor may a player stop his dribble and then resume dribbling. A dribble that touches both hands is considered stopping the dribble, giving this infraction the name double dribble. Within a dribble, the player cannot carry the ball by placing his hand on the bottom of the ball; doing so is known as carrying the ball. A team, once having established ball control in the front half of their court, may not return the ball to the backcourt and be the first to touch it. A violation of these rules results in loss of possession.\n\nThe ball may not be kicked, nor be struck with the fist. For the offense, a violation of these rules results in loss of possession; for the defense, most leagues reset the shot clock and the offensive team is given possession of the ball out of bounds.\n\nThere are limits imposed on the time taken before progressing the ball past halfway (8 seconds in FIBA and the NBA; 10 seconds in NCAA and high school for both sexes), before attempting a shot (24 seconds in FIBA, the NBA, and U Sports (Canadian universities) play for both sexes, and 30 seconds in NCAA play for both sexes), holding the ball while closely guarded (5 seconds), and remaining in the restricted area known as the free-throw lane, (or the \"key\") (3 seconds). These rules are designed to promote more offense.\n\nBasket interference, or goaltending is a violation charged when a player illegally interferes with a shot. This violation is incurred when a player touches the ball on its downward trajectory to the basket, unless it is obvious that the ball has no chance of entering the basket, if a player touches the ball while it is in the rim, or in the area extended upwards from the basket, or if a player reaches through the basket to interfere with the shot. When a defensive player is charged with goaltending, the basket is awarded. If an offensive player commits the infraction, the basket is cancelled. In either case possession of the ball is turned over to the defensive team.\n\nAn attempt to unfairly disadvantage an opponent through certain types of physical contact is illegal and is called a personal foul. These are most commonly committed by defensive players; however, they can be committed by offensive players as well. Players who are fouled either receive the ball to pass inbounds again, or receive one or more free throws if they are fouled in the act of shooting, depending on whether the shot was successful. One point is awarded for making a free throw, which is attempted from a line from the basket.\n\nThe referee is responsible for judging whether contact is illegal, sometimes resulting in controversy. The calling of fouls can vary between games, leagues and referees.\n\nThere is a second category of fouls called technical fouls, which may be charged for various rules violations including failure to properly record a player in the scorebook, or for unsportsmanlike conduct. These infractions result in one or two free throws, which may be taken by any of the five players on the court at the time. Repeated incidents can result in disqualification. A blatant foul involving physical contact that is either excessive or unnecessary is called an intentional foul (flagrant foul in the NBA). In FIBA, a foul resulting in ejection is called a disqualifying foul, while in leagues other than the NBA, such a foul is referred to as flagrant.\n\nIf a team exceeds a certain limit of team fouls in a given period (quarter or half) – four for NBA, NCAA women's, and international games – the opposing team is awarded one or two free throws on all subsequent non-shooting fouls for that period, the number depending on the league. In the US college men's game and high school games for both sexes, if a team reaches 7 fouls in a half, the opposing team is awarded one free throw, along with a second shot if the first is made. This is called shooting \"one-and-one\". If a team exceeds 10 fouls in the half, the opposing team is awarded two free throws on all subsequent fouls for the half.\n\nWhen a team shoots foul shots, the opponents may not interfere with the shooter, nor may they try to regain possession until the last or potentially last free throw is in the air.\n\nAfter a team has committed a specified number of fouls, the other team is said to be \"in the bonus\". On scoreboards, this is usually signified with an indicator light reading \"Bonus\" or \"Penalty\" with an illuminated directional arrow or dot indicating that team is to receive free throws when fouled by the opposing team. (Some scoreboards also indicate the number of fouls committed.)\n\nIf a team misses the first shot of a two-shot situation, the opposing team must wait for the completion of the second shot before attempting to reclaim possession of the ball and continuing play.\n\nIf a player is fouled while attempting a shot and the shot is unsuccessful, the player is awarded a number of free throws equal to the value of the attempted shot. A player fouled while attempting a regular two-point shot thus receives two shots, and a player fouled while attempting a three-point shot receives three shots.\n\nIf a player is fouled while attempting a shot and the shot is successful, typically the player will be awarded one additional free throw for one point. In combination with a regular shot, this is called a \"three-point play\" or \"four-point play\" (or more colloquially, an \"and one\") because of the basket made at the time of the foul (2 or 3 points) and the additional free throw (1 point).\n\nAlthough the rules do not specify any positions whatsoever, they have evolved as part of basketball. During the early years of basketball's evolution, two guards, two forwards, and one center were used. In more recent times specific positions evolved, but the current trend, advocated by many top coaches including Mike Krzyzewski is towards positionless basketball, where big guys are free to shoot from outside and dribble if their skill allows it. Popular descriptions of positions include:\n\nPoint guard (often called the \"1\") : usually the fastest player on the team, organizes the team's offense by controlling the ball and making sure that it gets to the right player at the right time.\n\nShooting guard (the \"2\") : creates a high volume of shots on offense, mainly long-ranged; and guards the opponent's best perimeter player on defense.\n\nSmall forward (the \"3\") : often primarily responsible for scoring points via cuts to the basket and dribble penetration; on defense seeks rebounds and steals, but sometimes plays more actively.\n\nPower forward (the \"4\"): plays offensively often with their back to the basket; on defense, plays under the basket (in a zone defense) or against the opposing power forward (in man-to-man defense).\n\nCenter (the \"5\"): uses height and size to score (on offense), to protect the basket closely (on defense), or to rebound.\n\nThe above descriptions are flexible. For most teams today, the shooting guard and small forward have very similar responsibilities and are often called the wings, as do the power forward and center, who are often called post players. While most teams describe two players as guards, two as forwards, and one as a center, on some occasions teams choose to call them by different designations.\n\nThere are two main defensive strategies: \"zone defense\" and \"man-to-man defense\". In a zone defense, each player is assigned to guard a specific area of the court. Zone defenses often allow the defense to double team the ball, a manoeuver known as a trap. In a man-to-man defense, each defensive player guards a specific opponent.\n\nOffensive plays are more varied, normally involving planned passes and movement by players without the ball. A quick movement by an offensive player without the ball to gain an advantageous position is known as a \"cut\". A legal attempt by an offensive player to stop an opponent from guarding a teammate, by standing in the defender's way such that the teammate cuts next to him, is a \"screen\" or \"pick\". The two plays are combined in the \"pick and roll\", in which a player sets a pick and then \"rolls\" away from the pick towards the basket. Screens and cuts are very important in offensive plays; these allow the quick passes and teamwork, which can lead to a successful basket. Teams almost always have several offensive plays planned to ensure their movement is not predictable. On court, the point guard is usually responsible for indicating which play will occur.\n\nShooting is the act of attempting to score points by throwing the ball through the basket, methods varying with players and situations.\n\nTypically, a player faces the basket with both feet facing the basket. A player will rest the ball on the fingertips of the dominant hand (the shooting arm) slightly above the head, with the other hand supporting the side of the ball. The ball is usually shot by jumping (though not always) and extending the shooting arm. The shooting arm, fully extended with the wrist fully bent, is held stationary for a moment following the release of the ball, known as a \"follow-through\". Players often try to put a steady backspin on the ball to absorb its impact with the rim. The ideal trajectory of the shot is somewhat controversial, but generally a proper arc is recommended. Players may shoot directly into the basket or may use the backboard to redirect the ball into the basket.\n\nThe two most common shots that use the above described setup are the \"set-shot\" and the \"jump-shot\". The set-shot is taken from a standing position, with neither foot leaving the floor, typically used for free throws, and in other circumstances while the jump-shot is taken in mid-air, the ball released near the top of the jump. This provides much greater power and range, and it also allows the player to elevate over the defender. Failure to release the ball before the feet return to the floor is considered a traveling violation.\n\nAnother common shot is called the \"lay-up\". This shot requires the player to be in motion toward the basket, and to \"lay\" the ball \"up\" and into the basket, typically off the backboard (the backboard-free, underhand version is called a \"finger roll\"). The most crowd-pleasing and typically highest-percentage accuracy shot is the \"slam dunk\", in which the player jumps very high and throws the ball downward, through the basket while touching it.\n\nAnother shot that is becoming common is the \"circus shot\". The circus shot is a low-percentage shot that is flipped, heaved, scooped, or flung toward the hoop while the shooter is off-balance, airborne, falling down, and/or facing away from the basket. A back-shot is a shot taken when the player is facing away from the basket, and may be shot with the dominant hand, or both; but there is a very low chance that the shot will be successful.\n\nA shot that misses both the rim and the backboard completely is referred to as an \"air-ball\". A particularly bad shot, or one that only hits the backboard, is jocularly called a brick. The \"'hang time\" is the length of time a player stays in the air after jumping, either to make a slam dunk, lay-up or jump shot.\n\nThe objective of rebounding is to successfully gain possession of the basketball after a missed field goal or free throw, as it rebounds from the hoop or backboard. This plays a major role in the game, as most possessions end when a team misses a shot. There are two categories of rebounds: offensive rebounds, in which the ball is recovered by the offensive side and does not change possession, and defensive rebounds, in which the defending team gains possession of the loose ball. The majority of rebounds are defensive, as the team on defense tends to be in better position to recover missed shots.\n\nA pass is a method of moving the ball between players. Most passes are accompanied by a step forward to increase power and are followed through with the hands to ensure accuracy.\n\nA staple pass is the \"chest pass\". The ball is passed directly from the passer's chest to the receiver's chest. A proper chest pass involves an outward snap of the thumbs to add velocity and leaves the defence little time to react.\n\nAnother type of pass is the \"bounce pass\". Here, the passer bounces the ball crisply about two-thirds of the way from his own chest to the receiver. The ball strikes the court and bounces up toward the receiver. The bounce pass takes longer to complete than the chest pass, but it is also harder for the opposing team to intercept (kicking the ball deliberately is a violation). Thus, players often use the bounce pass in crowded moments, or to pass around a defender.\n\nThe \"overhead pass\" is used to pass the ball over a defender. The ball is released while over the passer's head.\n\nThe \"outlet pass\" occurs after a team gets a defensive rebound. The next pass after the rebound is the \"outlet pass\".\n\nThe crucial aspect of any good pass is it being difficult to intercept. Good passers can pass the ball with great accuracy and they know exactly where each of their other teammates prefers to receive the ball. A special way of doing this is passing the ball without looking at the receiving teammate. This is called a \"no-look pass\".\n\nAnother advanced style of passing is the \"behind-the-back pass\", which, as the description implies, involves throwing the ball behind the passer's back to a teammate. Although some players can perform such a pass effectively, many coaches discourage no-look or behind-the-back passes, believing them to be difficult to control and more likely to result in turnovers or violations.\n\nDribbling is the act of bouncing the ball continuously with one hand, and is a requirement for a player to take steps with the ball. To dribble, a player pushes the ball down towards the ground with the fingertips rather than patting it; this ensures greater control.\n\nWhen dribbling past an opponent, the dribbler should dribble with the hand farthest from the opponent, making it more difficult for the defensive player to get to the ball. It is therefore important for a player to be able to dribble competently with both hands.\n\nGood dribblers (or \"ball handlers\") tend to bounce the ball low to the ground, reducing the distance of travel of the ball from the floor to the hand, making it more difficult for the defender to \"steal\" the ball. Good ball handlers frequently dribble behind their backs, between their legs, and switch directions suddenly, making a less predictable dribbling pattern that is more difficult to defend against. This is called a crossover, which is the most effective way to move past defenders while dribbling.\n\nA skilled player can dribble without watching the ball, using the dribbling motion or peripheral vision to keep track of the ball's location. By not having to focus on the ball, a player can look for teammates or scoring opportunities, as well as avoid the danger of having someone steal the ball away from him/her.\n\nA block is performed when, after a shot is attempted, a defender succeeds in altering the shot by touching the ball. In almost all variants of play, it is illegal to touch the ball after it is in the downward path of its arc; this is known as \"goaltending\". It is also illegal under NBA and Men's NCAA basketball to block a shot after it has touched the backboard, or when any part of the ball is directly above the rim. Under international rules it is illegal to block a shot that is in the downward path of its arc or one that has touched the backboard until the ball has hit the rim. After the ball hits the rim, it is again legal to touch it even though it is no longer considered as a block performed.\n\nTo block a shot, a player has to be able to reach a point higher than where the shot is released. Thus, height can be an advantage in blocking. Players who are taller and playing the power forward or center positions generally record more blocks than players who are shorter and playing the guard positions. However, with good timing and a sufficiently high vertical leap, even shorter players can be effective shot blockers.\n\nAt the professional level, most male players are above and most women above . Guards, for whom physical coordination and ball-handling skills are crucial, tend to be the smallest players. Almost all forwards in the top men's pro leagues are or taller. Most centers are over tall. According to a survey given to all NBA teams, the average height of all NBA players is just under , with the average weight being close to . The tallest players ever in the NBA were Manute Bol and Gheorghe Mureșan, who were both tall. The tallest current NBA player is Sim Bhullar, who stands at . At , Margo Dydek was the tallest player in the history of the WNBA.\n\nThe shortest player ever to play in the NBA is Muggsy Bogues at . Other short players have thrived at the pro level. Anthony \"Spud\" Webb was just tall, but had a vertical leap, giving him significant height when jumping. While shorter players are often at a disadvantage in certain aspects of the game, their ability to navigate quickly through crowded areas of the court and steal the ball by reaching low are strengths.\n\nThe composition of race and ethnicity in the National Basketball Association (NBA) has changed throughout the league's history. The first non-white player entered the league in 1947. According to racial equality activist Richard Lapchick, the NBA in 2011 was composed of 78 percent black players, 17 percent white players, four percent Latinos (of any race), and one percent Asian.\n\nHall of Fame player Larry Bird, who is white, stated in 2004 that the league needed more white players since the league's fans are mostly white. \"And if you just had a couple of white guys in there, you might get them [the fans, not the guys] a little excited. But it is a black man's game, and it will be forever. I mean, the greatest athletes in the world are African-American,\" said Bird.\n\nVariations of basketball are activities based on the game of basketball, using common basketball skills and equipment (primarily the ball and basket). Some variations are only superficial rules changes, while others are distinct games with varying degrees of basketball influences. Other variations include children's games, contests or activities meant to help players reinforce skills.\n\nThere are principal basketball sports with variations on basketball including Wheelchair basketball, Water basketball, Beach basketball, Slamball, Streetball and Unicycle basketball. An earlier version of basketball, played primarily by women and girls, was Six-on-six basketball. Horseball is a game played on horseback where a ball is handled and points are scored by shooting it through a high net (approximately 1.5m×1.5m). The sport is like a combination of polo, rugby, and basketball. There is even a form played on donkeys known as Donkey basketball, but that version has come under attack from animal rights groups.\n\n\nThere are also other basketball sports, such as:\n\n\nBeach basketball has grown to a very popular, widespread competitive sport. 15 Annual World Championships have been organized.\n\n\n\n\n\nSpin-offs from basketball that are now separate sports include:\n\n\nBasketball has been adopted by various social groups, which have established their own environments and sometimes their own rules. Such socialized forms of basketball include the following.\n\nFantasy basketball was popularized during the 1990s after the advent of the Internet. Those who play this game are sometimes referred to as General Managers, who draft actual NBA players and compute their basketball statistics. The game was popularized by ESPN Fantasy Sports, NBA.com, and Yahoo! Fantasy Sports. Other sports websites provided the same format keeping the game interesting with participants actually owning specific players.\n\n\n\n\n", "id": "3921", "title": "Basketball"}
{"url": "https://en.wikipedia.org/wiki?curid=3926", "text": "Blowfish (disambiguation)\n\nBlowfish are species of fish in the Tetraodontidae family.\n\nBlowfish may also refer to:\n\n\n", "id": "3926", "title": "Blowfish (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=3928", "text": "Ball\n\nA ball is a round object (usually spherical but sometimes ovoid) with various uses. It is used in ball games, where the play of the game follows the state of the ball as it is hit, kicked or thrown by players. Balls can also be used for simpler activities, such as catch, marbles and juggling. Balls made from hard-wearing materials are used in engineering applications to provide very low friction bearings, known as ball bearings. Black-powder weapons use stone and metal balls as projectiles.\n\nAlthough many types of balls are today made from rubber, this form was unknown outside the Americas until after the voyages of Columbus. The Spanish were the first Europeans to see bouncing rubber balls (albeit solid and not inflated) which were employed most notably in the Mesoamerican ballgame. Balls used in various sports in other parts of the world prior to Columbus were made from other materials such as animal bladders or skins, stuffed with various materials.\n\nAs balls are one of the most familiar spherical objects to humans, the word \"ball\" is used to refer to, or to describe, anything spherical or near-spherical.\n\nThe first known use of the word \"ball\" in English in the sense of a globular body that is played with was in 1205 in \"\" in the phrase, \"\" The word came from the Middle English \"bal\" (inflected as \"ball-e, -es\", in turn from Old Norse \"böllr\" (pronounced ; compare Old Swedish \"baller,\" and Swedish \"boll\") from Proto-Germanic \"ballu-z,\" (whence probably Middle High German \"bal, ball-es,\" Middle Dutch \"bal\"), a cognate with Old High German \"ballo, pallo,\" Middle High German balle from Proto-Germanic \"*ballon\" (weak masculine), and Old High German \"ballâ, pallâ,\" Middle High German \"balle,\" Proto-Germanic \"*ballôn\" (weak feminine). No Old English representative of any of these is known. (The answering forms in Old English would have been \"beallu, -a, -e\"—compare \"bealluc, ballock\".) If \"ball-\" was native in Germanic, it may have been a cognate with the Latin \"foll-is\" in sense of a \"thing blown up or inflated.\" In the later Middle English spelling \"balle\" the word coincided graphically with the French \"balle\" \"ball\" and \"bale\" which has hence been erroneously assumed to be its source. French \"balle\" (but not \"boule\") is assumed to be of Germanic origin, itself, however. In Ancient Greek the word πάλλα (\"palla\") for \"ball\" is attested besides the word \"σφαίρα\", \"sphere\".\n\nA ball, as the essential feature in many forms of gameplay requiring physical exertion, must date from the very earliest times. A rolling object appeals not only to a human baby but to a kitten and a puppy. Some form of game with a ball is found portrayed on Egyptian monuments, and is played among aboriginal tribes at the present day. In Homer, Nausicaa was playing at ball with her maidens when Odysseus first saw her in the land of the Phaeacians (Od. vi. 100). And Halios and Laodamas performed before Alcinous and Odysseus with ball play, accompanied with dancing (Od. viii. 370).\n\nAmong the Greeks games with balls (σφαῖραι) were regarded as a useful subsidiary to the more violent athletic exercises, as a means of keeping the body supple, and rendering it graceful, but were generally left to boys and girls. Of regular rules for the playing of ball games, little trace remains, if there were any such. The names in Greek for various forms, which have come down to us in such works as the Ὀνομαστικόν of Julius Pollux, imply little or nothing of such; thus, ἀπόρραξις (\"aporraxis\") only means the putting of the ball on the ground with the open hand, οὐρανία (\"ourania\"), the flinging of the ball in the air to be caught by two or more players; φαινίνδα (\"phaininda\") would seem to be a game of catch played by two or more, where feinting is used as a test of quickness and skill. Pollux (i. x. 104) mentions a game called episkyros (ἐπίσκυρος), which has often been looked on as the origin of football. It seems to have been played by two sides, arranged in lines; how far there was any form of \"goal\" seems uncertain.\n\nAmong the Romans, ball games were looked upon as an adjunct to the bath, and were graduated to the age and health of the bathers, and usually a place (sphaeristerium) was set apart for them in the baths (thermae). There appear to have been three types or sizes of ball, the pila, or small ball, used in catching games, the paganica, a heavy ball stuffed with feathers, and the follis, a leather ball filled with air, the largest of the three. This was struck from player to player, who wore a kind of gauntlet on the arm. There was a game known as trigon, played by three players standing in the form of a triangle, and played with the follis, and also one known as harpastum, which seems to imply a \"scrimmage\" among several players for the ball. These games are known to us through the Romans, though the names are Greek.\n\nThe various modern games played with a ball or balls and subject to rules are treated under their various names, such as polo, cricket, football, etc.\n\nSeveral sports use a ball in the shape of a prolate spheroid:\n", "id": "3928", "title": "Ball"}
{"url": "https://en.wikipedia.org/wiki?curid=3931", "text": "Binary relation\n\nIn mathematics, a binary relation on a set \"A\" is a collection of ordered pairs of elements of \"A\". In other words, it is a subset of the Cartesian product \"A\" = . More generally, a binary relation between two sets \"A\" and \"B\" is a subset of . The terms correspondence, dyadic relation and 2-place relation are synonyms for binary relation.\n\nAn example is the \"divides\" relation between the set of prime numbers P and the set of integers Z, in which every prime \"p\" is associated with every integer \"z\" that is a multiple of \"p\" (but with no integer that is not a multiple of \"p\"). In this relation, for instance, the prime 2 is associated with numbers that include −4, 0, 6, 10, but not 1 or 9; and the prime 3 is associated with numbers that include 0, 6, and 9, but not 4 or 13.\n\nBinary relations are used in many branches of mathematics to model concepts like \"is greater than\", \"is equal to\", and \"divides\" in arithmetic, \"is congruent to\" in geometry, \"is adjacent to\" in graph theory, \"is orthogonal to\" in linear algebra and many more. The concept of function is defined as a special kind of binary relation. Binary relations are also heavily used in computer science.\n\nA binary relation is the special case of an \"n\"-ary relation \"R\" ⊆ \"A\" × … × \"A\", that is, a set of \"n\"-tuples where the \"j\"th component of each \"n\"-tuple is taken from the \"j\"th domain \"A\" of the relation. An example for a ternary relation on Z×Z×Z is \" ... lies between ... and ...\", containing e.g. the triples , , and .\n\nIn some systems of axiomatic set theory, relations are extended to classes, which are generalizations of sets. This extension is needed for, among other things, modeling the concepts of \"is an element of\" or \"is a subset of\" in set theory, without running into logical inconsistencies such as Russell's paradox.\n\nA binary relation \"R\" between arbitrary sets (or classes) \"X\" (the set of departure) and \"Y\" (the set of destination or codomain) is specified by its graph \"G\", which is a subset of the Cartesian product \"X\" × \"Y\". The binary relation \"R\" itself is usually identified with its graph \"G\", but some authors define it as an ordered triple , which is otherwise referred to as a correspondence.\n\nThe statement is read \"\"x\" is \"R\"-related to \"y\"\", and is denoted by \"xRy\" or The latter notation corresponds to viewing \"R\" as the characteristic function of the subset \"G\" of i.e. equals to 1 (true), if and 0 (false) otherwise.\n\nThe order of the elements in each pair of \"G\" is important: if \"a\" ≠ \"b\", then \"aRb\" and \"bRa\" can be true or false, independently of each other. Resuming the above example, the prime 3 divides the integer 9, but 9 doesn't divide 3.\n\nThe domain of \"R\" is the set of all \"x\" such that \"xRy\" for at least one \"y\". The range of \"R\" is the set of all \"y\" such that \"xRy\" for at least one \"x\". The field of \"R\" is the union of its domain and its range.\n\nAccording to the definition above, two relations with identical graphs but different domains or different codomains are considered different. For example, if formula_1, then formula_2, formula_3, and formula_4 are three distinct relations, where formula_5 is the set of integers, formula_6 is the set of real numbers and formula_7 is the set of natural numbers.\n\nEspecially in set theory, binary relations are often defined as sets of ordered pairs, identifying binary relations with their graphs. The domain of a binary relation formula_8 is then defined as the set of all formula_9 such that there exists at least one formula_10 such that formula_11, the range of formula_8 is defined as the set of all formula_10 such that there exists at least one formula_9 such that formula_11, and the field of formula_8 is the union of its domain and its range.\n\nA special case of this difference in points of view applies to the notion of function. Many authors insist on distinguishing between a function's codomain and its range. Thus, a single \"rule,\" like mapping every real number \"x\" to \"x\", can lead to distinct functions formula_17 and formula_18, depending on whether the images under that rule are understood to be reals or, more restrictively, non-negative reals. But others view functions as simply sets of ordered pairs with unique first components. This difference in perspectives does raise some nontrivial issues. As an example, the former camp considers surjectivity—or being onto—as a property of functions, while the latter sees it as a relationship that functions may bear to sets.\n\nEither approach is adequate for most uses, provided that one attends to the necessary changes in language, notation, and the definitions of concepts like restrictions, composition, inverse relation, and so on. The choice between the two definitions usually matters only in very formal contexts, like category theory.\n\nExample: Suppose there are four objects {ball, car, doll, gun} and four persons {John, Mary, Ian, Venus}. Suppose that John owns the ball, Mary owns the doll, and Venus owns the car. Nobody owns the gun and Ian owns nothing. Then the binary relation \"is owned by\" is given as\n\nThus the first element of R is the set of objects, the second is the set of persons, and the last element is a set of ordered pairs of the form (object, owner).\n\nThe pair (ball, John), denoted by \"R\" means that the ball is owned by John.\n\nTwo different relations could have the same graph. For example: the relation\nis different from the previous one as everyone is an owner. But the graphs of the two relations are the same.\n\nNevertheless, \"R\" is usually identified or even defined as G(\"R\") and \"an ordered pair (\"x\", \"y\") ∈ G(\"R\")\" is usually denoted as \"(\"x\", \"y\") ∈ \"R\"\".\n\nSome important types of binary relations \"R\" between two sets \"X\" and \"Y\" are listed below. To emphasize that \"X\" and \"Y\" can be different sets, some authors call such binary relations heterogeneous.\n\nUniqueness properties:\n\nTotality properties (only definable if the sets of departure \"X\" resp. destination \"Y\" are specified; not to be confused with a total relation):\n\nUniqueness and totality properties:\n\nLess commonly encountered is the notion of difunctional (or regular) relation, defined as a relation \"R\" such that \"R\"=\"RR\"\"R\".\n\nTo understand this notion better, it helps to consider a relation as mapping every element \"x\"∈\"X\" to a set \"xR\" = { \"y\"∈\"Y\" | \"xRy\" }. This set is sometimes called the successor neighborhood of \"x\" in \"R\"; one can define the predecessor neighborhood analogously. Synonymous terms for these notions are afterset and respectively foreset.\n\nA difunctional relation can then be equivalently characterized as a relation \"R\" such that wherever \"x\"\"R\" and \"x\"\"R\" have a non-empty intersection, then these two sets coincide; formally \"x\"\"R\" ∩ \"x\"\"R\" ≠ ∅ implies \"x\"\"R\" = \"x\"\"R\".\n\nAs examples, any function or any functional (right-unique) relation is difunctional; the converse doesn't hold. If one considers a relation \"R\" from set to itself (\"X\" = \"Y\"), then if \"R\" is both transitive and symmetric (i.e. a partial equivalence relation), then it is also difunctional. The converse of this latter statement also doesn't hold.\n\nA characterization of difunctional relations, which also explains their name, is to consider two functions \"f\": \"A\" → \"C\" and \"g\": \"B\" → \"C\" and then define the following set which generalizes the kernel of a single function as joint kernel: ker(\"f\", \"g\") = { (\"a\", \"b\") ∈ \"A\" × \"B\" | \"f\"(\"a\") = \"g\"(\"b\") }. Every difunctional relation \"R\" ⊆ \"A\" × \"B\" arises as the joint kernel of two functions \"f\": \"A\" → \"C\" and \"g\": \"B\" → \"C\" for some set \"C\".\n\nIn automata theory, the term rectangular relation has also been used to denote a difunctional relation. This terminology is justified by the fact that when represented as a boolean matrix, the columns and rows of a difunctional relation can be arranged in such a way as to present rectangular blocks of true on the (asymmetric) main diagonal. Other authors however use the term \"rectangular\" to denote any heterogeneous relation whatsoever.\n\nIf \"X\" = \"Y\" then we simply say that the binary relation is over \"X\", or that it is an endorelation over \"X\". In computer science, such a relation is also called a homogeneous (binary) relation. Some types of endorelations are widely studied in graph theory, where they are known as simple directed graphs permitting loops.\n\nThe set of all binary relations Rel(\"X\") on a set \"X\" is the set 2 which is a Boolean algebra augmented with the involution of mapping of a relation to its inverse relation. For the theoretical explanation see Relation algebra.\n\nSome important properties that a binary relation \"R\" over a set \"X\" may have are:\n\n\nA relation that is reflexive, symmetric, and transitive is called an equivalence relation. A relation that is symmetric, transitive, and serial is also reflexive. A relation that is only symmetric and transitive (without necessarily being reflexive) is called a partial equivalence relation.\n\nA relation that is reflexive, antisymmetric, and transitive is called a partial order. A partial order that is total is called a total order, \"simple order\", linear order, or a chain. A linear order where every nonempty subset has a least element is called a well-order.\n\nIf \"R\", \"S\" are binary relations over \"X\" and \"Y\", then each of the following is a binary relation over \"X\" and \"Y\":\n\nIf \"R\" is a binary relation over \"X\" and \"Y\", and \"S\" is a binary relation over \"Y\" and \"Z\", then the following is a binary relation over \"X\" and \"Z\": (see main article \"composition of relations\")\nA relation \"R\" on sets \"X\" and \"Y\" is said to be contained in a relation \"S\" on \"X\" and \"Y\" if \"R\" is a subset of \"S\", that is, if \"x\" \"R\" \"y\" always implies \"x\" \"S\" \"y\". In this case, if \"R\" and \"S\" disagree, \"R\" is also said to be smaller than \"S\". For example, > is contained in ≥.\n\nIf \"R\" is a binary relation over \"X\" and \"Y\", then the following is a binary relation over \"Y\" and \"X\":\nIf \"R\" is a binary relation over \"X\", then each of the following is a binary relation over \"X\":\n\nIf \"R\" is a binary relation over \"X\" and \"Y\", then the following too:\n\nThe complement of the inverse is the inverse of the complement.\n\nIf \"X\" = \"Y\", the complement has the following properties:\n\nThe complement of the inverse has these same properties.\n\nThe restriction of a binary relation on a set \"X\" to a subset \"S\" is the set of all pairs (\"x\", \"y\") in the relation for which \"x\" and \"y\" are in \"S\".\n\nIf a relation is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), or an equivalence relation, its restrictions are too.\n\nHowever, the transitive closure of a restriction is a subset of the restriction of the transitive closure, i.e., in general not equal.\nFor example, restricting the relation \"\"x\" is parent of \"y\"\" to females yields the relation \"\"x\" is mother of the woman \"y\"\"; its transitive closure doesn't relate a woman with her paternal grandmother. On the other hand, the transitive closure of \"is parent of\" is \"is ancestor of\"; its restriction to females does relate a woman with her paternal grandmother.\n\nAlso, the various concepts of completeness (not to be confused with being \"total\") do not carry over to restrictions. For example, on the set of real numbers a property of the relation \"≤\" is that every non-empty subset \"S\" of R with an upper bound in R has a least upper bound (also called supremum) in R. However, for a set of rational numbers this supremum is not necessarily rational, so the same property does not hold on the restriction of the relation \"≤\" to the set of rational numbers.\n\nThe \"left-restriction\" (\"right-restriction\", respectively) of a binary relation between \"X\" and \"Y\" to a subset \"S\" of its domain (codomain) is the set of all pairs (\"x\", \"y\") in the relation for which \"x\" (\"y\") is an element of \"S\".\n\nVarious operations on binary endorelations can be treated as giving rise to an algebraic structure, known as relation algebra. It should not be confused with relation\"al\" algebra which deals in finitary relations (and in practice also finite and many-sorted).\n\nFor heterogenous binary relations, a category of relations arises.\n\nDespite their simplicity, binary relations are at the core of an abstract computation model known as an abstract rewriting system.\n\nCertain mathematical \"relations\", such as \"equal to\", \"member of\", and \"subset of\", cannot be understood to be binary relations as defined above, because their domains and codomains cannot be taken to be sets in the usual systems of axiomatic set theory. For example, if we try to model the general concept of \"equality\" as a binary relation =, we must take the domain and codomain to be the \"class of all sets\", which is not a set in the usual set theory.\n\nIn most mathematical contexts, references to the relations of equality, membership and subset are harmless because they can be understood implicitly to be restricted to some set in the context. The usual work-around to this problem is to select a \"large enough\" set \"A\", that contains all the objects of interest, and work with the restriction = instead of =. Similarly, the \"subset of\" relation ⊆ needs to be restricted to have domain and codomain \"P\"(\"A\") (the power set of a specific set \"A\"): the resulting set relation can be denoted ⊆. Also, the \"member of\" relation needs to be restricted to have domain \"A\" and codomain \"P\"(\"A\") to obtain a binary relation ∈ that is a set. Bertrand Russell has shown that assuming ∈ to be defined on all sets leads to a contradiction in naive set theory.\n\nAnother solution to this problem is to use a set theory with proper classes, such as NBG or Morse–Kelley set theory, and allow the domain and codomain (and so the graph) to be proper classes: in such a theory, equality, membership, and subset are binary relations without special comment. (A minor modification needs to be made to the concept of the ordered triple (\"X\", \"Y\", \"G\"), as normally a proper class cannot be a member of an ordered tuple; or of course one can identify the function with its graph in this context.) With this definition one can for instance define a function relation between every set and its power set.\n\nThe number of distinct binary relations on an \"n\"-element set is 2 :\nNotes:\n\nThe binary relations can be grouped into pairs (relation, complement), except that for \"n\" = 0 the relation is its own complement. The non-symmetric ones can be grouped into quadruples (relation, complement, inverse, inverse complement).\n\n\n", "id": "3931", "title": "Binary relation"}
{"url": "https://en.wikipedia.org/wiki?curid=3933", "text": "Braille\n\nBraille () is a tactile writing system used by people who are blind or visually impaired. It is traditionally written with embossed paper. Braille-users can read computer screens and other electronic supports thanks to refreshable braille displays. They can write braille with the original slate and stylus or type it on a braille writer, such as a portable braille note-taker, or on a computer that prints with a braille embosser.\n\nBraille is named after its creator, Frenchman Louis Braille, who lost his eyesight due to a childhood accident. In 1824, at the age of 15, Braille developed his code for the French alphabet as an improvement on night writing. He published his system, which subsequently included musical notation, in 1829. The second revision, published in 1837, was the first binary form of writing developed in the modern era.\n\nBraille characters are small rectangular blocks called \"cells\" that contain tiny palpable bumps called \"raised dots\". The number and arrangement of these dots distinguish one character from another. Since the various braille alphabets originated as transcription codes of printed writing systems, the mappings (sets of character designations) vary from language to language. Furthermore, in English Braille there are three levels of encoding: Grade 1 – a letter-by-letter transcription used for basic literacy; Grade 2 – an addition of abbreviations and contractions; and Grade 3 – various non-standardized personal shorthands.\n\nBraille cells are not the only thing to appear in braille text. There may be embossed illustrations and graphs, with the lines either solid or made of series of dots, arrows, bullets that are larger than braille dots, etc. A full Braille cell includes six raised dots arranged in two lateral rows each having three dots. The dot positions are identified by numbers from one through six. 64 solutions are possible from using one or more dots. A single cell can be used to represent an alphabet letter, number, punctuation mark, or even an entire word.\n\nIn the face of screen-reader software, braille usage has declined. However, because it teaches spelling and punctuation, braille education remains important for developing reading skills among blind and visually impaired children, and braille literacy correlates with higher employment rates.\n\nBraille was based on a tactile military code called night writing, developed by Charles Barbier in response to Napoleon's demand for a means for soldiers to communicate silently at night and without a light source. In Barbier's system, sets of 12 embossed dots encoded 36 different sounds. It proved to be too difficult for soldiers to recognize by touch, and was rejected by the military. In 1821 Barbier visited the Royal Institute for the Blind in Paris, where he met Louis Braille. Braille identified two major defects of the code: first, by representing only sounds, the code was unable to render the orthography of the words; second, the human finger could not encompass the whole 12-dot symbol without moving, and so could not move rapidly from one symbol to another. Braille's solution was to use 6-dot cells and to assign a specific pattern to each letter of the alphabet.\nAt first, braille was a one-to-one transliteration of French orthography, but soon various abbreviations, contractions, and even logograms were developed, creating a system much more like shorthand. The expanded English system, called Grade-2 Braille, was complete by 1905. For blind readers, Braille is an independent writing system, rather than a code of printed orthography.\n\nBraille is derived from the Latin alphabet, albeit indirectly. In Braille's original system, the dot patterns were assigned to letters according to their position within the alphabetic order of the French alphabet, with accented letters and \"w\" sorted at the end.\n\nThe first ten letters of the alphabet, \"a–j,\" use the upper four dot positions: (black dots in the table below). These stand for the ten digits \"1–9\" and \"0\" in a system parallel to Hebrew gematria and Greek isopsephy. (Though the dots are assigned in no obvious order, the cells with the fewest dots are assigned to the first three letters (and lowest digits), \"abc = 123\" (), and to the three vowels in this part of the alphabet, \"aei\" (), whereas the even digits, \"4, 6, 8, 0\" (), are corners/right angles.)\n\nThe next ten letters, \"k–t,\" are identical to \"a–j,\" respectively, apart from the addition of a dot at position 3 (red dots in the table): :\n\nThe next ten letters (the next \"decade\") are the same again, but with dots also at positions both 3 and 6 (green dots). Here \"w\" was left out as not being a part of the official French alphabet at the time of Braille's life; the French braille order is \"u v x y z ç é à è ù\" ().\n\nThe next ten, ending in \"w\", are the same again, except that for this series position 6 (purple dot) is used without position 3. These are \"â ê î ô û ë ï ü ö w\" ().\n\nThe \"a–j\" series lowered by one dot space () are used for punctuation. Letters \"a\" and \"c\" , which only use dots in the top row, were lowered two places for the apostrophe and hyphen: . (These are the decade diacritics, at left in the table below, of the second and third decade.)\n\nIn addition, there are ten patterns that are based on the first two letters () shifted to the right; these were assigned to non-French letters (\"ì ä ò\" ), or serve non-letter functions: (superscript; in English the accent mark), (currency prefix), (capital, in English the decimal point), (number sign), (emphasis mark), (symbol prefix).\n\nOriginally there had been nine decades. The fifth through ninth used dashes as well as dots, but proved to be impractical and were soon abandoned. These could be replaced with what we now know as the number sign (), though that only caught on for the digits (old 5th decade → modern 1st decade). The dash occupying the top row of the original sixth decade was simply dropped, producing the modern fifth decade. (See 1829 braille.)\n\nHistorically, there have been three principles in assigning the values of a linear script (print) to braille: Using Louis Braille's original French letter values; reassigning the braille letters according to the sort order of the print alphabet being transcribed; and reassigning the letters to improve the efficiency of writing in braille.\n\nUnder international consensus, most braille alphabets follow the French sorting order for the 26 letters of the basic Latin alphabet, and there have been attempts at unifying the letters beyond these 26 (see international braille), though differences remain, for example in German Braille and the contractions of English Braille. This unification avoids the chaos of each nation reordering the braille code to match the sorting order of its print alphabet, as happened in Algerian Braille, where braille codes were numerically reassigned to match the order of the Arabic alphabet and bear little relation to the values used in other countries (compare modern Arabic Braille, which uses the French sorting order), and as happened in an early American version of English Braille, where the letters \"w, x, y, z\" were reassigned to match English alphabetical order. A convention sometimes seen for letters beyond the basic 26 is to exploit the physical symmetry of braille patterns iconically, for example, by assigning a reversed \"n\" to \"ñ\" or an inverted \"s\" to \"sh\". (See Hungarian Braille and Bharati Braille, which do this to some extent.)\n\nA third principle was to assign braille codes according to frequency, with the simplest patterns (quickest ones to write) assigned to the most frequent letters of the alphabet. Such frequency-based alphabets were used in Germany and the United States in the 19th century (see American Braille), but none are attested in modern use. Finally, there are braille scripts which don't order the codes numerically at all, such as Japanese Braille and Korean Braille, which are based on more abstract principles of syllable composition.\n\nAcademic texts are sometimes written in a script of eight dots per cell rather than six, enabling them to encode a greater number of symbols. (See Gardner–Salinas braille codes.) Luxembourgish Braille has adopted eight-dot cells for general use; for example, it adds a dot below each letter to derive its capital variant.\n\nBraille was the first writing system with binary encoding. The system as devised by Braille consists of two parts:\n\nWithin an individual cell, the dot positions are arranged as two columns of three positions. A raised dot can appear in any of the six positions, producing sixty-four (2) possible patterns, including one in which there are no raised dots. For reference purposes, a pattern is commonly described by listing the positions where dots are raised, the positions being universally numbered, from top to bottom, as 1 to 3 on the left and 4 to 6 on the right. For example, dot pattern 1-3-4 describe a cell with three dots raised, at the top and bottom in the left column and at the top of the right column: that is, the letter \"m\". The lines of horizontal braille text are separated by a space, much like visible printed text, so that the dots of one line can be differentiated from the braille text above and below. Different assignments of braille codes (or code pages) are used to map the character sets of different printed scripts to the six-bit cells. Braille assignments have also been created for mathematical and musical notation. However, because the six-dot braille cell allows only 64 (2) patterns, including the space, the characters of a braille script commonly have multiple values, depending on their context. That is, character mapping between print and braille is not one-to-one. For example, the character corresponds in print to both the letter \"d\" and the digit \"4\".\n\nIn addition to simple encoding, many braille alphabets use contractions to reduce the size of braille texts and to increase reading speed. (See Contracted braille)\n\nBraille may be produced by hand using a slate and stylus in which each dot is created from the back of the page, writing in mirror image, or it may be produced on a braille typewriter or Perkins Brailler, or an electronic Brailler or eBrailler. Because braille letters cannot be effectively erased and written over if an error is made, an error is overwritten with all six dots (). \"Interpoint\" refers to braille printing that is offset, so that the paper can be embossed on both sides, with the dots on one side appearing between the divots that form the dots on the other (see the photo in the box at the top of this article for an example).\nUsing a computer or other electronic device, braille may be produced with a braille embosser (printer) or a refreshable braille display (screen).\n\nBraille has been extended to an 8-dot code, particularly for use with braille embossers and refreshable braille displays. In 8-dot braille the additional dots are added at the bottom of the cell, giving a matrix 4 dots high by 2 dots wide. The additional dots are given the numbers 7 (for the lower-left dot) and 8 (for the lower-right dot). Eight-dot braille has the advantages that the case of an individual letter is directly coded in the cell containing the letter and that all the printable ASCII characters can be represented in a single cell. All 256 (2) possible combinations of 8 dots are encoded by the Unicode standard. Braille with six dots is frequently stored as Braille ASCII.\n\nThe first 25 braille letters, up through the first half of the 3rd decade, transcribe \"a–z\" (skipping \"w\"). In English Braille, the rest of that decade is rounded out with the ligatures \"and, for, of, the,\" and \"with\". Omitting dot 3 from these forms the 4th decade, the ligatures \"ch, gh, sh, th, wh, ed, er, ou, ow\" and the letter \"w\".\n\nVarious formatting marks affect the values of the letters that follow them. They have no direct equivalent in print. The most important in English Braille are:\nThat is, is read as capital 'A', and as the digit '1'.\n\nBasic punctuation marks in English Braille include:\n\nPunctuation varies from language to language. For example, French Braille uses for its question mark and swaps the quotation marks and parentheses (to and ); it uses the period () for the decimal point, as in print, and the decimal point () to mark capitalization.\n\nBraille contractions are words and affixes that are shortened so that they take up fewer cells. In English Braille, for example, the word \"afternoon\" is written with just three letters, , much like stenoscript. There are also several abbreviation marks that create what are effectively logograms. The most common of these is dot 5, which combines with the first letter of words. With the letter \"m\", the resulting word is \"mother\". There are also ligatures (\"contracted\" letters), which are single letters in braille but correspond to more than one letter in print. The letter \"and\", for example, is used to write words with the sequence \"a-n-d\" in them, such as \"hand\".\n\nMost braille embossers support between 34 and 40 cells per line, and 25 lines per page.\n\nA manually operated Perkins braille typewriter supports a maximum of 42 cells per line (its margins are adjustable), and typical paper allows 25 lines per page.\n\nA large interlining Stainsby has 36 cells per line and 18 lines per page.\n\nAn A4-sized Marburg braille frame, which allows interpoint braille (dots on both sides of the page, offset so they do not interfere with each other) has 30 cells per line and 27 lines per page.\n\nA sighted child who is reading at a basic level should be able to understand common words and answer simple questions about the information presented. The child should also have enough fluency to get through the material in a timely manner. Over the course of a child's education, these foundations are built upon to teach higher levels of math, science, and comprehension skills. \n\nChildren who are blind not only have the education disadvantage of not being able to see — they also miss out on fundamental parts of early and advanced education if not provided with the necessary tools. Children who are blind or visually impaired can begin learning pre-braille skills from a very young age to become fluent braille readers as they get older.\n\nIn 1960, 50% of legally blind, school-age children were able to read braille in the U.S. According to the 2015 \"Annual Report\" from the American Printing House for the Blind, there were 61,739 legally blind students registered in the U.S. Of these, 8.6% (5,333) were registered as braille readers, 31% (19,109) as visual readers, 9.4% (5,795) as auditory readers, 17% (10,470) as pre-readers, and 34% (21,032) as non-readers.\n\nThere are numerous causes for the decline in braille usage, including school budget constraints, technology advancement, and different philosophical views over how blind children should be educated.\n\nA key turning point for braille literacy was the passage of the Rehabilitation Act of 1973, an act of Congress that moved thousands of children from specialized schools for the blind into mainstream public schools. Because only a small percentage of public schools could afford to train and hire braille-qualified teachers, braille literacy has declined since the law took effect. Braille literacy rates have improved slightly since the bill was passed, in part because of pressure from consumers and advocacy groups that has led 27 states to pass legislation mandating that children who are legally blind be given the opportunity to learn braille.\n\nIn 1998 there were 57,425 legally blind students registered in the United States, but only 10% (5,461) of them used braille as their primary reading medium.\n\nEarly braille education is crucial to literacy for a blind or low-vision child. A study conducted in the state of Washington found that people who learned braille at an early age did just as well, if not better, than their sighted peers in several areas, including vocabulary and comprehension. In the preliminary adult study, while evaluating the correlation between adult literacy skills and employment, it was found that 44% of the participants who had learned to read in braille were unemployed, compared to the 77% unemployment rate of those who had learned to read using print. Currently, among the estimated 85,000 blind adults in the United States, 90% of those who are braille-literate are employed. Among adults who do not know braille, only 33% are employed. Statistically, history has proven that braille reading proficiency provides an essential skill set that allows blind or low-vision children to compete with their sighted peers in a school environment and later in life as they enter the workforce.\n\nThough braille is thought to be the main way blind people read and write, in Britain (for example) out of the reported 2 million blind and low vision population, it is estimated that only around 15–20 thousand people use braille. Younger people are turning to electronic text on computers with screen reader software instead, a more portable communication method that they can use with their friends. A debate has started on how to make braille more attractive and for more teachers to be available to teach it.\n\nAlthough it is possible to transcribe print by simply substituting the equivalent braille character for its printed equivalent, in English such a character-by-character transcription (known as \"uncontracted braille\") is only used by beginners.\n\nBraille characters are much larger than their printed equivalents, and the standard 11\" by 11.5\" (28 cm × 30 cm) page has room for only 25 lines of 43 characters. To reduce space and increase reading speed, most braille alphabets and orthographies use ligatures, abbreviations, and contractions. Virtually all English Braille books are transcribed in this \"contracted braille,\" which adds an additional layer of complexity to English orthography: The Library of Congress's \"Instruction Manual for Braille Transcribing\" runs to over 300 pages and braille transcribers must pass certification tests.\n\nFully contracted braille is known as \"Grade 2 Braille\". There is an intermediate form between Computer Braille—one-for-one identity with print—and Grade 2, which is called Grade 1 Braille. In Grade 1 the capital-sign and Number sign are used, and most punctuation marks are shown using their Grade 2 values.\n\nThe system of contractions in English Braille begins with a set of 23 words which are contracted to single characters. Thus the word \"but\" is contracted to the single letter \"b,\" \"can\" to \"c\", \"do\" to \"d\", and so on. Even this simple rule creates issues requiring special cases; for example, \"d\" is, specifically, an abbreviation of the verb \"do;\" the noun \"do\" representing the note of the musical scale is a different word, and must be spelled out.\n\nPortions of words may be contracted, and many rules govern this process. For example, the character with dots 2-3-5 (the letter \"f\" lowered in the braille cell) stands for \"ff\" when used in the middle of a word. At the beginning of a word, this same character stands for the word \"to\"; the character is written in braille with no space following it. (This contraction was removed in the Unified English Braille Code.) At the end of a word, the same character represents an exclamation point.\n\nSome contractions are more similar than their print equivalents. For example, the contraction , meaning 'letter', differs from , meaning 'little', only in adding one dot to the second : \"little\", \"letter\". This causes greater confusion between the braille spellings of these words and can hinder the learning process of contracted braille.\n\nThe contraction rules take into account the linguistic structure of the word; thus, contractions are generally not to be used when their use would alter the usual braille form of a base word to which a prefix or suffix has been added. Some portions of the transcription rules are not fully codified and rely on the judgment of the transcriber. Thus, when the contraction rules permit the same word in more than one way, preference is given to \"the contraction that more nearly approximates correct pronunciation.\"\n\n\"Grade 3 Braille\" is a variety of non-standardized systems that include many additional shorthand-like contraction. They are not used for publication, but by individuals for their personal convenience.\n\nWhen people produce braille, this is called braille transcription. When computer software produces braille, this is called braille\ntranslation. Braille translation software exists to handle most of the common languages of the world, and many technical areas,\nsuch as mathematics (mathematical notation), for example WIMATS, music (musical notation), and tactile graphics.\n\nSince braille is one of the few writing systems where tactile perception is used, as opposed to visual perception, a braille reader must develop new skills. One skill important for braille readers is the ability to create smooth and even pressures when running one's fingers along the words. There are many different styles and techniques used for the understanding and development of braille, even though a study by B. F. Holland suggests that there is no specific technique that is superior to any other.\n\nAnother study by Lowenfield & Abel shows that braille could be read \"the fastest and best... by students who read using the index fingers of both hands.\" Another important reading skill emphasized in this study is to finish reading the end of a line with the right hand and to find the beginning of the next line with the left hand simultaneously. One final conclusion drawn by both Lowenfield and Abel is that children have difficulty using both hands independently where the right hand is the dominant hand. But this hand preference does not correlate to other activities.\n\nWhen braille was first adapted to languages other than French, many schemes were adopted, including mapping the native alphabet to the alphabetical order of French – e.g. in English W, which was not in the French alphabet at the time, is mapped to braille X, X to Y, Y to Z, and Z to the first French accented letter – or completely rearranging the alphabet such that common letters are represented by the simplest braille patterns. Consequently, mutual intelligibility was greatly hindered by this state of affairs. In 1878, the International Congress on Work for the Blind, held in Paris, proposed an international braille standard, where braille codes for different languages and scripts would be based, not on the order of a particular alphabet, but on phonetic correspondence and transliteration to Latin.\n\nThis unified braille has been applied to the languages of India and Africa, Arabic, Vietnamese, Hebrew, Russian, and Armenian, as well as nearly all Latin-script languages. Greek, for example, \"gamma\" is written as Latin \"g\", despite the fact that it has the alphabetic position of \"c\"; Hebrew \"bet\", the second letter of the alphabet and cognate with the Latin letter \"b\", is sometimes pronounced /b/ and sometimes /v/, and is written \"b\" or \"v\" accordingly; Russian \"ts\" is written as \"c\", which is the usual letter for /ts/ in those Slavic languages that use the Latin alphabet; and Arabic \"f\" is written as \"f\", despite being historically \"p\", and occurring in that part of the Arabic alphabet (between historic \"o\" and \"q\").\n\nOther systems for assigning values to braille patterns are also followed, beside the simple mapping of the alphabetical order onto the original French order. Some braille alphabets start with unified braille, and then diverge significantly based on the phonology of the target languages, while others diverge even further.\n\nIn the various Chinese systems, traditional braille values are used for initial consonants and the simple vowels. In both Mandarin and Cantonese Braille, however, characters have different readings depending on whether they are placed in syllable-initial (onset) or syllable-final (rime) position. For instance, the cell for Latin \"k\", , represents Cantonese \"k\" (\"g\" in Yale and other modern romanizations) when initial, but \"aak\" when final, while Latin \"j\", , represents Cantonese initial \"j\" but final \"oei\".\n\nNovel systems of braille mapping include Korean, which adopts separate syllable-initial and syllable-final forms for its consonants, explicitly grouping braille cells into syllabic groups in the same way as hangul. Japanese, meanwhile, combines independent vowel dot patterns and modifier consonant dot patterns into a single braille cell – an abugida representation of each Japanese mora.\n\nThe current series of Canadian banknotes has a tactile feature consisting of raised dots that indicate the denomination, allowing bills to be easily identified by blind or low vision people. It does not use standard braille; rather, the feature uses a system developed in consultation with blind and low vision Canadians after research indicated that braille was not sufficiently robust and that not all potential users read braille. Mexican bank notes, Indian rupee notes, Israeli New Shekel notes, Russian Ruble and Swiss Franc notes also have special raised symbols to make them identifiable by persons who are blind or low vision. \n\nIn India there are instances where the parliament acts have been published in braille, such as \"The Right to Information Act\".\n\nIn the United States, the Americans with Disabilities Act of 1990 requires various building signage to be in braille.\n\nIn the United Kingdom, it is required that medicines have the name of the medicine in Braille on the labelling.\n\nAustralia also recently introduced the tactile feature onto their five dollar banknote\n\nBraille was added to the Unicode Standard in September, 1999 with the release of version 3.0.\n\nMost braille embossers and refreshable braille displays do not support Unicode, using instead 6-dot braille ASCII. Some embossers have proprietary control codes for 8-dot braille or for full graphics mode, where dots may be placed anywhere on the page without leaving any space between braille cells, so that continuous lines can be drawn in diagrams, but these are rarely used and are not standard.\n\nThe Unicode standard encodes 8-dot braille glyphs according to their binary appearance, rather than following their assigned numeric order. Dot 1 corresponds to the least significant bit of the low byte of the Unicode scalar value, and dot 8 to the high bit of that byte.\n\nThe Unicode block for braille is U+2800 ... U+28FF:\n\nEvery year on 4 January, World Braille Day is observed internationally to commemorate the birth of Louis Braille and to recognize his efforts. However, the event is not considered a public holiday.\n\n", "id": "3933", "title": "Braille"}
{"url": "https://en.wikipedia.org/wiki?curid=3936", "text": "Bastille Day\n\nBastille Day is the common name given in English-speaking countries/lands to the French National Day, which is celebrated on 14 July each year. In France, it is formally called ' (; \"The National Celebration\") and commonly and legally ' (; \"the fourteenth of July\"). \n\nThe French National Day commemorates the Storming of the Bastille on 14 July 1789, a turning point of the French Revolution, as well as the Fête de la Fédération which celebrated the unity of the French people on 14 July 1790. Celebrations are held throughout France. The oldest and largest regular military parade in Europe is held on the morning of 14 July, on the Champs-Élysées in Paris in front of the President of the Republic, along with other French officials and foreign guests.\n\nOn 19 May 1789, Louis XVI invited Estates-General (') to air their grievances. The deputies of the Third Estate ('), representing the common people—the two others were the Catholic clergy (', Roman Catholicism being the state religion at that time) and the nobility (')—decided to break away and form a National Assembly. The Third Estate took the Tennis Court Oath (', 20 June 1789), swearing not to separate until a constitution had been established. They were gradually joined by (liberal) delegates of the other estates; Louis XVI started to recognize the validity of their concerns on 27 June. The assembly renamed itself the National Constituent Assembly (') on 9 July. \nJacques Necker, the finance minister, who was sympathetic to the Third Estate, was dismissed on 11 July. The people of Paris then stormed the Bastille, fearful that they and their representatives would be attacked by the royal army or by foreign regiments of mercenaries in the king's service, and seeking to gain ammunition and gunpowder for the general populace. The Bastille was a fortress-prison in Paris which had often held people jailed on the basis of \"lettres de cachet\" (literally \"signet letters\"), arbitrary royal indictments that could not be appealed and did not indicate the reason for the imprisonment. The Bastille held a large cache of ammunition and gunpowder, and was also known for holding political prisoners whose writings had displeased the royal government, and was thus a symbol of the absolutism of the monarchy. As it happened, at the time of the attack in July 1789 there were only seven inmates, none of great political significance.\n\nThe crowd was eventually reinforced by mutinous \"Gardes Françaises\" (\"French Guards\"), whose usual role was to protect public buildings. They proved a fair match for the fort's defenders, and Governor de Launay, the commander of the Bastille, capitulated and opened the gates to avoid a mutual massacre. However, possibly because of a misunderstanding, fighting resumed. According to the official documents, about 200 attackers and just one defender died in the actual fighting, but in the aftermath, de Launay and seven other defenders were killed, as was Jacques de Flesselles, the \"prévôt des marchands\" (\"provost of the merchants\"), the elected head of the city's guilds, who under the feudal monarchy also had the competences of a present-day mayor .\n\nShortly after the storming of the Bastille, late in the evening of 4 August, after a very stormy session of the \"Assemblée Constituante\", feudalism was abolished. On 26 August, the Declaration of the Rights of Man and of the Citizen (\"Déclaration des Droits de l'Homme et du Citoyen\") was proclaimed (\"homme\" meaning both \"man\" and \"human\").\n\nThe ' on 14 July 1790 was a celebration of the unity of the French nation during the French Revolution. The aim of this celebration, one year after the Storming of the Bastille, was to symbolize peace. The event took place on the Champ de Mars, which was at the time far outside Paris. The place had been transformed on a voluntary basis by the population of Paris, in what was recalled as the ' (\"Wheelbarrow Day\").\n\nA mass was celebrated by Talleyrand, bishop of Autun. The popular General Lafayette, as captain of the National Guard of Paris and a confidant of the king, took his oath to the constitution, followed by King Louis XVI. After the end of the official celebration, the day ended in a huge four-day popular feast and people celebrated with fireworks, as well as fine wine and running nude through the streets in order to display their great freedom.\n\nOn 30 June 1878, a feast was officially arranged in Paris to honour the French Republic (the event was commemorated in a painting by Claude Monet). On 14 July 1879, there was another feast, with a semi-official aspect. The day's events included a reception in the Chamber of Deputies, organised and presided over by Léon Gambetta, a military review at Longchamp, and a Republican Feast in the Pré Catelan. All through France, \"Le Figaro\" wrote, \"people feasted much to honour the storming of the Bastille\".\n\nOn 21 May 1880, Benjamin Raspail proposed a law to have \"the Republic choose the 14 July as a yearly national holiday\". The Assembly voted in favour of the proposal on 21 May and 8 June. The Senate approved it on 27 and 29 June, favouring 14 July against 4 August (which would have commemorated the end of the feudal system on 4 August 1789). The law was made official on 6 July 1880, and the Ministry of the Interior recommended to Prefects that the day should be \"celebrated with all the brilliance that the local resources allow\". Indeed, the celebrations of the new holiday in 1880 were particularly magnificent.\n\nIn the debate leading up to the adoption of the holiday, Henri Martin, chairman of the French Senate, addressed that chamber on 29 June 1880:\n\nThe Bastille Day Military Parade is the French military parade that has been held on the morning of 14 July each year in Paris since 1880. While previously held elsewhere within or near the capital city, since 1918 it has been held on the Champs-Élysées, with the participation of the Allies as represented in the Versailles Peace Conference, and with the exception of the period of German occupation from 1940 to 1944 (when the ceremony took place in London under the command of General Charles de Gaulle). The parade passes down the Champs-Élysées from the Arc de Triomphe to the Place de la Concorde, where the President of the French Republic, his government and foreign ambassadors to France stand. This is a popular event in France, broadcast on French TV, and is the oldest and largest regular military parade in Europe. In some years, invited detachments of foreign troops take part in the parade and foreign statesmen attend as guests.\n\nSmaller military parades are held in French garrison towns, including Toulon and Belfort, with local troops.\n\n\n\n\n\n", "id": "3936", "title": "Bastille Day"}
{"url": "https://en.wikipedia.org/wiki?curid=3940", "text": "Blowfish (cipher)\n\nBlowfish is a symmetric-key block cipher, designed in 1993 by Bruce Schneier and included in a large number of cipher suites and encryption products. Blowfish provides a good encryption rate in software and no effective cryptanalysis of it has been found to date. However, the Advanced Encryption Standard (AES) now receives more attention.\n\nSchneier designed Blowfish as a general-purpose algorithm, intended as an alternative to the aging DES and free of the problems and constraints associated with other algorithms. At the time Blowfish was released, many other designs were proprietary, encumbered by patents or were commercial or government secrets. Schneier has stated that, \"Blowfish is unpatented, and will remain so in all countries. The algorithm is hereby placed in the public domain, and can be freely used by anyone.\"\n\nNotable features of the design include key-dependent S-boxes and a highly complex key schedule.\n\nBlowfish has a 64-bit block size and a variable key length from 32 bits up to 448 bits. It is a 16-round Feistel cipher and uses large key-dependent S-boxes. In structure it resembles CAST-128, which uses fixed S-boxes.\n\nThe diagram to the left shows Blowfish's encryption routine. Each line represents 32 bits. There are five subkey-arrays: one 18-entry P-array (denoted as K in the diagram, to avoid confusion with the Plaintext) and four 256-entry S-boxes (S0, S1, S2 and S3).\n\nEvery round \"r\" consists of 4 actions: First, XOR the left half (L) of the data with the \"r\" th P-array entry, second, use the XORed data as input for Blowfish's F-function, third, XOR the F-function's output with the right half (R) of the data, and last, swap L and R.\n\nThe F-function splits the 32-bit input into four eight-bit quarters, and uses the quarters as input to the S-boxes. The S-boxes accept 8-bit input and produce 32-bit output. The outputs are added modulo 2 and XORed to produce the final 32-bit output (see image in the upper right corner).\n\nAfter the 16th round, undo the last swap, and XOR L with K18 and R with K17 (output whitening).\n\nDecryption is exactly the same as encryption, except that P1, P2..., P18 are used in the reverse order. This is not so obvious because xor is commutative and associative. A common misconception is to use inverse order of encryption as decryption algorithm (i.e. first XORing P17 and P18 to the ciphertext block, then using the P-entries in reverse order).\n\nBlowfish's key schedule starts by initializing the P-array and S-boxes with values derived from the hexadecimal digits of pi, which contain no obvious pattern (see nothing up my sleeve number). The secret key is then, byte by byte, cycling the key if necessary, XORed with all the P-entries in order. A 64-bit all-zero block is then encrypted with the algorithm as it stands. The resultant ciphertext replaces P and P. The same ciphertext is then encrypted again with the new subkeys, and the new ciphertext replaces P and P. This continues, replacing the entire P-array and all the S-box entries. In all, the Blowfish encryption algorithm will run 521 times to generate all the subkeys - about 4KB of data is processed.\n\nBecause the P-array is 576 bits long, and the key bytes are XORed through all these 576 bits during the initialization, many implementations support key sizes up to 576 bits. While this is certainly possible, the 448 bits limit is here to ensure that every bit of every subkey depends on every bit of the key, as the last four values of the P-array don't affect every bit of the ciphertext. This point should be taken in consideration for implementations with a different number of rounds, as even though it increases security against an exhaustive attack, it weakens the security guaranteed by the algorithm. And given the slow initialization of the cipher with each change of key, it is granted a natural protection against brute-force attacks, which doesn't really justify key sizes longer than 448 bits.\n\nBlowfish is a fast block cipher, except when changing keys. Each new key requires pre-processing equivalent to encrypting about 4 kilobytes of text, which is very slow compared to other block ciphers. This prevents its use in certain applications, but is not a problem in others.\n\nIn one application Blowfish's slow key changing is actually a benefit: the password-hashing method used in OpenBSD uses an algorithm derived from Blowfish that makes use of the slow key schedule; the idea is that the extra computational effort required gives protection against dictionary attacks. \"See\" key stretching.\n\nBlowfish has a memory footprint of just over 4 kilobytes of RAM. This constraint is not a problem even for older desktop and laptop computers, though it does prevent use in the smallest embedded systems such as early smartcards.\n\nBlowfish was one of the first secure block ciphers not subject to any patents and therefore freely available for anyone to use. This benefit has contributed to its popularity in cryptographic software.\n\nbcrypt is a cross-platform file encryption utility implementing Blowfish developed in 2002.\n\nBlowfish's use of a 64-bit block size (as opposed to e.g. AES's 128-bit block size) makes it vulnerable to birthday attacks, particularly in contexts like HTTPS. In 2016, the SWEET32 attack demonstrated how to leverage birthday attacks to perform plaintext recovery (i.e. decrypting ciphertext) against ciphers with a 64-bit block size such as Blowfish.\n\nA reduced-round variant of Blowfish is known to be susceptible to known-plaintext attacks on reflectively weak keys. Blowfish implementations use 16 rounds of encryption, and are not susceptible to this attack. Blowfish users are encouraged by Bruce Schneier, Blowfish's creator, to use the more modern and computationally efficient alternative Twofish. He is quoted in 2007 as saying: The FAQ for GnuPG (which features Blowfish as one of its algorithms) recommends that Blowfish should \"not\" be used to encrypt files that are larger than 4 Gb because of its small 64-bit block size.\n\n", "id": "3940", "title": "Blowfish (cipher)"}
{"url": "https://en.wikipedia.org/wiki?curid=3942", "text": "Bijection\n\nIn mathematics, a bijection, bijective function or one-to-one correspondence is a function between the elements of two sets, where each element of one set is paired with exactly one element of the other set, and each element of the other set is paired with exactly one element of the first set. There are no unpaired elements. In mathematical terms, a bijective function \"f\": \"X\" → \"Y\" is a one-to-one (injective) and onto (surjective) mapping of a set \"X\" to a set \"Y\".\n\nA bijection from the set \"X\" to the set \"Y\" has an inverse function from \"Y\" to \"X\". If \"X\" and \"Y\" are finite sets, then the existence of a bijection means they have the same number of elements. For infinite sets the picture is more complicated, leading to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.\n\nA bijective function from a set to itself is also called a \"permutation\".\n\nBijective functions are essential to many areas of mathematics including the definitions of isomorphism, homeomorphism, diffeomorphism, permutation group, and projective map.\n\nFor a pairing between \"X\" and \"Y\" (where \"Y\" need not be different from \"X\") to be a bijection, four properties must hold:\n\nSatisfying properties (1) and (2) means that a bijection is a function with domain \"X\". It is more common to see properties (1) and (2) written as a single statement: Every element of \"X\" is paired with exactly one element of \"Y\". Functions which satisfy property (3) are said to be \"onto \"Y\" \" and are called surjections (or surjective functions). Functions which satisfy property (4) are said to be \"one-to-one functions\" and are called injections (or injective functions). With this terminology, a bijection is a function which is both a surjection and an injection, or using other words, a bijection is a function which is both \"one-to-one\" and \"onto\".\n\nConsider the batting line-up of a baseball or cricket team (or any list of all the players of any sports team where every player holds a specific spot in a line-up). The set \"X\" will be the players on the team (of size nine in the case of baseball) and the set \"Y\" will be the positions in the batting order (1st, 2nd, 3rd, etc.) The \"pairing\" is given by which player is in what position in this order. Property (1) is satisfied since each player is somewhere in the list. Property (2) is satisfied since no player bats in two (or more) positions in the order. Property (3) says that for each position in the order, there is some player batting in that position and property (4) states that two or more players are never batting in the same position in the list.\n\nIn a classroom there are a certain number of seats. A bunch of students enter the room and the instructor asks them all to be seated. After a quick look around the room, the instructor declares that there is a bijection between the set of students and the set of seats, where each student is paired with the seat they are sitting in. What the instructor observed in order to reach this conclusion was that:\nThe instructor was able to conclude that there were just as many seats as there were students, without having to count either set.\n\n\nA bijection \"f\" with domain \"X\" (indicated by \"f\": \"X → Y\" in functional notation) also defines a relation starting in \"Y\" and going to \"X\" (by turning the arrows around). The process of \"turning the arrows around\" for an arbitrary function does not, \"in general\", yield a function, but properties (3) and (4) of a bijection say that this inverse relation is a function with domain \"Y\". Moreover, properties (1) and (2) then say that this inverse \"function\" is a surjection and an injection, that is, the inverse function exists and is also a bijection. Functions that have inverse functions are said to be invertible. A function is invertible if and only if it is a bijection.\n\nStated in concise mathematical notation, a function \"f\": \"X → Y\" is bijective if and only if it satisfies the condition\n\nContinuing with the baseball batting line-up example, the function that is being defined takes as input the name of one of the players and outputs the position of that player in the batting order. Since this function is a bijection, it has an inverse function which takes as input a position in the batting order and outputs the player who will be batting in that position.\n\nThe composition formula_3 of two bijections \"f\": \"X → Y\" and \"g\": \"Y → Z\" is a bijection. The inverse of formula_3 is formula_5.\nConversely, if the composition formula_3 of two functions is bijective, we can only say that \"f\" is injective and \"g\" is surjective.\n\nIf \"X\" and \"Y\" are finite sets, then there exists a bijection between the two sets \"X\" and \"Y\" if and only if \"X\" and \"Y\" have the same number of elements. Indeed, in axiomatic set theory, this is taken as the definition of \"same number of elements\" (equinumerosity), and generalising this definition to infinite sets leads to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.\n\n\nBijections are precisely the isomorphisms in the category Set of sets and set functions. However, the bijections are not always the isomorphisms for more complex categories. For example, in the category Grp of groups, the morphisms must be homomorphisms since they must preserve the group structure, so the isomorphisms are \"group isomorphisms\" which are bijective homomorphisms.\n\nThe notion of one-to-one correspondence generalizes to partial functions, where they are called partial bijections, although partial bijections are only required to be injective. The reason for this relaxation is that a (proper) partial function is already undefined for a portion of its domain; thus there is no compelling reason to constrain its inverse to be a total function, i.e. defined everywhere on its domain. The set of all partial bijections on a given base set is called the symmetric inverse semigroup.\n\nAnother way of defining the same notion is to say that a partial bijection from \"A\" to \"B\" is any relation \n\"R\" (which turns out to be a partial function) with the property that \"R\" is the graph of a bijection \"f\":\"A′\"→\"B′\", where \"A′\" is a subset of \"A\" and \"B′\" is a subset of \"B\".\n\nWhen the partial bijection is on the same set, it is sometimes called a one-to-one partial transformation. An example is the Möbius transformation simply defined on the complex plane, rather than its completion to the extended complex plane.\n\n\n\nThis topic is a basic concept in set theory and can be found in any text which includes an introduction to set theory. Almost all texts that deal with an introduction to writing proofs will include a section on set theory, so the topic may be found in any of these:\n\n", "id": "3942", "title": "Bijection"}
{"url": "https://en.wikipedia.org/wiki?curid=3943", "text": "Binary function\n\nIn mathematics, a binary function also called bivariate function, or function of two variables, is a function that takes two inputs.\n\nPrecisely stated, a function formula_1 is binary if there exists sets formula_2 such that\nwhere formula_4 is the Cartesian product of formula_5 and formula_6\n\nSet-theoretically, one may represent a binary function as a subset of the Cartesian product \"X\" × \"Y\" × \"Z\", where (\"x\",\"y\",\"z\") belongs to the subset if and only if \"f\"(\"x\",\"y\") = \"z\".\nConversely, a subset \"R\" defines a binary function if and only if for any \"x\" in \"X\" and \"y\" in \"Y\", there exists a unique \"z\" in \"Z\" such that (\"x\",\"y\",\"z\") belongs to \"R\".\nWe then define \"f\"(\"x\",\"y\") to be this \"z\".\n\nAlternatively, a binary function may be interpreted as simply a function from \"X\" × \"Y\" to \"Z\".\nEven when thought of this way, however, one generally writes \"f\" (\"x\",\"y\") instead of \"f\"((\"x\",\"y\")).\n\nDivision of whole numbers can be thought of as a function; if Z is the set of integers, N is the set of natural numbers (except for zero), and Q is the set of rational numbers, then division is a binary function from Z and N to Q.\n\nAnother example is that of inner products, or more generally functions of the form formula_7 where \"x,y\" are real-valued vectors of appropriate size and \"M\" is a matrix. If \"M\" is a positive definite matrix, this yields an inner product.\n\nFunctions whose domain is a subset of formula_8 are often also called functions of two variables even if their domain does not form a rectangle and thus the cartesian product of two sets.\n\nIn turn, one can also derive ordinary functions of one variable from a binary function.\nGiven any element \"x\" of \"X\", there is a function \"f\" , or \"f\" (\"x\",·), from \"Y\" to \"Z\", given by \"f\" (\"y\") := \"f\" (\"x\",\"y\").\nSimilarly, given any element \"y\" of \"Y\", there is a function \"f\" , or \"f\" (·,\"y\"), from \"X\" to \"Z\", given by \"f\" (\"x\") := \"f\" (\"x\",\"y\"). In computer science, this identification between a function from \"X\" × \"Y\" to \"Z\" and a function from \"X\" to \"Z\", where \"Z\" is the set of all functions from \"Y\" to \"Z\", is called \"currying\".\n\nThe various concepts relating to functions can also be generalised to binary functions.\nFor example, the division example above is \"surjective\" (or \"onto\") because every rational number may be expressed as a quotient of an integer and a natural number.\nThis example is \"injective\" in each input separately, because the functions \"f\" and \"f\" are always injective.\nHowever, it's not injective in both variables simultaneously, because (for example) \"f\" (2,4) = \"f\" (1,2).\n\nOne can also consider \"partial\" binary functions, which may be defined only for certain values of the inputs.\nFor example, the division example above may also be interpreted as a partial binary function from Z and N to Q, where N is the set of all natural numbers, including zero.\nBut this function is undefined when the second input is zero.\n\nA binary operation is a binary function where the sets \"X\", \"Y\", and \"Z\" are all equal; binary operations are often used to define algebraic structures.\n\nIn linear algebra, a bilinear transformation is a binary function where the sets \"X\", \"Y\", and \"Z\" are all vector spaces and the derived functions \"f\" and \"f\" are all linear transformations.\nA bilinear transformation, like any binary function, can be interpreted as a function from \"X\" × \"Y\" to \"Z\", but this function in general won't be linear.\nHowever, the bilinear transformation can also be interpreted as a single linear transformation from the tensor product formula_9 to \"Z\".\n\nThe concept of binary function generalises to \"ternary\" (or \"3-ary\") \"function\", \"quaternary\" (or \"4-ary\") \"function\", or more generally to \"n-ary function\" for any natural number \"n\".\nA \"0-ary function\" to \"Z\" is simply given by an element of \"Z\".\nOne can also define an \"A-ary function\" where \"A\" is any set; there is one input for each element of \"A\".\n\nIn category theory, \"n\"-ary functions generalise to \"n\"-ary morphisms in a multicategory.\nThe interpretation of an \"n\"-ary morphism as an ordinary morphisms whose domain is some sort of product of the domains of the original \"n\"-ary morphism will work in a monoidal category.\nThe construction of the derived morphisms of one variable will work in a closed monoidal category.\nThe category of sets is closed monoidal, but so is the category of vector spaces, giving the notion of bilinear transformation above.\n", "id": "3943", "title": "Binary function"}
{"url": "https://en.wikipedia.org/wiki?curid=3947", "text": "Blue Velvet (film)\n\nBlue Velvet is a 1986 American neo-noir mystery film, written and directed by David Lynch. Blending psychological horror with film noir, the film stars Kyle MacLachlan, Isabella Rossellini, Dennis Hopper and Laura Dern. The title is taken from Bobby Vinton's 1963 song of the same name.\n\nThe screenplay of \"Blue Velvet\" had been passed around multiple times in the late 1970s and early 1980s, with many major studios declining it because of its strong sexual and violent content. After the commercial and critical failure of Lynch's \"Dune\" (1984), the director made attempts at developing a more \"personal story\", somewhat characteristic of the surrealist style displayed in his debut \"Eraserhead\" (1977). The independent studio De Laurentiis Entertainment Group, owned at the time by Italian film producer Dino De Laurentiis, agreed to finance and produce the film.\n\n\"Blue Velvet\" initially received a divided critical response, with many stating that its objectionable content served little artistic purpose. It nevertheless earned Lynch his second Academy Award nomination for Best Director and came to achieve cult status. As an example of a director casting against the norm, it was credited for re-launching Hopper's career and for providing Rossellini with a dramatic outlet beyond her previous work as a fashion model and a cosmetics spokeswoman. In the years since, the film has generated significant academic attention with regard to its thematic symbolism, and is now widely regarded as one of Lynch's major works and one of the greatest films of the 1980s. Publications including \"Sight & Sound\", \"Time\", \"Entertainment Weekly\" and \"BBC Magazine\" have ranked it among the greatest American films of all time. In 2008, \"Blue Velvet\" was chosen by the American Film Institute as one of the greatest American mystery films ever made.\n\nJeffrey Beaumont (Kyle MacLachlan) returns to his logging home town of Lumberton, North Carolina, from Oak Lake College after his father suffers a near-fatal stroke. While walking home from the hospital, he cuts through a vacant lot and discovers a severed ear. Jeffrey takes the ear to police detective John Williams (George Dickerson) and becomes reacquainted with the detective's daughter, Sandy (Laura Dern). She tells him details about the ear case and a suspicious woman, Dorothy Vallens (Isabella Rossellini), who may be connected to the case. Increasingly curious, Jeffrey enters Dorothy's apartment by posing as an exterminator, and while Dorothy is distracted by a man dressed in a yellow suit at her door (whom Jeffrey later refers to as the Yellow Man), Jeffrey steals her spare key.\n\nJeffrey and Sandy attend Dorothy's nightclub act, in which she sings \"Blue Velvet\", and leave early so Jeffrey can sneak into her apartment to snoop. He hurriedly hides in a closet when she returns home. However, Dorothy, wielding a knife, discovers him and threatens to kill him. Believing his curiosity is merely sexual and aroused by his voyeurism, Dorothy makes Jeffrey undress at knifepoint and begins to fellate him before their encounter is interrupted by a knock at the door. Dorothy hides Jeffrey in the closet. From there he witnesses the visitor, Frank Booth (Dennis Hopper), inflict his bizarre sexual proclivities—which include inhaling an unidentified gas (possibly amyl nitrite), dry humping, and sadomasochism—upon Dorothy. Frank is an extremely foul-mouthed, violent sociopath whose orgasmic climax is a fit of both pleasure and rage. He continually refers to her as \"Mommy\" and to himself as both the \"Daddy\" and the \"Baby\", who \"wants to fuck.\" Frank has kidnapped Dorothy's husband and son to force her to perform sexual favors; to \"Do it for van Gogh.\" When Frank leaves, a sad and desperate Dorothy tries to seduce Jeffrey again and demands that he hit her, but when he refuses, she tells him to leave. When Jeffrey moves to leave, she asks him to stay, though he leaves anyway.\n\nJeffrey relays his experience to Sandy, asking her why there are people like Frank. Sandy in turn tells him of a wonderful dream she had about robins that she interprets as a sign of hope for humanity. Jeffrey and Sandy find themselves attracted to each other, though Sandy has a boyfriend.\n\nJeffrey again visits Dorothy's apartment and she tells him that although she knows nothing about him, she has been yearning for him. Jeffrey attends another of Dorothy's performances at the club, where she sings the same song. At the club, Jeffrey spots Frank in the audience fondling a piece of blue velvet fabric he cut from Dorothy's robe. Jeffrey follows Frank and spends the next few days spying on him. Shortly afterwards, two men that Jeffrey calls the Well-Dressed Man and the Yellow Man exit an industrial building that Frank frequently visits. Jeffrey concludes the men are criminal associates of Frank, and tells his new findings to Sandy. The two briefly kiss, though she feels uncomfortable about going any further. Jeffrey immediately visits Dorothy again, and the two have sex. However, when he refuses to hit her, she pressures him, becoming more emotional. In a blind rage he knocks her backwards and is instantly horrified, but Dorothy derives pleasure from it.\n\nAfterwards, Frank catches Dorothy and Jeffrey together and forces them both to accompany him to the apartment of Ben (Dean Stockwell), his suave, effeminate partner in crime who is holding Dorothy's son. Ben lip-syncs a performance of Roy Orbison's \"In Dreams\", sending Frank into maudlin sadness, then rage. Frank takes Jeffrey to a lumber yard and when he molests Dorothy, Jeffrey stands up to Frank by punching him. Frank's cronies drag Jeffrey out of the car and Frank kisses Jeffrey's face, intimidates him, and then savagely beats him to the overture of \"In Dreams\". Jeffrey wakes the next day at the same place and walks home, overcome with guilt and despair. He goes to the police station, where he notices that Sandy's father's partner is the Yellow Man—an officer named Lieutenant Detective Tom Gordon (Fred Pickler). Later, at Sandy's home, her father is amazed by Jeffrey's story, but warns Jeffrey to stop his amateur sleuthing lest he endanger himself and the investigation. Jeffrey and Sandy go to a dance together and profess their love, only to be confronted by Sandy's boyfriend. A confrontation is averted when the group finds Dorothy—naked, battered, and distressed—on Jeffrey's front lawn. Barely conscious, Dorothy reveals her intimacy with Jeffrey, causing Sandy to become upset and to slap Jeffrey, although she later forgives him.\n\nJeffrey insists on returning to Dorothy's apartment and tells Sandy to immediately send the police there, including her father. At Dorothy's apartment, Jeffrey finds Dorothy's husband (Don Vallens), who is dead from a gunshot to the head and identifiable by his missing ear, as well as the Yellow Man (Gordon), who bears a gruesome head wound and appears to have suffered a crude lobotomy. When Jeffrey tries to leave, he sees the Well-Dressed Man coming up the stairs and recognizes him as Frank in disguise. Jeffrey talks to Detective Williams over the Yellow Man's police radio, but lies about his location inside the apartment. Frank enters the apartment and brags about hearing Jeffrey's location over his own police radio. While Frank searches for him in the wrong room, Jeffrey retrieves the Yellow Man's gun and hides in the same closet in which he hid during his first visit to the apartment. Frank fires sporadically, knocking over the dead Yellow Man, who had still been standing up, and when he opens the closet door, Jeffrey fatally shoots him in the head. Detective Williams, gun drawn, enters with Sandy a moment later. Jeffrey and Sandy now go ahead with their relationship and note the unusual appearance of robins in their town. A montage sequence ends the film, which shows Dorothy and her son reunited.\n\n\nThe film's story originated from three ideas that crystallized in the filmmaker's mind over a period of time starting as early as 1973. The first idea was only \"a feeling\" and the title \"Blue Velvet\", Lynch told \"Cineaste\" in 1987. The second idea was an image of a severed, human ear lying in a field. \"I don't know why it had to be an ear. Except it needed to be an opening of a part of the body, a hole into something else ... The ear sits on the head and goes right into the mind so it felt perfect\", Lynch remarked in an interview. The third idea was Bobby Vinton's classic rendition of the song \"Blue Velvet\" and \"the mood that came with that song a mood, a time, and things that were of that time\". Lynch eventually spent two years writing two drafts, which, he stated, were not very good. The problem with them, Lynch has said, was that \"there was maybe all the unpleasantness in the film but nothing else. A lot was not there. And so it went away for a while.\"\n\nAfter completing \"The Elephant Man\" (1980), Lynch met producer Richard Roth over coffee. Roth had read and enjoyed Lynch's \"Ronnie Rocket\" script, but did not think it was something he wanted to produce. He asked Lynch if the filmmaker had any other scripts, but the director only had ideas. \"I told him I had always wanted to sneak into a girl's room to watch her into the night and that, maybe, at one point or another, I would see something that would be the clue to a murder mystery. Roth loved the idea and asked me to write a treatment. I went home and thought of the ear in the field.\" Production was announced in August 1984. Lynch wrote two more drafts before he was satisfied with the script of the film. Conditions at this point were ideal for Lynch's film: he had made a deal with Dino De Laurentiis that gave him complete artistic freedom and final cut privileges, with the stipulation that the filmmaker take a cut in his salary and work with a budget of only $6 million. This deal meant that \"Blue Velvet\" was the smallest film on the De Laurentiis' slate. Consequently, Lynch would be left mostly unsupervised during production. \"After \"Dune\" I was down so far that anything was up! So it was just a euphoria. And when you work with that kind of feeling, you can take chances. You can experiment.\" Because the material was completely different from anything that would be considered mainstream at the time, Laurentiis had to start his own company to distribute it.\n\nThe cast of \"Blue Velvet\" included several then-relatively unknown actors. Isabella Rossellini had gained some exposure before the film for her Lancôme ads in the early 1980s and for being the daughter of actress Ingrid Bergman and Italian film director Roberto Rossellini. After completion of the film, during test screenings, ICM Partners—the agency representing Rossellini—immediately dropped her as a client. Furthermore, the nuns at the school in Rome that Rossellini attended in her youth called to say they were praying for her. Dennis Hopper was the biggest \"name\" in the film, having starred in \"Easy Rider\" (1969), while Kyle MacLachlan had played the central role in Lynch's critical and commercial failure \"Dune\" (1984), a science fiction epic based on the novel of the same name. MacLachlan later became a recurring collaborator with Lynch, who remarked: \"Kyle plays innocents who are interested in the mysteries of life. He's the person you trust enough to go into a strange world with.\" Dennis Hopper—said to be Lynch's third choice—accepted the role, reportedly having exclaimed, \"I've got to play Frank! I am Frank!\" as Hopper confirmed in the \"Blue Velvet\" \"making-of\" documentary \"The Mysteries of Love\", produced for the 2002 special edition. For the role of Dorothy Vallens, Lynch met Isabella Rossellini at a restaurant, and she accepted the role. Laura Dern, then just nineteen years old, was cast after various successful actresses at the time turned it down, including Molly Ringwald.\n\nThe scene in which Dorothy appears naked outside was inspired by a real-life experience Lynch had during childhood when he and his brother saw a naked woman walking down a neighborhood street at night. The experience was so traumatic to the young Lynch that it made him cry, and he had never forgotten it. Principal photography of \"Blue Velvet\" began in February 1986 and completed in April. The film was shot at EUE/Screen Gems studio in Wilmington, North Carolina, which also provided the exterior scenes of Lumberton. The scene with a raped and battered Dorothy proved to be particularly challenging. Several townspeople arrived to watch the filming with picnic baskets and rugs, against the wishes of Rossellini and Lynch. However, they continued filming as normal, and when Lynch yelled cut, the townspeople had left. As a result, police told Lynch they were no longer permitted to shoot in any public areas of Wilmington.\n\nLynch's original rough cut ran for approximately four hours. He was contractually obligated to deliver a two-hour movie by De Laurentiis and cut many small subplots and character scenes. He also made cuts at the request of the MPAA. For example, when Frank slaps Dorothy after the first rape scene, the audience was supposed to see Frank actually hitting her. Instead, the film cuts away to Jeffrey in the closet, wincing at what he has just seen. This cut was made to satisfy the MPAA's concerns about violence. Lynch thought that the change only made the scene more disturbing. Lynch announced in 2011, that footage from the deleted scenes, long thought lost, had been discovered. It later appeared on the 2011 special edition Blu-ray disc release of the film. The final cut produced by Lynch runs for just under two hours.\n\nDespite \"Blue Velvet\"s initial appearance as a mystery, the film operates on a number of thematic levels. The film owes a large debt to 1950s film noir, containing and exploring such conventions as the femme fatale (Dorothy Vallens), a seemingly unstoppable villain (Frank Booth), and the questionable moral outlook of the hero (Jeffrey Beaumont), as well as its unusual use of shadowy, sometimes dark cinematography. \"Blue Velvet\" represents and establishes Lynch's famous \"askew vision\", and introduces several common elements of Lynch's work, some of which would later become his trademarks, including distorted characters, a polarized world, and debilitating damage to the skull or brain. Perhaps the most significant Lynchian trademark in the film is the depiction of unearthing a dark underbelly in a seemingly idealized small town; Jeffrey even proclaims in the film that he is \"seeing something that was always hidden\", alluding to the plot's central idea. Lynch's characterization of films, symbols, and motifs have become well-known, and his particular style, characterised largely in \"Blue Velvet\" for the first time, has been written about extensively using descriptions like \"dreamlike\", \"ultraweird\", \"dark\", and \"oddball\". Red curtains also show up in key scenes, specifically in Dorothy's apartment, which have since become a Lynch trademark. The film has been compared to Alfred Hitchcock's \"Psycho\" (1960) because of its stark treatment of psychotic evil. The premise of both films is curiosity, leading to an investigation that draws the lead characters into a hidden, voyeuristic underworld of crime.\n\nThe film's thematic framework hearkens back to Poe, James, and early gothic fiction, as well as films such as \"Shadow of a Doubt\" (1943) and \"The Night of the Hunter\" (1955) and the entire notion of film noir. Lynch has called it a \"film about things that are hidden—within a small city and within people\". Like many other Lynch films, \"Blue Velvet\" is immersed in pop culture imagery, both from the 1950s and the 1960s, as well as the 1980s.\n\nFeminist psychoanalytic film theorist Laura Mulvey argues that \"Blue Velvet\" establishes a metaphorical Oedipal family—\"the child\", Jeffrey Beaumont, and his \"parents\", Frank Booth and Dorothy Vallens—through deliberate references to film noir and its underlying Oedipal theme. The resulting violence, she claims, can be read as symbolic of domestic violence within real families. For instance, Frank's violent acts can be seen to reflect the different types of abuse within families, and the control he has over Dorothy might represent the hold an abusive husband has over his wife. Michael Atkinson reads Jeffrey as an innocent youth who is both horrified by the violence inflicted by Frank, but also tempted by it as the means of possessing Dorothy for himself. Atkinson takes a Freudian approach to the film; considering it to be an expression of the traumatised innocence which characterises Lynch's work. He states, \"Dorothy represents the sexual force of the mother [figure] because she is forbidden and because she becomes the object of the unhealthy, infantile impulses at work in Jeffrey's subconscious\".\n\nSymbolism is used heavily in \"Blue Velvet\". The most consistent symbolism in the film is an insect motif introduced at the end of the first scene, when the camera zooms in on a well-kept suburban lawn until it unearths a swarming underground nest of disgusting bugs. This is generally recognized as a metaphor for the seedy underworld that Jeffrey will soon discover under the surface of his own suburban, Reaganesque paradise. The severed ear he finds is being overrun by black ants. The bug motif is recurrent throughout the film, most notably in the bug-like gas mask that Frank wears, but also the excuse that Jeffrey uses to gain access to Dorothy's apartment: he claims to be an insect exterminator. One of Frank's sinister accomplices is also consistently identified through the yellow jacket he wears, possibly reminiscent of the name of a type of wasp. Finally, a robin eating a bug on a fence becomes a topic of discussion in the last scene of the film. The robin, mentioned earlier by Sandy when she recounted her dream, represents love conquering evil.\n\nThe severed ear that Jeffrey discovers is also a key symbolic element, leading Jeffrey into danger. Indeed, just as Jeffrey's troubles begin, the audience is treated to a nightmarish sequence in which the camera zooms into the canal of the severed, decomposing ear. Notably, the camera does not reemerge from the ear canal until the end of the film. When Jeffrey finally comes through his hellish ordeal unscathed, the ear canal shot is replayed, only in reverse, zooming out through Jeffrey's own ear as he relaxes in his yard on a summer day.\n\nThe \"Blue Velvet\" soundtrack was supervised by Angelo Badalamenti (who makes a brief cameo appearance as the pianist at the Slow Club where Dorothy performs). The soundtrack makes heavy usage of vintage pop songs, such as Bobby Vinton's \"Blue Velvet\" and Roy Orbison's \"In Dreams\", juxtaposed with an orchestral score inspired by Shostakovich. During filming, Lynch placed speakers on set and in streets and played Shostakovich to set the mood he wanted to convey. The score makes direct quotations from Shostakovich's 15th Symphony, which Lynch had been listening to regularly while writing the screenplay. Lynch had originally opted to use \"Song To The Siren\" by This Mortal Coil during the scene in which Sandy and Jeffrey share a dance, however he could not obtain the rights for the song at the time. He would go onto use this song in \"Lost Highway\", eleven years later.\n\n\"Entertainment Weekly\" ranked \"Blue Velvet's\" soundtrack on its list of the \"100 Greatest Film Soundtracks\", at the 100th position. Critic John Alexander wrote, \"the haunting soundtrack accompanies the title credits, then weaves through the narrative, accentuating the noir mood of the film\". Lynch worked with music composer Angelo Badalamenti for the first time in this film and asked him to write a score that had to be \"like Shostakovich, be very Russian, but make it the most beautiful thing but make it dark and a little bit scary\". Badalamenti's success with \"Blue Velvet\" would lead him to contribute to all of Lynch's future full-length films until \"Inland Empire\". Also included in the sound team was long time Lynch collaborator Alan Splet, a sound editor and designer who had won an Academy Award for his work on \"The Black Stallion\" (1979), and been nominated for \"Never Cry Wolf\" (1983).\n\n\"Blue Velvet\" premiered in competition at the Montréal World Film Festival in August 1986, and at the Toronto Festival of Festivals on September 12, 1986, and a few days later in the United States. It debuted commercially in both countries on September 19, 1986, in 98 theatres across the United States. In its opening weekend, the film grossed a total of $789,409. It eventually expanded to another fifteen theatres, and domestically grossed a total of $8,551,228. \"Blue Velvet\" was met with uproar during its audience reception, with lines formed around city blocks in New York City and Los Angeles. There were reports of mass walkouts and refund demands during its opening week. At a Chicago screening, a man fainted and had to have his pacemaker changed. Upon completion, he returned to the cinema to see the ending. At a Los Angeles cinema, two strangers became engaged in a heated disagreement, but decided to resolve the disagreement in order to return to the theatre.\n\nIt was also released internationally, in Australia, most of West Germany, China, Canada, Hong Kong, and Japan, followed by subsequent video releases. The film performed well overseas, grossing $900,000 in Australia, and $450,139 in Hong Kong.\n\n\"Blue Velvet\" was released to a polarized reception in the United States. The critics who did praise the film were often vociferous. \"The New York Times\" critic Janet Maslin directed much praise toward the performances of Hopper and Rossellini: \"Mr. Hopper and Miss Rossellini are so far outside the bounds of ordinary acting here that their performances are best understood in terms of sheer lack of inhibition; both give themselves entirely over to the material, which seems to be exactly what's called for.\" She called it \"an instant cult classic\". Maslin concluded by saying that \"Blue Velvet\" \"is as fascinating as it is freakish. It confirms Mr. Lynch's stature as an innovator, a superb technician, and someone best not encountered in a dark alley.\"\n\nSheila Benson of the \"Los Angeles Times\" called the film \"the most brilliantly disturbing film ever to have its roots in small-town American life\", describing it as \"shocking, visionary, rapturously controlled\". Film critic Gene Siskel included \"Blue Velvet\" on his list of the best films of 1986, at the fifth spot. Peter Travers, film critic for \"Rolling Stone\", named it the best film of the 1980s and referred to it as an \"American masterpiece\".\n\nHowever, the film was not without its detractors. Paul Attanasio of \"The Washington Post\" said \"the film showcases a visual stylist utterly in command of his talents\" and that Angelo Badalamenti \"contributes an extraordinary score, slipping seamlessly from slinky jazz to violin figures to the romantic sweep of a classic Hollywood score\", but stated that Lynch \"isn't interested in communicating, he's interested in parading his personality. The movie doesn't progress or deepen, it just gets weirder, and to no good end.\" A general criticism from U.S. critics was \"Blue Velvet\"s often vulgar approach to sexuality and violence. They asserted that this detracted from the film's seriousness as a work of art, and some condemned the film as pornographic. One of its detractors, Roger Ebert, praised Isabella Rossellini's performance as \"convincing and courageous\" but criticized how she was depicted in the film, even accusing David Lynch of misogyny: \"degraded, slapped around, humiliated and undressed in front of the camera. And when you ask an actress to endure those experiences, you should keep your side of the bargain by putting her in an important film\". While Ebert in later years came to consider Lynch a great filmmaker, his negative view of \"Blue Velvet\" remained unchanged after he revisited it in the 21st century.\n\nThe film is now considered a masterpiece and has a \"Certified Fresh\" score of 94% on Rotten Tomatoes based on 62 reviews with an average rating of 8.7 out of 10. The critical consensus states: \"If audiences walk away from this subversive, surreal shocker not fully understanding the story, they might also walk away with a deeper perception of the potential of film storytelling.\" The film also has a score of 75 out of 100 on Metacritic based on 14 critics indicating \"Generally favorable reviews\". Looking back in his \"Guardian/Observer\" review, critic Philip French wrote, \"The film is wearing well and has attained a classic status without becoming respectable or losing its sense of danger.\"\n\nLynch was nominated for a Best Director Oscar for the film. Isabella Rossellini won an Independent Spirit Award for the Best Female Lead in 1987. David Lynch and Dennis Hopper won a Los Angeles Film Critics Association award in 1987 for \"Blue Velvet\" in categories Best Director (Lynch) and Best Supporting Actor (Hopper). In 1987, National Society of Film Critics awarded Best Film, Best Director (David Lynch), Best Cinematography (Frederick Elmes), and Best Supporting Actor (Dennis Hopper) awards.\n\nAlthough it initially gained a relatively small theatrical audience in North America and was met with controversy over its artistic merit, \"Blue Velvet\" soon became the center of a \"national firestorm\" in 1986, and over time achieved status as an American classic. In the late 1980s, and early 1990s, after its release on videotape, the film became a widely known cult film, well known for its dark depiction of a suburban America. Followed by myriad VHS, Laserdisc and DVD releases, the film became increasingly well-known among American audiences. It marked the entrance of David Lynch into the Hollywood mainstream and the comeback of Dennis Hopper after a significant hiatus from work. Hopper's performance and the character of Frank Booth itself has left an imprint on popular culture, with countless tributes, cultural references and parodies. The success of the film alone has helped propel Hollywood mainstream toward more graphic displays of previously censored themes, a similar case to \"Psycho\" (1960), to which \"Blue Velvet\" has been frequently compared. It has become one of the most significant, well-recognized films of its era, spawning countless imitations and parodies in media. The film's dark, stylish and erotic production design has served as a benchmark for a number of films, parodies and even Lynch's own later work, notably \"Twin Peaks\" (1990–91), and \"Mulholland Drive\" (2001). Peter Travers of \"Rolling Stone\" magazine cited it as one of the most \"influential American films\", as did Michael Atkinson, who dedicated a book to the film's themes and motifs.\n\n\"Blue Velvet\" now frequently appears in various critical assessments of all-time great films, also ranked as one of the greatest films of the 1980s, one of the best examples of American surrealism and one of the finest examples of David Lynch's work. In a poll of two American critics ranking the \"most outstanding films of the decade\", \"Blue Velvet\" was placed third and fourth, behind \"Raging Bull\" (1980), \"E.T. The Extra Terrestrial\" (1982) and the German film \"Wings of Desire\" (1987). An \"Entertainment Weekly\" book special released in 1999 ranked \"Blue Velvet\" at thirty-seventh greatest films of all time. The film was ranked by \"The Guardian\" in its list of the 100 Greatest Films. \"Film Four's\" ranked it on their list of 100 Greatest Films. In a 2007 poll of the online film community held by \"Variety\", \"Blue Velvet\" came in at the ninety-fifth greatest film of all time. \"Total Film\" ranked \"Blue Velvet\" as one of the all-time best films in both a critics list and a public poll, in 2006 and 2007, respectively. In December 2002, a UK film critics poll in \"Sight & Sound\" ranked the film fifth on their list of the 10 Best Films of the Last 25 Years. In a special \"Entertainment Weekly\" issue, 100 new film classics were chosen from 1983 to 2008: \"Blue Velvet\" was ranked at fourth.\n\nIn addition to \"Blue Velvet's\" various \"all-time greatest films\" rankings, the American Film Institute has awarded the film three honors in its lists: ninety-sixth on \"100 Years... 100 Thrills\" in 2001, selecting cinema's most thrilling moments and ranked Frank Booth thirty-sixth of the 50 greatest villains in \"100 Years...100 Heroes and Villains\" in 2003. In June 2008, the AFI revealed its \"Ten top Ten\"—the best ten films in ten \"classic\" American film genres—after polling over 1,500 people from the creative community. \"Blue Velvet\" was acknowledged as the eighth best film in the mystery genre. \"Premiere\" magazine listed Frank Booth, played by Dennis Hopper, as the fifty-fourth on its list of The 100 Greatest Movie Characters of All Time, calling him one of \"the most monstrously funny creations in cinema history\". The film was ranked eighty-fourth on Bravo Television's four-hour program \"100 Scariest Movie Moments\" (2004). It is frequently sampled musically and an array of bands and solo artists have taken their names and inspiration from the film. In August 2012, \"Sight & Sound\" unveiled their latest list of the 250 greatest films of all time, with \"Blue Velvet\" ranking at sixty-ninth.\n\n\"Blue Velvet\" was also nominated for the following AFI lists:\n\n\"Blue Velvet\" was released on Blu-ray on November 8, 2011, in a special 25th anniversary edition featuring never-before-seen deleted scenes. The film had previously been released on DVD in 1999 and 2002 by MGM Home Entertainment. In early 2015, it was announced that a feature-length documentary film entitled \"Blue Velvet Revisited\", made up exclusively of behind the scenes footage, was in production for Autumn 2015 release. The film is based on extensive unreleased footage shot on set in 1985 by German filmmaker Peter Braatz, at David Lynch's invitation. Cult With No Name, Tuxedomoon and John Foxx were commissioned to provide the soundtrack for the film (which was released in October 2015), and a collection of previously unreleased photos is also to be published. The film was premiered at the London Film Festival in October 2016.\n\nInspired by the film, baroque pop singer Lana Del Rey recorded a cover version of Bobby Vinton's classic rendition of the song \"Blue Velvet\" in 2012. Used to endorse clothing line H&M, a music video accompanied the track and aired as a television commercial. Filmed in Post-war Americana, the video drew influence from Lynch and \"Blue Velvet\". In the video, Del Rey plays the role of Dorothy Vallens, performing a private concert similar to the scene where Ben (Dean Stockwell) pantomimes \"In Dreams\" for Frank Booth. Del Rey's version, however, has her lip-synching \"Blue Velvet\" when a little person dressed as Frank Sinatra approaches and unplugs a hidden victrola, revealing Del Rey as a fraud. When Lynch heard of the music video, he praised it, telling \"Artinfo\": \"Lana Del Rey, she's got some fantastic charisma and—this is a very interesting thing—it's like she's born out of another time. She's got something that's very appealing to people. And I didn't know she was influenced by me!\"\n\n\"Now It's Dark\", a song by American heavy metal band Anthrax on their 1988 album \"State of Euphoria\", was directly inspired by the film, and specifically the character of Frank Booth. The same phrase appeared in the liner notes of Rush's album \"Roll the Bones\", and drummer Neil Peart later explained: \"The phrase occurs in David Lynch's comedy classic \"Blue Velvet\".\"\n\nNotes\n\n", "id": "3947", "title": "Blue Velvet (film)"}
{"url": "https://en.wikipedia.org/wiki?curid=3948", "text": "Binary operation\n\nIn mathematics, a binary operation on a set is a calculation that combines two elements of the set (called operands) to produce another element of the set. More formally, a binary operation is an operation of arity two whose two domains and one codomain are the same set. Examples include the familiar elementary arithmetic operations of addition, subtraction, multiplication and division. Other examples are readily found in different areas of mathematics, such as vector addition, matrix multiplication and conjugation in groups.\n\nMore precisely, a binary operation on a set \"S\" is a map which sends elements of the Cartesian product to \"S\":\nBecause the result of performing the operation on a pair of elements of \"S\" is again an element of \"S\", the operation is called a closed binary operation on \"S\" (or sometimes expressed as having the property of closure). If \"f\" is not a function, but is instead a partial function, it is called a partial binary operation. For instance, division of real numbers is a partial binary operation, because one can't divide by zero: \"a\"/0 is not defined for any real \"a\". Note however that both in algebra and model theory the binary operations considered are defined on all of .\n\nSometimes, especially in computer science, the term is used for any binary function.\n\nBinary operations are the keystone of algebraic structures studied in abstract algebra: they are essential in the definitions of groups, monoids, semigroups, rings, and more. Most generally, a \"magma\" is a set together with some binary operation defined on it.\n\nTypical examples of binary operations are the addition (+) and multiplication (×) of numbers and matrices as well as composition of functions on a single set.\nFor instance,\n\nMany binary operations of interest in both algebra and formal logic are commutative, satisfying for all elements \"a\" and \"b\" in \"S\", or associative, satisfying for all \"a\", \"b\" and \"c\" in \"S\". Many also have identity elements and inverse elements.\n\nThe first three examples above are commutative and all of the above examples are associative.\n\nOn the set of real numbers R, subtraction, that is, , is a binary operation which is not commutative since, in general, . It is also not associative, since, in general, ; for instance, but .\n\nOn the set of natural numbers N, the binary operation exponentiation, , is not commutative since, in general, and is also not associative since . For instance, with , and , , but . By changing the set N to the set of integers Z, this binary operation becomes a partial binary operation since it is now undefined when and \"b\" is any negative integer. For either set, this operation has a \"right identity\" (which is 1) since for all \"a\" in the set, which is not an \"identity\" (two sided identity) since in general.\n\nDivision (/), a partial binary operation on the set of real or rational numbers, is not commutative or associative as well. Tetration (↑↑), as a binary operation on the natural numbers, is not commutative nor associative and has no identity element.\n\nBinary operations are often written using infix notation such as , , or (by juxtaposition with no symbol) \"ab\" rather than by functional notation of the form . Powers are usually also written without operator, but with the second argument as superscript.\n\nBinary operations sometimes use prefix or (probably more often) postfix notation, both of which dispense with parentheses. They are also called, respectively, Polish notation and reverse Polish notation.\n\nA binary operation, \"ab\", depends on the ordered pair (\"a, b\") and so (\"ab\")\"c\" (where the parentheses here mean first operate on the ordered pair (\"a\", \"b\") and then operate on the result of that using the ordered pair ((\"ab\"), \"c\")) depends in general on the ordered pair ((\"a\", \"b\"), \"c\"). Thus, for the general, non-associative case, binary operations can be represented with binary trees.\n\nHowever:\n\nA binary operation \"f\" on a set \"S\" may be viewed as a \"ternary\" relation on \"S\", that is, the set of triples (\"a\", \"b\", \"f(a,b)\") in \"S\" × \"S\" × \"S\" for all \"a\" and \"b\" in \"S\".\n\nAn external binary operation is a binary function from \"K\" × \"S\" to \"S\". This differs from a binary operation in the strict sense in that \"K\" need not be \"S\"; its elements come from \"outside\".\n\nAn example of an external binary operation is scalar multiplication in linear algebra. Here \"K\" is a field and \"S\" is a vector space over that field.\n\nAn external binary operation may alternatively be viewed as an action; \"K\" is acting on \"S\".\n\nNote that the dot product of two vectors is not a binary operation, external or otherwise, as it maps from \"S\"× \"S\" to \"K\", where \"K\" is a field and \"S\" is a vector space over \"K\".\n\n\n", "id": "3948", "title": "Binary operation"}
{"url": "https://en.wikipedia.org/wiki?curid=3950", "text": "Bagpipes\n\nBagpipes are a wind instrument using enclosed reeds fed from a constant reservoir of air in the form of a bag. Though the Scottish Great Highland bagpipes are the best known in the Anglophone world, bagpipes have been played for a millennium or more throughout large parts of Europe, northern Africa, and western Asia, including Turkey, the Caucasus, and around the Persian Gulf. The term \"bagpipe\" is equally correct in the singular or plural, though pipers usually refer to the bagpipes as \"the pipes\", \"a set of pipes\" or \"a stand of pipes\".\n\nA set of bagpipes minimally consists of an air supply, a bag, a chanter, and usually at least one drone. Many bagpipes have more than one drone (and, sometimes, more than one chanter) in various combinations, held in place in stocks—sockets that fasten the various pipes to the bag.\n\nThe most common method of supplying air to the bag is through blowing into a blowpipe, or blowstick. In some pipes the player must cover the tip of the blowpipe with their tongue while inhaling, but most blowpipes have a non-return valve that eliminates this need.\n\nAn innovation, dating from the 16th or 17th century, is the use of a bellows to supply air. In these pipes, sometimes called \"cauld wind pipes\", air is not heated or moistened by the player's breathing, so bellows-driven bagpipes can use more refined or delicate reeds. Such pipes include the Irish uilleann pipes, the Scottish border pipes and Lowland pipes; Northumbrian smallpipes, pastoral pipes and English Border pipes in Britain, and the musette de cour in France.\n\nThe bag is an airtight reservoir that holds air and regulates its flow via arm pressure, allowing the player to maintain continuous even sound. The player keeps the bag inflated by blowing air into it through a blowpipe or pumping air into it with a bellows. Materials used for bags vary widely, but the most common are the skins of local animals such as goats, dogs, sheep, and cows. More recently, bags made of synthetic materials including Gore-Tex have become much more common. A drawback of the synthetic bag is the potential for fungal spores to colonise the bag because of a reduction in necessary cleaning, with the associated danger of lung infection. An advantage of a synthetic bag is that they have a zip which allows the user to fit a more effective moisture trap to the inside of the bag.\n\nBags cut from larger materials are usually saddle-stitched with an extra strip folded over the seam and stitched (for skin bags) or glued (for synthetic bags) to reduce leaks. Holes are then cut to accommodate the stocks. In the case of bags made from largely intact animal skins, the stocks are typically tied into the points where limbs and the head joined the body of the whole animal, a construction technique common in Central Europe.\n\nThe chanter is the melody pipe, played with two hands. Almost all bagpipes have at least one chanter; some pipes have two chanters, particularly those in North Africa, the Balkans in Southern Europe, and Southwest Asia. A chanter can be bored internally so that the inside walls are parallel (or \"cylindrical\") for its full length, or it can be bored in a conical shape. \n\nThe chanter is usually open-ended, so there is no easy way for the player to stop the pipe from sounding. Thus most bagpipes share a constant, legato sound where there are no rests in the music. Primarily because of this inability to stop playing, technical movements are used to break up notes and to create the illusion of articulation and accents. Because of their importance, these embellishments (or \"ornaments\") are often highly technical systems specific to each bagpipe, and take many years of study to master. A few bagpipes (such as the musette de cour, the uilleann pipes, the Northumbrian smallpipe, the piva and the left chanter of the \"surdulina\") have closed ends or stop the end on the player's leg, so that when the player \"closes\" (covers all the holes) the chanter becomes silent.\n\nA practice chanter is a chanter without bag or drones, allowing a player to practice the instrument quietly and with no variables other than playing the chanter.\n\nThe term \"chanter\" is derived from the Latin \"cantare\", or \"to sing\", much like the modern French word \"chanteur\".\n\nThe note from the chanter is produced by a reed installed at its top. The reed may be a single (a reed with one vibrating tongue) or double reed (of two pieces that vibrate against each other). Double reeds are used with both conical- and parallel-bored chanters while single reeds are generally (although not exclusively) limited to parallel-bored chanters. In general, double-reed chanters are found in pipes of Western Europe while single-reed chanters appear in most other regions.\n\nMost bagpipes have at least one drone: a pipe which is generally not fingered but rather produces a constant harmonizing note throughout play (usually the tonic note of the chanter). Exceptions are generally those pipes which have a double-chanter instead. A drone is most commonly a cylindrically-bored tube with a single reed, although drones with double reeds exist. The drone is generally designed in two or more parts with a sliding joint so that the pitch of the drone can be adjusted.\n\nDepending on the type of pipes, the drones may lie over the shoulder, across the arm opposite the bag, or may run parallel to the chanter. Some drones have a tuning screw, which effectively alters the length of the drone by opening a hole, allowing the drone to be tuned to two or more distinct pitches. The tuning screw may also shut off the drone altogether. In most types of pipes, where there is one drone it is pitched two octaves below the tonic of the chanter. Additional drones often add the octave below and then a drone consonant with the fifth of the chanter.\n\nThe evidence for pre-Roman era bagpipes is still uncertain but several textual and visual clues have been suggested. The \"Oxford History of Music\" says that a sculpture of bagpipes has been found on a Hittite slab at Euyuk in the Middle East, dated to 1000 BC. Several authors identify the ancient Greek \"askaulos\" (ἀσκός \"askos\" – wine-skin, αὐλός \"aulos\" – reed pipe) with the bagpipe. In the 2nd century AD, Suetonius described the Roman emperor Nero as a player of the \"tibia utricularis\". Dio Chrysostom wrote in the 1st century of a contemporary sovereign (possibly Nero) who could play a pipe (tibia, Roman reedpipes similar to Greek and Etruscan instruments) with his mouth as well as by tucking a bladder beneath his armpit.\n\nIn the early part of the second millennium, definite clear attestations of bagpipes began to appear with frequency in Western European art and iconography. The Cantigas de Santa Maria, written in Galician-Portuguese and compiled in Castile in the mid-13th century, depicts several types of bagpipes. Several illustrations of bagpipes also appear in the \"Chronique dite de Baudoin d’Avesnes\", a 13th-century manuscript of northern French origin. Though evidence of bagpipes in the British Isles prior to the 14th century is contested, bagpipes are explicitly mentioned in \"The Canterbury Tales\" (written around 1380):\nBagpipes were also frequent subjects for carvers of wooden choir stalls in the late 15th and early 16th century throughout Europe, sometimes with animal musicians.\n\nActual examples of bagpipes from before the 18th century are extremely rare; however, a substantial number of paintings, carvings, engravings, manuscript illuminations, and so on survive. They make it clear that bagpipes varied hugely throughout Europe, and even within individual regions. Many examples of early folk bagpipes in continental Europe can be found in the paintings of Brueghel, Teniers, Jordaens, and Durer.\nThe first clear reference to the use of the Scottish Highland bagpipes is from a French history, which mentions their use at the Battle of Pinkie Cleugh in 1547. George Buchanan (1506–82) claimed that they had replaced the trumpet on the battlefield. This period saw the creation of the \"ceòl mór\" (great music) of the bagpipe, which reflected its martial origins, with battle-tunes, marches, gatherings, salutes and laments. The Highlands of the early seventeenth century saw the development of piping families including the MacCrimmonds, MacArthurs, MacGregors and the Mackays of Gairloch.\n\nEvidence of the bagpipe in Ireland occurs in 1581, when John Derrick's \"The Image of Irelande\" clearly depicts a bagpiper. Derrick's illustrations are considered to be reasonably faithful depictions of the attire and equipment of the English and Irish population of the 16th century. The \"Battell\" sequence from \"My Ladye Nevells Booke\" (1591) by William Byrd, which probably alludes to the Irish wars of 1578, contains a piece entitled \"The bagpipe: & the drone\". In 1760, the first serious study of the Scottish Highland bagpipe and its music was attempted, in Joseph MacDonald's \"Compleat Theory\". Further south, a manuscript from the 1730s by a William Dixon from Northumberland contains music that fits the border pipes, a nine-note bellows-blown bagpipe whose chanter is similar to that of the modern Great Highland bagpipe. However the music in Dixon's manuscript varied greatly from modern Highland bagpipe tunes, consisting mostly of extended variation sets of common dance tunes. Some of the tunes in the Dixon manuscript correspond to tunes found in early 19th century published and manuscript sources of Northumbrian smallpipe tunes, notably the rare book of 50 tunes, many with variations, by John Peacock.\n\nAs Western classical music developed, both in terms of musical sophistication and instrumental technology, bagpipes in many regions fell out of favour due to their limited range and function. This triggered a long, slow decline that continued, in most cases, into the 20th century.\n\nExtensive and documented collections of traditional bagpipes can be found in the Metropolitan Museum of Art in New York City, the International Bagpipe Museum in Gijón, Spain, the Pitt Rivers Museum in Oxford, England and the Morpeth Chantry Bagpipe Museum in Northumberland, and the Musical Instrument Museum in Phoenix, Arizona.\n\nDuring the expansion of the British Empire, spearheaded by British military forces that included Highland regiments, the Scottish Great Highland bagpipe became well-known worldwide. This surge in popularity was boosted by large numbers of pipers trained for military service in World War I and World War II. The surge coincided with a decline in the popularity of many traditional forms of bagpipe throughout Europe, which began to be displaced by instruments from the classical tradition and later by gramophone and radio.\n\nIn the United Kingdom and Commonwealth Nations such as Canada, New Zealand and Australia the Great Highland bagpipe is commonly used in the military and is often played in formal ceremonies. Foreign militaries patterned after the British Army have also taken the Highland bagpipe into use including Uganda, Sudan, India, Pakistan, Sri Lanka, Jordan, and Oman. Many police and fire services in Scotland, Canada, Australia, New Zealand, Hong Kong, and the United States have also adopted the tradition of fielding pipe bands.\nIn recent years, often driven by revivals of native folk music and dance, many types of bagpipes have enjoyed a resurgence in popularity and, in many cases, instruments that were on the brink of obscurity have become extremely popular. In Brittany, the Great Highland bagpipe and concept of the pipe band were appropriated to create a Breton interpretation, the bagad. The pipe band idiom has also been adopted and applied to the Galician gaita as well. Additionally, bagpipes have often been used in various films depicting moments from Scottish and Irish history; the film \"Braveheart\" and the theatrical show \"Riverdance\" have served to make the uilleann pipes more commonly known.\n\nBagpipes are sometimes played at formal events in Commonwealth universities, particularly in Canada. Because of the Scottish influences on the sport of curling, bagpipes are also the official instrument of the World Curling Federation and are commonly played during a ceremonial procession of teams before major curling championships.\n\nBagpipe making was once a craft that produced instruments in many distinctive local traditional styles. Today, the world's biggest producer of the instrument is Pakistan, where the industry was worth $6.8 million in 2010. In the late 20th century, various models of electronic bagpipes were invented. The first custom-built MIDI bagpipes were developed by the Asturian piper known as Hevia (José Ángel Hevia Velasco).\n\nDozens of types of bagpipes today are widely spread across Europe and the Middle East, as well as through much of the former British Empire. The name bagpipe has almost become synonymous with its best-known form, the Great Highland bagpipe, overshadowing the great number and variety of traditional forms of bagpipe. Despite the decline of these other types of pipes over the last few centuries, in recent years many of these pipes have seen a resurgence or revival as musicians have sought them out; for example, the Irish piping tradition, which by the mid 20th century had declined to a handful of master players is today alive, well, and flourishing a situation similar to that of the Asturian gaita, the Galician gaita, the Portuguese gaita transmontana, the Aragonese gaita de boto, Northumbrian smallpipes, the Breton biniou, the Balkan gaida, the Romanian cimpoi, the Black Sea tulum, the Scottish smallpipes and pastoral pipes, as well as other varieties.\n\nTraditionally, one of the purposes of the bagpipe was to provide music for dancing. This has declined with the growth of dance bands, recordings, and the decline of traditional dance. In turn, this has led to many types of pipes developing a performance-led tradition, and indeed much modern music based on the dance music tradition played on bagpipes is no longer suitable for use as dance music.\n\nSince the 1960s, bagpipes have also made appearances in other forms of music, including rock, metal, jazz, hip-hop, punk, and classical music, for example with Paul McCartney's \"Mull of Kintyre\", AC/DC's \"It's a Long Way to the Top (If You Wanna Rock 'n' Roll)\", and Peter Maxwell Davies's composition \"An Orkney Wedding, with Sunrise\". \n\n\"Periodicals covering specific types of bagpipes are addressed in the article for that bagpipe\"\n\n\n\n", "id": "3950", "title": "Bagpipes"}
{"url": "https://en.wikipedia.org/wiki?curid=3952", "text": "Bedrock Records\n\nBedrock Records is an English record label for trance, house and techno started by Nick Muir and John Digweed.\nIts name comes from a nightclub in London that is also called Bedrock. Bedrock Records has released many singles from artists such as Astro & Glyde, Brancaccio & Aisher, Steve Lawler, Shmuel Flash, Steve Porter, Guy J, Henry Saiz, Stelios Vassiloudis, Electric Rescue, The Japanese Popstars and Jerry Bonham. Bedrock is also the name that Digweed and Muir use as their production moniker.\n\nBedrock in the past had different Sub-labels: Bedrock Breaks, B_Rock and Black (Bedrock). Currently it has Bedrock Digital and one called Lost & Found belonging to Guy J\n\n\n", "id": "3952", "title": "Bedrock Records"}
{"url": "https://en.wikipedia.org/wiki?curid=3954", "text": "Biochemistry\n\nBiochemistry, sometimes called biological chemistry, is the study of chemical processes within and relating to living organisms. By controlling information flow through biochemical signaling and the flow of chemical energy through metabolism, biochemical processes give rise to the complexity of life. Over the last decades of the 20th century, biochemistry has become so successful at explaining living processes that now almost all areas of the life sciences from botany to medicine to genetics are engaged in biochemical research. Today, the main focus of pure biochemistry is on understanding how biological molecules give rise to the processes that occur within living cells, which in turn relates greatly to the study and understanding of tissues, organs, and whole organisms—that is, all of biology.\n\nBiochemistry is closely related to molecular biology, the study of the molecular mechanisms by which genetic information encoded in DNA is able to result in the processes of life. Depending on the exact definition of the terms used, molecular biology can be thought of as a branch of biochemistry, or biochemistry as a tool with which to investigate and study molecular biology.\n\nMuch of biochemistry deals with the structures, functions and interactions of biological macromolecules, such as proteins, nucleic acids, carbohydrates and lipids, which provide the structure of cells and perform many of the functions associated with life. The chemistry of the cell also depends on the reactions of smaller molecules and ions. These can be inorganic, for example water and metal ions, or organic, for example the amino acids, which are used to synthesize proteins. The mechanisms by which cells harness energy from their environment via chemical reactions are known as metabolism. The findings of biochemistry are applied primarily in medicine, nutrition, and agriculture. In medicine, biochemists investigate the causes and cures of diseases. In nutrition, they study how to maintain health and study the effects of nutritional deficiencies. In agriculture, biochemists investigate soil and fertilizers, and try to discover ways to improve crop cultivation, crop storage and pest control.\n\nAt its broadest definition, biochemistry can be seen as a study of the components and composition of living things and how they come together to become life, and the history of biochemistry may therefore go back as far as the ancient Greeks. However, biochemistry as a specific scientific discipline has its beginning sometime in the 19th century, or a little earlier, depending on which aspect of biochemistry is being focused on. Some argued that the beginning of biochemistry may have been the discovery of the first enzyme, diastase (today called amylase), in 1833 by Anselme Payen, while others considered Eduard Buchner's first demonstration of a complex biochemical process alcoholic fermentation in cell-free extracts in 1897 to be the birth of biochemistry. Some might also point as its beginning to the influential 1842 work by Justus von Liebig, \"Animal chemistry, or, Organic chemistry in its applications to physiology and pathology\", which presented a chemical theory of metabolism, or even earlier to the 18th century studies on fermentation and respiration by Antoine Lavoisier. Many other pioneers in the field who helped to uncover the layers of complexity of biochemistry have been proclaimed founders of modern biochemistry, for example Emil Fischer for his work on the chemistry of proteins, and F. Gowland Hopkins on enzymes and the dynamic nature of biochemistry.\n\nThe term \"biochemistry\" itself is derived from a combination of biology and chemistry. In 1877, Felix Hoppe-Seyler used the term (\"biochemie\" in German) as a synonym for physiological chemistry in the foreword to the first issue of \"Zeitschrift für Physiologische Chemie\" (Journal of Physiological Chemistry) where he argued for the setting up of institutes dedicated to this field of study. The German chemist Carl Neuberg however is often cited to have coined the word in 1903, while some credited it to Franz Hofmeister.\nIt was once generally believed that life and its materials had some essential property or substance (often referred to as the \"vital principle\") distinct from any found in non-living matter, and it was thought that only living beings could produce the molecules of life. Then, in 1828, Friedrich Wöhler published a paper on the synthesis of urea, proving that organic compounds can be created artificially. Since then, biochemistry has advanced, especially since the mid-20th century, with the development of new techniques such as chromatography, X-ray diffraction, dual polarisation interferometry, NMR spectroscopy, radioisotopic labeling, electron microscopy, and molecular dynamics simulations. These techniques allowed for the discovery and detailed analysis of many molecules and metabolic pathways of the cell, such as glycolysis and the Krebs cycle (citric acid cycle).\n\nAnother significant historic event in biochemistry is the discovery of the gene and its role in the transfer of information in the cell. This part of biochemistry is often called molecular biology. In the 1950s, James D. Watson, Francis Crick, Rosalind Franklin, and Maurice Wilkins were instrumental in solving DNA structure and suggesting its relationship with genetic transfer of information. In 1958, George Beadle and Edward Tatum received the Nobel Prize for work in fungi showing that one gene produces one enzyme. In 1988, Colin Pitchfork was the first person convicted of murder with DNA evidence, which led to the growth of forensic science. More recently, Andrew Z. Fire and Craig C. Mello received the 2006 Nobel Prize for discovering the role of RNA interference (RNAi), in the silencing of gene expression.\n\nAround two dozen of the 92 naturally occurring chemical elements are essential to various kinds of biological life. Most rare elements on Earth are not needed by life (exceptions being selenium and iodine), while a few common ones (aluminum and titanium) are not used. Most organisms share element needs, but there are a few differences between plants and animals. For example, ocean algae use bromine, but land plants and animals seem to need none. All animals require sodium, but some plants do not. Plants need boron and silicon, but animals may not (or may need ultra-small amounts).\n\nJust six elements—carbon, hydrogen, nitrogen, oxygen, calcium, and phosphorus—make up almost 99% of the mass of living cells, including those in the human body (see composition of the human body for a complete list). In addition to the six major elements that compose most of the human body, humans require smaller amounts of possibly 18 more.\n\nThe four main classes of molecules in biochemistry (often called biomolecules) are carbohydrates, lipids, proteins, and nucleic acids. Many biological molecules are polymers: in this terminology, monomers are relatively small micromolecules that are linked together to create large macromolecules known as polymers. When monomers are linked together to synthesize a biological polymer, they undergo a process called dehydration synthesis. Different macromolecules can assemble in larger complexes, often needed for biological activity.\n\nThe function of carbohydrates includes energy storage and providing structure. Sugars are carbohydrates, but not all carbohydrates are sugars. There are more carbohydrates on Earth than any other known type of biomolecule; they are used to store energy and genetic information, as well as play important roles in cell to cell interactions and communications.\n\nThe simplest type of carbohydrate is a monosaccharide, which among other properties contains carbon, hydrogen, and oxygen, mostly in a ratio of 1:2:1 (generalized formula CHO, where \"n\" is at least 3). Glucose (CHO) is one of the most important carbohydrates, others include fructose (CHO), the sugar commonly associated with the sweet taste of fruits, and deoxyribose (CHO).\n\nA monosaccharide can switch from the acyclic (open-chain) form to a cyclic form, through a nucleophilic addition reaction between the carbonyl group and one of the hydroxyls of the same molecule. The reaction creates a ring of carbon atoms closed by one bridging oxygen atom. The resulting molecule has an hemiacetal or hemiketal group, depending on whether the linear form was an aldose or a ketose. The reaction is easily reversed, yielding the original open-chain form.\n\nIn these cyclic forms, the ring usually has 5 or 6 atoms. These forms are called furanoses and pyranoses, respectively — by analogy with furan and pyran, the simplest compounds with the same carbon-oxygen ring (although they lack the double bonds of these two molecules). For example, the aldohexose glucose may form a hemiacetal linkage between the hydroxyl on carbon 1 and the oxygen on carbon 4, yielding a molecule with a 5-membered ring, called glucofuranose. The same reaction can take place between carbons 1 and 5 to form a molecule with a 6-membered ring, called glucopyranose. Cyclic forms with a 7-atom ring (the same of oxepane), rarely encountered, are called heptoses.\n\nWhen two monosaccharides undergo dehydration synthesis whereby a molecule of water is released, as two hydrogen atoms and one oxygen atom are lost from the two monosaccharides. The new molecule, consisting of two monosaccharides, is called a \"disaccharide\" and is conjoined together by a glycosidic or ether bond. The reverse reaction can also occur, using a molecule of water to split up a disaccharide and break the glycosidic bond; this is termed \"hydrolysis\". The most well-known disaccharide is sucrose, ordinary sugar (in scientific contexts, called \"table sugar\" or \"cane sugar\" to differentiate it from other sugars). Sucrose consists of a glucose molecule and a fructose molecule joined together. Another important disaccharide is lactose, consisting of a glucose molecule and a galactose molecule. As most humans age, the production of lactase, the enzyme that hydrolyzes lactose back into glucose and galactose, typically decreases. This results in lactase deficiency, also called \"lactose intolerance\".\n\nWhen a few (around three to six) monosaccharides are joined, it is called an \"oligosaccharide\" (\"oligo-\" meaning \"few\"). These molecules tend to be used as markers and signals, as well as having some other uses. Many monosaccharides joined together make a polysaccharide. They can be joined together in one long linear chain, or they may be branched. Two of the most common polysaccharides are cellulose and glycogen, both consisting of repeating glucose monomers. Examples are \"Cellulose\" which is an important structural component of plant's cell walls, and \"glycogen\", used as a form of energy storage in animals.\n\nSugar can be characterized by having reducing or non-reducing ends. A reducing end of a carbohydrate is a carbon atom that can be in equilibrium with the open-chain aldehyde (aldose) or keto form (ketose). If the joining of monomers takes place at such a carbon atom, the free hydroxy group of the pyranose or furanose form is exchanged with an OH-side-chain of another sugar, yielding a full acetal. This prevents opening of the chain to the aldehyde or keto form and renders the modified residue non-reducing. Lactose contains a reducing end at its glucose moiety, whereas the galactose moiety form a full acetal with the C4-OH group of glucose. Saccharose does not have a reducing end because of full acetal formation between the aldehyde carbon of glucose (C1) and the keto carbon of fructose (C2).\n\nLipids comprises a diverse range of molecules and to some extent is a catchall for relatively water-insoluble or nonpolar compounds of biological origin, including waxes, fatty acids, fatty-acid derived phospholipids, sphingolipids, glycolipids, and terpenoids (e.g., retinoids and steroids). Some lipids are linear aliphatic molecules, while others have ring structures. Some are aromatic, while others are not. Some are flexible, while others are rigid.\n\nLipids are usually made from one molecule of glycerol combined with other molecules. In triglycerides, the main group of bulk lipids, there is one molecule of glycerol and three fatty acids. Fatty acids are considered the monomer in that case, and may be saturated (no double bonds in the carbon chain) or unsaturated (one or more double bonds in the carbon chain).\n\nMost lipids have some polar character in addition to being largely nonpolar. In general, the bulk of their structure is nonpolar or hydrophobic (\"water-fearing\"), meaning that it does not interact well with polar solvents like water. Another part of their structure is polar or hydrophilic (\"water-loving\") and will tend to associate with polar solvents like water. This makes them amphiphilic molecules (having both hydrophobic and hydrophilic portions). In the case of cholesterol, the polar group is a mere -OH (hydroxyl or alcohol). In the case of phospholipids, the polar groups are considerably larger and more polar, as described below.\n\nLipids are an integral part of our daily diet. Most oils and milk products that we use for cooking and eating like butter, cheese, ghee etc., are composed of fats. Vegetable oils are rich in various polyunsaturated fatty acids (PUFA). Lipid-containing foods undergo digestion within the body and are broken into fatty acids and glycerol, which are the final degradation products of fats and lipids. Lipids, especially phospholipids, are also used in various pharmaceutical products, either as co-solubilisers (e.g., in parenteral infusions) or else as drug carrier components (e.g., in a liposome or transfersome).\n\nProteins are very large molecules – macro-biopolymers – made from monomers called amino acids. An amino acid consists of a carbon atom bound to four groups. One is an amino group, —NH, and one is a carboxylic acid group, —COOH (although these exist as —NH and —COO under physiologic conditions). The third is a simple hydrogen atom. The fourth is commonly denoted \"—R\" and is different for each amino acid. There are 20 standard amino acids, each containing a carboxyl group, an amino group, and a side-chain (known as an \"R\" group). The \"R\" group is what makes each amino acid different, and the properties of the side-chains greatly influence the overall three-dimensional conformation of a protein. Some amino acids have functions by themselves or in a modified form; for instance, glutamate functions as an important neurotransmitter. Amino acids can be joined via a peptide bond. In this dehydration synthesis, a water molecule is removed and the peptide bond connects the nitrogen of one amino acid's amino group to the carbon of the other's carboxylic acid group. The resulting molecule is called a \"dipeptide\", and short stretches of amino acids (usually, fewer than thirty) are called \"peptides\" or polypeptides. Longer stretches merit the title \"proteins\". As an example, the important blood serum protein albumin contains 585 amino acid residues.\n\nSome proteins perform largely structural roles. For instance, movements of the proteins actin and myosin ultimately are responsible for the contraction of skeletal muscle. One property many proteins have is that they specifically bind to a certain molecule or class of molecules—they may be \"extremely\" selective in what they bind. Antibodies are an example of proteins that attach to one specific type of molecule. In fact, the enzyme-linked immunosorbent assay (ELISA), which uses antibodies, is one of the most sensitive tests modern medicine uses to detect various biomolecules. Probably the most important proteins, however, are the enzymes. Virtually every reaction in a living cell requires an enzyme to lower the activation energy of the reaction. These molecules recognize specific reactant molecules called \"substrates\"; they then catalyze the reaction between them. By lowering the activation energy, the enzyme speeds up that reaction by a rate of 10 or more; a reaction that would normally take over 3,000 years to complete spontaneously might take less than a second with an enzyme. The enzyme itself is not used up in the process, and is free to catalyze the same reaction with a new set of substrates. Using various modifiers, the activity of the enzyme can be regulated, enabling control of the biochemistry of the cell as a whole.\n\nThe structure of proteins is traditionally described in a hierarchy of four levels. The primary structure of a protein simply consists of its linear sequence of amino acids; for instance, \"alanine-glycine-tryptophan-serine-glutamate-asparagine-glycine-lysine-…\". Secondary structure is concerned with local morphology (morphology being the study of structure). Some combinations of amino acids will tend to curl up in a coil called an α-helix or into a sheet called a β-sheet; some α-helixes can be seen in the hemoglobin schematic above. Tertiary structure is the entire three-dimensional shape of the protein. This shape is determined by the sequence of amino acids. In fact, a single change can change the entire structure. The alpha chain of hemoglobin contains 146 amino acid residues; substitution of the glutamate residue at position 6 with a valine residue changes the behavior of hemoglobin so much that it results in sickle-cell disease. Finally, quaternary structure is concerned with the structure of a protein with multiple peptide subunits, like hemoglobin with its four subunits. Not all proteins have more than one subunit.\n\nIngested proteins are usually broken up into single amino acids or dipeptides in the small intestine, and then absorbed. They can then be joined to make new proteins. Intermediate products of glycolysis, the citric acid cycle, and the pentose phosphate pathway can be used to make all twenty amino acids, and most bacteria and plants possess all the necessary enzymes to synthesize them. Humans and other mammals, however, can synthesize only half of them. They cannot synthesize isoleucine, leucine, lysine, methionine, phenylalanine, threonine, tryptophan, and valine. These are the essential amino acids, since it is essential to ingest them. Mammals do possess the enzymes to synthesize alanine, asparagine, aspartate, cysteine, glutamate, glutamine, glycine, proline, serine, and tyrosine, the nonessential amino acids. While they can synthesize arginine and histidine, they cannot produce it in sufficient amounts for young, growing animals, and so these are often considered essential amino acids.\n\nIf the amino group is removed from an amino acid, it leaves behind a carbon skeleton called an α-keto acid. Enzymes called transaminases can easily transfer the amino group from one amino acid (making it an α-keto acid) to another α-keto acid (making it an amino acid). This is important in the biosynthesis of amino acids, as for many of the pathways, intermediates from other biochemical pathways are converted to the α-keto acid skeleton, and then an amino group is added, often via transamination. The amino acids may then be linked together to make a protein.\n\nA similar process is used to break down proteins. It is first hydrolyzed into its component amino acids. Free ammonia (NH), existing as the ammonium ion (NH) in blood, is toxic to life forms. A suitable method for excreting it must therefore exist. Different tactics have evolved in different animals, depending on the animals' needs. Unicellular organisms, of course, simply release the ammonia into the environment. Likewise, bony fish can release the ammonia into the water where it is quickly diluted. In general, mammals convert the ammonia into urea, via the urea cycle.\n\nIn order to determine whether two proteins are related, or in other words to decide whether they are homologous or not, scientists use sequence-comparison methods. Methods like sequence alignments and structural alignments are powerful tools that help scientists identify homologies between related molecules. The relevance of finding homologies among proteins goes beyond forming an evolutionary pattern of protein families. By finding how similar two protein sequences are, we acquire knowledge about their structure and therefore their function.\n\nNucleic acids, so called because of its prevalence in cellular nuclei, is the generic name of the family of biopolymers. They are complex, high-molecular-weight biochemical macromolecules that can convey genetic information in all living cells and viruses. The monomers are called nucleotides, and each consists of three components: a nitrogenous heterocyclic base (either a purine or a pyrimidine), a pentose sugar, and a phosphate group.\n\nThe most common nucleic acids are deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). The phosphate group and the sugar of each nucleotide bond with each other to form the backbone of the nucleic acid, while the sequence of nitrogenous bases stores the information. The most common nitrogenous bases are adenine, cytosine, guanine, thymine, and uracil. The nitrogenous bases of each strand of a nucleic acid will form hydrogen bonds with certain other nitrogenous bases in a complementary strand of nucleic acid (similar to a zipper). Adenine binds with thymine and uracil; Thymine binds only with adenine; and cytosine and guanine can bind only with one another.\n\nAside from the genetic material of the cell, nucleic acids often play a role as second messengers, as well as forming the base molecule for adenosine triphosphate (ATP), the primary energy-carrier molecule found in all living organisms. Also, the nitrogenous bases possible in the two nucleic acids are different: adenine, cytosine, and guanine occur in both RNA and DNA, while thymine occurs only in DNA and uracil occurs in RNA.\n\nGlucose is the major energy source in most life forms. For instance, polysaccharides are broken down into their monomers (glycogen phosphorylase removes glucose residues from glycogen). Disaccharides like lactose or sucrose are cleaved into their two component monosaccharides.\n\nGlucose is mainly metabolized by a very important ten-step pathway called glycolysis, the net result of which is to break down one molecule of glucose into two molecules of pyruvate. This also produces a net two molecules of ATP, the energy currency of cells, along with two reducing equivalents of converting NAD (nicotinamide adenine dinucleotide:oxidised form) to NADH (nicotinamide adenine dinucleotide:reduced form). This does not require oxygen; if no oxygen is available (or the cell cannot use oxygen), the NAD is restored by converting the pyruvate to lactate (lactic acid) (e.g., in humans) or to ethanol plus carbon dioxide (e.g., in yeast). Other monosaccharides like galactose and fructose can be converted into intermediates of the glycolytic pathway.\n\nIn aerobic cells with sufficient oxygen, as in most human cells, the pyruvate is further metabolized. It is irreversibly converted to acetyl-CoA, giving off one carbon atom as the waste product carbon dioxide, generating another reducing equivalent as NADH. The two molecules acetyl-CoA (from one molecule of glucose) then enter the citric acid cycle, producing two more molecules of ATP, six more NADH molecules and two reduced (ubi)quinones (via FADH as enzyme-bound cofactor), and releasing the remaining carbon atoms as carbon dioxide. The produced NADH and quinol molecules then feed into the enzyme complexes of the respiratory chain, an electron transport system transferring the electrons ultimately to oxygen and conserving the released energy in the form of a proton gradient over a membrane (inner mitochondrial membrane in eukaryotes). Thus, oxygen is reduced to water and the original electron acceptors NAD and quinone are regenerated. This is why humans breathe in oxygen and breathe out carbon dioxide. The energy released from transferring the electrons from high-energy states in NADH and quinol is conserved first as proton gradient and converted to ATP via ATP synthase. This generates an additional \"28\" molecules of ATP (24 from the 8 NADH + 4 from the 2 quinols), totaling to 32 molecules of ATP conserved per degraded glucose (two from glycolysis + two from the citrate cycle). It is clear that using oxygen to completely oxidize glucose provides an organism with far more energy than any oxygen-independent metabolic feature, and this is thought to be the reason why complex life appeared only after Earth's atmosphere accumulated large amounts of oxygen.\n\nIn vertebrates, vigorously contracting skeletal muscles (during weightlifting or sprinting, for example) do not receive enough oxygen to meet the energy demand, and so they shift to anaerobic metabolism, converting glucose to lactate. The liver regenerates the glucose, using a process called gluconeogenesis. This process is not quite the opposite of glycolysis, and actually requires three times the amount of energy gained from glycolysis (six molecules of ATP are used, compared to the two gained in glycolysis). Analogous to the above reactions, the glucose produced can then undergo glycolysis in tissues that need energy, be stored as glycogen (or starch in plants), or be converted to other monosaccharides or joined into di- or oligosaccharides. The combined pathways of glycolysis during exercise, lactate's crossing via the bloodstream to the liver, subsequent gluconeogenesis and release of glucose into the bloodstream is called the Cori cycle.\n\nResearchers in biochemistry use specific techniques native to biochemistry, but increasingly combine these with techniques and ideas developed in the fields of genetics, molecular biology and biophysics. There has never been a hard-line among these disciplines in terms of content and technique. Today, the terms \"molecular biology\" and \"biochemistry\" are nearly interchangeable. The following figure is a schematic that depicts one possible view of the relationship between the fields:\n\n\n\na. Fructose is not the only sugar found in fruits. Glucose and sucrose are also found in varying quantities in various fruits, and indeed sometimes exceed the fructose present. For example, 32% of the edible portion of date is glucose, compared with 23.70% fructose and 8.20% sucrose. However, peaches contain more sucrose (6.66%) than they do fructose (0.93%) or glucose (1.47%).\n\n\n\n", "id": "3954", "title": "Biochemistry"}
{"url": "https://en.wikipedia.org/wiki?curid=3956", "text": "Badminton\n\nBadminton is a racquet sport played using racquets to hit a shuttlecock across a net. Although it may be played with larger teams, the most common forms of the game are \"singles\" (with one player per side) and \"doubles\" (with two players per side). Badminton is often played as a casual outdoor activity in a yard or on a beach; formal games are played on a rectangular indoor court. Points are scored by striking the shuttlecock with the racquet and landing it within the opposing side's half of the court.\n\nEach side may only strike the shuttlecock once before it passes over the net. Play ends once the shuttlecock has struck the floor or if a fault has been called by the umpire, service judge, or (in their absence) the opposing side.\n\nThe shuttlecock is a feathered or (in informal matches) plastic projectile which flies differently from the balls used in many other sports. In particular, the feathers create much higher drag, causing the shuttlecock to decelerate more rapidly. Shuttlecocks also have a high top speed compared to the balls in other racquet sports.\n\nThe game developed in British India from the earlier game of battledore and shuttlecock. European play came to be dominated by Denmark but the game has become very popular in Asia, with recent competition dominated by China. Since 1992, badminton has been a Summer Olympic sport with five events: men's singles, women's singles, men's doubles, women's doubles, and mixed doubles. At high levels of play, the sport demands excellent fitness: players require aerobic stamina, agility, strength, speed, and precision. It is also a technical sport, requiring good motor coordination and the development of sophisticated racquet movements.\n\nGames employing shuttlecocks have been played for centuries across Eurasia but the modern game of badminton developed in the mid-19th century among the British as a variant of the earlier game of battledore and shuttlecock. (\"Battledore\" was an older term for \"racquet\".) Its exact origin remains obscure. The name derives from the Duke of Beaufort's Badminton House in Gloucestershire, but why or when remains unclear. As early as 1860, a London toy dealer named Isaac Spratt published a booklet titled \"Badminton Battledore—A New Game\" but unfortunately no copy has survived. An 1863 article in \"The Cornhill Magazine\" describes badminton as \"battledore and shuttlecock played with sides, across a string suspended some five feet from the ground\".\n\nThe game may have originally developed among expatriate officers in British India, where it was very popular by the 1870s. Ball badminton, a form of the game played with a wool ball instead of a shuttlecock, was being played in Thanjavur as early as the 1850s and was at first played interchangeably with badminton by the British, the woollen ball being preferred in windy or wet weather.\n\nEarly on, the game was also known as Poona or Poonah after the garrison town of Pune, where it was particularly popular and where the first rules for the game were drawn up in 1873. By 1875, returning officers had started a badminton club in Folkestone. Initially, the sport was played with sides ranging from 1–4 players but it was quickly established that games between two or four competitors worked the best. The shuttlecocks were coated with India rubber and, in outdoor play, sometimes weighted with lead. Although the depth of the net was of no consequence, it was preferred that it should reach the ground.\n\nThe sport was played under the Pune rules until 1887, when the J.H.E. Hart of the Bath Badminton Club drew up revised regulations. In 1890, Hart and Bagnel Wild again revised the rules. The Badminton Association of England published these rules in 1893 and officially launched the sport at a house called \"Dunbar\" in Portsmouth on 13 September. The BAE started the first badminton competition, the All England Open Badminton Championships for gentlemen's doubles, ladies' doubles, and mixed doubles, in 1899. Singles competitions were added in 1900 and an England—Ireland championship match appeared in 1904.\n\nEngland, Scotland, Wales, Canada, Denmark, France, Ireland, the Netherlands, and New Zealand were the founding members of the International Badminton Federation in 1934, now known as the Badminton World Federation. India joined as an affiliate in 1936. The BWF now governs international badminton. Although initiated in England, competitive men's badminton has traditionally been dominated in Europe by Denmark. Worldwide, Asian nations have become dominant in international competition. China, Denmark, India, Indonesia, Malaysia, and South Korea are the nations which have consistently produced world-class players in the past few decades, with China being the greatest force in men's and women's competition recently.\n\nThe following information is a simplified summary of badminton rules based on the BWF Statutes publication, \"Laws of Badminton\".\n\nThe court is rectangular and divided into halves by a net. Courts are usually marked for both singles and doubles play, although badminton rules permit a court to be marked for singles only. The doubles court is wider than the singles court, but both are of same length. The exception, which often causes confusion to newer players, is that the doubles court has a shorter serve-length dimension.\n\nThe full width of the court is 6.1 metres (20 ft), and in singles this width is reduced to 5.18 metres (17 ft). The full length of the court is 13.4 metres (44 ft). The service courts are marked by a centre line dividing the width of the court, by a short service line at a distance of 1.98 metres (6 ft 6 inch) from the net, and by the outer side and back boundaries. In doubles, the service court is also marked by a long service line, which is 0.76 metres (2 ft 6 inch) from the back boundary.\n\nThe net is 1.55 metres (5 ft 1 inch) high at the edges and 1.524 metres (5 ft) high in the centre. The net posts are placed over the doubles sidelines, even when singles is played.\n\nThe minimum height for the ceiling above the court is not mentioned in the Laws of Badminton. Nonetheless, a badminton court will not be suitable if the ceiling is likely to be hit on a high serve.\n\nWhen the server serves, the shuttlecock must pass over the short service line on the opponents' court or it will count as a fault.\n\nAt the start of the rally, the server and receiver stand in diagonally opposite \"service courts\" (see court dimensions). The server hits the shuttlecock so that it would land in the receiver's service court. This is similar to tennis, except that a badminton serve must be hit below waist height and with the racquet shaft pointing downwards, the shuttlecock is not allowed to bounce and in badminton, the players stand inside their service courts unlike tennis.\n\nWhen the serving side loses a rally, the serve immediately passes to their opponent(s) (this differs from the old system where sometimes the serve passes to the doubles partner for what is known as a \"second serve\").\n\nIn singles, the server stands in their right service court when their score is even, and in her/his left service court when her/his score is odd.\n\nIn doubles, if the serving side wins a rally, the same player continues to serve, but he/she changes service courts so that she/he serves to a different opponent each time. If the opponents win the rally and their new score is even, the player in the right service court serves; if odd, the player in the left service court serves. The players' service courts are determined by their positions at the start of the previous rally, not by where they were standing at the end of the rally. A consequence of this system is that, each time a side regains the service, the server will be the player who did \"not\" serve last time.\n\nEach game is played to 21 points, with players scoring a point whenever they win a rally regardless of whether they served (this differs from the old system where players could only win a point on their serve and each game was played to 15 points). A match is the best of three games.\n\nIf the score reaches 20-all, then the game continues until one side gains a two-point lead (such as 24–22), except when there is a tie at 29-all, in which the game goes to a golden point. Whoever scores this point will win.\n\nAt the start of a match, the shuttlecock is cast and the side towards which the shuttlecock is pointing serves first. Alternatively, a coin may be tossed, with the winners choosing whether to serve or receive first, or choosing which end of the court to occupy first, and their opponents making the leftover the remaining choice.\n\nIn subsequent games, the winners of the previous game serve first. Matches are best out of three: a player or pair must win two games (of 21 points each) to win the match. For the first rally of any doubles game, the serving pair may decide who serves and the receiving pair may decide who receives. The players change ends at the start of the second game; if the match reaches a third game, they change ends both at the start of the game and when the leading player's or pair's score reaches 11 points.\n\nThe server and receiver must remain within their service courts, without touching the boundary lines, until the server strikes the shuttlecock. The other two players may stand wherever they wish, so long as they do not block the vision of the server or receiver.\n\nIf a let is called, the rally is stopped and replayed with no change to the score. Lets may occur because of some unexpected disturbance such as a shuttlecock landing on court (having been hit there by players playing in adjacent court) or in small halls the shuttle may touch an overhead rail which can be classed as a let.\n\nIf the receiver is not ready when the service is delivered, a let shall be called; yet, if the receiver attempts to return the shuttlecock, the receiver shall be judged to have been ready.\n\nBadminton rules restrict the design and size of racquets and shuttlecocks.\n\nBadminton racquets are lightweight, with top quality racquets weighing between not including grip or strings. They are composed of many different materials ranging from carbon fibre composite (graphite reinforced plastic) to solid steel, which may be augmented by a variety of materials. Carbon fibre has an excellent strength to weight ratio, is stiff, and gives excellent kinetic energy transfer. Before the adoption of carbon fibre composite, racquets were made of light metals such as aluminum. Earlier still, racquets were made of wood. Cheap racquets are still often made of metals such as steel, but wooden racquets are no longer manufactured for the ordinary market, because of their excessive mass and cost. Nowadays, nanomaterials such as fullerene and carbon nanotubes are added to racquets giving them greater durability.\n\nThere is a wide variety of racquet designs, although the laws limit the racquet size and shape. Different racquets have playing characteristics that appeal to different players. The traditional oval head shape is still available, but an isometric head shape is increasingly common in new racquets.\n\nBadminton strings are thin, high performing strings with thicknesses ranging from about 0.62 to 0.73 mm. Thicker strings are more durable, but many players prefer the feel of thinner strings. String tension is normally in the range of 80 to 160 N (18 to 36 lbf). Recreational players generally string at lower tensions than professionals, typically between 80 and 110 N (18 and 25 lbf). Professionals string between about 110 and 160 N (25 and 36 lbf). Some string manufacturers measure the thickness of their strings under tension so they are actually thicker than specified when slack. Ashaway Micropower is actually 0.7mm but Yonex BG-66 is about 0.72mm.\n\nIt is often argued that high string tensions improve control, whereas low string tensions increase power. The arguments for this generally rely on crude mechanical reasoning, such as claiming that a lower tension string bed is more bouncy and therefore provides more power. This is in fact incorrect, for a higher string tension can cause the shuttle to slide off the racquet and hence make it harder to hit a shot accurately. An alternative view suggests that the optimum tension for power depends on the player: the faster and more accurately a player can swing their racquet, the higher the tension for maximum power. Neither view has been subjected to a rigorous mechanical analysis, nor is there clear evidence in favour of one or the other. The most effective way for a player to find a good string tension is to experiment.\n\nThe choice of grip allows a player to increase the thickness of their racquet handle and choose a comfortable surface to hold. A player may build up the handle with one or several grips before applying the final layer.\n\nPlayers may choose between a variety of grip materials. The most common choices are PU synthetic grips or towelling grips. Grip choice is a matter of personal preference. Players often find that sweat becomes a problem; in this case, a drying agent may be applied to the grip or hands, sweatbands may be used, the player may choose another grip material or change his/her grip more frequently.\n\nThere are two main types of grip: \"replacement\" grips and \"overgrips\". Replacement grips are thicker, and are often used to increase the size of the handle. Overgrips are thinner (less than 1 mm), and are often used as the final layer. Many players, however, prefer to use replacement grips as the final layer. Towelling grips are always replacement grips. Replacement grips have an adhesive backing, whereas overgrips have only a small patch of adhesive at the start of the tape and must be applied under tension; overgrips are more convenient for players who change grips frequently, because they may be removed more rapidly without damaging the underlying material.\n\nA shuttlecock (often abbreviated to \"shuttle\"; also called a \"birdie\") is a high-drag projectile, with an open conical shape: the cone is formed from sixteen overlapping feathers embedded into a rounded cork base. The cork is covered with thin leather or synthetic material. Synthetic shuttles are often used by recreational players to reduce their costs as feathered shuttles break easily. These nylon shuttles may be constructed with either natural cork or synthetic foam base, and a plastic skirt.\n\nBadminton rules also provide for testing a shuttlecock for the correct speed:\n\nBadminton shoes are lightweight with soles of rubber or similar high-grip, non-marking materials.\n\nCompared to running shoes, badminton shoes have little lateral support. High levels of lateral support are useful for activities where lateral motion is undesirable and unexpected. Badminton, however, requires powerful lateral movements. A highly built-up lateral support will not be able to protect the foot in badminton; instead, it will encourage catastrophic collapse at the point where the shoe's support fails, and the player's ankles are not ready for the sudden loading, which can cause sprains. For this reason, players should choose badminton shoes rather than general trainers or running shoes, because proper badminton shoes will have a very thin sole, lower a person's centre of gravity, and therefore result in fewer injuries. Players should also ensure that they learn safe and proper footwork, with the knee and foot in alignment on all lunges. This is more than just a safety concern: proper footwork is also critical in order to move effectively around the court.\n\nBadminton offers a wide variety of basic strokes, and players require a high level of skill to perform all of them effectively. All strokes can be played either \"forehand\" or \"backhand\". A player's forehand side is the same side as their playing hand: for a right-handed player, the forehand side is their right side and the backhand side is their left side. Forehand strokes are hit with the front of the hand leading (like hitting with the palm), whereas backhand strokes are hit with the back of the hand leading (like hitting with the knuckles). Players frequently play certain strokes on the forehand side with a backhand hitting action, and vice versa.\n\nIn the forecourt and midcourt, most strokes can be played equally effectively on either the forehand or backhand side; but in the rear court, players will attempt to play as many strokes as possible on their forehands, often preferring to play a \"round-the-head\" forehand overhead (a forehand \"on the backhand side\") rather than attempt a backhand overhead. Playing a backhand overhead has two main disadvantages. First, the player must turn their back to their opponents, restricting their view of them and the court. Second, backhand overheads cannot be hit with as much power as forehands: the hitting action is limited by the shoulder joint, which permits a much greater range of movement for a forehand overhead than for a backhand. The \"backhand clear\" is considered by most players and coaches to be the most difficult basic stroke in the game, since precise technique is needed in order to muster enough power for the shuttlecock to travel the full length of the court. For the same reason, \"backhand smashes\" tend to be weak.\n\nThe choice of stroke depends on how near the shuttlecock is to the net, whether it is above net height, and where an opponent is currently positioned: players have much better attacking options if they can reach the shuttlecock well above net height, especially if it is also close to the net. In the forecourt, a high shuttlecock will be met with a \"net kill\", hitting it steeply downwards and attempting to win the rally immediately. This is why it is best to drop the shuttlecock just over the net in this situation. In the midcourt, a high shuttlecock will usually be met with a powerful \"smash\", also hitting downwards and hoping for an outright winner or a weak reply. Athletic \"jump smashes\", where players jump upwards for a steeper smash angle, are a common and spectacular element of elite men's doubles play. In the rearcourt, players strive to hit the shuttlecock while it is still above them, rather than allowing it to drop lower. This \"overhead\" hitting allows them to play smashes, \"clears\" (hitting the shuttlecock high and to the back of the opponents' court), and \"drop shots\" (hitting the shuttlecock softly so that it falls sharply downwards into the opponents' forecourt). If the shuttlecock has dropped lower, then a smash is impossible and a full-length, high clear is difficult.\n\nWhen the shuttlecock is well below net height, players have no choice but to hit upwards. \"Lifts\", where the shuttlecock is hit upwards to the back of the opponents' court, can be played from all parts of the court. If a player does not lift, his only remaining option is to push the shuttlecock softly back to the net: in the forecourt this is called a \"netshot\"; in the midcourt or rearcourt, it is often called a \"push\" or \"block\".\n\nWhen the shuttlecock is near to net height, players can hit \"drives\", which travel flat and rapidly over the net into the opponents' rear midcourt and rearcourt. Pushes may also be hit flatter, placing the shuttlecock into the front midcourt. Drives and pushes may be played from the midcourt or forecourt, and are most often used in doubles: they are an attempt to regain the attack, rather than choosing to lift the shuttlecock and defend against smashes. After a successful drive or push, the opponents will often be forced to lift the shuttlecock.\n\nBalls may be spun to alter their bounce (for example, topspin and backspin in tennis) or trajectory, and players may slice the ball (strike it with an angled racquet face) to produce such spin; but, since the shuttlecock is not allowed to bounce, this does not apply to badminton.\n\nSlicing the shuttlecock so that it spins, however, does have applications, and some are particular to badminton. (See Basic strokes for an explanation of technical terms.)\n\nDue to the way that its feathers overlap, a shuttlecock also has a slight natural spin about its axis of rotational symmetry. The spin is in a counter-clockwise direction as seen from above when dropping a shuttlecock. This natural spin affects certain strokes: a tumbling netshot is more effective if the slicing action is from right to left, rather than from left to right.\n\nBadminton biomechanics have not been the subject of extensive scientific study, but some studies confirm the minor role of the wrist in power generation and indicate that the major contributions to power come from internal and external rotations of the upper and lower arm. Recent guides to the sport thus emphasize forearm rotation rather than wrist movements.\n\nThe feathers impart substantial drag, causing the shuttlecock to decelerate greatly over distance. The shuttlecock is also extremely aerodynamically stable: regardless of initial orientation, it will turn to fly cork-first, and remain in the cork-first orientation.\n\nOne consequence of the shuttlecock's drag is that it requires considerable power to hit it the full length of the court, which is not the case for most racquet sports. The drag also influences the flight path of a lifted (\"lobbed\") shuttlecock: the parabola of its flight is heavily skewed so that it falls at a steeper angle than it rises. With very high serves, the shuttlecock may even fall vertically.\n\nWhen defending against a smash, players have three basic options: lift, block, or drive. In singles, a block to the net is the most common reply. In doubles, a lift is the safest option but it usually allows the opponents to continue smashing; blocks and drives are counter-attacking strokes, but may be intercepted by the smasher's partner. Many players use a backhand hitting action for returning smashes on both the forehand and backhand sides, because backhands are more effective than forehands at covering smashes directed to the body. Hard shots directed towards the body are difficult to defend.\n\nThe service is restricted by the Laws and presents its own array of stroke choices. Unlike in tennis, the server's racquet must be pointing in a downward direction to deliver the serve so normally the shuttle must be hit upwards to pass over the net. The server can choose a \"low serve\" into the forecourt (like a push), or a lift to the back of the service court, or a flat \"drive serve\". Lifted serves may be either \"high serves\", where the shuttlecock is lifted so high that it falls almost vertically at the back of the court, or \"flick serves\", where the shuttlecock is lifted to a lesser height but falls sooner.\n\nOnce players have mastered these basic strokes, they can hit the shuttlecock from and to any part of the court, powerfully and softly as required. Beyond the basics, however, badminton offers rich potential for advanced stroke skills that provide a competitive advantage. Because badminton players have to cover a short distance as quickly as possible, the purpose of many advanced strokes is to deceive the opponent, so that either he is tricked into believing that a different stroke is being played, or he is forced to delay his movement until he actually sees the shuttle's direction. \"Deception\" in badminton is often used in both of these senses. When a player is genuinely deceived, he will often lose the point immediately because he cannot change his direction quickly enough to reach the shuttlecock. Experienced players will be aware of the trick and cautious not to move too early, but the attempted deception is still useful because it forces the opponent to delay his movement slightly. Against weaker players whose intended strokes are obvious, an experienced player may move before the shuttlecock has been hit, anticipating the stroke to gain an advantage.\n\n\"Slicing\" and using a \"shortened hitting action\" are the two main technical devices that facilitate deception. Slicing involves hitting the shuttlecock with an angled racquet face, causing it to travel in a different direction than suggested by the body or arm movement. Slicing also causes the shuttlecock to travel more slowly than the arm movement suggests. For example, a good crosscourt \"sliced dropshot\" will use a hitting action that suggests a straight clear or smash, deceiving the opponent about both the power and direction of the shuttlecock. A more sophisticated slicing action involves brushing the strings around the shuttlecock during the hit, in order to make the shuttlecock spin. This can be used to improve the shuttle's trajectory, by making it dip more rapidly as it passes the net; for example, a sliced low serve can travel slightly faster than a normal low serve, yet land on the same spot. Spinning the shuttlecock is also used to create \"spinning netshots\" (also called \"tumbling netshots\"), in which the shuttlecock turns over itself several times (tumbles) before stabilizing; sometimes the shuttlecock remains inverted instead of tumbling. The main advantage of a spinning netshot is that the opponent will be unwilling to address the shuttlecock until it has stopped tumbling, since hitting the feathers will result in an unpredictable stroke. Spinning netshots are especially important for high level singles players.\n\nThe lightness of modern racquets allows players to use a very short hitting action for many strokes, thereby maintaining the option to hit a powerful or a soft stroke until the last possible moment. For example, a singles player may hold his racquet ready for a netshot, but then flick the shuttlecock to the back instead with a shallow lift when she or he notices the opponent has moved before the actual shot was played. A shallow lift takes less time to reach the ground and as mentioned above a rally is over when the shuttlecock touches the ground. This makes the opponent's task of covering the whole court much more difficult than if the lift was hit higher and with a bigger, obvious swing. A short hitting action is not only useful for deception: it also allows the player to hit powerful strokes when he has no time for a big arm swing. A big arm swing is also usually not advised in badminton because bigger swings make it more difficult to recover for the next shot in fast exchanges. The use of grip tightening is crucial to these techniques, and is often described as \"finger power\". Elite players develop finger power to the extent that they can hit some power strokes, such as net kills, with less than a racquet swing.\n\nIt is also possible to reverse this style of deception, by suggesting a powerful stroke before slowing down the hitting action to play a soft stroke. In general, this latter style of deception is more common in the rearcourt (for example, dropshots disguised as smashes), whereas the former style is more common in the forecourt and midcourt (for example, lifts disguised as netshots).\n\nDeception is not limited to slicing and short hitting actions. Players may also use \"double motion\", where they make an initial racquet movement in one direction before withdrawing the racquet to hit in another direction. Players will often do this to send opponents in the wrong direction. The racquet movement is typically used to suggest a straight angle but then play the stroke cross court, or vice versa. \"Triple motion\" is also possible, but this is very rare in actual play. An alternative to double motion is to use a \"racquet head fake\", where the initial motion is continued but the racquet is turned during the hit. This produces a smaller change in direction, but does not require as much time.\n\nTo win in badminton, players need to employ a wide variety of strokes in the right situations. These range from powerful jumping smashes to delicate tumbling net returns. Often rallies finish with a smash, but setting up the smash requires subtler strokes. For example, a netshot can force the opponent to lift the shuttlecock, which gives an opportunity to smash. If the netshot is tight and tumbling, then the opponent's lift will not reach the back of the court, which makes the subsequent smash much harder to return.\n\nDeception is also important. Expert players prepare for many different strokes that look identical, and use slicing to deceive their opponents about the speed or direction of the stroke. If an opponent tries to anticipate the stroke, he may move in the wrong direction and may be unable to change his body momentum in time to reach the shuttlecock.\n\nSince one person needs to cover the entire court, singles tactics are based on forcing the opponent to move as much as possible; this means that singles strokes are normally directed to the corners of the court. Players exploit the length of the court by combining lifts and clears with drop shots and net shots. Smashing tends to be less prominent in singles than in doubles because the smasher has no partner to follow up his effort and is thus vulnerable to a skillfully placed return. Moreover, frequent smashing can be exhausting in singles where the conservation of a player's energy is at a premium. However, players with strong smashes will sometimes use the shot to create openings, and players commonly smash weak returns to try to end rallies.\n\nIn singles, players will often start the rally with a forehand high serve or with a flick serve. Low serves are also used frequently, either forehand or backhand. Drive serves are rare.\n\nAt high levels of play, singles demands extraordinary fitness. Singles is a game of patient positional manoeuvring, unlike the all-out aggression of doubles.\n\nBoth pairs will try to gain and maintain the attack, smashing downwards when the opportunity arises. Whenever possible, a pair will adopt an ideal attacking formation with one player hitting down from the rearcourt, and his partner in the midcourt intercepting all smash returns except the lift. If the rearcourt attacker plays a dropshot, his partner will move into the forecourt to threaten the net reply. If a pair cannot hit downwards, they will use flat strokes in an attempt to gain the attack. If a pair is forced to lift or clear the shuttlecock, then they must defend: they will adopt a side-by-side position in the rear midcourt, to cover the full width of their court against the opponents' smashes. In doubles, players generally smash to the middle ground between two players in order to take advantage of confusion and clashes.\n\nAt high levels of play, the backhand serve has become popular to the extent that forehand serves have become fairly rare at a high level of play. The straight low serve is used most frequently, in an attempt to prevent the opponents gaining the attack immediately. Flick serves are used to prevent the opponent from anticipating the low serve and attacking it decisively.\n\nAt high levels of play, doubles rallies are extremely fast. Men's doubles is the most aggressive form of badminton, with a high proportion of powerful jump smashes and very quick reflex exchanges. Because of this, spectator interest is sometimes greater for men's doubles than for singles.\n\nIn mixed doubles, both pairs typically try to maintain an attacking formation with the woman at the front and the man at the back. This is because the male players are usually substantially stronger, and can therefore produce smashes that are more powerful. As a result, mixed doubles require greater tactical awareness and subtler positional play. Clever opponents will try to reverse the ideal position, by forcing the woman towards the back or the man towards the front. In order to protect against this danger, mixed players must be careful and systematic in their shot selection.\n\nAt high levels of play, the formations will generally be more flexible: the top women players are capable of playing powerfully from the back-court, and will happily do so if required. When the opportunity arises, however, the pair will switch back to the standard mixed attacking position, with the woman in front and men in the back.\n\nThe Badminton World Federation (BWF) is the internationally recognized governing body of the sport responsible for conduction of tournaments and approaching fair play. Five regional confederations are associated with the BWF:\n\n\nThe BWF organizes several international competitions, including the Thomas Cup, the premier men's international team event first held in 1948–1949, and the Uber Cup, the women's equivalent first held in 1956–1957. The competitions now take place once every two years. More than 50 national teams compete in qualifying tournaments within continental confederations for a place in the finals. The final tournament involves 12 teams, following an increase from eight teams in 2004.\n\nThe Sudirman Cup, a gender-mixed international team event held once every two years, began in 1989. Teams are divided into seven levels based on the performance of each country. To win the tournament, a country must perform well across all five disciplines (men's doubles and singles, women's doubles and singles, and mixed doubles). Like association football (soccer), it features a promotion and relegation system in every level.\n\nBadminton was a demonstration event in the 1972 and 1988 Summer Olympics. It became an official Summer Olympic sport at the Barcelona Olympics in 1992 and its gold medals now generally rate as the sport's most coveted prizes for individual players.\n\nIn the BWF World Championships, first held in 1977, currently only the highest ranked 64 players in the world, and a maximum of four from each country, can participate in any category. In both the Olympic and BWF World competitions restrictions on the number of participants from any one country have caused some controversy because they sometimes result in excluding elite world level players from the strongest badminton nations. The Thomas, Uber, and Sudirman Cups, the Olympics, and the BWF World (and World Junior Championships), are all categorized as level one tournaments.\n\nAt the start of 2007, the BWF introduced a new tournament structure for the highest level tournaments aside from those in level one: the BWF Super Series. This level two tournament series, a tour for the world's elite players, stages twelve open tournaments around the world with 32 players (half the previous limit). The players collect points that determine whether they can play in Super Series Finals held at the year end. Among the tournaments in this series is the venerable All-England Championships, first held in 1900, which was once considered the unofficial world championships of the sport.\n\nLevel three tournaments consist of Grand Prix Gold and Grand Prix event. Top players can collect the world ranking points and enable them to play in the BWF Super Series open tournaments. These include the regional competitions in Asia (Badminton Asia Championships) and Europe (European Badminton Championships), which produce the world's best players as well as the Pan America Badminton Championships.\n\nThe level four tournaments, known as International Challenge, International Series, and Future Series, encourage participation by junior players.\n\nBadminton is frequently compared to tennis. The following is a list of manifest differences:\n\nStatistics such as the smash speed, above, prompt badminton enthusiasts to make other comparisons that are more contentious. For example, it is often claimed that badminton is the fastest racquet sport. Although badminton holds the record for the fastest initial speed of a racquet sports projectile, the shuttlecock decelerates substantially faster than other projectiles such as tennis balls. In turn, this qualification must be qualified by consideration of the distance over which the shuttlecock travels: a smashed shuttlecock travels a shorter distance than a tennis ball during a serve.\n\nWhile fans of badminton and tennis often claim that their sport is the more physically demanding, such comparisons are difficult to make objectively because of the differing demands of the games. No formal study currently exists evaluating the physical condition of the players or demands during game play.\n\nBadminton and tennis techniques differ substantially. The lightness of the shuttlecock and of badminton racquets allow badminton players to make use of the wrist and fingers much more than tennis players; in tennis the wrist is normally held stable, and playing with a mobile wrist may lead to injury. For the same reasons, badminton players can generate power from a short racquet swing: for some strokes such as net kills, an elite player's swing may be less than . For strokes that require more power, a longer swing will typically be used, but the badminton racquet swing will rarely be as long as a typical tennis swing.\n\n\n\n", "id": "3956", "title": "Badminton"}
{"url": "https://en.wikipedia.org/wiki?curid=3957", "text": "Baroque\n\nThe Baroque ( or ) is often thought of as a period of artistic style which used exaggerated motion and clear, easily interpreted detail to produce drama, tension, exuberance, and grandeur in sculpture, painting, architecture, literature, dance, theatre, and music. The style began around 1600 in Rome and Italy, and spread to most of Europe.\n\nThe popularity and success of the Baroque style was encouraged by the Catholic Church, which had decided at the time of the Council of Trent, in response to the Protestant Reformation, that the arts should communicate religious themes with direct and emotional involvement. The aristocracy viewed the dramatic style of Baroque art and architecture as a means of impressing visitors by projecting triumph, power, and control. Baroque palaces are built around an entrance of courts, grand staircases, and reception rooms of sequentially increasing opulence. However, \"baroque\" has a resonance and application that extend beyond a simple reduction to either a style or period.\n\nThe French word \"baroque\" is derived from the Portuguese word \"barroco\" or Spanish \"barrueco\" both of which refer to a \"rough or imperfect pearl\", though whether it entered those languages via Latin, Arabic, or some other source is uncertain. It is also yields the Italian \"barocco\" and modern Spanish \"barroco\", German \"Barock\", Dutch \"Barok\", and so on. The 1911 \"Encyclopædia Britannica\" 11th edition thought the term was derived from the Spanish \"barrueco\", a large, irregularly-shaped pearl, and that it had for a time been confined to the craft of the jeweller. Others derive it from the mnemonic term \"Baroco\", a supposedly laboured form of syllogism in logical \"Scholastica\". The Latin root can be found in \"bis-roca\".\n\nIn informal usage, the word \"baroque\" can simply mean that something is \"elaborate\", with many details, without reference to the Baroque styles of the 17th and 18th centuries.\n\nThe word \"Baroque\", like most periodic or stylistic designations, was invented by later critics rather than practitioners of the arts in the 17th and early 18th centuries. It is a French transliteration of the Portuguese phrase \"pérola barroca\", which means \"irregular pearl\", and natural pearls that deviate from the usual, regular forms so they do not have an axis of rotation are known as \"baroque pearls\".\n\nThe term \"Baroque\" was initially used in a derogatory sense, to underline the excesses of its emphasis. In particular, the term was used to describe its eccentric redundancy and noisy abundance of details, which sharply contrasted the clear and sober rationality of the Renaissance. Although it was long thought that the word as a critical term was first applied to architecture, in fact it appears earlier in reference to music. In an anonymous satirical review of the première of Jean-Philippe Rameau's \"Hippolyte et Aricie\" in October 1733, which was printed in the \"Mercure de France\" in May 1734, the critic implied that the novelty in this opera was \"du barocque\", complaining that the music lacked coherent melody, was unsparing with dissonances, constantly changed key and meter, and speedily ran through every compositional device.\n\nAnother hypothesis says that the word comes from precursors of the style: Giacomo Barozzi da Vignola and Federico Barocci.\n\nThe Swiss-born art historian Heinrich Wölfflin (1864–1945) started the rehabilitation of the word \"Baroque\" in his \"Renaissance und Barock\" (1888); Wölfflin identified the Baroque as \"movement imported into mass\", an art antithetic to Renaissance art. He did not make the distinctions between Mannerism and Baroque that modern writers do, and he ignored the later phase, the academic Baroque that lasted into the 18th century. Long despised, Baroque art and architecture became fashionable between the two World Wars, and has largely remained in critical favour. For example, the often extreme Sicilian Baroque architecture is today recognised largely due to the work of Sir Sacheverall Sitwell, whose \"Southern Baroque Art\" of 1924 was the first book to appreciate the style, followed by the more academic work of Anthony Blunt. In painting the gradual rise in popular esteem of Caravaggio has been the best barometer of modern taste.\n\nIn art history it has become common to recognise \"Baroque\" stylistic phases, characterised by energetic movement and display, in earlier art, so that Sir John Boardman describes the ancient sculpture \"Laocoön and His Sons\" as \"one of the finest examples of the Hellenistic baroque\", and a later phase of Imperial Roman sculpture is also often called \"Baroque\". William Watson describes a late phase of Shang-dynasty Chinese ritual bronzes of the 11th century BC as \"baroque\".\n\nThe term \"Baroque\" may still be used, usually pejoratively, describing works of art, craft, or design that are thought to have excessive ornamentation or complexity of line.\n\nThe Baroque originated around 1600, several decades after the Council of Trent (1545–63), by which the Roman Catholic Church answered many questions of internal reform and formulated policy on the representational arts by demanding that paintings and sculptures in church contexts should speak to the illiterate rather than to the well-informed. Many art historians see this turn toward a populist conception of the function of ecclesiastical art as driving the innovations of Caravaggio and of the brothers Agostino and Annibale Carracci, all of whom were working (and competing for commissions) in Rome around 1600.\n\nThe appeal of Baroque style turned consciously from the witty, intellectual qualities of 16th-century Mannerist art to a visceral appeal aimed at the senses. It employed an iconography that was direct, simple, obvious, and theatrical (\"illustration, right\"). Baroque art drew on certain broad and heroic tendencies in Annibale Carracci and his circle, and found inspiration in other artists like Correggio and Caravaggio and Federico Barocci (\"illustration, right\"), nowadays sometimes termed 'proto-Baroque'. Germinal ideas of the Baroque can also be found in the work of Michelangelo. Some general parallels in music make the expression \"Baroque music\" useful: there are contrasting phrase lengths, harmony and counterpoint have ousted polyphony, and orchestral colour makes a stronger appearance. Even more generalised parallels perceived by some experts in philosophy, prose style and poetry, are harder to pinpoint.\n\nThough Baroque was superseded in many centers by the Rococo style, beginning in France in the late 1720s, especially for interiors, paintings and the decorative arts, the Baroque style continued in use in architecture until the advent of Neoclassicism in the later 18th century. See the Neapolitan palace of Caserta, a Baroque palace (though in a chaste exterior) whose construction began in 1752.\nIn paintings Baroque gestures are broader than Mannerist gestures: less ambiguous, less arcane and mysterious, more like the stage gestures of opera, a major Baroque art-form. Baroque poses depend on \"contrapposto\" (\"counterpoise\"), the tension within the figures that move the planes of shoulders and hips in counterdirections. (See Bernini's \"David\".)\n\nThe dryer, less dramatic and colouristic, chastened later stages of 18th century Baroque architectural style are often seen as a separate Late Baroque manifestation, for example in buildings by Claude Perrault. Academic characteristics in the neo-Palladian style, epitomised by William Kent, show a parallel development in Britain and the British colonies: within interiors, Kent's furniture designs are vividly influenced by the Baroque furniture of Rome and Genoa, hierarchical tectonic sculptural elements, meant never to be moved from their positions, completed the wall decoration. Baroque is a style of unity imposed upon rich, heavy detail.\n\nHeinrich Wölfflin defined the Baroque as the age where the oval replaced the circle as the center of composition, where centralization replaced balance, and where colouristic and \"painterly\" effects began to become more prominent.\nArt historians, often Protestant ones, have traditionally emphasized that the Baroque style evolved during a time in which the Roman Catholic Church had to react against the many revolutionary cultural movements that produced a new science and new forms of religion — the Reformation. It has been said that the monumental Baroque is a style that could give the Papacy, like secular absolute monarchies, a formal, imposing way of expression that could restore its prestige, at the point of becoming somehow symbolic of the Counter-Reformation.\n\nWhatever the truth of this interpretation, the Baroque was successfully developed in Rome, where Baroque architecture widely renewed the central areas with perhaps the most important urbanistic revision.\n\nThe Baroque era is sometimes divided into three approximate phases for convenience:\n\n\nThe term \"Late Baroque\" is also sometimes used synonymously with the succeeding Rococo movement.\n\nA defining statement of what \"Baroque\" signifies in painting is provided by the series of paintings executed by Peter Paul Rubens for Marie de Medici at the Luxembourg Palace in Paris (now at the Louvre), in which a Catholic painter satisfied a Catholic patron: Baroque-era conceptions of monarchy, iconography, handling of paint, and compositions as well as the depiction of space and movement.\n\nBaroque style featured \"exaggerated lighting, intense emotions, release from restraint, and even a kind of artistic sensationalism\". Baroque art did not really depict the life style of the people at that time; however, \"closely tied to the Counter-Reformation, this style melodramatically reaffirmed the emotional depths of the Catholic faith and glorified both church and monarchy\" of their power and influence.\n\nThere were highly diverse strands of Italian baroque painting, from Caravaggio to Cortona; both approaching emotive dynamism with different styles. The most prominent Spanish painter of the Baroque was Diego Velázquez.\n\nAnother frequently cited work of Baroque art is Bernini's \"Saint Theresa in Ecstasy\" for the Cornaro chapel in Saint Maria della Vittoria, which brings together architecture, sculpture, and theatre into one grand conceit.\n\nThe later Baroque style gradually gave way to a more decorative Rococo.\n\nA rather different art developed out of northern realist traditions in 17th century Dutch Golden Age painting, which had very little religious art, and little history painting, instead playing a crucial part in developing secular genres such as still life, genre paintings of everyday scenes, and landscape painting. While the Baroque nature of Rembrandt's art is clear, the label is less often used for Vermeer and many other Dutch artists. Flemish Baroque painting shared a part in this trend, while also continuing to produce the traditional categories.\n\nIn a similar way the French classical style of painting exemplified by Poussin is often classed as Baroque, and does share many qualities of the Italian painting of the same period, although the poise and restraint derived from following classical ideas typically give it a very different overall mood.\n\nIn Baroque sculpture, groups of figures assumed new importance and there was a dynamic movement and energy of human forms—they spiraled around an empty central vortex, or reached outwards into the surrounding space. For the first time, Baroque sculpture often had multiple ideal viewing angles. The characteristic Baroque sculpture added extra-sculptural elements, for example, concealed lighting, or water fountains. Aleijadinho in Brazil was also one of the great names of baroque sculpture, and his master work is the set of statues of the \"Santuário de Bom Jesus de Matosinhos\" in Congonhas. The soapstone sculptures of old testament prophets around the terrace are considered amongst his finest work.\n\nThe architecture, sculpture and fountains of Bernini (1598–1680) give highly charged characteristics of Baroque style. Bernini was undoubtedly the most important sculptor of the Baroque period. He approached Michelangelo in his omnicompetence: Bernini sculpted, worked as an architect, painted, wrote plays, and staged spectacles. In the late 20th century Bernini was most valued for his sculpture, both for his virtuosity in carving marble and his ability to create figures that combine the physical and the spiritual. He was also a fine sculptor of bust portraits in high demand among the powerful.\n\nA good example of Bernini's Baroque work is his \"St. Theresa in Ecstasy\" (1645–52), created for the Cornaro Chapel of the church of Santa Maria della Vittoria, Rome. Bernini designed the entire chapel, a subsidiary space along the side of the church, for the Cornaro family.\n\nSaint Theresa, the focal point of the chapel, is a soft white marble statue surrounded by a polychromatic marble architectural framing. This structure conceals a window which lights the statue from above. Figure-groups of the Cornaro family sculpted in shallow relief inhabit opera boxes on the two side walls of the chapel. The setting places the viewer as a spectator in front of the statue with the Cornaro family leaning out of their box seats and craning forward to see the mystical ecstasy of the saint.\n\nSt. Theresa is highly idealised and in an imaginary setting. She was a popular saint of the Catholic Reformation. She wrote of her mystical experiences for an audience of the nuns of her Carmelite Order; these writings had become popular reading among lay people interested in spirituality. In her writings, she described the love of God as piercing her heart like a burning arrow. Bernini materialises this by placing St. Theresa on a butt while a Cupid figure holds a golden arrow made of metal and smiles down at her. The angelic figure is not preparing to plunge the arrow into her heart—rather, he has withdrawn it. St. Theresa's face reflects not the anticipation of ecstasy, but her current fulfillment.\n\nThis work is widely considered a masterpiece of the Baroque, although the mix of religious and erotic imagery (faithful to St Teresa's own written account) may raise modern eyebrows. However, Bernini was a devout Catholic and was not attempting to satirize the experience of a chaste nun. Rather, he aimed to portray religious experience as an intensely physical one. Theresa described her bodily reaction to spiritual enlightenment in a language of ecstasy used by many mystics, and Bernini's depiction is earnest.\n\nThe Cornaro family promotes itself discreetly in this chapel; they are represented visually, but are placed on the sides of the chapel, witnessing the event from balconies. As in an opera house, the Cornaro have a privileged position in respect to the viewer, in their private reserve, closer to the saint; the viewer, however, has a better view from the front. They attach their name to the chapel, but St. Theresa is the focus. It is a private chapel in the sense that no one could say mass on the altar beneath the statue (in the 17th century and probably through the 19th) without permission from the family, but the only thing that divides the viewer from the image is the altar rail. The spectacle functions both as a demonstration of mysticism and as a piece of family pride.\n\nIn Baroque architecture, new emphasis was placed on bold massing, colonnades, domes, light-and-shade (\"chiaroscuro\"), 'painterly' colour effects, and the bold play of volume and void. In interiors, Baroque movement around and through a void informed monumental staircases that had no parallel in previous architecture. The other Baroque innovation in worldly interiors was the state apartment, a sequence of increasingly rich interiors that culminated in a presence chamber or throne room or a state bedroom. The sequence of monumental stairs followed by a state apartment was copied in smaller scale everywhere in aristocratic dwellings of any pretensions.\n\nBaroque architecture was taken up with enthusiasm in central Germany (see, e.g., Ludwigsburg Palace and Zwinger, Dresden), Austria and Russia (see, e.g., Peterhof). In England the culmination of Baroque architecture was embodied in work by Sir Christopher Wren, Sir John Vanbrugh and Nicholas Hawksmoor, from ca. 1660 to ca. 1725. Many examples of Baroque architecture and town planning are found in other European towns, and in Latin America. Town planning of this period featured radiating avenues intersecting in squares, which took cues from Baroque garden plans. In Sicily, Baroque developed new shapes and themes as in Noto, Ragusa and Acireale \"Basilica di San Sebastiano\".\n\nAnother example of Baroque architecture is the Cathedral of Morelia, in Michoacán, Mexico. Built in the 17th century by Vincenzo Barrochio, it is one of the many Baroque cathedrals in Mexico. Baroque churches built during the Spanish period are also seen in other former colonies of Spain.\n\nFrancis Ching described Baroque architecture as \"a style of architecture originating in Italy in the early 17th century and variously prevalent in Europe and the New World for a century and a half, characterised by free and sculptural use of the classical orders and ornament, dynamic opposition and interpenetration of spaces, and the dramatic combined effects of architecture, sculpture, painting, and the decorative arts.\"\nIn theatre, the elaborate conceits, multiplicity of plot turns and a variety of situations characteristic of Mannerism, in Shakespeare's tragedies for instance, were superseded by opera, which drew together all the arts into a unified whole.\n\nTheatre evolved in the Baroque era and became a multimedia experience, starting with the actual architectural space. In fact, much of the technology used in current Broadway or commercial plays was invented and developed during this era. The stage could change from a romantic garden to the interior of a palace in a matter of seconds. The entire space became a framed selected area that only allows the users to see a specific action, hiding all the machinery and technology – mostly ropes and pulleys.\n\nThis technology affected the content of the narrated or performed pieces, practicing at its best the Deus ex Machina solution. Gods were finally able to come down – literally – from the heavens and rescue the hero in the most extreme and dangerous, even absurd situations.\n\nThe term Theatrum Mundi – the world is a stage – was also created. The social and political realm in the real world is manipulated in exactly the same way the actor and the machines are presenting and limiting what is being presented on stage, hiding selectively all the machinery that makes the actions happen.\n\nThe films \"Vatel\" and \"Farinelli\" give a good idea of the style of productions of the Baroque period. The American musician William Christie and Les Arts Florissants have performed extensive research on all the French Baroque Opera, performing pieces from Charpentier and Lully, among others that are extremely faithful to the original 17th-century creations.\n\nThe influence of the Renaissance was also very late in England, and Baroque theatre is only partly a useful concept here, for example in discussing Restoration comedy. There was an 18-year break when the London theatres were closed during the English Civil War and English Commonwealth until the Restoration of Charles II in 1660.\n\nGerman theatre in the 17th century lacked major contributions. The best known playwright was Andreas Gryphius, who used the Jesuit model of the Dutch Joost van den Vondel and Pierre Corneille. There was also Johannes Velten who combined the traditions of the English comedians and the commedia del'arte with the classic theatre of Corneille and Molière. His touring company was perhaps the most significant and important of the 17th century.\n\nThe Baroque had a Catholic and conservative character in Spain, following an Italian literary models during the Renaissance. The Hispanic Baroque theatre aimed for a public content with an ideal reality that manifested fundamental three sentiments: Catholic religion, monarchist and national pride and honour originating from the chivalric, knightly world.\n\nTwo periods are known in the Baroque Spanish theatre, with the division occurring in 1630. The first period is represented chiefly by Lope de Vega, but also by Tirso de Molina, Gaspar Aguilar, Guillén de Castro, Antonio Mira de Amescua, Luis Vélez de Guevara, Juan Ruiz de Alarcón, Diego Jiménez de Enciso, Luis Belmonte Bermúdez, Felipe Godínez, Luis Quiñones de Benavente or Juan Pérez de Montalbán. The second period is represented by Pedro Calderón de la Barca and fellow dramatists Antonio Hurtado de Mendoza, Álvaro Cubillo de Aragón, Jerónimo de Cáncer, Francisco de Rojas Zorrilla, Juan de Matos Fragoso, Antonio Coello y Ochoa, Agustín Moreto, and Francisco Bances Candamo. These classifications are loose because each author had his own way and could occasionally adhere himself to the formula established by Lope. It may even be that the \"manner\" of Lope was more liberal and structured than Calderón's.\n\nLope de Vega introduced through his \"Arte nuevo de hacer comedias en este tiempo\" (1609) the \"new comedy\". He established a new dramatic formula that broke the three Aristotle unities of the Italian school of poetry (action, time and place) and a forth unity of Aristotle which is about style, mixing of tragic and comic elements showing different types of verses and stanzas upon what is represented. Although Lope has a great knowledge of the plastic arts, he did not use it during the major part of his career nor in theatre or scenography. The Lope's comedy granted a second role to the visual aspects of the theatrical representation.\n\nTirso de Molina, Lope de Vega, and Calderón were the most important play writers in Golden Era Spain. Their works, known for their subtle intelligence and profound comprehension of a person's humanity, could be considered a bridge between Lope's primitive comedy and the more elaborate comedy of Calderón. Tirso de Molina is best known for two works, \"The Convicted Suspicions\" and \"The Trickster of Seville\", one of the first versions of the Don Juan myth.\n\nUpon his arrival to Madrid, Cosimo Lotti brought to the Spanish court the most advanced theatrical techniques of Europe. His techniques and mechanic knowledge were applied in palace exhibitions called \"Fiestas\" and in lavish exhibitions of rivers or artificial fountains called \"Naumaquias\". He was in charge of styling the Gardens of Buen Retiro, of Zarzuela and of Aranjuez and the construction of the theatrical building of Coliseo del Buen Retiro. Lope's formulas begins with a verse that it unbefitting of the palace theatre foundation and the birth of new concepts that begun the careers of some play writers like Calderón de la Barca. Marking the principal innovations of the New Lopesian Comedy, Calderón's style marked many differences, with a great deal of constructive care and attention to his internal structure. Calderón's work is in formal perfection and a very lyric and symbolic language. Liberty, vitality and openness of Lope gave a step to Calderón's intellectual reflection and formal precision. In his comedy it reflected his ideological and doctrine intentions in above the passion and the action, the work of Autos sacramentales achieved high ranks. The genre of Comedia is political, multi-artistic and in a sense hybrid. The poetic text interweaved with Medias and resources originating from architecture, music and painting freeing the deception that is in the Lopesian comedy was made up from the lack of scenery and engaging the dialogue of action.\n\nThe most important English authors of the 17th century were playwright William Shakespeare and epic poet John Milton.\n\nIn France it was a brilliant period known as Grand Siècle. Molière, Pierre Corneille and Jean Racine wrote famous plays while Jean de La Fontaine and Charles Perrault – fables.\n\nBaroque was the greatest era in the history of Spanish literature which is called Siglo de Oro with playwrights Pedro Calderon de la Barca and Lope de Vega, poet Juana Inés de la Cruz as well as Miguel de Cervantes who is regarded as the first novelist.\n\nRené Descartes, John Locke, Gottfried Wilhelm Leibniz, Baruch Spinoza, Thomas Hobbes, Francis Bacon, Galileo Galilei and Isaac Newton are the most appreciated thinkers of the 17th century. This period was characterised by mixing new ideas with religious tradition. Neostoicism of Justus Lipsius, scholasticism of Francisco Suárez and casuistry of Jesuits were predominant.\n\nThe term \"Baroque\" is also used to designate the style of music composed during a period that overlaps with that of Baroque art, but usually encompasses a slightly later period.\n\nIt is a still-debated question as to what extent Baroque music shares aesthetic principles with the visual and literary arts of the Baroque period. A fairly clear, shared element is a love of ornamentation, and it is perhaps significant that the role of ornament was greatly diminished in both music and architecture as the Baroque gave way to the Classical period.\n\nThe application of the term \"Baroque\" to music is a relatively recent development, although it has recently been pointed out that the first use of the word \"baroque\" in criticism of any of the arts related to music, in an anonymous, satirical review of the première in October 1733 of Rameau's \"Hippolyte et Aricie,\" printed in the \"Mercure de France\" in May 1734. The critic implied that the novelty in this opera was \"du barocque,\" complaining that the music lacked coherent melody, was filled with unremitting dissonances, constantly changed key and meter, and speedily ran through every compositional device.\n\nHowever this was an isolated reference, and consistent use of the term as a period designator was only begun in 1919, by Curt Sachs, and it was not until 1940 that it was first used in English (in an article published by Manfred Bukofzer).\n\nMany musical forms were born in that era, like the concerto and sinfonia. Forms such as the sonata, cantata and oratorio flourished. Also, opera was born out of the experimentation of the Florentine Camerata, the creators of monody, who attempted to recreate the theatrical arts of the Ancient Greeks. An important technique used in baroque music was the use of ground bass, a repeated bass line. \"Dido's Lament\" by Henry Purcell is a famous example of this technique.\n\n\n", "id": "3957", "title": "Baroque"}
{"url": "https://en.wikipedia.org/wiki?curid=3959", "text": "Boolean algebra (structure)\n\nIn abstract algebra, a Boolean algebra or Boolean lattice is a complemented distributive lattice. This type of algebraic structure captures essential properties of both set operations and logic operations. A Boolean algebra can be seen as a generalization of a power set algebra or a field of sets, or its elements can be viewed as generalized truth values. It is also a special case of a De Morgan algebra and a Kleene algebra (with involution).\n\nEvery Boolean algebra gives rise to a Boolean ring, and vice versa, with ring multiplication corresponding to conjunction or meet ∧, and ring addition to exclusive disjunction or symmetric difference (not disjunction ∨). However, the theory of Boolean rings has an inherent asymmetry between the two operators, while the axioms and theorems of Boolean algebra express the symmetry of the theory described by the duality principle.\n\nThe term \"Boolean algebra\" honors George Boole (1815–1864), a self-educated English mathematician. He introduced the algebraic system initially in a small pamphlet, \"The Mathematical Analysis of Logic\", published in 1847 in response to an ongoing public controversy between Augustus De Morgan and William Hamilton, and later as a more substantial book, \"The Laws of Thought\", published in 1854. Boole's formulation differs from that described above in some important respects. For example, conjunction and disjunction in Boole were not a dual pair of operations. Boolean algebra emerged in the 1860s, in papers written by William Jevons and Charles Sanders Peirce. The first systematic presentation of Boolean algebra and distributive lattices is owed to the 1890 \"Vorlesungen\" of Ernst Schröder. The first extensive treatment of Boolean algebra in English is A. N. Whitehead's 1898 \"Universal Algebra\". Boolean algebra as an axiomatic algebraic structure in the modern axiomatic sense begins with a 1904 paper by Edward V. Huntington. Boolean algebra came of age as serious mathematics with the work of Marshall Stone in the 1930s, and with Garrett Birkhoff's 1940 \"Lattice Theory\". In the 1960s, Paul Cohen, Dana Scott, and others found deep new results in mathematical logic and axiomatic set theory using offshoots of Boolean algebra, namely forcing and Boolean-valued models.\n\nA Boolean algebra is a six-tuple consisting of a set \"A\", equipped with two binary operations ∧ (called \"meet\" or \"and\"), ∨ (called \"join\" or \"or\"), a unary operation ¬ (called \"complement\" or \"not\") and two elements 0 and 1 (called \"bottom\" and \"top\", or \"least\" and \"greatest\" element, also denoted by the symbols ⊥ and ⊤, respectively), such that for all elements \"a\", \"b\" and \"c\" of \"A\", the following axioms hold:\nNote, however, that the absorption law can be excluded from the set of axioms as it can be derived by the other axioms.\n\nA Boolean algebra with only one element is called a trivial Boolean algebra or a degenerate Boolean algebra. (Some authors require 0 and 1 to be \"distinct\" elements in order to exclude this case.)\n\nIt follows from the last three pairs of axioms above (identity, distributivity and complements), or from the absorption axiom, that\nThe relation ≤ defined by \"a\" ≤ \"b\" if these equivalent conditions hold, is a partial order with least element 0 and greatest element 1. The meet \"a\" ∧ \"b\" and the join \"a\" ∨ \"b\" of two elements coincide with their infimum and supremum, respectively, with respect to ≤.\n\nThe first four pairs of axioms constitute a definition of a bounded lattice.\n\nIt follows from the first five pairs of axioms that any complement is unique.\n\nThe set of axioms is self-dual in the sense that if one exchanges ∨ with ∧ and 0 with 1 in an axiom, the result is again an axiom. Therefore, by applying this operation to a Boolean algebra (or Boolean lattice), one obtains another Boolean algebra with the same elements; it is called its dual.\n\n\n\n\nA \"homomorphism\" between two Boolean algebras \"A\" and \"B\" is a function \"f\" : \"A\" → \"B\" such that for all \"a\", \"b\" in \"A\":\n\nIt then follows that \"f\"(¬\"a\") = ¬\"f\"(\"a\") for all \"a\" in \"A\". The class of all Boolean algebras, together with this notion of morphism, forms a full subcategory of the category of lattices.\n\nEvery Boolean algebra (A, ∧, ∨) gives rise to a ring (\"A\", +, ·) by defining \"a\" + \"b\" := (\"a\" ∧ ¬\"b\") ∨ (\"b\" ∧ ¬\"a\") = (\"a\" ∨ \"b\") ∧ ¬(\"a\" ∧ \"b\") (this operation is called symmetric difference in the case of sets and XOR in the case of logic) and \"a\" · \"b\" := \"a\" ∧ \"b\". The zero element of this ring coincides with the 0 of the Boolean algebra; the multiplicative identity element of the ring is the 1 of the Boolean algebra. This ring has the property that \"a\" · \"a\" = \"a\" for all \"a\" in \"A\"; rings with this property are called Boolean rings.\n\nConversely, if a Boolean ring \"A\" is given, we can turn it into a Boolean algebra by defining \"x\" ∨ \"y\" := \"x\" + \"y\" + (\"x\" · \"y\") and \"x\" ∧ \"y\" := \"x\" · \"y\".\n\nSince these two constructions are inverses of each other, we can say that every Boolean ring arises from a Boolean algebra, and vice versa. Furthermore, a map \"f\" : \"A\" → \"B\" is a homomorphism of Boolean algebras if and only if it is a homomorphism of Boolean rings. The categories of Boolean rings and Boolean algebras are equivalent.\n\nHsiang (1985) gave a rule-based algorithm to check whether two arbitrary expressions denote the same value in every Boolean ring.\n\nMore generally, Boudet, Jouannaud, and Schmidt-Schauß (1989) gave an algorithm to solve equations between arbitrary Boolean-ring expressions.\nEmploying the similarity of Boolean rings and Boolean algebras, both algorithms have applications in automated theorem proving.\n\nAn \"ideal\" of the Boolean algebra \"A\" is a subset \"I\" such that for all \"x\", \"y\" in \"I\" we have x ∨ y in \"I\" and for all \"a\" in \"A\" we have \"a\" ∧ \"x\" in \"I\". This notion of ideal coincides with the notion of ring ideal in the Boolean ring \"A\". An ideal \"I\" of \"A\" is called \"prime\" if \"I\" ≠ \"A\" and if \"a\" ∧ \"b\" in \"I\" always implies \"a\" in \"I\" or \"b\" in \"I\". Furthermore, for every \"a\" ∈ \"A\" we have that \"a\" ∧ \"-a\" = 0 ∈ \"I\" and then \"a\" ∈ \"I\" or \"-a\" ∈ \"I\" for every \"a\" ∈ \"A\", if \"I\" is prime. An ideal \"I\" of \"A\" is called \"maximal\" if \"I\" ≠ \"A\" and if the only ideal properly containing \"I\" is \"A\" itself. For an ideal \"I\", if \"a\" ∉ \"I\" and \"-a\" ∉ \"I\", then \"I\" ∪ {\"a\"} or \"I\" ∪ {\"-a\"} is properly contained in another ideal \"J\". Hence, that an \"I\" is not maximal and therefore the notions of prime ideal and maximal ideal are equivalent in Boolean algebras. Moreover, these notions coincide with ring theoretic ones of prime ideal and maximal ideal in the Boolean ring \"A\".\n\nThe dual of an \"ideal\" is a \"filter\". A \"filter\" of the Boolean algebra \"A\" is a subset \"p\" such that for all \"x\", \"y\" in \"p\" we have \"x\" ∧ \"y\" in \"p\" and for all \"a\" in \"A\" we have \"a\" ∨ \"x\" in \"p\". The dual of a \"maximal\" (or \"prime\") \"ideal\" in a Boolean algebra is \"ultrafilter\". Ultrafilters can alternatively be described as 2-valued morphisms from \"A\" to the two-element Boolean algebra. The statement \"every filter in a Boolean algebra can be extended to an ultrafilter\" is called the \"Ultrafilter Theorem\" and can not be proved in ZF, if ZF is consistent. Within ZF, it is strictly weaker than the axiom of choice.\nThe Ultrafilter Theorem has many equivalent formulations: \"every Boolean algebra has an ultrafilter\", \"every ideal in a Boolean algebra can be extended to a prime ideal\", etc.\n\nIt can be shown that every \"finite\" Boolean algebra is isomorphic to the Boolean algebra of all subsets of a finite set. Therefore, the number of elements of every finite Boolean algebra is a power of two.\n\nStone's celebrated \"representation theorem for Boolean algebras\" states that \"every\" Boolean algebra \"A\" is isomorphic to the Boolean algebra of all clopen sets in some (compact totally disconnected Hausdorff) topological space.\n\nThe first axiomatization of Boolean lattices/algebras in general was given by Alfred North Whitehead in 1898.\nIt included the above axioms and additionally \"x\"∨1=1 and \"x\"∧0=0.\nIn 1904, the American mathematician Edward V. Huntington (1874–1952) gave probably the most parsimonious axiomatization based on ∧, ∨, ¬, even proving the associativity laws (see box).\nHe also proved that these axioms are independent of each other.\nIn 1933, Huntington set out the following elegant axiomatization for Boolean algebra. It requires just one binary operation + and a unary functional symbol \"n\", to be read as 'complement', which satisfy the following laws:\n\n\nHerbert Robbins immediately asked: If the Huntington equation is replaced with its dual, to wit:\n\ndo (1), (2), and (4) form a basis for Boolean algebra? Calling (1), (2), and (4) a \"Robbins algebra\", the question then becomes: Is every Robbins algebra a Boolean algebra? This question (which came to be known as the Robbins conjecture) remained open for decades, and became a favorite question of Alfred Tarski and his students. In 1996, William McCune at Argonne National Laboratory, building on earlier work by Larry Wos, Steve Winker, and Bob Veroff, answered Robbins's question in the affirmative: Every Robbins algebra is a Boolean algebra. Crucial to McCune's proof was the automated reasoning program EQP he designed. For a simplification of McCune's proof, see Dahn (1998).\n\nRemoving the requirement of existence of a unit from the axioms of Boolean algebra yields \"generalized Boolean algebras\". Formally, a distributive lattice \"B\" is a generalized Boolean lattice, if it has a smallest element 0 and for any elements \"a\" and \"b\" in \"B\" such that \"a\" ≤ \"b\", there exists an element \"x\" such that a ∧ x = 0 and a ∨ x = b. Defining a ∖ b as the unique \"x\" such that (a ∧ b) ∨ x = a and (a ∧ b) ∧ x = 0, we say that the structure (B,∧,∨,∖,0) is a \"generalized Boolean algebra\", while (B,∨,0) is a \"generalized Boolean semilattice\". Generalized Boolean lattices are exactly the ideals of Boolean lattices.\n\nA structure that satisfies all axioms for Boolean algebras except the two distributivity axioms is called an orthocomplemented lattice. Orthocomplemented lattices arise naturally in quantum logic as lattices of closed subspaces for separable Hilbert spaces.\n\n\n\n\nA monograph available free online:\n", "id": "3959", "title": "Boolean algebra (structure)"}
{"url": "https://en.wikipedia.org/wiki?curid=3960", "text": "Bank of Italy\n\nThe Bank of Italy, known in Italian as Banca d'Italia (), also known as Bankitalia, is the central bank of Italy and part of the European System of Central Banks. It is located in Palazzo Koch, via Nazionale, Rome. The bank's current governor is Ignazio Visco, who took the office on 1 November 2011.\n\nAfter the charge of monetary and exchange rate policies was shifted in 1998 to the European Central Bank, within the European institutional framework, the bank implements the decisions, issues euro banknotes and withdraws and destroys worn pieces.\n\nThe main function has thus become banking and financial supervision. The objective is to ensure the stability and efficiency of the system and compliance to rules and regulations; the bank pursues it through secondary legislation, controls and cooperation with governmental authorities.\n\nFollowing reform in 2005, which was prompted by takeover scandals, the bank has lost exclusive antitrust authority in the credit sector, which is now shared with Italian Competition Authority ().\n\nOther functions include, market supervision, oversight of the payment system and provision of settlement services, State treasury service, Central Credit Register, economic analysis and institutional consultancy.\n\nBank of Italy gold reserves are 2,451.8 tonnes (2006).\n\nThe institution was established in 1893 from the combination of three major banks in Italy (after the Banca Romana scandal). The new central bank first issued bank-notes during 1926. Until 1928, it was directed by a general manager, after this time instead by a governor elected by an internal commission of managers, with a decree from the President of the Italian Republic, for a term of seven years.\n\n\n\nThe Bank's governing bodies are the General Meeting of Shareholders, the Board of Directors, the Governor, the Director General and three Deputy Directors General; the last five constitute the Directorate. \n\nThe general meeting takes place yearly and with the purpose of approving accounts and appointing the auditors. The Board of Directors has administrative powers and is chaired by the governor (or by the Director General in his absence). Following reform in 2005, the governor lost exclusive responsibility regarding decisions of external relevance (i.e. banking and financial supervision), which has been transferred to the Directorate (by majority vote). The Director General is responsible for the day-to-day administration of the bank, and acts as governor when absent.\n\nThe Board of Auditors assesses the bank's administration and compliance with the law, regulations and the statute.\n\nThe Directorate's term of office lasts six years and is renewable once. The appointment of the governor is the responsibility of the government, head of the Board of Directors, with the approval of the President of the Republic (formally a decree of the President). The Board of Directors is elected by the shareholders according to the bank statute.\n\nOn 25 October 2011, Silvio Berlusconi named Ignazio Visco as new governor of the bank, replacing Mario Draghi.\n\nBanca d'Italia had 300,000 number of shares with a nominal value of €25,000. Original scattered around the banks of whole Italy, the shares now accumulated due to the merger of the banks since 1990s. The status of the bank states that a minimum of 54% of profits would go to the Italian government, and only a maximum of 6% of profits would distributed as dividends according to shares ratio.\n\n", "id": "3960", "title": "Bank of Italy"}
{"url": "https://en.wikipedia.org/wiki?curid=3962", "text": "British\n\nBritish may refer to:\n\n\n\n", "id": "3962", "title": "British"}
{"url": "https://en.wikipedia.org/wiki?curid=3963", "text": "Beachcomber (pen name)\n\nBeachcomber was the \"nom de plume\" used by two humorous columnists, D. B. Wyndham Lewis and, chiefly, J. B. Morton, as authors of the \"Daily Express\" column \"By the Way\" in the period 1919–1975. Two other authors have also used the name: Major John Bernard Arbuthnot MVO, the column's founder, and William Hartston, the current author of the column in its revived form.\n\n\"By the Way\" was originally a column in \"The Globe\", consisting of unsigned humorous pieces; P. G. Wodehouse was assistant editor of the column from August 1903 and editor from August 1904 to May 1909, during which time he was assisted by Herbert Westbrook. After the \"Globe\"'s closure, it was reestablished as a society news column in the \"Daily Express\" from 1917 onwards, initially written by social correspondent Major John Arbuthnot, who invented the name \"Beachcomber\".\n\nAfter Arbuthnot was promoted to deputy editor, it was taken over sometime in 1919 by Wyndham-Lewis, who reinvented it as an outlet for his wit and humour. It was then passed to Morton during 1924, though it is likely there was a period when they overlapped. Morton wrote the column until 1975; it was revived in January 1996 and continues today, written by William Hartston. The column is unsigned except by \"Beachcomber\" and it was not publicly known that Morton or Wyndham-Lewis wrote it until the 1930s. The name is mainly associated with Morton, who has been credited as an influence by Spike Milligan amongst others. Morton introduced the recurring characters and continuing stories that were a major feature of the column during his 51-year run.\n\nThe format of the column was a random assortment of small paragraphs which were otherwise unconnected. These could be anything, such as:\n\n\nMorton's other interest, France, was occasionally represented by epic tales of his rambling walks through the French countryside. These were not intended as humour.\n\n\"By the Way\" was popular with the readership, and of course, one of the reasons it lasted so long. Its style and randomness could be off-putting, however, and it is safe to say the humour could be something of an acquired taste. Oddly, one of the column's greatest opponents was the \"Express\" newspaper's owner, Lord Beaverbrook, who had to keep being assured the column was indeed funny. A prominent critic was George Orwell, who frequently referred to him in his essays and diaries as \"A Catholic Apologist\" and accused him of being \"silly-clever\", in line with his criticisms of G. K. Chesterton, Hilaire Belloc, Ronald Knox and Wyndham-Lewis. \n\nBut \"By the Way\" was one of the few features kept continuously running in the often seriously reduced \"Daily Express\" throughout World War II, when Morton's lampooning of Hitler, including the British invention of bracerot to make the Nazi's trousers fall down at inopportune moments, was regarded as valuable for morale.\n\nThe column appeared daily until 1965 when it was changed to weekly. It was cancelled in 1975 and revived as a daily piece in the early 1990s. It continues to the present day in much the same format, but is now entitled \"Beachcomber\", not \"By the Way\".\n\n\nThe Will Hay film \"Boys Will Be Boys\" (1935) was set at Morton's Narkover school.\n\nAccording to Spike Milligan, the columns had been an influence on the comedic style of his radio series, \"The Goon Show\". \n\nIn 1969, Milligan based a BBC television series named \"The World of Beachcomber\" on the columns. A small selection was issued on a 1971 LP and a 2-cassette set of the series' soundtrack was made available in the late 1990s.\n\nIn 1989, BBC Radio 4 broadcast the first of three series based on Morton's work. This featured Richard Ingrams, John Wells, Patricia Routledge and John Sessions playing the perennial Prodnose. The compilations prepared by Mike Barfield. Series 1 was also made available as a 2-cassette set.\n\n\n\n\n", "id": "3963", "title": "Beachcomber (pen name)"}
{"url": "https://en.wikipedia.org/wiki?curid=3965", "text": "Bill Joy\n\nWilliam Nelson \"Bill\" Joy (born November 8, 1954) is an American computer scientist. Joy co-founded Sun Microsystems in 1982 along with Vinod Khosla, Scott McNealy and Andreas von Bechtolsheim, and served as chief scientist at the company until 2003. He played an integral role in the early development of BSD UNIX while a graduate student at Berkeley, and he is the original author of the vi text editor. He also wrote the 2000 essay \"Why the Future Doesn't Need Us\", in which he expressed deep concerns over the development of modern technologies.\n\nJoy was born in the Detroit suburb of Farmington Hills, Michigan, to William Joy, a school vice-principal and counselor, and Ruth Joy. Joy received a Bachelor of Science in electrical engineering from the University of Michigan and a Master of Science in electrical engineering and computer science from the University of California, Berkeley in 1979. Joy's graduate advisor was Bob Fabry.\n\nAs a UC Berkeley graduate student, Joy worked for Fabry's Computer Systems Research Group CSRG on the Berkeley Software Distribution (BSD) version of the Unix operating system. He initially worked on a Pascal compiler left at Berkeley by Ken Thompson, who had been visiting the University when Joy had just started his graduate work. He later moved on to improving the Unix kernel, and also handled BSD distributions. Some of his most notable contributions were the ex and vi editors and csh. Joy's prowess as a computer programmer is legendary, with an oft-told anecdote that he wrote the vi editor in a weekend. Joy denies this assertion. Other of his accomplishments have also been sometimes exaggerated; Eric Schmidt, CEO of Novell at the time, inaccurately reported during an interview in PBS's documentary \"Nerds 2.0.1\" that Joy had personally rewritten the BSD kernel in a weekend.\n\nAccording to a \"Salon\" article, during the early 1980s, DARPA had contracted the company Bolt, Beranek and Newman (BBN) to add TCP/IP to Berkeley UNIX. Joy had been instructed to plug BBN's stack into Berkeley Unix, but he refused to do so, as he had a low opinion of BBN's TCP/IP. So, Joy wrote his own high-performance TCP/IP stack. According to John Gage,\n\nBBN had a big contract to implement TCP/IP, but their stuff didn't work, and grad student Joy's stuff worked. So they had this big meeting and this grad student in a T-shirt shows up, and they said, \"How did you do this?\" And Bill said, \"It's very simple — you read the protocol and write the code.\"\n\nRob Gurwitz, who was working at BBN at the time, disputes this version of events.\n\nIn 1982, after the firm had been going for six months, Joy was brought in with full co-founder status at Sun Microsystems. At Sun, Joy was an inspiration for the development of NFS, the SPARC microprocessors, the Java programming language, Jini / JavaSpaces and JXTA.\n\nIn 1986, Joy was awarded a Grace Murray Hopper Award by the ACM for his work on the Berkeley UNIX Operating System.\n\nOn September 9, 2003 Sun announced that Bill Joy was leaving the company and that he \"is taking time to consider his next move and has no definite plans\".\n\nIn 1999, Joy co-founded a venture capital firm, HighBAR Ventures, with two Sun colleagues, Andreas von Bechtolsheim and Roy Thiele-Sardiña. In January 2005 he was named a partner in venture capital firm Kleiner Perkins Caufield & Byers. There, Joy has made several investments in green energy industries, even though he does not have any credentials in the field. He once said, \"My method is to look at something that seems like a good idea and assume it's true\".\n\nIn 2011, he was inducted as a Fellow of the Computer History Museum for his work on the Berkeley Software Distribution (BSD) Unix system and the co-founding of Sun Microsystems.\n\nIn 2000, Joy gained notoriety with the publication of his article in \"Wired Magazine\", \"Why the future doesn't need us\", in which he declared, in what some have described as a \"neo-Luddite\" position, that he was convinced that growing advances in genetic engineering and nanotechnology would bring risks to humanity. He argued that intelligent robots would replace humanity, at the very least in intellectual and social dominance, in the relatively near future. He advocates a position of relinquishment of GNR (genetics, nanotechnology, and robotics) technologies, rather than going into an arms race between negative uses of the technology and defense against those negative uses (good nano-machines patrolling and defending against Grey Goo \"bad\" nano-machines). This stance of broad relinquishment was criticized by technologists such as technological-singularity thinker Ray Kurzweil, who instead advocates fine-grained relinquishment and ethical guidelines. Joy was also criticized by the conservative \"American Spectator\", which characterized Joy's essay as a (possibly unwitting) rationale for statism.\n\nA bar-room discussion of these technologies with Ray Kurzweil started to set Joy's thinking along this path. He states in his essay that during the conversation, he became surprised that other serious scientists were considering such possibilities likely, and even more astounded at what he felt was a lack of consideration of the contingencies. After bringing the subject up with a few more acquaintances, he states that he was further alarmed by what he felt was the fact that although many people considered these futures possible or probable, that very few of them shared as serious a concern for the dangers as he seemed to. This concern led to his in-depth examination of the issue and the positions of others in the scientific community on it, and eventually, to his current activities regarding it.\n\nDespite this, he is a venture capitalist, investing in GNR technology companies. He has also raised a specialty venture fund to address the dangers of pandemic diseases, such as the H5N1 avian influenza and biological weapons.\n\nIn his 2013 book \"Makers\", author Chris Anderson credited Joy with establishing \"Joy's theory\" based on a quip: \"No matter who you are, most of the smartest people work for someone else [other than you].\" His argument was that companies use an inefficient process by not hiring the best employees, only those they are able to hire. His \"law\" was a continuation of Friedrich Hayek's \"\"The Use of Knowledge in Society\"\" and warned that the competition outside of a company would always have the potential to be greater than the company itself.\n\n", "id": "3965", "title": "Bill Joy"}
{"url": "https://en.wikipedia.org/wiki?curid=3967", "text": "Bandwidth (signal processing)\n\nBandwidth is the difference between the upper and lower frequencies in a continuous set of frequencies. It is typically measured in hertz, and may sometimes refer to \"passband bandwidth\", sometimes to \"baseband bandwidth\", depending on context. Passband bandwidth is the difference between the upper and lower cutoff frequencies of, for example, a band-pass filter, a communication channel, or a signal spectrum. In the case of a low-pass filter or baseband signal, the bandwidth is equal to its upper cutoff frequency.\n\nBandwidth in hertz is a central concept in many fields, including electronics, information theory, digital communications, radio communications, signal processing, and spectroscopy and is one of the determinants of the capacity of a given communication channel.\n\nA key characteristic of bandwidth is that any band of a given width can carry the same amount of information, regardless of where that band is located in the frequency spectrum. For example, a 3 kHz band can carry a telephone conversation whether that band is at baseband (as in a POTS telephone line) or modulated to some higher frequency.\n\nBandwidth is a key concept in many telecommunications applications. In radio communications, for example, bandwidth is the frequency range occupied by a modulated carrier signal. An FM radio receiver's tuner spans a limited range of frequencies. A government agency (such as the Federal Communications Commission in the United States) may apportion the regionally available bandwidth to broadcast license holders so that their signals do not mutually interfere. Each transmitter owns a slice of bandwidth.\n\nFor different applications there are different precise definitions, which are necessarily different for signals than for systems. One definition of bandwidth, for a system, could be the range of frequencies over which the system produces a specified level of performance. A less strict and more practically useful definition will refer to the frequencies beyond which frequency response is \"small\". Small could mean less than 3 dB below the maximum value, or more rarely 10 dB below, or it could mean below a certain absolute value. As with any definition of the \"width\" of a function, many definitions are suitable for different purposes.\n\nBandwidth typically refers to baseband bandwidth in the context of, for example, the sampling theorem and Nyquist sampling rate, while it refers to passband bandwidth in the context of Nyquist symbol rate or Shannon-Hartley channel capacity for communication systems.\n\nIn some contexts, the signal bandwidth in hertz refers to the frequency range in which the signal's spectral density (in W/Hz or V/Hz) is nonzero or above a small threshold value. That definition is used in calculations of the lowest sampling rate that will satisfy the sampling theorem. The threshold value is often defined relative to the maximum value, and is most commonly the 3dB point, that is the point where the spectral density is half its maximum value (or the spectral amplitude, in V or V/Hz, is 70.7% of its maximum).\n\nThe word bandwidth applies to signals as described above, but it could also apply to \"systems\", for example filters or communication channels. To say that a system has a certain bandwidth means that the system can process signals of that bandwidth, or that the system reduces the bandwidth of a white noise input to that bandwidth.\n\nThe 3 dB bandwidth of an electronic filter or communication channel is the part of the system's frequency response that lies within 3 dB of the response at its peak, which in the passband filter case is typically at or near its center frequency, and in the lowpass filter is near 0 hertz. If the maximum gain is 0 dB, the 3 dB bandwidth is the frequency range where the gain is more than −3 dB, or the attenuation is less than 3 dB. This is also the range of frequencies where the amplitude gain is above 70.7% of the maximum amplitude gain, and the power gain is above half the maximum power gain. This same \"half power gain\" convention is also used in spectral width, and more generally for extent of functions as full width at half maximum (FWHM).\n\nIn electronic filter design, a filter specification may require that within the filter passband, the gain is nominally 0 dB ± a small number of dB, for example within the ±1 dB interval. In the stopband(s), the required attenuation in dB is above a certain level, for example >100 dB. In a transition band the gain is not specified. In this case, the filter bandwidth corresponds to the passband width, which in this example is the 1 dB-bandwidth. If the filter shows amplitude ripple within the passband, the \"x\" dB point refers to the point where the gain is \"x\" dB below the nominal passband gain rather than \"x\" dB below the maximum gain.\n\nA commonly used quantity is fractional bandwidth. This is the bandwidth of a device divided by its center frequency. E.g., a passband filter that has a bandwidth of 2 MHz with center frequency 10 MHz will have a fractional bandwidth of 2/10, or 20%.\n\nIn communication systems, in calculations of the Shannon–Hartley channel capacity, bandwidth refers to the 3 dB-bandwidth. In calculations of the maximum symbol rate, the Nyquist sampling rate, and maximum bit rate according to the Hartley formula, the bandwidth refers to the frequency range within which the gain is non-zero, or the gain in dB is below a very large value.\n\nThe fact that in equivalent baseband models of communication systems, the signal spectrum consists of both negative and positive frequencies, can lead to confusion about bandwidth, since they are sometimes referred to only by the positive half, and one will occasionally see expressions such as formula_1, where formula_2 is the total bandwidth (i.e. the maximum passband bandwidth of the carrier-modulated RF signal and the minimum passband bandwidth of the physical passband channel), and formula_3 is the positive bandwidth (the baseband bandwidth of the equivalent channel model). For instance, the baseband model of the signal would require a lowpass filter with cutoff frequency of at least formula_3 to stay intact, and the physical passband channel would require a passband filter of at least formula_2 to stay intact.\n\nIn signal processing and control theory the bandwidth is the frequency at which the closed-loop system gain drops 3 dB below peak.\n\nIn basic electric circuit theory, when studying band-pass and band-reject filters, the bandwidth represents the distance between the two points in the frequency domain where the signal is formula_6 of the maximum signal amplitude (half power).\n\nIn the field of antennas, two different methods of expressing relative bandwidth are used for narrowband and wideband antennas. For either, a set of criteria is established to define the extents of the bandwidth, such as input impedance, pattern, or polarization.\n\n\"Percent bandwidth\", usually used for narrowband antennas, is used defined as formula_7. The theoretical limit to percent bandwidth is 200%, which occurs for formula_8.\n\n\"Fractional bandwidth or Ratio bandwidth\", usually used for wideband antennas, is defined as formula_9 and is typically presented in the form of formula_10. Fractional bandwidth is used for wideband antennas because of the compression of the percent bandwidth that occurs mathematically with percent bandwidths above 100%, which corresponds to a fractional bandwidth of 3:1.\n\nIf formula_11\n\nthen formula_12.\n\nIn photonics, the term \"bandwidth\" occurs in a variety of meanings:\n\nA related concept is the spectral linewidth of the radiation emitted by excited atoms.\n\n", "id": "3967", "title": "Bandwidth (signal processing)"}
{"url": "https://en.wikipedia.org/wiki?curid=3968", "text": "Bodhisattva\n\nIn Buddhism, bodhisattva is the Sanskrit term for anyone who, motivated by great compassion, has generated bodhicitta, which is a spontaneous wish and a compassionate mind to attain buddhahood for the benefit of all sentient beings. Bodhisattvas are a popular subject in Buddhist art.\n\nIn early Indian Buddhism, the term \"bodhisattva\" was primarily used to refer specifically to Gautama Buddha in his former life. The Jataka tales, which are the stories of the Buddha's past lives, depict the various attempts of the bodhisattva to embrace qualities like self-sacrifice and morality.\n\nAccording to the Jataka tales, the term \"bodhisattva\" originally referred to the pre-enlightened practitioner of austerities that surpassed Śrāvakayana and Pratyekabuddhayana by far and completed Bodhisattvayana. Mount Potalaka, for example, is one of Bodhisattvayana. The term for practitioners who have not yet reached Bodhisattvayana was not fixed, but the terms \"Śrāvaka-Bodhisattva\" (聲聞菩薩) and \"Pratyekabuddha-Bodhisattva\" (縁覚菩薩) had already appeared in the Āgama scriptures of early Indian Buddhism.\n\nMahayana Buddhism did not place much emphasis in honoring Śrāvakayana and Pratyekabuddhayana since they were classified as part of the Hinayana, but praise of the general Bodhisattvayana was commonplace. Because Hinayana was disliked and the terms Śrāvaka-Bodhisattva or Pratyekabuddha-Bodhisattva were not widely used, while usage of the general term \"bodhisattva\" had grown in popularity. Nevertheless, \"bodhisattva\" retained an implied reference to someone on the path to become an arhat or pratyekabuddha. In contrast, the goal of Mahayana's bodhisattva path is to achieve \"Samyaksambodhiṃ.\" \n\nIn Theravada Buddhism, the equivalent Pali term \"bodhisatta\" is used in the Pāli Canon to refer to Gautama Buddha in his previous lives and as a young man in his current life in the period during which he was working towards his own liberation. When, during his discourses, he recounts his experiences as a young aspirant, he regularly uses the phrase \"When I was an unenlightened bodhisatta...\" The term therefore connotes a being who is \"bound for enlightenment\", in other words, a person whose aim is to become fully enlightened. In the Pāli canon, the bodhisatta is also described as someone who is still subject to birth, illness, death, sorrow, defilement, and delusion. Some of the previous lives of the Buddha as a bodhisattva are featured in the Jataka tales.\n\nAccording to the Theravāda monk Bhikkhu Bodhi, the bodhisattva path is not taught in the earliest strata of Buddhist texts such as the Pali Nikayas (and their counterparts such as the Chinese Āgamas) which instead focus on the ideal of the Arahant. In later Theravada literature, the term \"bodhisatta\" is used fairly frequently in the sense of someone on the path to liberation. The later tradition of commentary also recognizes the existence of two additional types of bodhisattas: the \"paccekabodhisatta\", who will attain Paccekabuddhahood, and the \"savakabodhisatta\", who will attain enlightenment as a disciple of a Buddha. In the 1st-2nd century BCE Sri Lankan work, the Buddhavamsa, the idea of the person who makes a Bodhisatta vow to become a fully enlightened Buddha out of compassion for all sentient beings is presented. Another related concept outlined in the Buddhavamsa and in another text called the Cariyapitaka is the need to cultivate certain Bodhisatta perfections or paramitas\n\nKings of Sri Lanka were often described as bodhisattvas, starting at least as early as Sirisanghabodhi (r. 247-249), who was renowned for his compassion, who took vows for the welfare of the citizens, and was regarded as a mahāsatta (Sanskrit \"mahāsattva\"), an epithet used almost exclusively in Mahayana Buddhism. Many other kings of Sri Lanka from the 3rd century until the 15th century were also described as bodhisattvas and their royal duties were sometimes clearly associated with the practice of the Ten Pāramitās.\n\nTheravadin bhikkhu and scholar Walpola Rahula stated that the bodhisattva ideal has traditionally been held to be higher than the state of a \"śrāvaka\" not only in Mahayana but also in Theravada Buddhism. He also quotes an inscription from the 10th Century king of Sri Lanka, Mahinda IV (956-972 CE), who had the words inscribed \"none but the bodhisattvas will become kings of a prosperous Lanka,\" among other examples.\nPaul Williams writes that some modern Theravada meditation masters in Thailand are popularly regarded as bodhisattvas.\n\nAccording to Jeffrey Samuels, it \"may more accurately portray the differences that exist between the two yanas by referring to Mahayana Buddhism as a vehicle in which the bodhisattva ideal is more universally applied, and to Theravada Buddhism as a vehicle in which the bodhisattva ideal is reserved for and appropriated by certain exceptional people.\"\n\nMahāyāna Buddhism is based principally upon the path of a bodhisattva.\nAccording to Jan Nattier, the term \"Mahāyāna\" (\"Great Vehicle\") was originally even an honorary synonym for \"Bodhisattvayāna\", or the \"Bodhisattva Vehicle.\" The \"\" contains a simple and brief definition for the term \"bodhisattva\", which is also the earliest known Mahāyāna definition. This definition is given as the following.\n\nThe earliest depiction of the Bodhisattva path in texts such as the Ugraparipṛcchā Sūtra describe it as an arduous, difficult monastic path suited only for the few which is nevertheless the most glorious path one can take. Three kinds of Bodhisattvas are mentioned in the early Mahayana texts, the forest, city, and monastery Bodhisattvas - with forest dwelling being promoted a superior, even necessary path in sutras such as the Samadhiraja Sutra and the Ugraparipṛcchā Sūtra. The early Rastrapalapariprccha sutra also promotes a solitary life of meditation in the forests, far away from the distractions of the householder life. The Rastrapala is also highly critical of monks living in monasteries and in cities who are seen as not practicing meditation and morality. The Ratnagunasamcayagatha also says the Bodhisattva should undertake ascetic practices (dhutanga), \"wander freely without a home\", practice the paramitas and train under a guru in order to perfect his meditation practice and realization of prajnaparamita. These texts seem to indicate the initial Bodhisattva ideal was associated with a strict forest asceticism.\n\nMahāyāna Buddhism encourages everyone to become bodhisattvas and to take the bodhisattva vows. With these vows, one makes the promise to work for the complete enlightenment of all sentient beings by practicing the six perfections. Indelibly entwined with the bodhisattva vow is merit transference (\"pariṇāmanā\").\n\nIn Mahāyāna Buddhism life in this world is compared to people living in a house that is on fire. People take this world as reality pursuing worldly projects and pleasures without realizing that the house is on fire and will soon burn down (due to the inevitability of death). A bodhisattva is one who has a determination to free sentient beings from samsara and its cycle of death, rebirth and suffering. This type of mind is known as the mind of awakening (\"bodhicitta\"). Bodhisattvas take bodhisattva vows in order to progress on the spiritual path towards buddhahood.\n\nThere are a variety of different conceptions of the nature of a bodhisattva in Mahāyāna. According to some Mahāyāna sources a bodhisattva is someone on the path to full Buddhahood. Others speak of bodhisattvas renouncing Buddhahood. According to the \"Kun-bzang bla-ma'i zhal-lung\", a bodhisattva can choose any of three paths to help sentient beings in the process of achieving buddhahood. They are:\n\nAccording to the doctrine of some Tibetan schools (like Theravāda but for different reasons), only the first of these is recognized. It is held that Buddhas remain in the world, able to help others, so there is no point in delay. Geshe Kelsang Gyatso notes:\n\nIn reality, the second two types of bodhicitta are wishes that are impossible to fulfill because it is only possible to lead others to enlightenment once we have attained enlightenment ourself. Therefore, only king-like bodhicitta is actual bodhicitta. Je Tsongkhapa says that although the other Bodhisattvas wish for that which is impossible, their attitude is sublime and unmistaken.\nThe Nyingma school, however, holds that the lowest level is the way of the king, who primarily seeks his own benefit but who recognizes that his benefit depends crucially on that of his kingdom and his subjects. The middle level is the path of the boatman, who ferries his passengers across the river and simultaneously, of course, ferries himself as well. The highest level is that of the shepherd, who makes sure that all his sheep arrive safely ahead of him and places their welfare above his own.\n\nAccording to many traditions within Mahāyāna Buddhism, on the way to becoming a Buddha, a bodhisattva proceeds through ten, or sometimes fourteen, grounds or \"bhūmis.\" Below is the list of the ten \"bhūmis\" and their descriptions according to the \"Avataṃsaka Sūtra\" and \"The Jewel Ornament of Liberation,\" a treatise by Gampopa, an influential teacher of the Tibetan Kagyu school. (Other schools give slightly variant descriptions.)\n\nBefore a bodhisattva arrives at the first ground, he or she first must travel the first two of the five paths:\n\nThe ten grounds of the bodhisattva then can be grouped into the next three paths\n\nThe chapter of ten grounds in the \"Avataṃsaka Sūtra\" refers to 52 stages. The 10 grounds are:\n\n\nAfter the ten \"bhūmis\", according to Mahāyāna Buddhism, one attains complete enlightenment and becomes a Buddha.\n\nWith the 52 stages, the \"Śūraṅgama Sūtra\" recognizes 57 stages. With the 10 grounds, various Vajrayāna schools recognize 3–10 additional grounds, mostly 6 more grounds with variant descriptions.\n\nA bodhisattva above the 7th ground is called a \"mahāsattva\". Some bodhisattvas such as Samantabhadra are also said to have already attained buddhahood.\n\nSome sutras said a beginner would take 3–22 countless eons (\"mahāsaṃkhyeya kalpas\") to become a buddha. Pure Land Buddhism suggests buddhists go to the pure lands to practice as bodhisattvas. Tiantai, Huayan, Zen and Vajrayāna schools say they teach ways to attain buddhahood within one karmic cycle.\n\nVarious traditions within Buddhism believe in specific bodhisattvas. Some bodhisattvas appear across traditions, but due to language barriers may be seen as separate entities. For example, Tibetan Buddhists believe in various forms of Chenrezig, who is Avalokiteśvara in Sanskrit, Guanyin in China, Gwan-eum in Korea, Quan Am in Vietnam, and Kannon in Japan. Followers of Tibetan Buddhism consider the Dalai Lamas and the Karmapas to be an emanation of Chenrezig, the Bodhisattva of Compassion.\n\nKṣitigarbha is another popular bodhisattva in Japan and China. He is known for aiding those who are lost. His greatest compassionate vow is:\n\nThe place of a bodhisattva's earthly deeds, such as the achievement of enlightenment or the acts of Dharma, is known as a \"bodhimaṇḍa\", and may be a site of pilgrimage. Many temples and monasteries are famous as bodhimaṇḍas. Perhaps the most famous bodhimaṇḍa of all is the Bodhi Tree under which Śākyamuṇi achieved buddhahood. In the tradition of Chinese Buddhism, there are four mountains that are regarded as bodhimaṇḍas for bodhisattvas, with each site having major monasteries and being popular for pilgrimages by both monastics and laypeople. These four bodhimandas are:\n\n\n\n\n", "id": "3968", "title": "Bodhisattva"}
{"url": "https://en.wikipedia.org/wiki?curid=3969", "text": "Buckingham Palace\n\nBuckingham Palace () is the London residence and administrative headquarters of the reigning monarch of the United Kingdom. Located in the City of Westminster, the palace is often at the centre of state occasions and royal hospitality. It has been a focal point for the British people at times of national rejoicing and mourning.\n\nOriginally known as Buckingham House, the building at the core of today's palace was a large townhouse built for the Duke of Buckingham in 1703 on a site that had been in private ownership for at least 150 years. It was acquired by King George III in 1761 as a private residence for Queen Charlotte and became known as \"The Queen's House\". During the 19th century it was enlarged, principally by architects John Nash and Edward Blore, who constructed three wings around a central courtyard. Buckingham Palace became the London residence of the British monarch on the accession of Queen Victoria in 1837.\n\nThe last major structural additions were made in the late 19th and early 20th centuries, including the East front, which contains the well-known balcony on which the royal family traditionally congregates to greet crowds. The palace chapel was destroyed by a German bomb during World War II; the Queen's Gallery was built on the site and opened to the public in 1962 to exhibit works of art from the Royal Collection.\n\nThe original early 19th-century interior designs, many of which survive, include widespread use of brightly coloured scagliola and blue and pink lapis, on the advice of Sir Charles Long. King Edward VII oversaw a partial redecoration in a \"Belle Époque\" cream and gold colour scheme. Many smaller reception rooms are furnished in the Chinese regency style with furniture and fittings brought from the Royal Pavilion at Brighton and from Carlton House. The palace has 775 rooms, and the garden is the largest private garden in London. The state rooms, used for official and state entertaining, are open to the public each year for most of August and September and on some days in winter and spring.\n\nIn the Middle Ages, the site of the future palace formed part of the Manor of Ebury (also called Eia). The marshy ground was watered by the river Tyburn, which still flows below the courtyard and south wing of the palace. Where the river was fordable (at Cow Ford), the village of Eye Cross grew. Ownership of the site changed hands many times; owners included Edward the Confessor and his queen consort Edith of Wessex in late Saxon times, and, after the Norman Conquest, William the Conqueror. William gave the site to Geoffrey de Mandeville, who bequeathed it to the monks of Westminster Abbey.\n\nIn 1531, King Henry VIII acquired the Hospital of St James (later St James's Palace) from Eton College, and in 1536 he took the Manor of Ebury from Westminster Abbey. These transfers brought the site of Buckingham Palace back into royal hands for the first time since William the Conqueror had given it away almost 500 years earlier.\n\nVarious owners leased it from royal landlords and the freehold was the subject of frenzied speculation during the 17th century. By then, the old village of Eye Cross had long since fallen into decay, and the area was mostly wasteland. Needing money, James I sold off part of the Crown freehold but retained part of the site on which he established a mulberry garden for the production of silk. (This is at the northwest corner of today's palace.) Clement Walker in \"Anarchia Anglicana\" (1649) refers to \"new-erected sodoms and spintries at the Mulberry Garden at S. James's\"; this suggests it may have been a place of debauchery. Eventually, in the late 17th century, the freehold was inherited from the property tycoon Sir Hugh Audley by the great heiress Mary Davies.\n\nPossibly the first house erected within the site was that of a Sir William Blake, around 1624. The next owner was Lord Goring, who from 1633 extended Blake's house and developed much of today's garden, then known as Goring Great Garden. He did not, however, obtain the freehold interest in the mulberry garden. Unbeknown to Goring, in 1640 the document \"failed to pass the Great Seal before King Charles I fled London, which it needed to do for legal execution\". It was this critical omission that helped the British royal family regain the freehold under King George III.\n\nThe improvident Goring defaulted on his rents; Henry Bennet, 1st Earl of Arlington obtained the mansion and was occupying it, now known as Goring House, when it burned down in 1674. Arlington House rose on the site—the location of the southern wing of today's palace—the next year. In 1698, John Sheffield, later the first Duke of Buckingham and Normanby, acquired the lease.\n\nThe house which forms the architectural core of the palace was built for the first Duke of Buckingham and Normanby in 1703 to the design of William Winde. The style chosen was of a large, three-floored central block with two smaller flanking service wings. Buckingham House was eventually sold by Buckingham's natural son, Sir Charles Sheffield, in 1761 to George III for £21,000. Sheffield's leasehold on the mulberry garden site, the freehold of which was still owned by the royal family, was due to expire in 1774.\n\nUnder the new crown ownership, the building was originally intended as a private retreat for King George III's wife, Queen Charlotte, and was accordingly known as The Queen's House. Remodelling of the structure began in 1762. In 1775, an Act of Parliament settled the property on Queen Charlotte, in exchange for her rights to Somerset House (see \"Old and New London\" (below), and 14 of their 15 children were born there. Some furnishings were transferred from Carlton House, and others had been bought in France after the French Revolution of 1789. While St James's Palace remained the official and ceremonial royal residence, the name \"Buckingham-palace\" began from at least 1791.\n\nAfter his accession to the throne in 1820, King George IV continued the renovation with the idea in mind of a small, comfortable home. While the work was in progress, in 1826, the King decided to modify the house into a palace with the help of his architect John Nash. The external façade was designed keeping in mind the French neo-classical influence preferred by George IV. The cost of the renovations grew dramatically, and by 1829 the extravagance of Nash's designs resulted in his removal as architect. On the death of George IV in 1830, his younger brother King William IV hired Edward Blore to finish the work. After the destruction of the Palace of Westminster by fire in 1834, William considered converting the palace into the new Houses of Parliament.\n\nBuckingham Palace finally became the principal royal residence in 1837, on the accession of Queen Victoria, who was the first monarch to reside there; her predecessor William IV had died before its completion. While the state rooms were a riot of gilt and colour, the necessities of the new palace were somewhat less luxurious. For one thing, it was reported the chimneys smoked so much that the fires had to be allowed to die down, and consequently the court shivered in icy magnificence. Ventilation was so bad that the interior smelled, and when it was decided to install gas lamps, there was a serious worry about the build-up of gas on the lower floors. It was also said that staff were lax and lazy and the palace was dirty. Following the Queen's marriage in 1840, her husband, Prince Albert, concerned himself with a reorganisation of the household offices and staff, and with the design faults of the palace. The problems were all rectified by the close of 1840. However, the builders were to return within the decade.\n\nBy 1847, the couple had found the palace too small for court life and their growing family, and consequently the new wing, designed by Edward Blore, was built by Thomas Cubitt, enclosing the central quadrangle. The large East Front, facing The Mall, is today the \"public face\" of Buckingham Palace, and contains the balcony from which the royal family acknowledge the crowds on momentous occasions and after the annual Trooping the Colour. The ballroom wing and a further suite of state rooms were also built in this period, designed by Nash's student Sir James Pennethorne.\n\nBefore Prince Albert's death, the palace was frequently the scene of musical entertainments, and the greatest contemporary musicians entertained at Buckingham Palace. The composer Felix Mendelssohn is known to have played there on three occasions. Johann Strauss II and his orchestra played there when in England. Strauss's \"Alice Polka\" was first performed at the palace in 1849 in honour of the queen's daughter, Princess Alice. Under Victoria, Buckingham Palace was frequently the scene of lavish costume balls, in addition to the usual royal ceremonies, investitures and presentations.\n\nWidowed in 1861, the grief-stricken Queen withdrew from public life and left Buckingham Palace to live at Windsor Castle, Balmoral Castle and Osborne House. For many years the palace was seldom used, even neglected. In 1864, a note was found pinned to the fence of Buckingham Palace, saying: \"These commanding premises to be let or sold, in consequence of the late occupant's declining business.\" Eventually, public opinion persuaded the Queen to return to London, though even then she preferred to live elsewhere whenever possible. Court functions were still held at Windsor Castle, presided over by the sombre Queen habitually dressed in mourning black, while Buckingham Palace remained shuttered for most of the year.\n\nThe palace measures by , is high and contains over of floorspace. The floor area is smaller than the Royal Palace of Madrid, the Papal Palace and Quirinal Palace in Rome, the Louvre in Paris, the Hofburg Palace in Vienna, and the Forbidden City. There are 775 rooms, including 19 state rooms, 52 principal bedrooms, 188 staff bedrooms, 92 offices, and 78 bathrooms. The palace also has its own post office, cinema, swimming pool, doctor's surgery, and jeweller's workshop.\n\nThe principal rooms are contained on the \"piano nobile\" behind the west-facing garden façade at the rear of the palace. The centre of this ornate suite of state rooms is the Music Room, its large bow the dominant feature of the façade. Flanking the Music Room are the Blue and the White Drawing Rooms. At the centre of the suite, serving as a corridor to link the state rooms, is the Picture Gallery, which is top-lit and long. The Gallery is hung with numerous works including some by Rembrandt, van Dyck, Rubens and Vermeer; other rooms leading from the Picture Gallery are the Throne Room and the Green Drawing Room. The Green Drawing Room serves as a huge anteroom to the Throne Room, and is part of the ceremonial route to the throne from the Guard Room at the top of the Grand Staircase. The Guard Room contains white marble statues of Queen Victoria and Prince Albert, in Roman costume, set in a tribune lined with tapestries. These very formal rooms are used only for ceremonial and official entertaining, but are open to the public every summer.\nDirectly underneath the State Apartments is a suite of slightly less grand rooms known as the semi-state apartments. Opening from the Marble Hall, these rooms are used for less formal entertaining, such as luncheon parties and private audiences. Some of the rooms are named and decorated for particular visitors, such as the \"1844 Room\", decorated in that year for the State visit of Tsar Nicholas I of Russia, and, on the other side of the Bow Room, the \"1855 Room\", in honour of the visit of Emperor Napoleon III of France. At the centre of this suite is the Bow Room, through which thousands of guests pass annually to the Queen's Garden Parties in the Gardens. The Queen and Prince Philip use a smaller suite of rooms in the north wing.\n\nBetween 1847 and 1850, when Blore was building the new east wing, the Brighton Pavilion was once again plundered of its fittings. As a result, many of the rooms in the new wing have a distinctly oriental atmosphere. The red and blue Chinese Luncheon Room is made up from parts of the Brighton Banqueting and Music Rooms with a large oriental chimney piece sculpted by Richard Westmacott. The Yellow Drawing Room has wallpaper supplied in 1817 for the Brighton Saloon, and a chimney piece which is a European vision of how the Chinese chimney piece may appear. It has nodding mandarins in niches and fearsome winged dragons, designed by Robert Jones.\n\nAt the centre of this wing is the famous balcony with the Centre Room behind its glass doors. This is a Chinese-style saloon enhanced by Queen Mary, who, working with the designer Sir Charles Allom, created a more \"binding\" Chinese theme in the late 1920s, although the lacquer doors were brought from Brighton in 1873. Running the length of the \"piano nobile\" of the east wing is the great gallery, modestly known as the Principal Corridor, which runs the length of the eastern side of the quadrangle. It has mirrored doors, and mirrored cross walls reflecting porcelain pagodas and other oriental furniture from Brighton. The Chinese Luncheon Room and Yellow Drawing Room are situated at each end of this gallery, with the Centre Room obviously placed in the centre.\n\nThe original early 19th-century interior designs, many of which still survive, included widespread use of brightly coloured scagliola and blue and pink lapis, on the advice of Sir Charles Long. King Edward VII oversaw a partial redecoration in a Belle époque cream and gold colour scheme.\n\nWhen paying a state visit to Britain, foreign heads of state are usually entertained by the Queen at Buckingham Palace. They are allocated a large suite of rooms known as the Belgian Suite, situated at the foot of the Minister's Staircase, on the ground floor of the north-facing Garden Wing. The rooms of the suite are linked by narrow corridors, one of them is given extra height and perspective by saucer domes designed by Nash in the style of Soane. A second corridor in the suite has Gothic influenced cross over vaulting. The Belgian Rooms themselves were decorated in their present style and named after Prince Albert's uncle Léopold I, first King of the Belgians. In 1936, the suite briefly became the private apartments of the palace when they were occupied by King Edward VIII.\n\nInvestitures, which include the conferring of knighthoods by dubbing with a sword, and other awards take place in the palace's Ballroom, built in 1854. At long, wide and high, it is the largest room in the palace. It has replaced the throne room in importance and use. During investitures, the Queen stands on the throne dais beneath a giant, domed velvet canopy, known as a shamiana or a baldachin, that was used at the Delhi Durbar in 1911. A military band plays in the musicians' gallery as award recipients approach the Queen and receive their honours, watched by their families and friends.\n\nState banquets also take place in the Ballroom; these formal dinners are held on the first evening of a state visit by a foreign head of state. On these occasions, for up to 170 guests in formal \"white tie and decorations\", including tiaras, the dining table is laid with the Grand Service, a collection of silver-gilt plate made in 1811 for the Prince of Wales, later George IV. The largest and most formal reception at Buckingham Palace takes place every November when the Queen entertains members of the diplomatic corps. On this grand occasion, all the state rooms are in use, as the royal family proceed through them, beginning at the great north doors of the Picture Gallery. As Nash had envisaged, all the large, double-mirrored doors stand open, reflecting the numerous crystal chandeliers and sconces, creating a deliberate optical illusion of space and light.\n\nSmaller ceremonies such as the reception of new ambassadors take place in the \"1844 Room\". Here too, the Queen holds small lunch parties, and often meetings of the Privy Council. Larger lunch parties often take place in the curved and domed Music Room, or the State Dining Room. On all formal occasions, the ceremonies are attended by the Yeomen of the Guard in their historic uniforms, and other officers of the court such as the Lord Chamberlain.\n\nSince the bombing of the palace chapel in World War II, royal christenings have sometimes taken place in the Music Room. The Queen's first three children were all baptised there.\n\nThe largest functions of the year are the Queen's Garden Parties for up to 8,000 invitees in the Garden.\n\nFormerly, men not wearing military uniform wore knee breeches of an 18th-century design. Women's evening dress included trains and tiaras or feathers in their hair (or both). The dress code governing formal court uniform and dress has progressively relaxed. After World War I, when Queen Mary wished to follow fashion by raising her skirts a few inches from the ground, she requested a lady-in-waiting to shorten her own skirt first to gauge the king's reaction. King George V was horrified, so the queen kept her hemline unfashionably low. Following their accession in 1936, King George VI and his consort, Queen Elizabeth, allowed the hemline of daytime skirts to rise. Today, there is no official dress code. Most men invited to Buckingham Palace in the daytime choose to wear service uniform or lounge suits; a minority wear morning coats, and in the evening, depending on the formality of the occasion, black tie or white tie.\n\nDébutantes were aristocratic young ladies making their first entrée into society through presentation to the monarch at court. These occasions, known as \"coming out\", took place at the palace from the reign of Edward VII. Wearing full court dress, with three ostrich feathers in their hair, débutantes entered, curtsied, and performed a backwards walk and a further curtsey, while manoeuvring a dress train of prescribed length. (The ceremony, known as an evening court, corresponded to the \"court drawing rooms\" of Victoria's reign.) After World War II, the ceremony was replaced by less formal afternoon receptions, usually omitting curtsies and court dress. In 1958, the Queen abolished the presentation parties for débutantes, replacing them with Garden Parties.\n\nThe boy Jones was an intruder who gained entry to the palace on three occasions between 1838 and 1841.\n\nIn 1982, Michael Fagan broke into the palace twice and on the second occasion entered the Queen's bedroom; news media reported at the time that he had a long conversation with the Queen while she waited for security officers to arrive, but in a 2012 interview with \"The Independent\", Fagan claimed that the Queen ran out of the room and no conversation took place. It was only in 2007 that trespassing on the palace grounds became a criminal offence.\n\nAt the rear of the palace is the large and park-like garden, which together with its lake is the largest private garden in London. There, the Queen hosts her annual garden parties each summer, and also holds large functions to celebrate royal milestones, such as jubilees. It covers , and includes a helicopter landing area, a lake, and a tennis court.\n\nAdjacent to the palace is the Royal Mews, also designed by Nash, where the royal carriages, including the Gold State Coach, are housed. This rococo gilt coach, designed by Sir William Chambers in 1760, has painted panels by G. B. Cipriani. It was first used for the State Opening of Parliament by George III in 1762 and has been used by the monarch for every coronation since George IV. It was last used for the Golden Jubilee of Elizabeth II. Also housed in the mews are the coach horses used at royal ceremonial processions.\n\nThe Mall, a ceremonial approach route to the palace, was designed by Sir Aston Webb and completed in 1911 as part of a grand memorial to Queen Victoria. It extends from Admiralty Arch, across St James's Park to the Victoria Memorial. This route is used by the cavalcades and motorcades of visiting heads of state, and by the royal family on state occasions such as the annual Trooping the Colour.\n\nIn 1901 the accession of Edward VII saw new life breathed into the palace. The new King and his wife Queen Alexandra had always been at the forefront of London high society, and their friends, known as \"the Marlborough House Set\", were considered to be the most eminent and fashionable of the age. Buckingham Palace—the Ballroom, Grand Entrance, Marble Hall, Grand Staircase, vestibules and galleries redecorated in the Belle époque cream and gold colour scheme they retain today—once again became a setting for entertaining on a majestic scale but leaving some to feel King Edward's heavy redecorations were at odds with Nash's original work.\n\nThe last major building work took place during the reign of King George V when, in 1913, Sir Aston Webb redesigned Blore's 1850 East Front to resemble in part Giacomo Leoni's Lyme Park in Cheshire. This new, refaced principal façade (of Portland stone) was designed to be the backdrop to the Victoria Memorial, a large memorial statue of Queen Victoria, placed outside the main gates. George V, who had succeeded Edward VII in 1910, had a more serious personality than his father; greater emphasis was now placed on official entertaining and royal duties than on lavish parties. He arranged a series of command performances featuring jazz musicians such as the Original Dixieland Jazz Band (1919) – the first jazz performance for a head of state, Sidney Bechet, and Louis Armstrong (1932), which earned the palace a nomination in 2009 for a (Kind of) Blue Plaque by the Brecon Jazz Festival as one of the venues making the greatest contribution to jazz music in the United Kingdom. George V's wife Queen Mary was a connoisseur of the arts, and took a keen interest in the Royal Collection of furniture and art, both restoring and adding to it. Queen Mary also had many new fixtures and fittings installed, such as the pair of marble Empire-style chimneypieces by Benjamin Vulliamy, dating from 1810, which the Queen had installed in the ground floor Bow Room, the huge low room at the centre of the garden façade. Queen Mary was also responsible for the decoration of the Blue Drawing Room. This room, long, previously known as the South Drawing Room, has a ceiling designed specially by Nash, coffered with huge gilt console brackets.\nDuring World War I, the palace, then the home of King George V and Queen Mary, escaped unscathed. Its more valuable contents were evacuated to Windsor but the royal family remained \"in situ\". The King imposed rationing at the palace, much to the dismay of his guests and household. To the King's later regret, David Lloyd George persuaded him to go further by ostentatiously locking the wine cellars and refraining from alcohol, to set a good example to the supposedly inebriated working class. The workers continued to imbibe and the King was left unhappy at his enforced abstinence. In 1938, the north-west pavilion, designed by Nash as a conservatory, was converted into a swimming pool.\n\nDuring World War II, the palace was bombed nine times, the most serious and publicised of which resulted in the destruction of the palace chapel in 1940. Coverage of this event was played in cinemas all over the UK to show the common suffering of rich and poor. One bomb fell in the palace quadrangle while King George VI and Queen Elizabeth were in residence, and many windows were blown in and the chapel destroyed. War-time coverage of such incidents was severely restricted, however. The King and Queen were filmed inspecting their bombed home, the smiling Queen, as always, immaculately dressed in a hat and matching coat seemingly unbothered by the damage around her. It was at this time the Queen famously declared: \"I'm glad we have been bombed. Now I can look the East End in the face\". The royal family were seen as sharing their subjects' hardship, as \"The Sunday Graphic\" reported:\n\nOn 15 September 1940, known as the Battle of Britain Day, an RAF pilot, Ray Holmes of No. 504 Squadron RAF rammed a German bomber he believed was going to bomb the Palace. Holmes had run out of ammunition and made the quick decision to ram it. Holmes bailed out. Both aircraft crashed. In fact the Dornier Do 17 bomber was empty. It had already been damaged, two of its crew had been killed and the remainder bailed out. Its pilot, Feldwebel Robert Zehbe, landed, only to die later of wounds suffered during the attack. During the Dornier's descent, it somehow unloaded its bombs, one of which hit the Palace. It then crashed into the forecourt of London Victoria station. The bomber's engine was later exhibited at the Imperial War Museum in London. The British pilot became a King's Messenger after the war, and died at the age of 90 in 2005.\n\nOn VE Day—8 May 1945—the palace was the centre of British celebrations. The King, Queen, Princess Elizabeth (the future Queen), and Princess Margaret appeared on the balcony, with the palace's blacked-out windows behind them, to the cheers from a vast crowd in the Mall. The damaged Palace was carefully restored after the War by John Mowlem & Co. It was designated a Grade I listed building in 1970.\n\nEvery year, some 50,000 invited guests are entertained at garden parties, receptions, audiences, and banquets. Three Garden Parties are held in the summer, usually in July. The Forecourt of Buckingham Palace is used for Changing of the Guard, a major ceremony and tourist attraction (daily from April to July; every other day in other months).\n\nThe palace, like Windsor Castle, is owned by the reigning monarch in right of the Crown. It is not the monarch's personal property, unlike Sandringham House and Balmoral Castle. Many of the contents from Buckingham Palace, Windsor Castle, Kensington Palace, and St James's Palace are part of the Royal Collection, held in trust by the Sovereign; they can, on occasion, be viewed by the public at the Queen's Gallery, near the Royal Mews. Unlike the palace and the castle, the purpose-built gallery is open continually and displays a changing selection of items from the collection. It occupies the site of the chapel destroyed by an air raid in World War II. The palace's state rooms have been open to the public during August and September and on selected dates throughout the year since 1993. The money raised in entry fees was originally put towards the rebuilding of Windsor Castle after the 1992 fire devastated many of its state rooms. In the year to 31 March 2016, 519,000 people visited the palace.\n\nHer Majesty's Government is responsible for maintaining the palace in exchange for the profits made by the Crown Estate. In November 2015, the State Dining Room was closed for six months because its ceiling had become potentially dangerous. A 10-year schedule of maintenance work, including new plumbing, wiring, boilers, and radiators, and the installation of solar panels on the roof, has been estimated to cost £369 million and was approved by the prime minister in November 2016. It will be funded by a temporary increase in the Sovereign Grant paid from the income of the Crown Estate and is intended to extend the building's working life by at least 50 years. In March 2017, MPs backed funding for Buckingham Palace 464 votes to 56.\n\nThus, Buckingham Palace is a symbol and home of the British monarchy, an art gallery, and a tourist attraction. Behind the gilded railings and gates that were completed by the Bromsgrove Guild in 1911 and Webb's famous façade, which has been described in a book published by the Royal Collection as looking \"like everybody's idea of a palace\", is not only a weekday home of the Queen and Prince Philip but also the London residence of the Duke of York and the Earl and Countess of Wessex. The palace also houses the offices of the Queen, Prince Philip, the Duke of York, the Earl and Countess of Wessex, the Princess Royal, and Princess Alexandra and is the workplace of more than 800 people.\n\n\n", "id": "3969", "title": "Buckingham Palace"}
{"url": "https://en.wikipedia.org/wiki?curid=3970", "text": "British Airways\n\nBritish Airways, often shortened to BA, is the flag carrier and the largest airline in the United Kingdom based on fleet size. When measured by passengers carried, it is second-largest in the United Kingdom behind easyJet. The airline is based in Waterside near its main hub at London Heathrow Airport.\n\nA British Airways Board was established by the United Kingdom government in 1972 to manage the two nationalised airline corporations, British Overseas Airways Corporation and British European Airways, and two smaller, regional airlines, Cambrian Airways, from Cardiff, and Northeast Airlines, from Newcastle upon Tyne. On 31 March 1974, all four companies were merged to form British Airways. After almost 13 years as a state company, British Airways was privatised in February 1987 as part of a wider privatisation plan by the Conservative government. The carrier soon expanded with the acquisition of British Caledonian in 1987, followed by Dan-Air in 1992 and British Midland International in 2012.\n\nBritish Airways is a founding member of the Oneworld airline alliance, along with American Airlines, Cathay Pacific, Qantas, and the now defunct Canadian Airlines. The alliance has since grown to become the third-largest, after SkyTeam and Star Alliance. British Airways merged with Iberia on 21 January 2011, formally creating the International Airlines Group (IAG), the world's third-largest airline group in terms of annual revenue and the second-largest in Europe. IAG is listed on the London Stock Exchange and in the FTSE 100 Index.\n\nA long-time Boeing customer, British Airways ordered 59 Airbus A320 family aircraft in August 1998. In 2007 it purchased 12 Airbus A380s and 24 Boeing 787 Dreamliners, marking the start of its long-haul fleet replacement. The centrepiece of the airline's long-haul fleet is the Boeing 777, with 58 in the fleet. British Airways is the largest operator of the Boeing 747-400, with 51 registered to the airline.\n\nIn January 1972, a British Airways Board was established by the United Kingdom government following the passing of the Civil Aviation Act of 1971, to manage British Overseas Airways Corporation (BOAC) and British European Airways (BEA). On 1 September 1972 the management service functions of both BOAC and BEA were combined under the newly formed \"British Airways Group\".\n\nBritish Airways was established as an airline on 31 March 1974 by the dissolution of BOAC and BEA. Following two years of fierce competition with British Caledonian, the second-largest airline in the United Kingdom at the time, the Government changed its aviation policy in 1976 so that the two carriers would no longer compete on long-haul routes.\n\nBritish Airways and Air France operated the supersonic airliner Aerospatiale-BAC Concorde, and the world's first supersonic passenger service flew in January 1976 from London Heathrow to Bahrain. Services to the US began on 24 May 1976 with a flight to Washington Dulles airport, and flights to New York JFK airport followed on 22 September 1977. Service to Singapore was established in co-operation with Singapore Airlines as a continuation of the flight to Bahrain. Following the Air France Concorde crash in Paris and a slump in air travel following the 11 September attacks in New York in 2001, it was decided to cease Concorde operations in 2003 after 27 years of service. The final commercial Concorde flight was BA002 from New York JFK to London Heathrow on 24 October 2003.\nIn 1981 the airline was instructed to prepare for privatisation by the Conservative Thatcher government. Sir John King, later Lord King, was appointed chairman, charged with bringing the airline back into profitability. While many other large airlines struggled, King was credited with transforming British Airways into one of the most profitable air carriers in the world. The flag carrier was privatised and was floated on the London Stock Exchange in February 1987. British Airways effected the takeover of the UK's \"second\" airline, British Caledonian, in July of that same year.\n\nThe formation of Richard Branson's Virgin Atlantic Airways in 1984 created a competitor for BA. The intense rivalry between British Airways and Virgin Atlantic culminated in the former being sued for libel in 1993, arising from claims and counter claims over a \"dirty tricks\" campaign against Virgin. This campaign included allegations of poaching Virgin Atlantic customers, tampering with private files belonging to Virgin and undermining Virgin's reputation in the City. As a result of the case BA management apologised \"unreservedly\", and the company agreed to pay £110,000 damages to Virgin, £500,000 to Branson personally and £3 million legal costs. Lord King stepped down as chairman in 1993 and was replaced by his deputy, Colin Marshall, while Bob Ayling took over as CEO. Virgin filed a separate action in the US that same year regarding BA's domination of the trans-Atlantic routes, but it was thrown out in 1999.\n\nIn 1992 British Airways expanded through the acquisition of the financially troubled Dan-Air, giving BA a much larger presence at Gatwick airport. British Asia Airways, a subsidiary based in Taiwan, was formed in March 1993 to operate between London and Taipei. That same month BA purchased a 25% stake in the Australian airline Qantas and, with the acquisition of Brymon Airways in May, formed British Airways Citiexpress (later BA Connect). In September 1998, British Airways, along with American Airlines, Cathay Pacific, Qantas, and Canadian Airlines, formed the Oneworld airline alliance. Oneworld began operations on 1 February 1999, and is the third largest airline alliance in the world, behind SkyTeam and Star Alliance.\n\nBob Ayling's leadership led to a cost savings of £750m and the establishment of a budget airline, Go, in 1998. The next year, however, British Airways reported an 84% drop in profits in its first quarter alone, its worst in seven years. In March 2000, Ayling was removed from his position and British Airways announced Rod Eddington as his successor. That year, British Airways and KLM conducted talks on a potential merger, reaching a decision in July to file an official merger plan with the European Commission. The plan fell through in September 2000. British Asia Airways ceased operations in 2001 after BA suspended flights to Taipei. Go was sold to its management and the private equity firm 3i in June 2001. Eddington would make further workforce cuts due to reduced demand following 11 September attacks in 2001, and BA sold its stake in Qantas in September 2004. In 2005 Willie Walsh, managing director of Aer Lingus and a former pilot, became the chief executive officer of British Airways. BA unveiled its new subsidiary OpenSkies in January 2008, taking advantage of the liberalisation of transatlantic traffic rights between Europe and the United States. OpenSkies flies non-stop from Paris to New York's JFK and Newark airports.\n\nOn July 2008 British Airways announced a merger plan with Iberia, another flag carrier airline in the Oneworld alliance, wherein each airline would retain its original brand. The agreement was confirmed in April 2010, and in July the European Commission and US Department of Transport permitted the merger and began to co-ordinate transatlantic routes with American Airlines. On 6 October 2010 the alliance between British Airways, American Airlines and Iberia formally began operations. The alliance generates an estimated £230 million in annual cost-saving for BA, in addition to the £330 million which would be saved by the merger with Iberia. This merger was finalised on 21 January 2011, resulting in the International Airlines Group (IAG), the world's third-largest airline in terms of annual revenue and the second-largest airline group in Europe. Prior to merging, British Airways owned a 13.5% stake in Iberia, and thus received ownership of 55% of the combined International Airlines Group; Iberia's other shareholders received the remaining 45%. As a part of the merger, British Airways ceased trading independently on the London Stock Exchange after 23 years as a constituent of the FTSE 100 Index.\n\nIn September 2010 Willie Walsh, now CEO of IAG, announced that the group was considering acquiring other airlines and had drawn up a shortlist of twelve possible acquisitions. In November 2011 IAG announced an agreement in principle to purchase British Midland International from Lufthansa. A contract to purchase the airline was agreed the next month, and the sale was completed for £172.5 million on 30 March 2012. The airline established a new subsidiary based at London City Airport operating Airbus A318s.\n\nBritish Airways was the official airline partner of the London 2012 Olympic Games. On 18 May 2012 it flew the Olympic flame from Athens International Airport to RNAS Culdrose while carrying various dignitaries, including Lord Sebastian Coe, Princess Anne, the Olympics minister Hugh Robertson and the London Mayor Boris Johnson, along with the footballer David Beckham.\n\nBritish Airways is the largest airline based in the United Kingdom in terms of fleet size, international flights, and international destinations and was, until 2008, the largest airline by passenger numbers. The airline carried 34.6 million passengers in 2008, but, rival carrier easyJet transported 44.5 million passengers that year, passing British Airways for the first time. British Airways holds a United Kingdom Civil Aviation Authority Type A Operating Licence, it is permitted to carry passengers, cargo, and mail on aircraft with 20 or more seats.\n\nThe airlines' head office, Waterside, stands in Harmondsworth, a village that is near London Heathrow Airport. Waterside was completed in June 1998 to replace British Airways' previous head office, Speedbird House, which was located on the grounds of Heathrow.\n\nBritish Airways' main base is at London Heathrow Airport, but it also has a major presence at Gatwick Airport. It also has a base at London City Airport (LCY), where its subsidiary BA CityFlyer is the largest operator. BA had previously operated a significant hub at Manchester Airport. Manchester to New York (JFK) services were withdrawn; later all international services outside London ceased when the subsidiary BA Connect was sold. Passengers wishing to travel internationally with BA either to or from regional UK destinations must now transfer in London. Heathrow Airport is dominated by British Airways, which owns 40% of the slots available at the airport. The majority of BA services operate from Terminal 5, with the exception of some short-haul and mid-haul flights at Terminal 1 arising from the purchase of BMI and some short-haul flights at Terminal 3, owing to a lack of capacity at Terminal 5. With the imminent opening of the brand-new Terminal 2 in 2014, Star Alliance airlines will progressively be moving all their services into the new terminal and Terminal 1 will be closed for demolition in due course. British Airways' services will then be concentrated in Terminals 3 and 5.\n\nIn August 2014 Willie Walsh advised the airline would continue to use flight paths over Iraq despite the hostilities there. A few days earlier Qantas announced it would avoid Iraqi airspace, while other airlines did likewise. The issue arose following the downing of Malaysia Airlines Flight 17 over Ukraine, and a temporary suspension of flights to and from Ben Gurion Airport during the 2014 Israel–Gaza conflict.\n\nBA CityFlyer, a wholly owned subsidiary, offers flights from its base at London City Airport to 23 destinations throughout Europe. It flies 17 Embraer E-170/E-190 aircraft and two leased Saab 2000. The airline focuses on serving the financial market, though it has recently expanded into the leisure market, offering routes to Ibiza, Palma and Venice. The onboard product is identical to that of the BA Short Haul product from both LHR and LGW.\n\nIn March 2015, Qatar Airways purchased a 10% stake in International Airlines Group, the parent of British Airways and Iberia, for €1.2 billion (US$1.26 billion).\n\nBEA Helicopters was renamed British Airways Helicopters in 1974 and operated passenger and offshore oil support services until it was sold in 1986. Other former subsidiaries include the German airline Deutsche BA from 1997 until 2003 and the French airline Air Liberté from 1997 to 2001. British Airways also owned Airways Aero Association, the operator of the British Airways flying club based at Wycombe Air Park in High Wycombe, until it was sold to Surinder Arora in 2007.\n\nSouth Africa's Comair and Denmark's Sun Air of Scandinavia have been franchisees of British Airways since 1996. British Airways obtained a 15% stake in UK regional airline Flybe from the sale of BA Connect in March 2007. It sold the stake in 2014. BA also owned a 10% stake in InterCapital and Regional Rail (ICRR), the company that managed the operations of Eurostar (UK) Ltd from 1998 to 2010, when the management of Eurostar was restructured.\n\nWith the creation of an Open Skies agreement between Europe and the United States in March 2008, British Airways started a new subsidiary airline called OpenSkies (previously known as \"Project Lauren\"). The airline started operations in June 2008, and now flies direct from Paris to New York's JFK and Newark airports.\n\nBritish Airways Limited was established in 2012 to take over the operation of the premium service between London City Airport and New York-JFK. BA began the service in September 2009, using two Airbus A318s fitted with 32 lie-flat beds in an all business class cabin. Flights operate under the numbers previously reserved for Concorde: BA001 — BA004.\n\nBritish Airways provides cargo services under the British Airways World Cargo brand. The division has been part of IAG Cargo since 2012, and is the world's twelfth-largest cargo airline based on total freight tonne-kilometres flown. BA World Cargo operates using the main BA fleet. Until the end of March 2014 they also operated three Boeing 747-8 freighter aircraft providing dedicated long-haul services under a wet lease arrangement from Global Supply Systems. The division operates an automated cargo centre at London Heathrow Airport and handles freight at Gatwick and Stansted airports.\n\nThe key trends for the British Airways Plc Group are shown below.\n\nOn the merger with Iberia, the accounting reference date was changed from 31 March to 31 December; figures below are therefore for the years to 31 March up to 2010, for the nine months to 31 December 2010, and for the years to 31 December thereafter:\n\nStaff working for British Airways are represented by a number of trade unions, pilots are represented by British Air Line Pilots' Association, cabin crew by British Airlines Stewards and Stewardesses Association (a branch of Unite the Union), while other branches of Unite the Union and the GMB Union represent other employees. Bob Ayling's management faced strike action by cabin crew over a £1 billion cost-cutting drive to return BA to profitability in 1997; this was the last time BA cabin crew would strike until 2009, although staff morale has reportedly been unstable since that incident. In an effort to increase interaction between management, employees, and the unions, various conferences and workshops have taken place, often with thousands in attendance.\n\nIn 2005, wildcat action was taken by union members over a decision by Gate Gourmet not to renew the contracts of 670 workers and replace them with agency staff; it is estimated that the strike cost British Airways £30 million and caused disruption to 100,000 passengers. In October 2006, BA became involved in a civil rights dispute when a Christian employee was forbidden to wear a necklace bearing the cross, a religious symbol. BA's practice of forbidding such symbols has been publicly questioned by British politicians such as the former Home Secretary John Reid and the former Foreign Secretary Jack Straw.\n\nRelations have been turbulent between BA and Unite. In 2007, cabin crew threatened strike action over salary changes to be imposed by BA management. The strike was called off at the last minute, British Airways losing £80 million. In December 2009, a ballot for strike action over Christmas received a high level of support, action was blocked by a court injunction that deemed the ballot illegal. Negotiations failed to stop strike action in March, BA withdrew perks for strike participants. Allegations were made by the \"Guardian\" newspaper that BA had consulted outside firms methods to undermine the unions, the story was later withdrawn. A strike was announced for May 2010, British Airways again sought an injunction. Members of the Socialist Workers Party disrupted negotiations between BA management and Unite to prevent industrial action. Further disruption struck when Derek Simpson, a Unite co-leader, was discovered to have leaked details of confidential negotiations online via Twitter.\n\nBritish Airways serves over 160 destinations, including six domestic. It is one of the few airlines to fly to all six permanently inhabited continents, along with Air China, Air Canada, Delta Air Lines, Emirates, Etihad Airways, Korean Air, Qantas, Qatar Airways, Singapore Airlines, and South African Airways.\n\nBritish Airways codeshares with the following airlines:\n\nBritish Airways is a member and one of the founders of Oneworld, an airline alliance.\n\nWith the exception of the Boeing 707 and early Boeing 747 variants from BOAC, British Airways inherited a mainly UK-built fleet of aircraft when it was formed in 1974. The airline introduced the Boeing 737 and Boeing 757 into the fleet in the 1980s, followed by the Boeing 747-400, Boeing 767 and Boeing 777 in the 1990s. BA is now the largest operator of Boeing 747-400s, with 57 in its fleet. Prior to the introduction of the 787, when Boeing built an aircraft for British Airways, it was allocated the customer code \"36\", which appeared in their aircraft designation as a suffix, such as 737–436.\n\nIn 1991, British Airways placed its first order for 777-200 aircraft, ordering another four for fleet expansion in 2007 at a cost of around US$800 million. BA's first 777s were fitted with General Electric GE90 engines, but BA switched to Rolls-Royce Trent 800s for subsequent aircraft.\n\nLater in 2007, BA announced their order of thirty-six new long-haul aircraft, including twelve Airbus A380s and twenty-four Boeing 787 Dreamliners. Rolls-Royce Trent engines were again selected for both orders with Trent 900s powering the A380s and Trent 1000s powering the 787s. The Boeing 787s will replace 14 of British Airways' Boeing 767 fleet, while the Airbus A380s will replace 20 of BA's Boeing 747-400s and will most likely be used to increase capacity on key routes from London Heathrow.\n\nOn 1 August 2008, BA announced orders for six Boeing 777-300ERs and options for four more as an interim measure to cover for delays over the deliveries of their 787-8/9s. Of the six that have been ordered, four will be leased and two will be fully acquired by British Airways.\n\nOn 22 April 2013, IAG confirmed that it had signed a memorandum of understanding to order 18 A350-1000 aircraft for British Airways, with an option for a further 18. The aircraft would replace some of the airline's fleet of Boeing 747-400s. Options for 18 Boeing 787 aircraft, part of the original contract signed in 2007, have been converted into firm orders for delivery between 2017 and 2021.\n\nOn 26 June 2013, British Airways took delivery of its first 787s. The aircraft began operations to Toronto on 1 September 2013, and began service to Newark on 1 October 2013. BA's first A380 was delivered on 4 July 2013. It began regular services to Los Angeles on 24 September 2013, followed by Hong Kong on 22 October 2013.\n\nThe combined International Airlines Group entity (that BA is now a part of), operates around 400 aircraft, carries over 62 million passengers annually, and serves more than 200 destinations.\n\nAs of December 2016, the British Airways fleet consists of the following aircraft: \n\nIAG's cargo division, IAG Cargo, handles cargo operations using capacity on British Airways' passenger aircraft. IAG reached an agreement with Qatar Airways in 2014 to operate flights for IAG Cargo using Boeing 777F of Qatar Airways Cargo.\n\nBritish Airways World Cargo was the airline's freight division prior to its merger with Iberia Cargo to form IAG Cargo. Aircraft types used by the division between 1974 and 1983 were Vickers 953C, Boeing 707-300C and Boeing 747-200F while the Boeing 747-400F was operated from the 1990s to 2001 through Atlas Air and 2002 to early 2012 by Global Supply Systems, of these only one of Atlas Air's aircraft wore BA livery, the others flew in Atlas and Global Supply's own colours. From 2012 until the termination of Global Supply System's contract in 2014, three Boeing 747-8F aircraft were flown for British Airways World Cargo.\n\nThe company has its own engineering branch to maintain its aircraft fleet, this includes line maintenance at over 70 airports around the world. As well as hangar facilities at Heathrow and Gatwick airport it has two major maintenance centres at Glasgow and Cardiff Airports.\n\nThe musical theme predominantly used on British Airways advertising is \"\"The Flower Duet\"\" by Léo Delibes. This, and the slogan \"The World's Favourite Airline\" were introduced in 1989 with the launch of the iconic \"Face\" advertisement. The slogan was dropped in 2001 after Lufthansa overtook BA in terms of passenger numbers. \"Flower Duet\" is still used by the airline, and has been through several different arrangements since 1989. The most recent version of this melody was shown in 2007 with a new slogan: \"Upgrade to British Airways\". Other advertising slogans have included \"The World's Best Airline\", \"We'll Take More Care of You\", and \"Fly the Flag\".\n\nBA had an account for 23 years with Saatchi & Saatchi, an agency that created many of their most famous advertisements, including the influential \"Face\" campaign. Saatchi & Saatchi later imitated this advert for Silverjet, a rival of BA, after BA discontinued their business activities. Since 2007 BA has used Bartle Bogle Hegarty as its advertising agency.\n\nBritish Airways purchased the internet domain ba.com in 2002 from previous owner Bell Atlantic, 'BA' being the company's acronym and its IATA Airline code. In 2011 BA launched its biggest advertising campaign in a decade, including a 90-second cinematic advert celebrating the airline's ninety-year heritage and a new slogan \"To Fly. To Serve\".\n\nBritish Airways is the official airline of the Wimbledon Championship tennis tournament, and was the official airline and tier one partner of the 2012 Summer Olympics and Paralympics. British Airways was also the official airline of England's bid to host the 2018 Football World Cup.\n\n\"High Life\", founded in 1973, is the official in-flight magazine of the airlines.\n\nSince its formation in 1974, British Airways' aeroplanes carried a Union Jack scheme painted on their tail fins. The original tail scheme was changed in 1984 as part of a new livery designed by Landor Associates.\n\nIn 1997, there was a controversial change from the use of the former British Airways branding (which incorporated stylised elements of the Union Jack) to a new livery which was intended mainly to reflect the diversity of places served by the airline – so-called \"World Images\". This involved a range of different designs appearing on tailfins and elsewhere, although the bodies of all the planes would use the corporate colours consistently; the exception was the Concorde fleet, which would have a new tailfin design based on a stylised, fluttering Union flag. What became known as the \"ethnic images\" included Delftware or Chinese calligraphy, related to countries on the company's network of routes. This was reported to have caused problems with air traffic control: previously controllers had been able to tell pilots to follow a BA plane, but they were now harder to visually identify. Several people spoke out against the change, including the former Prime Minister Margaret Thatcher, who famously covered the tail of a model 747 at an event with a handkerchief, to show her displeasure. BA's traditional rival, Virgin Atlantic, took advantage of the negative press coverage by applying the Union flag to the winglets of their aircraft along with the slogan \"Britain's national flagcarrier\". In 1999 the CEO of British Airways, Bob Ayling, announced that all BA planes would adopt the tailfin design originally intended to be used only on the Concorde, based on the Union Flag.\n\nBritish Airways' tiered loyalty programme, called the Executive Club, includes access to special lounges and dedicated \"fast\" queues. BA also invites its top corporate accounts to join a \"Premier\" incentive programme. British Airways operates airside lounges for passengers travelling in premium cabins, and these are available to certain tiers of Executive Club members. First class passengers, as well as Gold Executive Club members, are entitled to use First Class Lounges. Business class passengers (called Club World or Club Europe in BA terms) as well as Silver Executive Club members may use Business lounges. At airports in which BA does not operate a departure lounge, a third party lounge is often provided for premium or status passengers. In 2011, due to the merger with Iberia, British Airways announced changes to the Executive Club to maximise integration between the airlines. This included the combination and rebranding of Air Miles, BA Miles and Iberia Plus points as the IAG operated loyalty programme Avios.\n\nhigh life Magazine is British Airways' complimentary inflight magazine. It is available to all customers across all cabins and aircraft types.\n\nhigh life shop Magazine is British Airways' inflight shopping magazine. It is available to all customers on all aircraft where the inflight shopping range can be carried.\n\nFirst life is a complimentary magazine offered to all customers travelling in the First cabin. It has a range of articles including fashion, trends and technology with an upmarket target audience.\n\nBusiness life is a complimentary magazine targeted at business travellers and frequent flyers. The magazine can be found in all short haul aircraft seat pockets, in the magazine selection for Club World customers and in lounges operated by British Airways.\n\nUK Domestic is British Airways' economy class on domestic UK flights. Flights into Heathrow are mostly operated by Airbus A320 series aircraft – a small number of peak services to Glasgow and Edinburgh are operated by Boeing 767-300ERs configured in an all-economy layout. Flights into Gatwick are operated by Airbus A320 series aircraft in a one-class configuration. On flights departing before 09:30 a complimentary hot breakfast item is served with orange juice and tea or coffee. For all flights departing after 09:30 a complimentary bar service is offered along with a light snack item.\n\nEuro Traveller is British Airways' economy class offering on flights from the UK to the rest of Europe.\nHeathrow based flights are operated by the Airbus A320 series aircraft and Boeing 767-300ER aircraft. Gatwick based flights are operated by Airbus A320 series aircraft. Standard seat pitch varies from 29\" to 34\" depending on aircraft type and location of seat. All Heathrow flights have a buy on board system where drinks and snacks are payable, with tea and coffee charged £2.30, bottle of water £1.80 and sandwiches starting from £3.00.\n\nFlights from London City Airport and London Stansted Airport will switch to buy on board in the summer of 2017. Passengers may also use Frequent Flyer Avios points to buy items. Simon Calder of \"The Independent\" stated that this may cause an increase in terminal-based food establishments at London Heathrow Terminal 5.\n\nClub Europe is the short-haul business class on all short-haul flights (excluding those within the UK). This class allows for access to business lounges at most airports. Club Europe provides seats in a 2–2 configuration on narrowbody aircraft, with the middle seat not used. Instead, a table folds up from under the middle seat on refurbished aircraft. Pillows and blankets are available on longer flights.\n\nIn-flight entertainment is offered on selected longer flights operated by the Boeing 767-300ER and some A320 aircraft. Headphones are provided to all customers on services where IFE is available.\n\nIn 2012, British Airways launched a new mid-haul product for the Airbus A321s on routes formerly operated by BMI. These aircraft have been designated to serve routes such as Almaty, Tbilisi, Baku, Cairo, Amman, Beirut and Tel Aviv.\n\nThe 'Club World' business class on these narrowbody aircraft is slightly different from the product operated on the rest of the long-haul fleet: An alternating 1:2 / 2:1 configuration of 23 seats is used in the front section of the aircraft. The full Club World bar and main meal service is offered. Larger individual LCD TV screens are fitted to each seat back.\n\nThe 'World Traveller' service is also offered on these flights with complimentary drinks, food and individual IFE screens. All seats on the aircraft are fitted with the Thales i5000 or Rockwell Collins in-flight entertainment system.\n\nFirst class is offered on all Airbus A380s, all Boeing 747-400s, some Boeing 777-200s, and all Boeing 777-300ERs and Boeing 787-9s on launch. There are fourteen (eight on 787-9) private \"demi-cabins\" on most of these aircraft, each with a bed, a wide entertainment screen, and in-seat power.\nDedicated British Airways 'Galleries First' lounges are available at some airports. The exclusive 'Concorde Room' lounges at Heathrow Terminal 5 and New York JFK airports extend the offering with waiter service pre-flight dining and more intimate space. Business lounges are used where these are not available.\n\nClub World is the long-haul business class, and is offered on all long-haul configured Boeing 767, Boeing 777, Boeing 787, Boeing 747-400, Airbus A318 and A380 aircraft. The cabin features fully flat beds. In 2006 British Airways launched \"Next Generation New Club World\", featuring larger seats. The Club World cabins are all configured in a similar design on widebody aircraft with aisle seats facing forwards while middle seats and window seats face backwards (British Airways is one of only three carriers with backwards facing Business class seats; American Airlines and United Airlines are the others).\n\nWorld Traveller Plus is the premium economy offering provided on all long-haul aircraft except the Airbus A318. This cabin offers wider seats, extended leg-room, additional seat comforts such as larger IFE screen (on most aircraft) a foot rest and power sockets. A complimentary 'World Traveller' bar is offered along with an upgraded main meal course.\n\nWorld Traveller is the long-haul economy class offered on international flights to destinations outside Europe. It offers seat-back entertainment, complimentary drinks and meals. AVOD is offered on all A380s, 747s, long-haul 767s, and all of the 777s and 787s. On the Airbus A380, Boeing 787, Boeing 777-300ER and refurbished 777-200 aircraft, AC power outlets and USB plug-in points are offered in every seat row. The outlets accept both UK and US plugs. The outlets in World Traveller are located between each seat.\n\n\n\n", "id": "3970", "title": "British Airways"}
{"url": "https://en.wikipedia.org/wiki?curid=3973", "text": "Bicycle\n\nA bicycle, often called a bike or cycle, is a human-powered, pedal-driven, single-track vehicle, having two wheels attached to a frame, one behind the other. A is called a cyclist, or bicyclist. \n\nBicycles were introduced in the 19th century in Europe and as of 2003, more than 1 billion have been produced worldwide, twice as many as the number of automobiles that have been produced. They are the principal means of transportation in many regions. They also provide a popular form of recreation, and have been adapted for use as children's toys, general fitness, military and police applications, courier services, and bicycle racing.\n\nThe basic shape and configuration of a typical upright or \"safety bicycle\", has changed little since the first chain-driven model was developed around 1885. But many details have been improved, especially since the advent of modern materials and computer-aided design. These have allowed for a proliferation of specialized designs for many types of cycling.\n\nThe bicycle's invention has had an enormous effect on society, both in terms of culture and of advancing modern industrial methods. Several components that eventually played a key role in the development of the automobile were initially invented for use in the bicycle, including ball bearings, pneumatic tires, chain-driven sprockets, and tension-spoked wheels.\n\nThe word \"bicycle\" first appeared in English print in \"The Daily News\" in 1868, to describe \"Bysicles and trysicles\" on the \"Champs Elysées and Bois de Boulogne.\" The word was first used in 1847 in a French publication to describe an unidentified two-wheeled vehicle, possibly a carriage. The design of the bicycle was an advance on the velocipede, although the words were used with some degree of overlap for a time.\n\nOther words for bicycle include \"bike\", \"pushbike\", \"pedal cycle\", or \"cycle\". In Unicode, the code point for \"bicycle\" is 0x1F6B2. The entity codice_1 in HTML produces 🚲.\n\nThe \"Dandy horse\", also called Draisienne or Laufmaschine, was the first human means of transport to use only two wheels in tandem and was invented by the German Baron Karl von Drais. It is regarded as the modern bicycle's forerunner; Drais introduced it to the public in Mannheim in summer 1817 and in Paris in 1818. Its rider sat astride a wooden frame supported by two in-line wheels and pushed the vehicle along with his or her feet while steering the front wheel.\nThe first mechanically-propelled, two-wheeled vehicle may have been built by Kirkpatrick MacMillan, a Scottish blacksmith, in 1839, although the claim is often disputed. He is also associated with the first recorded instance of a cycling traffic offense, when a Glasgow newspaper in 1842 reported an accident in which an anonymous \"gentleman from Dumfries-shire... bestride a velocipede... of ingenious design\" knocked over a little girl in Glasgow and was fined five shillings.\n\nIn the early 1860s, Frenchmen Pierre Michaux and Pierre Lallement took bicycle design in a new direction by adding a mechanical crank drive with pedals on an enlarged front wheel (the velocipede). Another French inventor named Douglas Grasso had a failed prototype of Pierre Lallement's bicycle several years earlier. Several inventions followed using rear-wheel drive, the best known being the rod-driven velocipede by Scotsman Thomas McCall in 1869. In that same year, bicycle wheels with wire spokes were patented by Eugène Meyer of Paris. The French \"vélocipède\", made of iron and wood, developed into the \"penny-farthing\" (historically known as an \"ordinary bicycle\", a retronym, since there was then no other kind). It featured a tubular steel frame on which were mounted wire-spoked wheels with solid rubber tires. These bicycles were difficult to ride due to their high seat and poor weight distribution. In 1868 Rowley Turner, a sales agent of the Coventry Sewing Machine Company (which soon became the Coventry Machinists Company), brought a Michaux cycle to Coventry, England. His uncle, Josiah Turner, and business partner James Starley, used this as a basis for the 'Coventry Model' in what became Britain's first cycle factory.\n\nThe \"dwarf ordinary\" addressed some of these faults by reducing the front wheel diameter and setting the seat further back. This, in turn, required gearing—effected in a variety of ways—to efficiently use pedal power. Having to both pedal and steer via the front wheel remained a problem. Englishman J. K. Starley (nephew of James Starley), J. H. Lawson, and Shergold solved this problem by introducing the chain drive (originated by the unsuccessful \"bicyclette\" of Englishman Henry Lawson), connecting the frame-mounted cranks to the rear wheel. These models were known as safety bicycles, dwarf safeties, or upright bicycles for their lower seat height and better weight distribution, although without pneumatic tires the ride of the smaller-wheeled bicycle would be much rougher than that of the larger-wheeled variety. Starley's 1885 Rover, manufactured in Coventry is usually described as the first recognizably modern bicycle. Soon the \"seat tube\" was added, creating the modern bike's double-triangle \"diamond frame\".\n\nFurther innovations increased comfort and ushered in a second bicycle craze, the 1890s \"Golden Age of Bicycles\". In 1888, Scotsman John Boyd Dunlop introduced the first practical pneumatic tire, which soon became universal. Soon after, the rear freewheel was developed, enabling the rider to coast. This refinement led to the 1890s invention of coaster brakes. Dérailleur gears and hand-operated Bowden cable-pull brakes were also developed during these years, but were only slowly adopted by casual riders.\n\nThe Svea Velocipede with vertical pedal arrangement and locking hubs was introduced in 1892 by the Swedish engineers Fredrik Ljungström and Birger Ljungström. It attracted attention at the World Fair and was produced in a few thousand units.\n\nBy the turn of the century, cycling clubs flourished on both sides of the Atlantic, and touring and racing became widely popular.\n\nBicycles and horse buggies were the two mainstays of private transportation just prior to the automobile, and the grading of smooth roads in the late 19th century was stimulated by the widespread advertising, production, and use of these devices.\n\nFrom the beginning and still today, bicycles have been and are employed for many uses. In a utilitarian way, bicycles are used for transportation, bicycle commuting, and utility cycling. It can be used as a 'work horse', used by mail carriers, paramedics, police, messengers, and general delivery services. Military uses of bicycles include communications, reconnaissance, troop movement, supply of provisions, and patrol. See also: bicycle infantry.\n\nThe bicycle is also used for recreational purposes, such as bicycle touring, mountain biking, physical fitness, and play. Bicycle competition includes racing, BMX racing, track racing, criterium, roller racing, sportives and time trials. Major multi-stage professional events are the Giro d'Italia, the Tour de France, the Vuelta a España, the Tour de Pologne, and the Volta a Portugal.\n\nBikes can be used for entertainment and pleasure, such as in organised mass rides, artistic cycling and freestyle BMX.\n\nThe bicycle has undergone continual adaptation and improvement since its inception. These innovations have continued with the advent of modern materials and computer-aided design, allowing for a proliferation of specialized bicycle types.\n\nBicycles can be categorized in many different ways: by function, by number of riders, by general construction, by gearing or by means of propulsion. The more common types include utility bicycles, mountain bicycles, racing bicycles, touring bicycles, hybrid bicycles, cruiser bicycles, and BMX bikes. Less common are tandems, low riders, tall bikes, fixed gear, folding models, amphibious bicycles, recumbents and electric bicycles.\n\nUnicycles, tricycles and quadracycles are not strictly bicycles, as they have respectively one, three and four wheels, but are often referred to informally as \"bikes\".\n\nA bicycle stays upright while moving forward by being steered so as to keep its center of mass over the wheels. This steering is usually provided by the rider, but under certain conditions may be provided by the bicycle itself.\n\nThe combined center of mass of a bicycle and its rider must lean into a turn to successfully navigate it. This lean is induced by a method known as countersteering, which can be performed by the rider turning the handlebars directly with the hands or indirectly by leaning the bicycle.\n\nShort-wheelbase or tall bicycles, when braking, can generate enough stopping force at the front wheel to flip longitudinally. The act of purposefully using this force to lift the rear wheel and balance on the front without tipping over is a trick known as a stoppie, endo, or front wheelie.\n\nThe bicycle is extraordinarily efficient in both biological and mechanical terms. The bicycle is the most efficient human-powered means of transportation in terms of energy a person must expend to travel a given distance. From a mechanical viewpoint, up to 99% of the energy delivered by the rider into the pedals is transmitted to the wheels, although the use of gearing mechanisms may reduce this by 10–15%.\nIn terms of the ratio of cargo weight a bicycle can carry to total weight, it is also an efficient means of cargo transportation.\n\nA human traveling on a bicycle at low to medium speeds of around uses only the power required to walk. Air drag, which is proportional to the square of speed, requires dramatically higher power outputs as speeds increase. If the rider is sitting upright, the rider's body creates about 75% of the total drag of the bicycle/rider combination. Drag can be reduced by seating the rider in a more aerodynamically streamlined position. Drag can also be reduced by covering the bicycle with an aerodynamic fairing. The fastest recorded unpaced speed on a flat surface is \n\nIn addition, the carbon dioxide generated in the production and transportation of the food required by the bicyclist, per mile traveled, is less than 1/10 that generated by energy efficient motorcars.\n\nThe great majority of today's bicycles have a frame with upright seating that looks much like the first chain-driven bike. These upright bicycles almost always feature the \"diamond frame\", a truss consisting of two triangles: the front triangle and the rear triangle. The front triangle consists of the head tube, top tube, down tube, and seat tube. The head tube contains the headset, the set of bearings that allows the fork to turn smoothly for steering and balance. The top tube connects the head tube to the seat tube at the top, and the down tube connects the head tube to the bottom bracket. The rear triangle consists of the seat tube and paired chain stays and seat stays. The chain stays run parallel to the chain, connecting the bottom bracket to the rear dropout, where the axle for the rear wheel is held. The seat stays connect the top of the seat tube (at or near the same point as the top tube) to the rear fork ends.\nHistorically, women's bicycle frames had a top tube that connected in the middle of the seat tube instead of the top, resulting in a lower standover height at the expense of compromised structural integrity, since this places a strong bending load in the seat tube, and bicycle frame members are typically weak in bending. This design, referred to as a \"step-through frame\" or as an \"open frame\", allows the rider to mount and dismount in a dignified way while wearing a skirt or dress. While some women's bicycles continue to use this frame style, there is also a variation, the \"mixte\", which splits the top tube laterally into two thinner top tubes that bypass the seat tube on each side and connect to the rear fork ends. The ease of stepping through is also appreciated by those with limited flexibility or other joint problems. Because of its persistent image as a \"women's\" bicycle, step-through frames are not common for larger frames.\n\nStep-throughs were popular partly for practical reasons and partly for social mores of the day. For most of the history of bicycles' popularity women have worn long skirts, and the lower frame accommodated these better than the top-tube. Furthermore, it was considered \"unladylike\" for women to open their legs to mount and dismount - in more conservative times women who rode bicycles at all were vilified as immoral or immodest. These practices were akin to the older practice of riding horse sidesaddle.\n\nAnother style is the recumbent bicycle. These are inherently more aerodynamic than upright versions, as the rider may lean back onto a support and operate pedals that are on about the same level as the seat. The world's fastest bicycle is a recumbent bicycle but this type was banned from competition in 1934 by the Union Cycliste Internationale.\n\nHistorically, materials used in bicycles have followed a similar pattern as in aircraft, the goal being high strength and low weight. Since the late 1930s alloy steels have been used for frame and fork tubes in higher quality machines. By the 1980s aluminum welding techniques had improved to the point that aluminum tube could safely be used in place of steel. Since then aluminum alloy frames and other components have become popular due to their light weight, and most mid-range bikes are now principally aluminum alloy of some kind. More expensive bikes use carbon fibre due to its significantly lighter weight and profiling ability, allowing designers to make a bike both stiff and compliant by manipulating the lay-up. Other exotic frame materials include titanium and advanced alloys. Bamboo, a natural composite material with high strength-to-weight ratio and stiffness has been used for bicycles since 1894. \nRecent versions use bamboo for the primary frame with glued metal connections and parts, priced as exotic models.\n\nThe \"drivetrain\" begins with pedals which rotate the cranks, which are held in axis by the bottom bracket. Most bicycles use a chain to transmit power to the rear wheel. A very small number of bicycles use a shaft drive to transmit power, or special belts. Hydraulic bicycle transmissions have been built, but they are currently inefficient and complex.\n\nSince cyclists' legs are most efficient over a narrow range of pedaling speeds, or cadence, a variable gear ratio helps a cyclist to maintain an optimum pedalling speed while covering varied terrain. Some, mainly utility, bicycles use hub gears with between 3 and 14 ratios, but most use the generally more efficient dérailleur system, by which the chain is moved between different cogs called chainrings and sprockets in order to select a ratio. A dérailleur system normally has two dérailleurs, or mechs, one at the front to select the chainring and another at the back to select the sprocket. Most bikes have two or three chainrings, and from 5 to 11 sprockets on the back, with the number of theoretical gears calculated by multiplying front by back. In reality, many gears overlap or require the chain to run diagonally, so the number of usable gears is fewer.\n\nAn alternative to chaindrive is to use a synchronous belt. These are toothed and work much the same as a chain - popular with commuters and long distance cyclists they require little maintenance. They can't be shifted across a cassette of sprockets, and are used either as single speed or with a hub gear.\n\nDifferent gears and ranges of gears are appropriate for different people and styles of cycling. Multi-speed bicycles allow gear selection to suit the circumstances: a cyclist could use a high gear when cycling downhill, a medium gear when cycling on a flat road, and a low gear when cycling uphill. In a lower gear every turn of the pedals leads to fewer rotations of the rear wheel. This allows the energy required to move the same distance to be distributed over more pedal turns, reducing fatigue when riding uphill, with a heavy load, or against strong winds. A higher gear allows a cyclist to make fewer pedal turns to maintain a given speed, but with more effort per turn of the pedals.\n\nWith a \"chain drive\" transmission, a \"chainring\" attached to a crank drives the chain, which in turn rotates the rear wheel via the rear sprocket(s) (cassette or freewheel). There are four gearing options: two-speed hub gear integrated with chain ring, up to 3 chain rings, up to 11 sprockets, hub gear built into rear wheel (3-speed to 14-speed). The most common options are either a rear hub or multiple chain rings combined with multiple sprockets (other combinations of options are possible but less common).\n\nThe handlebars turn the fork and the front wheel via the stem, which rotates within the headset. Three styles of handlebar are common. \"Upright handlebars\", the norm in Europe and elsewhere until the 1970s, curve gently back toward the rider, offering a natural grip and comfortable upright position. \"Drop handlebars\" \"drop\" as they curve forward and down, offering the cyclist best braking power from a more aerodynamic \"crouched\" position, as well as more upright positions in which the hands grip the brake lever mounts, the forward curves, or the upper flat sections for increasingly upright postures. Mountain bikes generally feature a 'straight handlebar' or 'riser bar' with varying degrees of sweep backwards and centimeters rise upwards, as well as wider widths which can provide better handling due to increased leverage against the wheel.\n\nSaddles also vary with rider preference, from the cushioned ones favored by short-distance riders to narrower saddles which allow more room for leg swings. Comfort depends on riding position. With comfort bikes and hybrids, cyclists sit high over the seat, their weight directed down onto the saddle, such that a wider and more cushioned saddle is preferable. For racing bikes where the rider is bent over, weight is more evenly distributed between the handlebars and saddle, the hips are flexed, and a narrower and harder saddle is more efficient. Differing saddle designs exist for male and female cyclists, accommodating the genders' differing anatomies, although bikes typically are sold with saddles most appropriate for men.\n\nA recumbent bicycle has a reclined chair-like seat that some riders find more comfortable than a saddle, especially riders who suffer from certain types of seat, back, neck, shoulder, or wrist pain. Recumbent bicycles may have either under-seat or over-seat steering.\n\nBicycle brakes may be rim brakes, in which friction pads are compressed against the wheel rims; hub brakes, where the mechanism is contained within the wheel hub, or disc brakes, where pads act on a rotor attached to the hub. Most road bicycles use rim brakes, but some use disk brakes. Disc brakes are more common for mountain bikes, tandems and recumbent bicycles than on other types of bicycles, due to their increased power, coupled with an increased weight and complexity.\nWith hand-operated brakes, force is applied to brake levers mounted on the handlebars and transmitted via Bowden cables or hydraulic lines to the friction pads, which apply pressure to the braking surface, causing friction which slows the bicycle down. A rear hub brake may be either hand-operated or pedal-actuated, as in the back pedal \"coaster brakes\" which were popular in North America until the 1960s.\n\nTrack bicycles do not have brakes, because all riders ride in the same direction around a track which does not necessitate sharp deceleration. Track riders are still able to slow down because all track bicycles are fixed-gear, meaning that there is no freewheel. Without a freewheel, coasting is impossible, so when the rear wheel is moving, the cranks are moving. To slow down, the rider applies resistance to the pedals, acting as a braking system which can be as effective as a conventional rear wheel brake, but not as effective as a front wheel brake.\n\nBicycle suspension refers to the system or systems used to \"suspend\" the rider and all or part of the bicycle. This serves two purposes: to keep the wheels in continuous contact with the ground, improving control, and to isolate the rider and luggage from jarring due to rough surfaces, improving comfort.\n\nBicycle suspensions are used primarily on mountain bicycles, but are also common on hybrid bicycles, as they can help deal with problematic vibration from poor surfaces. Suspension is especially important on recumbent bicycles, since while an upright bicycle rider can stand on the pedals to achieve some of the benefits of suspension, a recumbent rider cannot.\n\nBasic mountain bicycles and hybrids usually have front suspension only, whilst more sophisticated ones also have rear suspension. Road bicycles tend to have no suspension.\n\nThe wheel axle fits into fork ends in the frame and fork. A pair of wheels may be called a wheelset, especially in the context of ready-built \"off the shelf\", performance-oriented wheels.\n\nTires vary enormously depending on their intended purpose. Road bicycles use tires 18 to 25 millimeters wide, most often completely smooth, or slick, and inflated to high pressure in order to roll fast on smooth surfaces. Off-road tires are usually between wide, and have treads for gripping in muddy conditions or metal studs for ice.\n\nSome components, which are often optional accessories on sports bicycles, are standard features on utility bicycles to enhance their usefulness and comfort. Mudguards, or fenders, protect the cyclist and moving parts from spray when riding through wet areas and chainguards protect clothes from oil on the chain while preventing clothing from being caught between the chain and crankset teeth. Kick stands keep bicycles upright when parked, and bike locks deter theft. Front-mounted baskets, front or rear luggage carriers or racks, and panniers mounted above either or both wheels can be used to carry equipment or cargo. Pegs can be fastened to one, or both of the wheel hubs to either help the rider perform certain tricks, or allow a place for extra riders to stand, or rest. Parents sometimes add rear-mounted child seats, an auxiliary saddle fitted to the crossbar, or both to transport children.\n\n\"Toe-clips\" and \"toestraps\" and clipless pedals help keep the foot locked in the proper pedal position and enable cyclists to pull and push the pedals. Technical accessories include cyclocomputers for measuring speed, distance, heart rate, GPS data etc. Other accessories include lights, reflectors, mirrors, water bottles and cages, and bell.\n\nBicycle helmets can reduce injury in the event of a collision or accident, and a suitable helmet is legally required of riders in many jurisdictions. Helmets may be classified as an accessory or as an item of clothing.\n\nBike trainers are used to enable cyclists to cycle while the bike remains stationary. They are frequently used to warm up before races or indoors when riding conditions are unfavorable.\n\nBicycles can also be fitted with a hitch to tow a trailer for carrying cargo, a child, or both.\n\nA number of formal and industry standards exist for bicycle components to help make spare parts exchangeable and to maintain a minimum product safety.\n\nThe International Organization for Standardization (ISO) has a special technical committee for cycles, TC149, that has the following scope: \"Standardization in the field of cycles, their components and accessories with particular reference to terminology, testing methods and requirements for performance and safety, and interchangeability.\"\n\nThe European Committee for Standardization (CEN) also has a specific Technical Committee, TC333, that defines European standards for cycles. Their mandate states that EN cycle standards shall harmonize with ISO standards. Some CEN cycle standards were developed before ISO published their standards, leading to strong European influences in this area. European cycle standards tend to describe minimum safety requirements, while ISO standards have historically harmonized parts geometry.\n\nMaintenance of adequate tire inflation is the most frequent and troublesome concern for cyclists and many means and methods are employed to preserve pneumatic integrity. Thicker tires, thicker tubes, tire liners (of a number of rather impenetrable devices installed between the tire and tube), liquid sealing compounds squeezed into the tube, and automotive-style patch kits are all used to reliably contain the typical tire pressures of 40 to 60 pounds per square inch that are required for bicycle operation. Thin, light bicycle tires are particularly vulnerable to penetration and subsequent deflation caused by goat's heads and other burs, colloquially known as stickers. Inflation of bicycle tires to pressures higher than typical for automotive use requires special pumps. The complexity of bicycle tire maintenance and repair may cause many to not consider the bicycle for transport or leisure.\n\nSome bicycle parts, particularly hub-based gearing systems, require considerable torque for dis-assembly and may thus need professional services. Self-service and assisted-service maintenance and repair may be available.\n\n\nThere are specialized bicycle tools for use both in the shop and on the road. Many cyclists carry tool kits. These may include a tire patch kit (which, in turn, may contain any combination of a hand pump or CO Pump, tire levers, spare tubes, self-adhesive patches, or tube-patching material, an adhesive, a piece of sandpaper or a metal grater (for roughing the tube surface to be patched), Special, thin wrenches are often required for maintaining various screw fastened parts, specifically, the frequently lubricated ball-bearing \"cones.\" and sometimes even a block of French chalk.), wrenches, hex keys, screwdrivers, and a chain tool. There are also cycling specific multi-tools that combine many of these implements into a single compact device. More specialized bicycle components may require more complex tools, including proprietary tools specific for a given manufacturer.\n\nThe bicycle has had a considerable effect on human society, in both the cultural and industrial realms.\n\nAround the turn of the 20th century, bicycles reduced crowding in inner-city tenements by allowing workers to commute from more spacious dwellings in the suburbs. They also reduced dependence on horses. Bicycles allowed people to travel for leisure into the country, since bicycles were three times as energy efficient as walking and three to four times as fast.\n\nIn built up cities around the world, urban planning uses cycling infrastructure like bikeways to reduce traffic congestion and air pollution. A number of cities around the world have implemented schemes known as bicycle sharing systems or community bicycle programs. The first of these was the White Bicycle plan in Amsterdam in 1965. It was followed by yellow bicycles in La Rochelle and green bicycles in Cambridge. These initiatives complement public transport systems and offer an alternative to motorized traffic to help reduce congestion and pollution. In Europe, especially in the Netherlands and parts of Germany and Denmark, bicycle commuting is common. In Copenhagen, a cyclists' organization runs a Cycling Embassy that promotes biking for commuting and sightseeing. The United Kingdom has a tax break scheme (IR 176) that allows employees to buy a new bicycle tax free to use for commuting.\n\nIn the Netherlands all train stations offer free bicycle parking, or a more secure parking place for a small fee, with the larger stations also offering bicycle repair shops. Cycling is so popular that the parking capacity may be exceeded, while in some places such as Delft the capacity is usually exceeded. In Trondheim in Norway, the Trampe bicycle lift has been developed to encourage cyclists by giving assistance on a steep hill. Buses in many cities have bicycle carriers mounted on the front.\n\nThere are towns in some countries where bicycle culture has been an integral part of the landscape for generations, even without much official support. That is the case of Ílhavo, in Portugal.\n\nIn cities where bicycles are not integrated into the public transportation system, commuters often use bicycles as elements of a mixed-mode commute, where the bike is used to travel to and from train stations or other forms of rapid transit. Some students who commute several miles drive a car from home to a campus parking lot, then ride a bicycle to class. Folding bicycles are useful in these scenarios, as they are less cumbersome when carried aboard. Los Angeles removed a small amount of seating on some trains to make more room for bicycles and wheel chairs.\nSome US companies, notably in the tech sector, are developing both innovative cycle designs and cycle-friendliness in the workplace. Foursquare, whose CEO Dennis Crowley \"pedaled to pitch meetings ... [when he] was raising money from venture capitalists\" on a two-wheeler, chose a new location for its New York headquarters \"based on where biking would be easy\". Parking in the office was also integral to HQ planning. Mitchell Moss, who runs the Rudin Center for Transportation Policy & Management at New York University, said in 2012: \"Biking has become the mode of choice for the educated high tech worker.\"\n\nBicycles offer an important mode of transport in many developing countries. Until recently, bicycles have been a staple of everyday life throughout Asian countries. They are the most frequently used method of transport for commuting to work, school, shopping, and life in general. In Europe, bicycles are commonly used. They also offer a degree of exercise to keep individuals healthy.\n\nBicycles are also celebrated in the visual arts. An example of this is the Bicycle Film Festival, a film festival hosted all around the world.\n\nExperiments done in Uganda, Tanzania, and Sri Lanka on hundreds of households have shown that a bicycle can increase a poor family's income as much as 35%. Transport, if analyzed for the cost-benefit analysis for rural poverty alleviation, has given one of the best returns in this regard. For example, road investments in India were a staggering 3-10 times more effective than almost all other investments and subsidies in rural economy in the decade of the 1990s. What a road does at a macro level to increase transport, the bicycle supports at the micro level. The bicycle, in that sense, can be an important poverty-eradication tool in poor nations.\n\nThe safety bicycle gave women unprecedented mobility, contributing to their emancipation in Western nations. As bicycles became safer and cheaper, more women had access to the personal freedom that bicycles embodied, and so the bicycle came to symbolize the New Woman of the late 19th century, especially in Britain and the United States. The bicycle craze in the 1890s also led to a movement for so-called rational dress, which helped liberate women from corsets and ankle-length skirts and other restrictive garments, substituting the then-shocking bloomers.\n\nThe bicycle was recognized by 19th-century feminists and suffragists as a \"freedom machine\" for women. American Susan B. Anthony said in a \"New York World\" interview on February 2, 1896:\n\"I think it has done more to emancipate woman than any one thing in the world. I rejoice every time I see a woman ride by on a wheel. It gives her a feeling of self-reliance and independence the moment she takes her seat; and away she goes, the picture of untrammelled womanhood.\" In 1895 Frances Willard, the tightly laced president of the Woman’s Christian Temperance Union, wrote \"A Wheel Within a Wheel: How I Learned to Ride the Bicycle, with Some Reflections by the Way\", a 75-page illustrated memoir praising \"Gladys\", her bicycle, for its \"gladdening effect\" on her health and political optimism. Willard used a cycling metaphor to urge other suffragists to action.\n\nBicycle manufacturing proved to be a training ground for other industries and led to the development of advanced metalworking techniques, both for the frames themselves and for special components such as ball bearings, washers, and sprockets. These techniques later enabled skilled metalworkers and mechanics to develop the components used in early automobiles and aircraft.\n\nWilbur and Orville Wright, a pair of businessmen, ran the Wright Cycle Company which designed, manufactured and sold their bicycles during the bike boom of the 1890s.\n\nThey also served to teach the industrial models later adopted, including mechanization and mass production (later copied and adopted by Ford and General Motors), vertical integration (also later copied and adopted by Ford), aggressive advertising (as much as 10% of all advertising in U.S. periodicals in 1898 was by bicycle makers), lobbying for better roads (which had the side benefit of acting as advertising, and of improving sales by providing more places to ride), all first practiced by Pope. In addition, bicycle makers adopted the annual model change (later derided as planned obsolescence, and usually credited to General Motors), which proved very successful.\n\nEarly bicycles were an example of conspicuous consumption, being adopted by the fashionable elites. In addition, by serving as a platform for accessories, which could ultimately cost more than the bicycle itself, it paved the way for the likes of the Barbie doll.\n\nBicycles helped create, or enhance, new kinds of businesses, such as bicycle messengers, traveling seamstresses, riding academies, and racing rinks. Their board tracks were later adapted to early motorcycle and automobile racing. There were a variety of new inventions, such as spoke tighteners, and specialized lights, socks and shoes, and even cameras, such as the Eastman Company's Poco. Probably the best known and most widely used of these inventions, adopted well beyond cycling, is Charles Bennett's Bike Web, which came to be called the jock strap.\nThey also presaged a move away from public transit that would explode with the introduction of the automobile.\n\nJ. K. Starley's company became the Rover Cycle Company Ltd. in the late 1890s, and then simply the Rover Company when it started making cars. Morris Motors Limited (in Oxford) and Škoda also began in the bicycle business, as did the Wright brothers. Alistair Craig, whose company eventually emerged to become the engine manufacturers Ailsa Craig, also started from manufacturing bicycles, in Glasgow in March 1885.\n\nIn general, U.S. and European cycle manufacturers used to assemble cycles from their own frames and components made by other companies, although very large companies (such as Raleigh) used to make almost every part of a bicycle (including bottom brackets, axles, etc.) In recent years, those bicycle makers have greatly changed their methods of production. Now, almost none of them produce their own frames.\n\nMany newer or smaller companies only design and market their products; the actual production is done by Asian companies. For example, some 60% of the world's bicycles are now being made in China. Despite this shift in production, as nations such as China and India become more wealthy, their own use of bicycles has declined due to the increasing affordability of cars and motorcycles.\nOne of the major reasons for the proliferation of Chinese-made bicycles in foreign markets is the lower cost of labor in China.\n\nIn line with the European financial crisis, in Italy in 2011 the number of bicycle sales (1.75 million) just passed the number of new car sales.\n\nOne of the profound economic implications of bicycle use is that it liberates the user from oil consumption.(Ballantine, 1972) The bicycle is an inexpensive, fast, healthy and environmentally friendly mode of transport. Ivan Illich stated that bicycle use extended the usable physical environment for people, while alternatives such as cars and motorways degraded and confined people's environment and mobility.\nCurrently, two billion bicycles are in use around the world. Children, students, professionals, laborers, civil servants and seniors are pedaling around their communities. They all experience the freedom and the natural opportunity for exercise that the bicycle easily provides. Bicycle also has lowest carbon intensity of travel.\n\nThe proper Islamic bicycle for the Iranian women is a topic of heated discussion in both Sunni and Shia Islam.\n\nThe global bicycle market is $61 billion in 2011. 130 million bicycles were sold every year globally and 66% of them were made in China.\n\nEarly in its development, as with automobiles, there were restrictions on the operation of bicycles. Along with advertising, and to gain free publicity, Albert A. Pope litigated on behalf of cyclists.\n\nThe 1968 Vienna Convention on Road Traffic of the United Nations considers a bicycle to be a vehicle, and a person controlling a bicycle (whether actually riding or not) is considered an operator. The traffic codes of many countries reflect these definitions and demand that a bicycle satisfy certain legal requirements before it can be used on public roads. In many jurisdictions, it is an offense to use a bicycle that is not in a roadworthy condition.\n\nIn most jurisdictions, bicycles must have functioning front and rear lights when ridden after dark. As some generator or dynamo-driven lamps only operate while moving, rear reflectors are frequently also mandatory. Since a moving bicycle makes little noise, some countries insist that bicycles have a warning bell for use when approaching pedestrians, equestrians, and other cyclists, though sometimes a car horn can be used when a 12 volt battery is available.\n\nSome countries require child and/or adult cyclists to wear helmets, as this may protect riders from head trauma. Countries which require adult cyclists to wear helmets include Spain, New Zealand and Australia. Mandatory helmet wearing is one of the most controversial topics in the cycling world, with proponents arguing that it reduces head injuries and thus is an acceptable requirement, while opponents argue that by making cycling seem more dangerous and cumbersome, it reduces cyclist numbers on the streets, creating an overall negative health effect (fewer people cycling for their own health, and the remaining cyclists being more exposed through a reversed safety in numbers effect).\n\nBicycles are popular targets for theft, due to their value and ease of resaleThe number of bicycles stolen annually is difficult to quantify as a large number of crimes are not reported. Around 50% of the participants in the Montreal International Journal of Sustainable Transportation survey were subjected to a bicycle theft in their lifetime as active cyclists.\n\nThe worlds longest bicycle was created by Santos and University of South Australia. It measures at 41.42 m (135 ft 10.7 in) long.\n\n\n\n\n", "id": "3973", "title": "Bicycle"}
{"url": "https://en.wikipedia.org/wiki?curid=3974", "text": "Biopolymer\n\nBiopolymers are polymers produced by living organisms; in other words, they are polymeric biomolecules. Since they are polymers, biopolymers contain monomeric units that are covalently bonded to form larger structures. There are three main classes of biopolymers, classified according to the monomeric units used and the structure of the biopolymer formed: polynucleotides (RNA and DNA), which are long polymers composed of 13 or more nucleotide monomers; polypeptides, which are short polymers of amino acids; and polysaccharides, which are often linear bonded polymeric carbohydrate structures.\n\nOther examples of biopolymers include rubber, suberin, melanin and lignin.\n\nCellulose is the most common organic compound and biopolymer on Earth. About 33 percent of all plant matter is cellulose. The cellulose content of cotton is 90 percent, for wood it is 50 percent.\n\nA major defining difference between biopolymers and synthetic polymers can be found in their structures. All polymers are made of repetitive units called monomers. Biopolymers often have a well-defined structure, though this is not a defining characteristic (example: lignocellulose): \nThe exact chemical composition and the sequence in which these units are arranged is called the primary structure, in the case of proteins. Many biopolymers spontaneously fold into characteristic compact shapes (see also \"protein folding\" as well as secondary structure and tertiary structure), which determine their biological functions and depend in a complicated way on their primary structures. Structural biology is the study of the structural properties of the biopolymers.\nIn contrast, most synthetic polymers have much simpler and more random (or stochastic) structures. This fact leads to a molecular mass distribution that is missing in biopolymers.\nIn fact, as their synthesis is controlled by a template-directed process in most \"in vivo\" systems, all biopolymers of a type (say one specific protein) are all alike: they all contain the similar sequences and numbers of monomers and thus all have the same mass. This phenomenon is called monodispersity in contrast to the polydispersity encountered in synthetic polymers. As a result, biopolymers have a polydispersity index of 1.\n\nThe convention for a polypeptide is to list its constituent amino acid residues as they occur from the amino terminus to the carboxylic acid terminus. The amino acid residues are always joined by peptide bonds. Protein, though used colloquially to refer to any polypeptide, refers to larger or fully functional forms and can consist of several polypeptide chains as well as single chains. Proteins can also be modified to include non-peptide components, such as saccharide chains and lipids.\n\nThe convention for a nucleic acid sequence is to list the nucleotides as they occur from the 5' end to the 3' end of the polymer chain, where 5' and 3' refer to the numbering of carbons around the ribose ring which participate in forming the phosphate diester linkages of the chain. Such a sequence is called the primary structure of the biopolymer.\n\nSugar-based biopolymers are often difficult with regards to convention. Sugar polymers can be linear or branched and are typically joined with glycosidic bonds. The exact placement of the linkage can vary, and the orientation of the linking functional groups is also important, resulting in α- and β-glycosidic bonds with numbering definitive of the linking carbons' location in the ring. In addition, many saccharide units can undergo various chemical modifications, such as amination, and can even form parts of other molecules, such as glycoproteins.\n\nThere are a number of biophysical techniques for determining sequence information. Protein sequence can be determined by Edman degradation, in which the N-terminal residues are hydrolyzed from the chain one at a time, derivatized, and then identified. Mass spectrometer techniques can also be used. Nucleic acid sequence can be determined using gel electrophoresis and capillary electrophoresis. Lastly, mechanical properties of these biopolymers can often be measured using optical tweezers or atomic-force microscopy. Dual polarization interferometry can be used to measure the conformational changes or self-assembly of these materials when stimulated by pH, temperature, ionic strength or other binding partners.\n\nSome biopolymers- such as PLA, naturally occurring zein, and poly-3-hydroxybutyrate can be used as plastics, replacing the need for polystyrene or polyethylene based plastics.\n\nSome plastics are now referred to as being 'degradable', 'oxy-degradable' or 'UV-degradable'. This means that they break down when exposed to light or air, but these plastics are still primarily (as much as 98 per cent) oil-based and are not currently certified as 'biodegradable' under the European Union directive on Packaging and Packaging Waste (94/62/EC). Biopolymers will break down, and some are suitable for domestic composting.\n\nBiopolymers (also called renewable polymers) are produced from biomass for use in the packaging industry. Biomass comes from crops such as sugar beet, potatoes or wheat: when used to produce biopolymers, these are classified as non food crops. These can be converted in the following pathways:\n\nSugar beet > Glyconic acid > Polyglyconic acid\n\nStarch > (fermentation) > Lactic acid > Polylactic acid (PLA)\n\nBiomass > (fermentation) > Bioethanol > Ethene > Polyethylene\n\nMany types of packaging can be made from biopolymers: food trays, blown starch pellets for shipping fragile goods, thin films for wrapping.\n\nBiopolymers can be sustainable, carbon neutral and are always renewable, because they are made from plant materials which can be grown indefinitely. These plant materials come from agricultural non food crops. Therefore, the use of biopolymers would create a sustainable industry. In contrast, the feedstocks for polymers derived from petrochemicals will eventually deplete. In addition, biopolymers have the potential to cut carbon emissions and reduce CO quantities in the atmosphere: this is because the CO released when they degrade can be reabsorbed by crops grown to replace them: this makes them close to carbon neutral.\n\nBiopolymers are biodegradable, and some are also compostable. Some biopolymers are biodegradable: they are broken down into CO and water by microorganisms. Some of these biodegradable biopolymers are compostable: they can be put into an industrial composting process and will break down by 90% within six months. Biopolymers that do this can be marked with a 'compostable' symbol, under European Standard EN 13432 (2000). Packaging marked with this symbol can be put into industrial composting processes and will break down within six months or less. An example of a compostable polymer is PLA film under 20μm thick: films which are thicker than that do not qualify as compostable, even though they are biodegradable. In Europe there is a home composting standard and associated logo that enables consumers to identify and dispose of packaging in their compost heap.\n\n", "id": "3974", "title": "Biopolymer"}
