{"url": "https://en.wikipedia.org/wiki?curid=7519", "text": "Convolution\n\nIn mathematics (and, in particular, functional analysis) convolution is a mathematical operation on two functions (\"f\" and \"g\"); it produces a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated. Convolution is similar to cross-correlation. It has applications that include probability, statistics, computer vision, natural language processing, image and signal processing, engineering, and differential equations.\n\nThe convolution can be defined for functions on groups other than Euclidean space. For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by \"periodic convolution\". (See row 11 at DTFT § Properties.)  A \"discrete convolution\" can be defined for functions on the set of integers. Generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.\n\nComputing the inverse of the convolution operation is known as deconvolution.\n\nThe convolution of \"f\" and \"g\" is written \"f\"∗\"g\", using an asterisk or star. It is defined as the integral of the product of the two functions after one is reversed and shifted. As such, it is a particular kind of integral transform:\n\nWhile the symbol \"t\" is used above, it need not represent the time domain. But in that context, the convolution formula can be described as a weighted average of the function \"f\"(\"τ\") at the moment \"t\" where the weighting is given by \"g\"(−\"τ\") simply shifted by amount \"t\". As \"t\" changes, the weighting function emphasizes different parts of the input function.\n\nFor functions \"f\", \"g\" supported on only formula_2 (i.e., zero for negative arguments), the integration limits can be truncated, resulting in\n\nIn this case, the Laplace transform is more appropriate than the Fourier transform below and boundary terms become relevant.\n\nFor the multi-dimensional formulation of convolution, see \"domain of definition\" (below).\n\nA primarily engineering convention that one often sees is:\n\nwhich has to be interpreted carefully to avoid confusion. For instance,  \"ƒ\"(\"t\")*\"g\"(\"t\" − \"t\") is equivalent to (\"ƒ\"*\"g\")(\"t\" − \"t\"), but \"ƒ\"(\"t\" − \"t\")*\"g\"(\"t\" − \"t\") is in fact equivalent to (\"ƒ\"*\"g\")(\"t-2\"t\"\").\n\nConvolution describes the output (in terms of the input) of an important class of operations known as \"linear time-invariant\" (LTI). See LTI system theory for a derivation of convolution as the result of LTI constraints. In terms of the Fourier transforms of the input and output of an LTI operation, no new frequency components are created. The existing ones are only modified (amplitude and/or phase). In other words, the output transform is the pointwise product of the input transform with a third transform (known as a transfer function). See Convolution theorem for a derivation of that property of convolution. Conversely, convolution can be derived as the inverse Fourier transform of the pointwise product of two Fourier transforms.\n\nOne of the earliest uses of the convolution integral appeared in D'Alembert's derivation of Taylor's theorem in \"Recherches sur différents points importants du système du monde,\" published in 1754.\n\nAlso, an expression of the type:\n\nis used by Sylvestre François Lacroix on page 505 of his book entitled \"Treatise on differences and series\", which is the last of 3 volumes of the encyclopedic series: \"Traité du calcul différentiel et du calcul intégral\", Chez Courcier, Paris, 1797-1800. Soon thereafter, convolution operations appear in the works of Pierre Simon Laplace, Jean-Baptiste Joseph Fourier, Siméon Denis Poisson, and others. The term itself did not come into wide use until the 1950s or 60s. Prior to that it was sometimes known as \"faltung\" (which means \"folding\" in German), \"composition product\", \"superposition integral\", and \"Carson's integral\".\nYet it appears as early as 1903, though the definition is rather unfamiliar in older uses.\n\nThe operation:\n\nis a particular case of composition products considered by the Italian mathematician Vito Volterra in 1913.\n\nWhen a function \"g\" is periodic, with period \"T\", then for functions, \"f\", such that \"f\"∗\"g\" exists, the convolution is also periodic and identical to:\n\nwhere \"t\" is an arbitrary choice. The summation is called a periodic summation of the function \"f\".\n\nWhen \"g\" is a periodic summation of another function, \"g\", then \"f\"∗\"g\" is known as a \"circular\" or \"cyclic\" convolution of \"f\" and \"g\".<br>\nAnd if the periodic summation above is replaced by \"f\", the operation is called a \"periodic\" convolution of \"f\" and \"g\".\n\nFor complex-valued functions \"f\", \"g\" defined on the set Z of integers, the discrete convolution of \"f\" and \"g\" is given by:\n\nThe convolution of two finite sequences is defined by extending the sequences to finitely supported functions on the set of integers. When the sequences are the coefficients of two polynomials, then the coefficients of the ordinary product of the two polynomials are the convolution of the original two sequences. This is known as the Cauchy product of the coefficients of the sequences.\n\nThus when \"g\" has finite support in the set formula_9 (representing, for instance, a finite impulse response), a finite summation may be used:\n\nWhen a function \"g\" is periodic, with period \"N\", then for functions, \"f\", such that \"f\"∗\"g\" exists, the convolution is also periodic and identical to:\n\nThe summation on \"k\" is called a periodic summation of the function \"f\".\n\nIf \"g\" is a periodic summation of another function, \"g\", then \"f\"∗\"g\" is known as a circular convolution of \"f\" and \"g\".\n\nWhen the non-zero durations of both \"f\" and \"g\" are limited to the interval [0, \"N\" − 1], \"f\"∗\"g\" reduces to these common forms:\n\n] \\equiv (f *_N g)[n]\n\nThe notation (\"f\" ∗ \"g\") for \"cyclic convolution\" denotes convolution over the cyclic group of integers modulo \"N\".\n\nCircular convolution arises most often in the context of fast convolution with an FFT algorithm.\n\nIn many situations, discrete convolutions can be converted to circular convolutions so that fast transforms with a convolution property can be used to implement the computation. For example, convolution of digit sequences is the kernel operation in multiplication of multi-digit numbers, which can therefore be efficiently implemented with transform techniques (; ).\n\nThe most common fast convolution algorithms use fast Fourier transform (FFT) algorithms via the circular convolution theorem. Specifically, the circular convolution of two finite-length sequences is found by taking an FFT of each sequence, multiplying pointwise, and then performing an inverse FFT. Convolutions of the type defined above are then efficiently implemented using that technique in conjunction with zero-extension and/or discarding portions of the output. Other fast convolution algorithms, such as the Schönhage–Strassen algorithm or the Mersenne transform, use fast Fourier transforms in other rings.\n\nIf one sequence is much longer than the other, zero-extension of the shorter sequence and fast circular convolution is not the most computationally efficient method available. Instead, decomposing the longer sequence into blocks and convolving each block allows for faster algorithms such as the Overlap–save method and Overlap–add method. A hybrid convolution method that combines block and FIR algorithms allows for a zero input-output latency that is useful for real-time convolution computations.\n\nThe convolution of two complex-valued functions on R is itself a complex-valued function on R, defined by:\n\nis well-defined only if \"f\" and \"g\" decay sufficiently rapidly at infinity in order for the integral to exist. Conditions for the existence of the convolution may be tricky, since a blow-up in \"g\" at infinity can be easily offset by sufficiently rapid decay in \"f\". The question of existence thus may involve different conditions on \"f\" and \"g\":\n\nIf \"f\" and \"g\" are compactly supported continuous functions, then their convolution exists, and is also compactly supported and continuous . More generally, if either function (say \"f\") is compactly supported and the other is locally integrable, then the convolution \"f\"∗\"g\" is well-defined and continuous.\n\nConvolution of \"f\" and \"g\" is also well defined when both functions are locally square integrable on R and supported on an interval of the form [a, +∞) (or both supported on [-∞, a]).\n\nThe convolution of \"f\" and \"g\" exists if \"f\" and \"g\" are both Lebesgue integrable functions in L(R), and in this case \"f\"∗\"g\" is also integrable . This is a consequence of Tonelli's theorem. This is also true for functions in formula_13, under the discrete convolution, or more generally for the convolution on any group.\n\nLikewise, if \"f\" ∈ \"L\"(R) and \"g\" ∈ \"L\"(R) where 1 ≤ \"p\" ≤ ∞, then \"f\"∗\"g\" ∈ \"L\"(R) and\n\nIn the particular case \"p\" = 1, this shows that \"L\" is a Banach algebra under the convolution (and equality of the two sides holds if \"f\" and \"g\" are non-negative almost everywhere).\nMore generally, Young's inequality implies that the convolution is a continuous bilinear map between suitable \"L\" spaces. Specifically, if 1 ≤ \"p\",\"q\",\"r\" ≤ ∞ satisfy\n\nthen\n\nso that the convolution is a continuous bilinear mapping from \"L\"×\"L\" to \"L\".\nThe Young inequality for convolution is also true in other contexts (circle group, convolution on Z). The preceding inequality is not sharp on the real line: when , there exists a constant such that:\n\nThe optimal value of was discovered in 1975.\n\nA stronger estimate is true provided :\nwhere formula_19 is the weak \"L\" norm. Convolution also defines a bilinear continuous map formula_20 for formula_21, owing to the weak Young inequality:\n\nIn addition to compactly supported functions and integrable functions, functions that have sufficiently rapid decay at infinity can also be convolved. An important feature of the convolution is that if \"f\" and \"g\" both decay rapidly, then \"f\"∗\"g\" also decays rapidly. In particular, if \"f\" and \"g\" are rapidly decreasing functions, then so is the convolution \"f\"∗\"g\". Combined with the fact that convolution commutes with differentiation (see Properties), it follows that the class of Schwartz functions is closed under convolution .\n\nUnder some circumstances, it is possible to define the convolution of a function with a distribution, or of two distributions. If \"f\" is a compactly supported function and \"g\" is a distribution, then \"f\"∗\"g\" is a smooth function defined by a distributional formula analogous to\n\nMore generally, it is possible to extend the definition of the convolution in a unique way so that the associative law\n\nremains valid in the case where \"f\" is a distribution, and \"g\" a compactly supported distribution .\n\nThe convolution of any two Borel measures μ and ν of bounded variation is the measure λ defined by \nThis agrees with the convolution defined above when μ and ν are regarded as distributions, as well as the convolution of L functions when μ and ν are absolutely continuous with respect to the Lebesgue measure.\n\nThe convolution of measures also satisfies the following version of Young's inequality\nwhere the norm is the total variation of a measure. Because the space of measures of bounded variation is a Banach space, convolution of measures can be treated with standard methods of functional analysis that may not apply for the convolution of distributions.\n\nThe convolution defines a product on the linear space of integrable functions. This product satisfies the following algebraic properties, which formally mean that the space of integrable functions with the product given by convolution is a commutative algebra without identity . Other linear spaces of functions, such as the space of continuous functions of compact support, are closed under the convolution, and so also form commutative algebras.\n\n\nProof: By definition\nChanging the variable of integration to formula_29 and the result follows.\n\n\nProof: This follows from using Fubini's theorem (i.e., double integrals can be evaluated as\niterated integrals in either order).\n\nProof: This follows from linearity of the integral.\n\nfor any real (or complex) number formula_33.\n\nNo algebra of functions possesses an identity for the convolution. The lack of identity is typically not a major inconvenience, since most collections of functions on which the convolution is performed can be convolved with a delta distribution or, at the very least (as is the case of \"L\") admit approximations to the identity. The linear space of compactly supported distributions does, however, admit an identity under the convolution. Specifically,\nwhere δ is the delta distribution.\n\n\nSome distributions have an inverse element for the convolution, \"S\", which is defined by\nThe set of invertible distributions forms an abelian group under the convolution.\n\n\n\nProof:\n\n\nIf \"f\" and \"g\" are integrable functions, then the integral of their convolution on the whole space is simply obtained as the product of their integrals:\n\nThis follows from Fubini's theorem. The same result holds if \"f\" and \"g\" are only assumed to be nonnegative measurable functions, by Tonelli's theorem.\n\nIn the one-variable case,\n\nwhere \"d\"/\"dx\" is the derivative. More generally, in the case of functions of several variables, an analogous formula holds with the partial derivative:\n\nA particular consequence of this is that the convolution can be viewed as a \"smoothing\" operation: the convolution of \"f\" and \"g\" is differentiable as many times as \"f\" and \"g\" are in total.\n\nThese identities hold under the precise condition that \"f\" and \"g\" are absolutely integrable and at least one of them has an absolutely integrable (L) weak derivative, as a consequence of Young's inequality. For instance, when \"f\" is continuously differentiable with compact support, and \"g\" is an arbitrary locally integrable function,\nThese identities also hold much more broadly in the sense of tempered distributions if one of \"f\" or \"g\" is a compactly supported distribution or a Schwartz function and the other is a tempered distribution. On the other hand, two positive integrable and infinitely differentiable functions may have a nowhere continuous convolution.\n\nIn the discrete case, the difference operator \"D\" \"f\"(\"n\") = \"f\"(\"n\" + 1) − \"f\"(\"n\") satisfies an analogous relationship:\n\nThe convolution theorem states that\n\nwhere formula_50 denotes the Fourier transform of formula_51, and formula_52 is a constant that depends on the specific normalization of the Fourier transform. Versions of this theorem also hold for the Laplace transform, two-sided Laplace transform, Z-transform and Mellin transform.\n\nSee also the less trivial Titchmarsh convolution theorem.\n\nThe convolution commutes with translations, meaning that\n\nwhere τf is the translation of the function \"f\" by \"x\" defined by\n\nIf \"f\" is a Schwartz function, then τ\"f\" is the convolution with a translated Dirac delta function τ\"f\" = \"f\"∗\"τ\" \"δ\". So translation invariance of the convolution of Schwartz functions is a consequence of the associativity of convolution.\n\nFurthermore, under certain conditions, convolution is the most general translation invariant operation. Informally speaking, the following holds\n\n\nThus any translation invariant operation can be represented as a convolution. Convolutions play an important role in the study of time-invariant systems, and especially LTI system theory. The representing function \"g\" is the impulse response of the transformation \"S\".\n\nA more precise version of the theorem quoted above requires specifying the class of functions on which the convolution is defined, and also requires assuming in addition that \"S\" must be a continuous linear operator with respect to the appropriate topology. It is known, for instance, that every continuous translation invariant continuous linear operator on \"L\" is the convolution with a finite Borel measure. More generally, every continuous translation invariant continuous linear operator on \"L\" for 1 ≤ \"p\" < ∞ is the convolution with a tempered distribution whose Fourier transform is bounded. To wit, they are all given by bounded Fourier multipliers.\n\nIf \"G\" is a suitable group endowed with a measure λ, and if \"f\" and \"g\" are real or complex valued integrable functions on \"G\", then we can define their convolution by\n\nIt is not commutative in general. In typical cases of interest \"G\" is a locally compact Hausdorff topological group and λ is a (left-) Haar measure. In that case, unless \"G\" is unimodular, the convolution defined in this way is not the same as formula_56. The preference of one over the other is made so that convolution with a fixed function \"g\" commutes with left translation in the group:\n\nFurthermore, the convention is also required for consistency with the definition of the convolution of measures given below. However, with a right instead of a left Haar measure, the latter integral is preferred over the former.\n\nOn locally compact abelian groups, a version of the convolution theorem holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The circle group T with the Lebesgue measure is an immediate example. For a fixed \"g\" in \"L\"(T), we have the following familiar operator acting on the Hilbert space \"L\"(T):\n\nThe operator \"T\" is compact. A direct calculation shows that its adjoint \"T*\" is convolution with\n\nBy the commutativity property cited above, \"T\" is normal: \"T\"*\"T\" = \"TT\"*. Also, \"T\" commutes with the translation operators. Consider the family \"S\" of operators consisting of all such convolutions and the translation operators. Then \"S\" is a commuting family of normal operators. According to spectral theory, there exists an orthonormal basis {\"h\"} that simultaneously diagonalizes \"S\". This characterizes convolutions on the circle. Specifically, we have\n\nwhich are precisely the characters of T. Each convolution is a compact multiplication operator in this basis. This can be viewed as a version of the convolution theorem discussed above.\n\nA discrete example is a finite cyclic group of order \"n\". Convolution operators are here represented by circulant matrices, and can be diagonalized by the discrete Fourier transform.\n\nA similar result holds for compact groups (not necessarily abelian): the matrix coefficients of finite-dimensional unitary representations form an orthonormal basis in \"L\" by the Peter–Weyl theorem, and an analog of the convolution theorem continues to hold, along with many other aspects of harmonic analysis that depend on the Fourier transform.\n\nLet \"G\" be a topological group.\nIf μ and ν are finite Borel measures on \"G\", then their convolution μ∗ν is defined by\n\nfor each measurable subset \"E\" of \"G\". The convolution is also a finite measure, whose total variation satisfies\n\nIn the case when \"G\" is locally compact with (left-)Haar measure λ, and μ and ν are absolutely continuous with respect to a λ, so that each has a density function, then the convolution μ∗ν is also absolutely continuous, and its density function is just the convolution of the two separate density functions.\n\nIf μ and ν are probability measures on the topological group then the convolution μ∗ν is the probability distribution of the sum \"X\" + \"Y\" of two independent random variables \"X\" and \"Y\" whose respective distributions are μ and ν.\n\nLet (\"X\", Δ, ∇, \"ε\", \"η\") be a bialgebra with comultiplication Δ, multiplication ∇, unit η, and counit ε. The convolution is a product defined on the endomorphism algebra End(\"X\") as follows. Let φ, ψ ∈ End(\"X\"), that is, φ,ψ : \"X\" → \"X\" are functions that respect all algebraic structure of \"X\", then the convolution φ∗ψ is defined as the composition\n\nThe convolution appears notably in the definition of Hopf algebras . A bialgebra is a Hopf algebra if and only if it has an antipode: an endomorphism \"S\" such that\n\nConvolution and related operations are found in many applications in science, engineering and mathematics.\n\n\n\n\n", "id": "7519", "title": "Convolution"}
{"url": "https://en.wikipedia.org/wiki?curid=7521", "text": "Calico\n\nCalico (in British usage, 1505,) is a plain-woven textile made from unbleached and often not fully processed cotton. It may contain unseparated husk parts, for example. The fabric is far less fine than muslin, but less coarse and thick than canvas or denim, but it is still very cheap owing to its unfinished and undyed appearance.\n\nThe fabric was originally from the city of Kozhikode (known by the English as \"Calicut\") in southwestern India. It was made by the traditional weavers called cāliyans. The raw fabric was dyed and printed in bright hues, and calico prints became popular in Europe.\n\nCalico originated in Kozhikode (also known as Calicut, from which the name of the textile came) in southwestern India (in present-day Kerala) during the 11th century. The cloth was known as \"cāliyan\" to the natives.\n\nIt was mentioned in Indian literature by the 12th century when the writer Hēmacandra described calico fabric prints with a lotus design. By the 15th century calico from Gujǎrāt made its appearance in Egypt. Trade with Europe followed from the 17th century onwards.\n\nCalico was woven using Sūrat cotton for both the warp and weft.\n\nIn the 18th century, England was famous for its woollen and worsted cloth. That industry, centred in the east and south in towns such as Norwich, jealously protected their product. Cotton processing was tiny: in 1701 only of cottonwool was imported into England, and by 1730 this had fallen to . This was due to commercial legislation to protect the woollen industry. Cheap calico prints, imported by the East India Company from Hindustān (India), had become popular. In 1700 an Act of Parliament passed to prevent the importation of dyed or printed calicoes from India, China or Persia. This caused demand to switch to imported grey cloth instead—calico that had not been finished—dyed or printed. These were printed with popular patterns in southern England. Also, Lancashire businessmen produced grey cloth with linen warp and cotton weft, known as fustian, which they sent to London for finishing. Cottonwool imports recovered though, and by 1720 were almost back to their 1701 levels. Again the woolen manufacturers, in true protectionist fashion, claimed that the imports were taking jobs away from workers in Coventry. A new law passed, enacting fines against anyone caught wearing printed or stained calico muslins. Neckcloths and fustians were exempted. The Lancashire manufacturers exploited this exemption; coloured cotton weft with linen warp were specifically permitted by the 1736 Manchester Act. There now was an artificial demand for woven cloth.\n\nIn 1764, of cotton-wool were imported. It has been noted that this change in consumer demand a key part of the process that brought the Indian economy from sophisticated textile production to supply of raw materials. These events occurred under colonial rule and were described by Nehru and also some more recent scholars as \"de-industrialization.\"\n\nEarly Indian chintz, that is, glazed calico with a large floral pattern. were primarily produced by painting techniques. Later, the hues were applied by wooden blocks, and the cloth manufacturers in Britain printing calico used wooden block printing. Calico printers at work are depicted in one of the stained glass windows made by Stephen Adam for the Maryhill Burgh Halls, Glasgow. Confusingly, linen and silk printed this way were known as \"linen calicoes\" and \"silk calicoes\". Early European calicoes (1680) would be cheap plain-weave white cotton fabric with equal weft and warp plain weave cotton fabric in, or cream or unbleached cotton, with a design block-printed using a single alizarin dye fixed with two mordants, giving a red and black pattern. Polychromatic prints were possible, using two sets of blocks and an additional blue dye. The Indian taste was for dark printed backgrounds while the European market preferred a pattern on a cream base. As the century progressed the European preference moved from the large chintz patterns to smaller, tighter patterns.\n\nThomas Bell patented a printing technique in 1783 that used copper rollers, and Livesey, Hargreaves and Company put the first machine that used it into operation near Preston, Lancashire in 1785. The production volume for printed cloth in Lancashire in 1750 was estimated at 50,000 pieces of In 1850 it was 20,000,000 pieces. After 1888, block printing was only used for short-run specialized jobs. After 1880, profits from printing fell due to overcapacity and the firms started to form combines. In the first, three Scottish firms formed the United Turkey Red Co. Ltd in 1897, and the second, in 1899, was the much larger Calico Printers' Association 46 printing concerns and 13 merchants combined, representing 85% of the British printing capacity. Some of this capacity was removed and in 1901 Calico had 48% of the printing trade. In 1916, they and the other printers formed and joined a trade association, which then set minimum prices for each 'price section' of the industry.\n\nThe trade association remained in operation until 1954, when the arrangement was challenged by the government Monopolies Commission. Over the intervening period much trade had been lost overseas.\n\nIn the UK, Australia and New Zealand:\n\nIn the US:\n\nPrinted calico was imported into the United States from Lancashire in the 1780s, and here a linguistic separation occurred, while Europe maintained the word calico for the fabric, in the States it was used to refer to the printed design.\n\nThese colorful, small-patterned printed fabrics gave rise to the use of the word calico to describe a cat coat color: \"calico cat\". The patterned fabric also gave its name to two species of North American crabs; see the calico crab.\n\n\n", "id": "7521", "title": "Calico"}
{"url": "https://en.wikipedia.org/wiki?curid=7522", "text": "Calorimetry\n\nCalorimetry is the science or act of measuring changes in state variables of a body for the purpose of deriving the heat transfer associated with changes of its state due, for example, to chemical reactions, physical changes, or phase transitions under specified constraints. Calorimetry is performed with a calorimeter. The word \"calorimetry\" is derived from the Latin word \"calor\", meaning heat and the Greek word \"μέτρον\" (metron), meaning measure. Scottish physician and scientist Joseph Black, who was the first to recognize the distinction between heat and temperature, is said to be the founder of the science of calorimetry.\n\nIndirect Calorimetry calculates heat that living organisms produce by measuring either their production of carbon dioxide and nitrogen waste (frequently ammonia in aquatic organisms, or urea in terrestrial ones), or from their consumption of oxygen. \nLavoisier noted in 1780 that heat production can be predicted from oxygen consumption this way, using multiple regression. The Dynamic Energy Budget theory explains why this procedure is correct. Heat generated by living organisms may also be measured by direct calorimetry, in which the entire organism is placed inside the calorimeter for the measurement.\n\nA widely used modern instrument is the differential scanning calorimeter, a device which allows thermal data to be obtained on small amounts of material. It involves heating the sample at a controlled rate and recording the heat flow either into or from the specimen.\n\nCalorimetry requires that a reference material that changes temperature have known definite thermal constitutive properties. The classical rule, recognized by Clausius and by Kelvin, is that the pressure exerted by the calorimetric material is fully and rapidly determined solely by its temperature and volume; this rule is for changes that do not involve phase change, such as melting of ice. There are many materials that do not comply with this rule, and for them, the present formula of classical calorimetry does not provide an adequate account. Here the classical rule is assumed to hold for the calorimetric material being used, and the propositions are mathematically written:\n\nThe thermal response of the calorimetric material is fully described by its pressure formula_1 as the value of its constitutive function formula_2 of just the volume formula_3 and the temperature formula_4. All increments are here required to be very small. This calculation refers to a domain of volume and temperature of the body in which no phase change occurs, and there is only one phase present. An important assumption here is continuity of property relations. A different analysis is needed for phase change\n\nWhen a small increment of heat is gained by a calorimetric body, with small increments, formula_5 of its volume, and formula_6 of its temperature, the increment of heat, formula_7, gained by the body of calorimetric material, is given by\n\nwhere\n\nThe latent heat with respect to volume is the heat required for unit increment in volume at constant temperature. It can be said to be 'measured along an isotherm', and the pressure the material exerts is allowed to vary freely, according to its constitutive law formula_18. For a given material, it can have a positive or negative sign or exceptionally it can be zero, and this can depend on the temperature, as it does for water about 4 C. The concept of latent heat with respect to volume was perhaps first recognized by Joseph Black in 1762. The term 'latent heat of expansion' is also used. The latent heat with respect to volume can also be called the 'latent energy with respect to volume'. For all of these usages of 'latent heat', a more systematic terminology uses 'latent heat capacity'.\n\nThe heat capacity at constant volume is the heat required for unit increment in temperature at constant volume. It can be said to be 'measured along an isochor', and again, the pressure the material exerts is allowed to vary freely. It always has a positive sign. This means that for an increase in the temperature of a body without change of its volume, heat must be supplied to it. This is consistent with common experience.\n\nQuantities like formula_7 are sometimes called 'curve differentials', because they are measured along curves in the formula_20 surface.\n\nConstant-volume calorimetry is calorimetry performed at a constant volume. This involves the use of a constant-volume calorimeter. Heat is still measured by the above-stated principle of calorimetry.\n\nThis means that in a suitably constructed calorimeter, called a bomb calorimeter, the increment of volume formula_5 can be made to vanish, formula_22. For constant-volume calorimetry:\n\nwhere\n\nFrom the above rule of calculation of heat with respect to volume, there follows one with respect to pressure.\n\nIn a process of small increments, formula_26 of its pressure, and formula_6 of its temperature, the increment of heat, formula_7, gained by the body of calorimetric material, is given by\n\nwhere\n\nThe new quantities here are related to the previous ones:\n\nwhere\n\nand\n\nThe latent heats formula_9 and formula_30 are always of opposite sign.\n\nIt is common to refer to the ratio of specific heats as\n\nAn early calorimeter was that used by Laplace and Lavoisier, as shown in the figure above. It worked at constant temperature, and at atmospheric pressure. The latent heat involved was then not a latent heat with respect to volume or with respect to pressure, as in the above account for calorimetry without phase change. The latent heat involved in this calorimeter was with respect to phase change, naturally occurring at constant temperature. This kind of calorimeter worked by measurement of mass of water produced by the melting of ice, which is a phase change.\n\nFor a time-dependent process of heating of the calorimetric material, defined by a continuous joint progression formula_53 of formula_54 and formula_55, starting at time formula_56 and ending at time formula_57, there can be calculated an accumulated quantity of heat delivered, formula_58 . This calculation is done by mathematical integration along the progression with respect to time. This is because increments of heat are 'additive'; but this does not mean that heat is a conservative quantity. The idea that heat was a conservative quantity was invented by Lavoisier, and is called the 'caloric theory'; by the middle of the nineteenth century it was recognized as mistaken. Written with the symbol formula_59, the quantity formula_58 is not at all restricted to be an increment with very small values; this is in contrast with formula_7.\n\nOne can write\n\nThis expression uses quantities such as formula_65 which are defined in the section below headed 'Mathematical aspects of the above rules'.\n\nThe use of 'very small' quantities such as formula_7 is related to the physical requirement for the quantity formula_2 to be 'rapidly determined' by formula_3 and formula_4; such 'rapid determination' refers to a physical process. These 'very small' quantities are used in the Leibniz approach to the infinitesimal calculus. The Newton approach uses instead 'fluxions' such as formula_70, which makes it more obvious that formula_2 must be 'rapidly determined'.\n\nIn terms of fluxions, the above first rule of calculation can be written\n\nwhere\n\nThe increment formula_7 and the fluxion formula_65 are obtained for a particular time formula_73 that determines the values of the quantities on the righthand sides of the above rules. But this is not a reason to expect that there should exist a mathematical function formula_82. For this reason, the increment formula_7 is said to be an 'imperfect differential' or an 'inexact differential'. Some books indicate this by writing formula_84 instead of formula_7. Also, the notation \"đQ\" is used in some books. Carelessness about this can lead to error.<ref name=\"Planck 1923/1926 57\">Planck, M. (1923/1926), page 57.</ref>\n\nThe quantity formula_62 is properly said to be a functional of the continuous joint progression formula_53 of formula_54 and formula_55, but, in the mathematical definition of a function, formula_62 is not a function of formula_20. Although the fluxion formula_65 is defined here as a function of time formula_73, the symbols formula_94 and formula_82 respectively standing alone are not defined here.\n\nThe above rules refer only to suitable calorimetric materials. The terms 'rapidly' and 'very small' call for empirical physical checking of the domain of validity of the above rules.\n\nThe above rules for the calculation of heat belong to pure calorimetry. They make no reference to thermodynamics, and were mostly understood before the advent of thermodynamics. They are the basis of the 'thermo' contribution to thermodynamics. The 'dynamics' contribution is based on the idea of work, which is not used in the above rules of calculation.\n\nEmpirically, it is convenient to measure properties of calorimetric materials under experimentally controlled conditions.\n\nFor measurements at experimentally controlled volume, one can use the assumption, stated above, that the pressure of the body of calorimetric material is can be expressed as a function of its volume and temperature.\n\nFor measurement at constant experimentally controlled volume, the isochoric coefficient of pressure rise with temperature, is defined by\n\nFor measurements at experimentally controlled pressure, it is assumed that the volume formula_3 of the body of calorimetric material can be expressed as a function formula_98 of its temperature formula_4 and pressure formula_1. This assumption is related to, but is not the same as, the above used assumption that the pressure of the body of calorimetric material is known as a function of its volume and temperature; anomalous behaviour of materials can affect this relation.\n\nThe quantity that is conveniently measured at constant experimentally controlled pressure, the isobaric volume expansion coefficient, is defined by\n\nFor measurements at experimentally controlled temperature, it is again assumed that the volume formula_3 of the body of calorimetric material can be expressed as a function formula_98 of its temperature formula_4 and pressure formula_1, with the same provisos as mentioned just above.\n\nThe quantity that is conveniently measured at constant experimentally controlled temperature, the isothermal compressibility, is defined by\n\nAssuming that the rule formula_18 is known, one can derive the function formula_108 that is used above in the classical heat calculation with respect to pressure. This function can be found experimentally from the coefficients formula_109 and formula_110 through the mathematically deducible relation\n\nThermodynamics developed gradually over the first half of the nineteenth century, building on the above theory of calorimetry which had been worked out before it, and on other discoveries. According to Gislason and Craig (2005): \"Most thermodynamic data come from calorimetry...\" According to Kondepudi (2008): \"Calorimetry is widely used in present day laboratories.\"\n\nIn terms of thermodynamics, the internal energy formula_112 of the calorimetric material can be considered as the value of a function formula_113 of formula_20, with partial derivatives formula_115 and formula_116.\n\nThen it can be shown that one can write a thermodynamic version of the above calorimetric rules:\n\nwith\n\nand\n\nAgain, further in terms of thermodynamics, the internal energy formula_112 of the calorimetric material can sometimes, depending on the calorimetric material, be considered as the value of a function formula_121 of formula_122, with partial derivatives formula_123 and formula_116, and with formula_3 being expressible as the value of a function formula_126 of formula_122, with partial derivatives formula_128 and formula_129 .\n\nThen, according to Adkins (1975), it can be shown that one can write a further thermodynamic version of the above calorimetric rules:\n\nwith\n\nand\n\nBeyond the calorimetric fact noted above that the latent heats formula_9 and formula_30 are always of opposite sign, it may be shown, using the thermodynamic concept of work, that also\n\nCalorimetry has a special benefit for thermodynamics. It tells about the heat absorbed or emitted in the isothermal segment of a Carnot cycle.\n\nA Carnot cycle is a special kind of cyclic process affecting a body composed of material suitable for use in a heat engine. Such a material is of the kind considered in calorimetry, as noted above, that exerts a pressure that is very rapidly determined just by temperature and volume. Such a body is said to change reversibly. A Carnot cycle consists of four successive stages or segments:\n\n(3) another isothermal change in volume from formula_140 to a volume formula_142 at constant temperature formula_143 such as to incur a flow or heat out of the body and just such as to precisely prepare for the following change\n\n(4) another adiabatic change of volume from formula_142 back to formula_136 just such as to return the body to its starting temperature formula_138.\n\nIn isothermal segment (1), the heat that flows into the body is given by\n\nand in isothermal segment (3) the heat that flows out of the body is given by\n\nBecause the segments (2) and (4) are adiabats, no heat flows into or out of the body during them, and consequently the net heat supplied to the body during the cycle is given by\n\nThis quantity is used by thermodynamics and is related in a special way to the net work done by the body during the Carnot cycle. The net change of the body's internal energy during the Carnot cycle, formula_150, is equal to zero, because the material of the working body has the special properties noted above.\n\nThe quantity formula_9, the latent heat with respect to volume, belongs to classical calorimetry. It accounts for the occurrence of energy transfer by work in a process in which heat is also transferred; the quantity, however, was considered before the relation between heat and work transfers was clarified by the invention of thermodynamics. In the light of thermodynamics, the classical calorimetric quantity is revealed as being tightly linked to the calorimetric material's equation of state formula_18. Provided that the temperature formula_153 is measured in the thermodynamic absolute scale, the relation is expressed in the formula\n\nAdvanced thermodynamics provides the relation\n\nFrom this, further mathematical and thermodynamic reasoning leads to another relation between classical calorimetric quantities. The difference of specific heats is given by\n\nConstant-volume calorimetry is calorimetry performed at a constant volume. This involves the use of a constant-volume calorimeter.\n\nNo work is performed in constant-volume calorimetry, so the heat measured equals the change in internal energy of the system. The heat capacity at constant volume is assumed to be independent of temperature.\n\nHeat is measured by the principle of calorimetry.\n\nwhere \n\nIn \"constant-volume calorimetry\" the pressure is not held constant. If there is a pressure difference between initial and final states, the heat measured needs adjustment to provide the \"enthalpy change\". One then has\n\nwhere \n\n", "id": "7522", "title": "Calorimetry"}
{"url": "https://en.wikipedia.org/wiki?curid=7525", "text": "Charles Evans Hughes\n\nCharles Evans Hughes, Sr. (April 11, 1862 – August 27, 1948) was an American statesman, lawyer, and Republican politician from New York. He served as the 36th Governor of New York (1907–1910), Associate Justice of the Supreme Court of the United States (1910–1916), United States Secretary of State (1921–1925), a judge on the Court of International Justice (1928–1930), and the 11th Chief Justice of the United States (1930–1941). He was the Republican nominee in the 1916 U.S. Presidential election, losing narrowly to incumbent President Woodrow Wilson.\n\nHughes was a professor in the 1890s, a staunch supporter of Britain's New Liberalism, an important leader of the progressive movement of the 20th century, a leading diplomat and New York lawyer in the days of Harding and Coolidge, and was known for being a swing voter when dealing with cases related to the New Deal in the 1930s. Historian Clinton Rossiter has hailed him as a leading American conservative.\n\nCharles Evans Hughes was born in Glens Falls, New York, the son of a Welsh immigrant minister Rev. David C. Hughes and Mary C. (Connelly) Hughes, a sister of State Senator Henry C. Connelly (1832–1912). He was active in the Northern Baptist church, a Mainline Protestant denomination. \n\nHughes' early education included attending Lafayette School in Newark, NJ. At the age of 14, he enrolled at Madison University (now Colgate University), where he became a member of Delta Upsilon fraternity. He then transferred to Brown University, continuing as a member of Delta Upsilon. He graduated third in his class at the age of 19, having been elected to Phi Beta Kappa in his junior year. He read law and entered Columbia Law School in 1882, where he graduated in 1884 with highest honors.\n\nIn 1885, Hughes met Antoinette Carter, the daughter of a senior partner of the law firm where he worked, and they were married in 1888. They had one son, Charles Evans Hughes Jr. and three daughters. Their youngest child, Elizabeth Hughes Gossett, was one of the first humans injected with insulin, and later served as president of the Supreme Court Historical Society. Hughes was the grandfather of Charles Evans Hughes III and H. Stuart Hughes.\n\nAfter graduating Hughes began working for Chamberlain, Carter & Hornblower where he met his future wife. In 1888, shortly after he was married, he became a partner in the firm, and the name was changed to Carter, Hughes & Cravath. Later the name was changed to Hughes, Hubbard & Reed. In 1891, Hughes left the practice of law to become a professor at Cornell Law School. In 1893, he returned to his old law firm in New York City to continue practicing until he ran for governor in 1906. He continued his association with Cornell as a special lecturer at the Law School from 1893 to 1895. He was also a special lecturer for New York University Law School, 1893–1900.\n\nAt that time, in addition to practicing law, Hughes taught at New York Law School with Woodrow Wilson, who would later defeat him for the Presidency. In 1905, he was appointed as counsel to the New York state legislative \"Stevens Gas Commission\", a committee investigating utility rates. His uncovering of corruption led to lower gas rates in New York City. In 1905, he was appointed to the Armstrong Insurance Commission to investigate the insurance industry in New York as a special assistant to U.S. Attorney General.\n\nHughes served as the Governor of New York from 1907 to 1910. He defeated William Randolph Hearst in the 1906 election, and was the only Republican statewide candidate to win office. An admirer of Britain's New Liberal philosophy, Hughes campaigned on a platform to improve the state of New York's standard of living by moving it away from laissez-faire tradition and enacting social reforms similar to that which had been enacted in Britain. As a supporter of progressive policies, Hughes was able to play on the popularity of Theodore Roosevelt and weaken the power of the state's conservative Republican officials. In 1908, he was offered the vice-presidential nomination by William Howard Taft, but he declined it to run again for Governor. Theodore Roosevelt became an important supporter of Hughes.\n\nAs the Governor, Hughes produced important reform legislation in three areas: improvement of the machinery and processes of government; extension of the state's regulatory authority over businesses engaged in public services; and expansion of governmental police and welfare functions. To counter political corruption, he secured campaign laws in 1906 and 1907 that limited political contributions by corporations and forced candidates to account for their receipts and expenses, legislation that was quickly copied in fifteen other states. He pushed the passage of the Moreland Act, which enabled the governor to oversee city and county officials as well as officials in semi-autonomous state bureaucracies. This allowed him to fire many corrupt officials. He also managed to have the powers of the state's Public Service Commissions increased and fought strenuously, if not completely successfully, to get their decisions exempted from judicial review.\n\nWhen two bills were passed to reduce railroad fares, Hughes vetoed them on the grounds that the rates should be set by expert commissioners rather than by elected ones. His ideal was not government by the people but for the people. As Hughes put it, \"you must have administration by administrative officers.\"\n\nHughes, however, would be unsuccessful in achieving one of his main goals as governor: primary voting reform. Hoping to achieve a compromise with the state's party bosses, Hughes rejected the option of a direct primary in which voters could choose between declared candidates and instead proposed a complicated system of nominations by party committees. The state's party bosses, however, rejected this compromise and the state legislature rejected the plan on three occasions in 1909 and 1910.\n\nOn social issues, Hughes strongly supported relatively limited social reforms. He endorsed the Page-Prentice Act of 1907, which set an eight-hour day and forty-eight-hour week for factory workers—but only for those under the age of sixteen. By employing the well-established legal distinction between ordinary and hazardous work, the governor also won legislative approval for a Dangerous Trades Act that barred young workers from thirty occupations. To enforce these and other regulations, in 1907 Hughes reorganized the Department of Labor and appointed a well-qualified commissioner. Two years later, the governor created a new bureau for immigrant issues in the Department of Labor and appointed reformer Frances Kellor to head it.\n\nIn his final year as the Governor, he had the state comptroller draw up an executive budget. This began a rationalization of state government and eventually it led to an enhancement of executive authority. He also signed the Worker's Compensation Act of 1910, which required a compulsory, employer-paid plan of compensation for workers injured in hazardous industries and a voluntary system for other workers; after the New York Court of Appeals ruled the law unconstitutional in 1911, a popular referendum was held that successfully made the law an amendment in the New York Constitution.\n\nIn 1908, Governor Hughes reviewed the clemency petition of Chester Gillette concerning the murder of Grace Brown. The governor denied the petition as well as an application for reprieve, and Gillette was electrocuted in March of that year.\n\nWhen Hughes left office, a prominent journal remarked \"One can distinctly see the coming of a New Statism ... [of which] Gov. Hughes has been a leading prophet and exponent\". In 1926, Hughes was appointed by New York Governor Alfred E. Smith to be the chairman of a \"State Reorganization Commission\" through which Smith's plan to place the Governor as the head of a rationalized state government, was accomplished, bringing to realization what Hughes himself had envisioned.\n\nIn 1909, Hughes led an effort to incorporate Delta Upsilon fraternity. This was the first fraternity to incorporate, and he served as its first international president.\n\nOn April 25, 1910, President William H. Taft nominated Hughes for Associate Justice to fill the vacancy left by the death of Justice David J. Brewer. The Senate confirmed the nomination on May 2, 1910, and Hughes received his commission the same day. As an associate justice of the Supreme Court from 1910 to 1916, Hughes remained an advocate of regulation and authored decisions that weakened the legal foundations of laissez-faire capitalism. He also mastered a new set of issues regarding the Commerce Clause and, in a deliberately restrained manner, wrote constitutional decisions that expanded the regulatory powers of both the state and federal governments.\n\nHe wrote for the court in \"Bailey v. Alabama\" , which held that involuntary servitude encompassed more than just slavery, and \"Interstate Commerce Comm. v. Atchison T & SF R Co.\" , holding that the Interstate Commerce Commission could regulate intrastate rates if they were significantly intertwined with interstate commerce.\n\nOn April 15, 1915 in the case of \"Frank v. Mangum\", the Supreme Court decided (7-2) to deny an appeal made by Leo Frank's attorneys, and instead upheld the decision of lower courts to sustain the guilty verdict against Frank. Justice Hughes and Justice Oliver Wendell Holmes Jr. were the two dissenting votes.\n\nHughes resigned from the Supreme Court on June 10, 1916, to be the Republican candidate for President in 1916. He is the last sitting Supreme Court justice to surrender his or her seat to run for elected office. He was also endorsed by the Progressive Party, thanks to the support given to him from former President Theodore Roosevelt. Other Republican figures such as former President William Howard Taft endorsed Hughes and felt the accomplishments he made as Governor of New York would establish him as formidable progressive alternative to Wilson. Many former leaders of the Progressive Party, however, endorsed Wilson because Hughes opposed the Adamson Act, the Sixteenth Amendment and diverted his focus away from progressive issues during the course of the campaign. Hughes was defeated by Woodrow Wilson in a close election (separated by 23 electoral votes and 594,188 popular votes). The election hinged on California, where Wilson managed to win by 3,800 votes and its 13 electoral votes and thus was returned for a second term; Hughes had lost the endorsement of the California governor and Roosevelt's 1912 Progressive running mate Hiram Johnson when he failed to show up for an appointment with him.\n\nDespite coming close to winning the presidency, Hughes did not seek the Republican nomination again in 1920. Hughes also advocated ways to prevent the return of President Wilson's expanded government control over important industries such as the nation's railroads, which he felt would lead to the eventual destruction of individualism and political self-rule. After Robert LaFollette's Progressive Party advocated the return of such regulations during the 1924 US Presidential election, Hughes shifted rightwards believing that the federal bureaucracy should now have limited powers over individual liberties and property rights and that common law should be strictly enforced.\n\nHughes returned to government office in March 1921 as Secretary of State under President Harding. On November 11, 1921, Armistice Day (later changed to Veterans Day), the Washington Naval Conference for the limitation of naval armament among the Great Powers began. The major naval powers of Britain, France, Italy, Japan and the United States were in attendance as well as other nations with concerns about territories in the Pacific — Belgium, the Netherlands, Portugal and China.\n\nThe American delegation was headed by Hughes and included Elihu Root, Henry Cabot Lodge, and Oscar Underwood, the Democratic minority leader in the Senate. The conference continued until February 1922 and included the Four-power pact (December 13, 1921), Shantung Treaty (February 4, 1922), Five-Power Treaty, the Nine-Power Treaty (February 6, 1922), the \"Six-power pact\" that was an agreement between the Big Five Nations plus China to divide the German cable routes in the Pacific, and the Yap Island agreement.\n\nHughes continued in office after Harding died and was succeeded by Coolidge, but resigned after Coolidge was elected to a full term. On June 30, 1922, he signed the \"Hughes–Peynado agreement\" that ended the United States's six-year occupation of Dominican Republic.\n\nIn 1907, Gov. Charles Evans Hughes became the first president of the newly formed Northern Baptist Convention—based at Calvary Baptist Church in Washington, DC, of which Hughes was a member. He also served as President of the New York State Bar Association.\n\nAfter leaving the State Department, he again rejoined his old partners at the Hughes firm, which included his son and future United States Solicitor General Charles E. Hughes Jr., and was one of the nation's most sought-after advocates. From 1925 to 1930, for example, Hughes argued over 50 times before the U.S. Supreme Court. From 1926 to 1930, Hughes also served as a member of the Permanent Court of Arbitration and as a judge of the Permanent Court of International Justice in The Hague, Netherlands from 1928 to 1930. He was additionally a delegate to the Pan American Conference on Arbitration and Conciliation from 1928 to 1930. He was one of the co-founders in 1927 of the National Conference on Christians and Jews, now known as the National Conference for Community and Justice (NCCJ), along with S. Parkes Cadman and others, to oppose the Ku Klux Klan, anti-Catholicism, and anti-Semitism in the 1920s and 1930s.\n\nIn 1925–1926, Charles Evans Hughes represented the API (American Petroleum Institute) before the FOCB (Federal Oil Conservation Board).\n\nIn 1928 conservative business interests tried to interest Hughes in the Republican presidential nomination of 1928 instead of Herbert Hoover. Hughes, citing his age, turned down the offer.\n\nHerbert Hoover, who had appointed Hughes's son as Solicitor General in 1929, appointed Hughes Chief Justice of the United States on February 3, 1930. Hughes was confirmed by the United States Senate on February 13, 1930, and received commission the same day, serving in this capacity until 1941. Hughes replaced former President William Howard Taft, a fellow Republican who had also lost a presidential election to Woodrow Wilson (in 1912) and who, in 1910, had appointed Hughes to his first tenure on the Supreme Court.\n\nHughes' appointment was opposed by progressive elements in both parties who felt that he was too friendly to big business. Idaho Republican William E. Borah said on the United States Senate floor that confirming Hughes would constitute \"placing upon the Court as Chief Justice one whose views are known upon these vital and important questions and whose views, in my opinion, however sincerely entertained, are not which ought to be incorporated in and made a permanent part of our legal and economic system.\" In addition to his politics, at 67, Hughes was the oldest man ever nominated as Chief Justice. Nonetheless Hughes was confirmed as Chief Justice with a vote of 52 to 26.\n\nHughes as Chief Justice swore in President Franklin D. Roosevelt in 1933, 1937 and 1941.\n\nUpon his return to the court, more progressives had joined the bench. Hughes seemed determined again to vote progressive and soon bring an end to the longstanding pro-business Lochner era. During his early years as Chief Justice, however, the fear he had developed for an overblown bureaucracy during World War I undermined his optimism. Showing his old progressive image, he upheld legislation protecting civil rights and civil liberties and wrote the opinion for the Court in \"Near v. Minnesota\" , which held prior restraint against the press is unconstitutional. Concerning economic regulation, he was still willing to uphold legislation that supported \"freedom of opportunity\" for individuals on the one hand and the \"police power\" of the state on the other but did not personally favor legislation that linked national economic planning and bureaucratic social welfare together. At first resisting Roosevelt's New Deal and building a consensus of centrist members of the court, Hughes used his influence to limit the collectivist scope of Roosevelt's changes and would often strike down New Deal legislation he felt was poorly drafted and did not clearly specify how they were constitutional. By 1935, Hughes felt the court's four conservative Justices had disregarded common law and sought to curb their power.\n\nHughes was often aligned with the court's three liberal Justices — Louis Brandeis, Harlan Fiske Stone, and Benjamin Cardozo — in finding some New Deal measures (such as the violation of the gold clauses in contracts and the confiscation of privately owned monetary gold) constitutional. On one occasion, Hughes would side with the conservatives in striking down the New Deal's Agricultural Adjustment Act in the 1936 case \"United States v. Butler\", which held that the law was unconstitutional because its so-called tax policy was a coercive regulation rather than a tax measure and the federal government lacked authority to regulate agriculture. But surprisingly he did not assign the majority opinion, a practice usually required for court's most senior justice who agrees with the majority opinion, and allowed Associate Justice Owen Roberts to speak for the entire majority in his own words. It was accepted that he did not agree with the argument that the federal government lacked authority over agriculture and was going to write a separate opinion upholding the act's regulation policy while striking down the act's taxation policy on the grounds that it was a coercive regulation rather than a tax measure. However, Roberts convinced Hughes that he would side with him and the three liberal justices in future cases pertaining to the nation's agriculture that involved the Constitution's General Welfare Clause if he agreed to join his opinion.\n\nBy 1936, Hughes sensed the growing hostility in the court and could do little about it. In the 1936 case \"Carter v. Carter Coal Company\", Hughes took a middle ground for doctrinal and court-management reasons. Writing his own opinion, he joined the three liberal justices in upholding the Bituminous Coal Conservation Act's marketing provision but sided with Roberts and the four conservatives in striking down the act's provision that regulated local labor. By 1937, as the court leaned more in his favor, Hughes would renounce the position he took in the \"Carter\" case regarding local labor and ruled that the procedural methods that governed the Wagner Act's labor regulation provisions bore resemblance to the procedural methods which governed the railroad rates that the Interstate Commerce Commission was allowed to maintain in the 1914 \"Shreveport\" decision; he thus demonstrated that Congress could use its commerce power to regulate local industrial labor as well.\n\nIn 1937, when Roosevelt attempted to pack the Court with six additional justices, Hughes worked behind the scenes to defeat the effort, which failed in the Senate, by rushing important New Deal legislation — such as Wagner Act and the Social Security Act — through the court and ensuring that the court's majority would uphold their constitutionality. The month after Roosevelt's court-packing announcement, Roberts, who had joined the four conservative Justices in striking down important New Deal legislation, shocked the American public by siding with Hughes and the court's three liberal justices in striking down the court's ruling in the 1923 \"Adkins v. Children's Hospital\" case — which held that laws requiring minimum wage violated the Fifth Amendment's due process clause — and upholding the constitutionality of Washington state's minimum wage law in \"West Coast Hotel Co. v. Parrish\". Because Roberts had previously sided with the four conservative justices and used the \"Adkins\" decision as the basis for striking down a similar minimum wage law the state of New York enforced in \"Morehead v. New York ex rel. Tipaldo\", it was widely perceived that he only agreed to uphold the constitutionality of minimum wage as a result of the pressure that was put on the Supreme Court by the court-packing plan. However, Hughes and Roberts acknowledged that the Chief Justice had already convinced Roberts to change his method of voting months before Roosevelt announced his court-packing plan and that the effort he put into defeating the plan played only a small significance in determining how the court's majority made their decisions in future cases pertaining to New Deal legislation.\n\nFollowing the overwhelming support that voters showed for the New Deal through Roosevelt's overwhelming re-election in November 1936, Hughes was not able to persuade Roberts to base his votes on political maneuvering and to side with him in future cases regarding New Deal-related policies. Roberts had voted to grant \"certiorari\" to hear the \"Parrish\" case before the election of 1936. Oral arguments occurred on December 16 and 17, 1936, with counsel for Parrish specifically asking the court to reconsider its decision in \"Adkins v. Children's Hospital\", which had been the basis for striking down a New York minimum wage law in \"Morehead v. New York ex rel. Tipaldo\" in the late spring of 1936.\n\nRoberts indicated his desire to overturn \"Adkins\" immediately after oral arguments ended for the Parrish case on December 17, 1936. The initial conference vote on December 19, 1936 was 4-4; with this even division on the Court, the holding of the Washington Supreme Court, finding the minimum wage statute constitutional, would stand. The eight voting justices anticipated Justice Stone — absent due to illness — would be the fifth vote necessary for a majority opinion affirming the constitutionality of the minimum wage law. As Hughes desired a clear and strong 5-4 affirmation of the Washington Supreme Court's judgment, rather than a 4-4 default affirmation, he convinced the other justices to wait until Stone's return before deciding and announcing the case. In one of his notes from 1936, Hughes wrote that Roosevelt's re-election forced the court to depart from its \"fortress in public opinion\" and severely weakened its capability to base its rulings on personal or political beliefs.\n\nPresident Roosevelt announced his court reform bill on February 5, 1937, the day of the first conference vote after Stone's February 1, 1937 return to the bench. Roosevelt later made his justifications for the bill to the public on March 9, 1937 during his ninth Fireside Chat. The Court's opinion in \"Parrish\" was not published until March 29, 1937, after Roosevelt's radio address. Hughes wrote in his autobiographical notes that Roosevelt's court reform proposal \"had not the slightest effect on our [the court's] decision,\" but due to the delayed announcement of its decision the Court was characterized as retreating under fire.\n\nAlthough Hughes wrote the opinion invalidating the National Recovery Administration in \"Schechter Poultry Corp. v. United States\" — though the decision was unanimously upheld by all of the court's Justices — he also wrote the opinions for the Court in \"NLRB v. Jones & Laughlin Steel Corp.\", \"NLRB v. Friedman-Harry Marks Clothing Co.\" and \"West Coast Hotel Co. v. Parrish\" which approved some New Deal measures. Hughes supervised the move of the Court from its former quarters at the U.S. Capitol to the newly constructed Supreme Court building.\n\nHughes wrote 199 majority opinions in his time on the bench, from 1930 to 1941. \"His opinions, in the view of one commentator, were concise and admirable, placing Hughes in the pantheon of great justices.\" His \"remarkable intellectual and social gifts...made him a superb leader and administrator. He had a photographic memory that few, if any, of his colleagues could match. Yet he was generous, kind, and forebearing in an institution where egos generally come in only one size: extra large!\"\n\nFor many years, he was a member of the Union League Club of New York and served as its president from 1917 to 1919.\n\nIn 1907 Hughes, then the Governor of New York, was elected to honorary membership in the Empire State Society of the Sons of the American Revolution. He was assigned national membership number 18,977.\n\nOn August 27, 1948, at the age of 86, Hughes died in what is now the Tiffany Cottage of the Wianno Club in Osterville, Massachusetts. He is interred at Woodlawn Cemetery in The Bronx, New York City.\n\n\n\n\n", "id": "7525", "title": "Charles Evans Hughes"}
{"url": "https://en.wikipedia.org/wiki?curid=7527", "text": "Concept album\n\nA concept album is an album unified by a larger purpose or meaning to the album collectively than to its tracks individually. This may be achieved through a single central narrative or theme, or through a sense of artistic cohesiveness. The exact criteria of a \"concept album\" varies, with no discernible consensus.\n\nThe format originates with folk singer Woody Guthrie's \"Dust Bowl Ballads\" (1940) and subsequently popularized by traditional pop singer Frank Sinatra's 1940s–50s string of albums, but the term is more often associated with rock music. In the 1960s, several well-regarded concept albums were released by various rock bands, which eventually led to the invention of progressive rock and the rock opera. Since then, many concept albums have been released across many different musical genres.\n\nConcepts are general ideas, thoughts, or abstract notions. There is no clear definition of what constitutes a \"concept album\". Fiona Sturges of \"The Independent\" stated that the concept album \"was originally defined as a long-player where the songs were based on one dramatic idea – but the term is subjective.\" A precursor to this type of album can be found in the 19th century song cycle which ran into some of the same difficulties in classification. The extremely broad definitions of a \"concept album\" could potentially encompass all soundtracks, compilations, cast recordings, greatest hits albums, tribute albums, Christmas albums, and live albums.\n\nThe most common definitions refer to an expanded approach to the rock album format (as a story, play, or opus), or a project that either revolves around a specific theme or a collection of related materials. AllMusic writes, \"A concept album could be a collection of songs by an individual songwriter or a particular theme -- these are the concept LPs that reigned in the '50s ... the phrase 'concept album' is inextricably tied to the late 1960s, when rock & rollers began stretching the limits of their art form.\" Author Jim Cullen describes it: \"a collection of discrete but thematically unified songs whose whole is greater than the sum of its parts ... sometimes [erroneously] assumed to be a product of the rock era.\" Author Roy Shuker defines concept albums and rock operas as albums that are \"unified by a theme, which can be instrumental, compositional, narrative, or lyrical. ... In this form, the album changed from a collection of heterogeneous songs into a narrative work with a single theme, in which individual songs segue into one another.\"\n\nYes keyboardist Rick Wakeman considers the first concept album to be Woody Guthrie's 1940 album \"Dust Bowl Ballads\". \"The Independent\" regards it as \"perhaps\" one of the first concept albums, consisting exclusively of semi-autobiographical songs about the hardships of American migrant labourers during the 1930s.\n\nIn the late 1940s, the LP record was introduced, with space age pop composers making concept albums soon after. Some of the themes were about exploring wild life; some were made to be played while dining or relaxing; others were more abstract and centered on emotions. This was accompanied in the mid 1950s with the invention of the gatefold, which allowed room for liner notes to explain the concept.\n\nSinger Frank Sinatra recorded several concept albums prior to the 1960s rock era, including \"In the Wee Small Hours\" (1955) and \"Frank Sinatra Sings for Only the Lonely\" (1958). Sinatra is sometimes credited as the inventor of the concept album, beginning with \"The Voice of Frank Sinatra\" (1946), which led to similar work by Bing Crosby. According to biographer Will Friedwald: \"Sinatra sequenced the songs so that the lyrics created a flow from track to track, affording an impression of a narrative, as in musical comedy or opera. ... [He was the] first pop singer to bring a consciously artistic attitude to recording.\"\n\nThe author Carys Wyn Jones writes that the Beach Boys' \"Pet Sounds\" (May 1966), the Beatles' \"Revolver\" (August 1966) and \"Sgt. Pepper's Lonely Hearts Club Band\" (1967), and the Who's \"Tommy\" (1969) are variously cited as \"the first concept album\", usually for their \"uniform excellence rather than some lyrical theme or underlying musical motif\". Commentators and historians frequently cite \"Pet Sounds\" as the first concept album of rock music. According to music critic Tim Riley, \"Strictly speaking, the Mothers of Invention's \"Freak Out!\" [June 1966] has claims as the first 'concept album', but \"Sgt. Pepper\" was the record that made that idea convincing to most ears.\" Musicologist Allan Moore says that \"Even though previous albums had set a unified mood, it was on the basis of the influence of \"Sgt. Pepper\" that the penchant for the concept album was born.\"\n\nThere exists claims for other records as \"early\" or \"first\" concept albums. In the early 1960s, concept albums began featuring highly in American country music, but the fact has been largely ignored by rock/pop fans and critics who would only begin acknowledging \"concept albums\" as a phenomenon nearly ten years later. \"The 100 Greatest Bands of All Time\" (2015) states that the Ventures \"pioneered the idea of the rock concept album years before the genre is generally acknowledged to have been born\". \"Ultimate Guitar\" Matt Springer says that the Beach Boys' \"Little Deuce Coupe\" (1963) is \"considered to be one of the earliest examples of a rock concept album\". Brian Boyd of \"The Irish Times\" names the Kinks' \"Face to Face\" (October 1966) as the first concept album: \"Written entirely by Ray Davies, the songs were supposed to be linked by pieces of music, so that the album would play without gaps, but the record company baulked at such radicalism. It’s not one of the band’s finest works, but it did have an impact.\"\n\nAuthor Bill Martin relates the assumed concept albums of the 1960s to progressive rock:\n\n\"Popmatters\" Sarah Zupko notes that while the Who's \"Tommy\" is \"popularly thought of as the first rock opera, an extra-long concept album with characters, a consistent storyline, and a slight bit of pomposity\", it is preceded by the shorter concept albums \"Ogdens' Nut Gone Flake\" (Small Faces, 1968) and \"S.F. Sorrow\" (The Pretty Things, 1968). Author Jim Cullen states: \"The concept album reached its apogee in the 1970s in ambitious records like Pink Floyd's \"Dark Side of the Moon\" (1973) and the Eagles' \"Hotel California\" (1977).\" In 2015, \"Rolling Stone\" ranked \"Dark Side of the Moon\" number one among the 50 greatest prog albums of all-time, also noting the LP's stature as the second best-selling album of all time. Pink Floyd's \"The Wall\" (1979), a semi-autobiographical story modeled after the band's Roger Waters and Syd Barrett, is one of the most famous concept albums by any artist.\n\nAccording to author Edward Macan, concept albums as a recurrent theme in progressive rock was directly inspired by the counterculture associated with \"the proto-progressive bands of the 1960s\", observing: \"the consistent use of lengthy forms such as the programmatic song cycle of the concept album and the multimovement suite underscores the hippies' new, drug-induced conception of time.\"\n\nWith the emergence of MTV as a music video network which valued single over album, concept albums became less dominant in the 1980s. Some artists, however, still released concept albums and experienced success in the 1990s and 2000s. \"NME\"s Emily Barker cites Green Day's \"American Idiot\" (2004) as one of the \"more notable\" examples, having brought the concept album back to high-charting positions. Dorian Lynskey, writing for \"GQ\", noted a resurgence of concept albums in the 2010s due to streaming: \"This is happening not in spite of the rise of streaming and playlists, but because of it. Threatened with redundancy in the digital era, albums have fought back by becoming more album-like.\" Cucchiara argues that \"concept albums\" should also describe \"this new generation of concept albums, for one key reason. This is because the unison between the songs on a particular album has now been expanded into a broader field of visual and artistic design and marketing strategies that play into the themes and stories that form the album.\"\n\n", "id": "7527", "title": "Concept album"}
{"url": "https://en.wikipedia.org/wiki?curid=7530", "text": "Cro-hook\n\nThe cro-hook, is a special double-ended crochet hook used to make double-sided crochet. \nIt employs the use of a long double-ended hook, which permits the maker to work stitches on or off from either end. Because the hook has two ends, two alternating colors of thread can be used simultaneously and freely interchanged, working loops over the hook. Crafts using a double-ended hook are commercially marketed as Cro-hook and Crochenit. Cro-hook is a variation of Tunisian crochet and also shows similarities with the Afghan stitch used to make Afghan scarves, but the fabric is typically softer with greater elasticity.\n\n", "id": "7530", "title": "Cro-hook"}
{"url": "https://en.wikipedia.org/wiki?curid=7531", "text": "Clavichord\n\nThe clavichord is a European stringed keyboard instrument known from the late Medieval, through the Renaissance, Baroque and Classical eras. Historically, it was mostly used as a practice instrument and as an aid to composition, not being loud enough for larger performances. The clavichord produces sound by striking brass or iron strings with small metal blades called tangents. Vibrations are transmitted through the bridge(s) to the soundboard.\n\nThe name is derived from the Latin word \"clavis\", meaning \"key\" (associated with more common \"clavus\", meaning \"nail, rod, etc.\") and \"chorda\" (from Greek χορδή) meaning \"string, especially of a musical instrument\". An analogous name is used in other European languages (It. \"clavicordio\", \"clavicordo\"; Fr. \"clavicorde\"; Germ. \"Klavichord\"; Lat. \"clavicordium\"; Port. \"clavicórdio\"; Sp. \"clavicordio\"). Many languages also have another name derived from Latin \"manus\", meaning \"hand\" (It. \"manicordo\"; Fr. \"manicorde\", \"manicordion\"; Sp. \"manicordio\", \"manucordio\"). Other names refer to the monochord-like nature of a fully fretted clavichord (It. \"monacordo\" or \"monocordo\"; Sp. \"monacordio\"). Italian also used \"sordino\", a reference to its quiet sound (sordino usually designates a mute).\n\nThe clavichord was invented in the early fourteenth century. In 1504, the German poem \"\" mentions the terms clavicimbalum (a term used mainly for the harpsichord) and clavichordium, designating them as the best instruments to accompany melodies.\n\nOne of the earliest references to the clavichord in England occurs in the privy-purse expenses of Elizabeth of York, queen of Henry VII, in an entry dated August 1502:\nItem. The same day, Hugh Denys for money by him delivered to a stranger that gave the queen a payre of clavycordes. In crowns form his reward iiii libres.\n\nThe clavichord was very popular from the 16th century to the 18th century, but mainly flourished in German-speaking lands, Scandinavia, and the Iberian Peninsula in the latter part of this period. It had fallen out of use by 1850. In the late 1890s, Arnold Dolmetsch revived clavichord construction and Violet Gordon-Woodhouse, among others, helped to popularize the instrument. Although most of the instruments built before the 1730s were small (four octaves, four feet long), the latest instruments were built up to seven feet long with a six octave range.\n\nToday clavichords are played primarily by Renaissance, Baroque, and Classical music enthusiasts. They attract many interested buyers, and are manufactured worldwide. There are now numerous clavichord societies around the world, and some 400 recordings of the instrument have been made in the past 70 years. Leading modern exponents of the instrument have included Derek Adlam, Christopher Hogwood, Richard Troeger, Thurston Dart, Wim Winters and Miklos Spányi.\n\nThe clavichord has also gained attention in other genres of music, in the form of the Clavinet, which is essentially an electric clavichord that uses a magnetic pickup to produce a signal for amplification. Stevie Wonder uses a Clavinet in many of his songs, such as \"Superstition\" and \"Higher Ground\". A Clavinet played through an instrument amplifier with guitar effect pedals is often associated with funky, disco-infused 1970s rock.\n\nGuy Sigsworth has played clavichord in a modern setting with Björk, notably on the studio recording of \"All Is Full of Love\". Björk also made extensive use of and even played the instrument herself on the song \"My Juvenile\" of her 2007 album \"Volta\".\n\nTori Amos uses the instrument on \"Little Amsterdam\" from the album \"Boys for Pele\" and on the song \"Smokey Joe\" from her 2007 album \"American Doll Posse\". Amos also featured her use of the Clavinet on her 2004 recording \"Not David Bowie\", released as part of her 2006 box set, \"\".\n\nIn 1976 Oscar Peterson played (with Joe Pass on acoustic guitar) songs from \"Porgy And Bess\" on the clavichord. Keith Jarrett also recorded an album entitled \"Book of Ways\" (1987) in which he plays a series of clavichord improvisations. The Beatles' \"For No One\" (1966) features Paul McCartney playing the clavichord. Rick Wakeman plays the Clavinet in the track \"The Battle\" from the album \"Journey to the Centre of the Earth\".\n\nIn the clavichord, strings run transversely from the hitchpin rail at the left-hand end to tuning pegs on the right. Towards the right end they pass over a curved wooden bridge. The action is simple, with the keys being levers with a small brass tangent, a small piece of metal similar in shape and size to the head of a flat-bladed screwdriver, at the far end. The strings, which are usually of brass, or else a combination of brass and iron, are usually arranged in pairs, like a lute or mandolin. When the key is pressed, the tangent strikes the strings above, causing them to sound in a similar fashion to the \"hammering\" technique on a guitar. Unlike in a piano action, the tangent does not rebound from the string; rather, it stays in contact with the string as long as the key is held, acting as both the nut and as the initiator of sound. The volume of the note can be changed by striking harder or softer, and the pitch can also be affected by varying the force of the tangent against the string (known as \"Bebung\"). When the key is released, the tangent loses contact with the string and the vibration of the string is silenced by strips of damping cloth.\n\nThe action of the clavichord is unique among all keyboard instruments in that one part of the action simultaneously initiates the sound vibration while at the same time defining the endpoint of the vibrating string, and thus its pitch. Because of this intimate contact between the player's hand and the production of sound, the clavichord has been referred to as the most intimate of keyboard instruments. Despite its many (serious) limitations, including extremely low volume, it has considerable expressive power, the player being able to control attack, duration, volume, and even provide certain subtle effects of swelling of tone and a type of vibrato unique to the clavichord.\n\nSince the string vibrates from the bridge only as far as the tangent, multiple keys with multiple tangents can be assigned to the same string. This is called \"fretting\". Early clavichords frequently had many notes played on each string, even going so far as the keyed monochord — an instrument with only one string—though most clavichords were triple- or double-fretted. Since only one note can be played at a time on each string, the fretting pattern is generally chosen so that notes rarely heard together (such as C and C) share a string pair. The advantages of this system compared with unfretted instruments (see below) include relative ease of tuning (with around half as many strings to keep in tune), greater volume (though still not really enough for use in chamber music), and a clearer, more direct sound. Among the disadvantages: temperament could not be re-set without bending the tangents; and playing required a further refinement of touch, since notes sharing a single string played in quick succession had to be slightly separated to avoid a disagreeable deadening of the sound, potentially disturbing a legato line.\n\nSome clavichords have been built with a single pair of strings for each note. The first known reference to one was by Johann Speth in 1693 and the earliest such extant signed and dated clavichord was built in 1716 by Johann Michael Heinitz. Such instruments are referred to as \"unfretted\" whereas instruments using the same strings for several notes are called \"fretted\". Among the advantages to unfretted instruments are flexibility in tuning (the temperament can be easily altered) and the ability to play any music exactly as written without concern for \"bad\" notes. Disadvantages include a smaller volume, even though many or most unfretted instruments tend to be significantly larger than fretted instruments; and \"many\" more strings to keep in tune. Unfretted instruments tend to have a sweeter, less incisive tone due to the greater load on the bridge resulting from the greater number of strings, though the large, late (early 19th century) Swedish clavichords tend to be the loudest of any of the historic clavichords.\n\nWhile clavichords were typically single manual instruments, they could be stacked, one clavichord on top of another, to provide multiple keyboards. With the addition of a pedal clavichord, which included a pedal keyboard for the lower notes, a clavichord could be used to practice organ repertoire. Most often, the addition of a pedal keyboard only involved connecting the keys of the pedalboard to the lower notes on the manual clavichord using string so the lower notes on the manual instrument could be operated by the feet. In the era of pipe organs, which used man-powered bellows that required several people to operate, and of churches only heated during church services if at all, organists used pedal harpsichords and pedal clavichords as practice instruments (see also: pedal piano). There is speculation that some works written for organ may have been intended for pedal clavichord. An interesting case is made by that Bach's \"Eight Little Preludes and Fugues\", now thought spurious, may actually be authentic. The keyboard writing seems unsuited to organ, but Speerstra argues that they are idiomatic on the pedal clavichord. As Speerstra and also note, the compass of the keyboard parts of Bach's six trio sonatas for organ (BWV 525–530) rarely go below the tenor C, so they could have been played on a single manual pedal clavichord, by moving the left hand down an octave, a customary practice in the 18th century.\n\nMuch of the musical repertoire written for harpsichord and organ from the period circa 1400–1800 can be played on the clavichord; however, it does not have enough (unamplified) volume to participate in chamber music, with the possible exception of providing accompaniment to a soft baroque flute, recorder, or single singer. J. S. Bach's son Carl Philipp Emanuel Bach was a great proponent of the instrument, and most of his German contemporaries regarded it as a central keyboard instrument, for performing, teaching, composing and practicing. The fretting of a clavichord provides new problems for some repertoire, but scholarship suggests that these problems are not insurmountable in Bach's Well-Tempered Clavier (). Among recent clavichord recordings, those by Christopher Hogwood (\"The Secret Bach\", \"The Secret Handel\", and \"The Secret Mozart\"), break new ground. In his liner notes, Hogwood pointed out that these composers would typically have played the clavichord in the privacy of their homes. In England, the composer Herbert Howells (1892–1983) wrote two significant collections of pieces for clavichord (\"Lambert's Clavichord\" and \"Howells' Clavichord\"), and Stephen Dodgson (1924–2013) wrote two clavichord suites.\n\nNotes\nSources\n\n", "id": "7531", "title": "Clavichord"}
{"url": "https://en.wikipedia.org/wiki?curid=7534", "text": "Centripetal force\n\nA centripetal force (from Latin \"centrum\", \"center\" and \"petere\", \"to seek\") is a force that makes a body follow a curved path. Its direction is always orthogonal to the motion of the body and towards the fixed point of the instantaneous center of curvature of the path. Isaac Newton described it as \"a force by which bodies are drawn or impelled, or in any way tend, towards a point as to a centre\". In Newtonian mechanics, gravity provides the centripetal force responsible for astronomical orbits.\n\nOne common example involving centripetal force is the case in which a body moves with uniform speed along a circular path. The centripetal force is directed at right angles to the motion and also along the radius towards the centre of the circular path. The mathematical description was derived in 1659 by the Dutch physicist Christiaan Huygens.\n\nThe magnitude of the centripetal force on an object of mass \"m\" moving at tangential speed \"v\" along a path with radius of curvature \"r\" is:\n\nwhere formula_2 is the centripetal acceleration.\nThe direction of the force is toward the center of the circle in which the object is moving, or the osculating circle (the circle that best fits the local path of the object, if the path is not circular).\nThe speed in the formula is squared, so twice the speed needs four times the force. The inverse relationship with the radius of curvature shows that half the radial distance requires twice the force. This force is also sometimes written in terms of the angular velocity \"ω\" of the object about the center of the circle, related to the tangential velocity by the formula\n\nso that\n\nExpressed using the orbital period \"T\" for one revolution of the circle,\n\nthe equation becomes\n\nIn particle accelerators, velocity can be very high (close to the speed of light in vacuum) so the same rest mass now exerts greater inertia (relativistic mass) thereby requiring greater force for the same centripetal acceleration, so the equation becomes:\n\nwhere\n\nis called the Lorentz factor.\n\nMore intuitively:\n\nwhich is the rate of change of relativistic momentum (formula_10)\n\nIn the case of an object that is swinging around on the end of a rope in a horizontal plane, the centripetal force on the object is supplied by the tension of the rope. The rope example is an example involving a 'pull' force. The centripetal force can also be supplied as a 'push' force, such as in the case where the normal reaction of a wall supplies the centripetal force for a wall of death rider.\n\nNewton's idea of a centripetal force corresponds to what is nowadays referred to as a central force. When a satellite is in orbit around a planet, gravity is considered to be a centripetal force even though in the case of eccentric orbits, the gravitational force is directed towards the focus, and not towards the instantaneous center of curvature.\n\nAnother example of centripetal force arises in the helix that is traced out when a charged particle moves in a uniform magnetic field in the absence of other external forces. In this case, the magnetic force is the centripetal force that acts towards the helix axis.\n\nBelow are three examples of increasing complexity, with derivations of the formulas governing velocity and acceleration.\n\nUniform circular motion refers to the case of constant rate of rotation. Here are two approaches to describing this case.\n\nIn two dimensions, the position vector formula_11, which has magnitude (length) formula_12 and directed at an angle formula_13 above the x-axis, can be expressed in Cartesian coordinates using the unit vectors formula_14 and formula_15:\n\nAssume uniform circular motion, which requires three things.\n\nNow find the velocity formula_21 and acceleration formula_22 of the motion by taking derivatives of position with respect to time.\n\nNotice that the term in parenthesis is the original expression of formula_11 in Cartesian coordinates. Consequently,\n\nnegative shows that the acceleration is pointed towards the center of the circle (opposite the radius), hence it is called \"centripetal\" (i.e. \"center-seeking\"). While objects naturally follow a straight path (due to inertia), this centripetal acceleration describes the circular motion path caused by a centripetal force.\n\nThe image at right shows the vector relationships for uniform circular motion. The rotation itself is represented by the angular velocity vector Ω, which is normal to the plane of the orbit (using the right-hand rule) and has magnitude given by:\n\nwith \"θ\" the angular position at time \"t\". In this subsection, d\"θ\"/d\"t\" is assumed constant, independent of time. The distance traveled dℓ of the particle in time d\"t\" along the circular path is\n\nwhich, by properties of the vector cross product, has magnitude \"r\"d\"θ\" and is in the direction tangent to the circular path.\n\nConsequently,\n\nDifferentiating with respect to time,\n\nLagrange's formula states:\n\nApplying Lagrange's formula with the observation that Ω • r(\"t\") = 0 at all times,\n\nIn words, the acceleration is pointing directly opposite to the radial displacement r at all times, and has a magnitude:\n\nwhere vertical bars |...| denote the vector magnitude, which in the case of r(\"t\") is simply the radius \"r\" of the path. This result agrees with the previous section, though the notation is slightly different.\n\nWhen the rate of rotation is made constant in the analysis of nonuniform circular motion, that analysis agrees with this one.\n\nA merit of the vector approach is that it is manifestly independent of any coordinate system.\n\nThe upper panel in the image at right shows a ball in circular motion on a banked curve. The curve is banked at an angle \"θ\" from the horizontal, and the surface of the road is considered to be slippery. The objective is to find what angle the bank must have so the ball does not slide off the road. Intuition tells us that, on a flat curve with no banking at all, the ball will simply slide off the road; while with a very steep banking, the ball will slide to the center unless it travels the curve rapidly.\n\nApart from any acceleration that might occur in the direction of the path, the lower panel of the image above indicates the forces on the ball. There are \"two\" forces; one is the force of gravity vertically downward through the center of mass of the ball \"mg, where \"m\" is the mass of the ball and g is the gravitational acceleration; the second is the upward normal force exerted by the road perpendicular to the road surface \"ma. The centripetal force demanded by the curved motion is also shown above. This centripetal force is not a third force applied to the ball, but rather must be provided by the net force on the ball resulting from vector addition of the normal force and the force of gravity. The resultant or net force on the ball found by vector addition of the normal force exerted by the road and vertical force due to gravity must equal the centripetal force dictated by the need to travel a circular path. The curved motion is maintained so long as this net force provides the centripetal force requisite to the motion.\n\nThe horizontal net force on the ball is the horizontal component of the force from the road, which has magnitude |F| = \"m\"|a|sin\"θ\". The vertical component of the force from the road must counteract the gravitational force: |F| = \"m\"|a|cos\"θ\" = \"m\"|g|, which implies |a|=|g| / cos\"θ\". Substituting into the above formula for |F| yields a horizontal force to be:\n\nOn the other hand, at velocity |v| on a circular path of radius \"r\", kinematics says that the force needed to turn the ball continuously into the turn is the radially inward centripetal force F of magnitude:\n\nConsequently, the ball is in a stable path when the angle of the road is set to satisfy the condition:\n\nor,\n\nAs the angle of bank \"θ\" approaches 90°, the tangent function approaches infinity, allowing larger values for |v|/\"r\". In words, this equation states that for faster speeds (bigger |v|) the road must be banked more steeply (a larger value for \"θ\"), and for sharper turns (smaller \"r\") the road also must be banked more steeply, which accords with intuition. When the angle \"θ\" does not satisfy the above condition, the horizontal component of force exerted by the road does not provide the correct centripetal force, and an additional frictional force tangential to the road surface is called upon to provide the difference. If friction cannot do this (that is, the coefficient of friction is exceeded), the ball slides to a different radius where the balance can be realized.\n\nThese ideas apply to air flight as well. See the FAA pilot's manual.\n\nAs a generalization of the uniform circular motion case, suppose the angular rate of rotation is not constant. The acceleration now has a tangential component, as shown the image at right. This case is used to demonstrate a derivation strategy based on a polar coordinate system.\n\nLet r(\"t\") be a vector that describes the position of a point mass as a function of time. Since we are assuming circular motion, let r(\"t\") = \"R\"·u, where \"R\" is a constant (the radius of the circle) and u is the unit vector pointing from the origin to the point mass. The direction of u is described by \"θ\", the angle between the x-axis and the unit vector, measured counterclockwise from the x-axis. The other unit vector for polar coordinates, u is perpendicular to u and points in the direction of increasing \"θ\". These polar unit vectors can be expressed in terms of Cartesian unit vectors in the \"x\" and \"y\" directions, denoted i and j respectively:\n\nand\n\nOne can differentiate to find velocity:\n\nwhere \"ω\" is the angular velocity d\"θ\"/d\"t\".\n\nThis result for the velocity matches expectations that the velocity should be directed tangentially to the circle, and that the magnitude of the velocity should be \"rω\". Differentiating again, and noting that\n\nwe find that the acceleration, a is:\n\nThus, the radial and tangential components of the acceleration are:\n\nwhere |v| = \"r\" ω is the magnitude of the velocity (the speed).\n\nThese equations express mathematically that, in the case of an object that moves along a circular path with a changing speed, the acceleration of the body may be decomposed into a perpendicular component that changes the direction of motion (the centripetal acceleration), and a parallel, or tangential component, that changes the speed.\n\nThe above results can be derived perhaps more simply in polar coordinates, and at the same time extended to general motion within a plane, as shown next. Polar coordinates in the plane employ a radial unit vector u and an angular unit vector u, as shown above. A particle at position r is described by:\n\nwhere the notation \"ρ\" is used to describe the distance of the path from the origin instead of \"R\" to emphasize that this distance is not fixed, but varies with time. The unit vector u travels with the particle and always points in the same direction as r(\"t\"). Unit vector u also travels with the particle and stays orthogonal to u. Thus, u and u form a local Cartesian coordinate system attached to the particle, and tied to the path traveled by the particle. By moving the unit vectors so their tails coincide, as seen in the circle at the left of the image above, it is seen that u and u form a right-angled pair with tips on the unit circle that trace back and forth on the perimeter of this circle with the same angle \"θ\"(\"t\") as r(\"t\").\n\nWhen the particle moves, its velocity is\n\nTo evaluate the velocity, the derivative of the unit vector u is needed. Because u is a unit vector, its magnitude is fixed, and it can change only in direction, that is, its change du has a component only perpendicular to u. When the trajectory r(\"t\") rotates an amount d\"θ\", u, which points in the same direction as r(\"t\"), also rotates by d\"θ\". See image above. Therefore, the change in u is\n\nor\n\nIn a similar fashion, the rate of change of u is found. As with u, u is a unit vector and can only rotate without changing size. To remain orthogonal to u while the trajectory r(\"t\") rotates an amount d\"θ\", u, which is orthogonal to r(\"t\"), also rotates by d\"θ\". See image above. Therefore, the change du is orthogonal to u and proportional to d\"θ\" (see image above):\n\nThe image above shows the sign to be negative: to maintain orthogonality, if du is positive with d\"θ\", then du must decrease.\n\nSubstituting the derivative of u into the expression for velocity:\n\nTo obtain the acceleration, another time differentiation is done:\n\nSubstituting the derivatives of u and u, the acceleration of the particle is:\n\nAs a particular example, if the particle moves in a circle of constant radius \"R\", then d\"ρ\"/d\"t\" = 0, v = v, and:\n\nwhere formula_60\n\nThese results agree with those above for nonuniform circular motion. See also the article on non-uniform circular motion. If this acceleration is multiplied by the particle mass, the leading term is the centripetal force and the negative of the second term related to angular acceleration is sometimes called the Euler force.\n\nFor trajectories other than circular motion, for example, the more general trajectory envisioned in the image above, the instantaneous center of rotation and radius of curvature of the trajectory are related only indirectly to the coordinate system defined by u and u and to the length |r(\"t\")| = \"ρ\". Consequently, in the general case, it is not straightforward to disentangle the centripetal and Euler terms from the above general acceleration equation.\n\nLocal coordinates mean a set of coordinates that travel with the particle, and have orientation determined by the path of the particle. Unit vectors are formed as shown in the image at right, both tangential and normal to the path. This coordinate system sometimes is referred to as \"intrinsic\" or \"path coordinates\" or \"nt-coordinates\", for \"normal-tangential\", referring to these unit vectors. These coordinates are a very special example of a more general concept of local coordinates from the theory of differential forms.\n\nDistance along the path of the particle is the arc length \"s\", considered to be a known function of time.\n\nA center of curvature is defined at each position \"s\" located a distance \"ρ\" (the radius of curvature) from the curve on a line along the normal u (\"s\"). The required distance \"ρ\"(\"s\") at arc length \"s\" is defined in terms of the rate of rotation of the tangent to the curve, which in turn is determined by the path itself. If the orientation of the tangent relative to some starting position is \"θ\"(\"s\"), then \"ρ\"(\"s\") is defined by the derivative d\"θ\"/d\"s\":\n\nThe radius of curvature usually is taken as positive (that is, as an absolute value), while the \"curvature\" \"κ\" is a signed quantity.\n\nA geometric approach to finding the center of curvature and the radius of curvature uses a limiting process leading to the osculating circle. See image above.\n\nUsing these coordinates, the motion along the path is viewed as a succession of circular paths of ever-changing center, and at each position \"s\" constitutes non-uniform circular motion at that position with radius \"ρ\". The local value of the angular rate of rotation then is given by:\n\nwith the local speed \"v\" given by:\n\nAs for the other examples above, because unit vectors cannot change magnitude, their rate of change is always perpendicular to their direction (see the left-hand insert in the image above):\n\nConsequently, the velocity and acceleration are:\nand using the chain-rule of differentiation:\n\nIn this local coordinate system, the acceleration resembles the expression for nonuniform circular motion with the local radius \"ρ\"(\"s\"), and the centripetal acceleration is identified as the second term.\n\nExtending this approach to three dimensional space curves leads to the Frenet–Serret formulas.\n\nLooking at the image above, one might wonder whether adequate account has been taken of the difference in curvature between \"ρ\"(\"s\") and \"ρ\"(\"s\" + d\"s\") in computing the arc length as d\"s\" = \"ρ\"(\"s\")d\"θ\". Reassurance on this point can be found using a more formal approach outlined below. This approach also makes connection with the article on curvature.\n\nTo introduce the unit vectors of the local coordinate system, one approach is to begin in Cartesian coordinates and describe the local coordinates in terms of these Cartesian coordinates. In terms of arc length \"s\", let the path be described as:\n\nThen an incremental displacement along the path d\"s\" is described by:\n\nwhere primes are introduced to denote derivatives with respect to \"s\". The magnitude of this displacement is d\"s\", showing that:\n\nThis displacement is necessarily a tangent to the curve at \"s\", showing that the unit vector tangent to the curve is:\n\nwhile the outward unit vector normal to the curve is\n\nOrthogonality can be verified by showing that the vector dot product is zero. The unit magnitude of these vectors is a consequence of Eq. 1. Using the tangent vector, the angle \"θ\" of the tangent to the curve is given by:\n\nThe radius of curvature is introduced completely formally (without need for geometric interpretation) as:\n\nThe derivative of \"θ\" can be found from that for sin\"θ\":\n\nNow:\n\nin which the denominator is unity. With this formula for the derivative of the sine, the radius of curvature becomes:\n\nwhere the equivalence of the forms stems from differentiation of Eq. 1:\nWith these results, the acceleration can be found:\nas can be verified by taking the dot product with the unit vectors u(\"s\") and u(\"s\"). This result for acceleration is the same as that for circular motion based on the radius \"ρ\". Using this coordinate system in the inertial frame, it is easy to identify the force normal to the trajectory as the centripetal force and that parallel to the trajectory as the tangential force. From a qualitative standpoint, the path can be approximated by an arc of a circle for a limited time, and for the limited time a particular radius of curvature applies, the centrifugal and Euler forces can be analyzed on the basis of circular motion with that radius.\n\nThis result for acceleration agrees with that found earlier. However, in this approach, the question of the change in radius of curvature with \"s\" is handled completely formally, consistent with a geometric interpretation, but not relying upon it, thereby avoiding any questions the image above might suggest about neglecting the variation in \"ρ\".\nTo illustrate the above formulas, let \"x\", \"y\" be given as:\n\nThen:\n\nwhich can be recognized as a circular path around the origin with radius \"α\". The position \"s\" = 0 corresponds to [\"α\", 0], or 3 o'clock. To use the above formalism, the derivatives are needed:\n\nWith these results, one can verify that:\n\nThe unit vectors can also be found:\n\nwhich serve to show that \"s\" = 0 is located at position [\"ρ\", 0] and \"s\" = \"ρ\"π/2 at [0, \"ρ\"], which agrees with the original expressions for \"x\" and \"y\". In other words, \"s\" is measured counterclockwise around the circle from 3 o'clock. Also, the derivatives of these vectors can be found:\n\nTo obtain velocity and acceleration, a time-dependence for \"s\" is necessary. For counterclockwise motion at variable speed \"v\"(\"t\"):\nwhere \"v\"(\"t\") is the speed and \"t\" is time, and \"s\"(\"t\" = 0) = 0. Then:\nwhere it already is established that α = ρ. This acceleration is the standard result for non-uniform circular motion.\n\n\n\n", "id": "7534", "title": "Centripetal force"}
{"url": "https://en.wikipedia.org/wiki?curid=7535", "text": "Commodore\n\nCommodore generally refers to Commodore (rank), a naval rank. It may also refer to:\n\n\n\n\n\n\n\n", "id": "7535", "title": "Commodore"}
{"url": "https://en.wikipedia.org/wiki?curid=7536", "text": "Conditioning\n\nConditioning may refer to:\n\n\n\n\n\n", "id": "7536", "title": "Conditioning"}
{"url": "https://en.wikipedia.org/wiki?curid=7538", "text": "Checksum\n\nA checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors which may have been introduced during its transmission or storage. It is usually applied to an installation file after it is received from the download server. By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity.\n\nThe actual procedure which yields the checksum from a data input is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions, which may be used to detect many data corruption errors and verify overall data integrity; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted.\n\nChecksum functions are related to hash functions, fingerprints, randomization functions, and cryptographic hash functions. However, each of those concepts has different applications and therefore different design goals. For instance a function returning the start of a string can provide a hash appropriate for some applications but will never be a suitable checksum. Checksums are used as cryptographic primitives in larger authentication algorithms. For cryptographic systems with these two specific design goals, see HMAC.\n\nCheck digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers, bank account numbers, computer words, single bytes, etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.\n\nThe simplest checksum algorithm is the so-called longitudinal parity check, which breaks the data into \"words\" with a fixed number \"n\" of bits, and then computes the exclusive or (XOR) of all those words. The result is appended to the message as an extra word. To check the integrity of a message, the receiver computes the exclusive or of all its words, including the checksum; if the result is not a word with \"n\" zeros, the receiver knows a transmission error occurred.\n\nWith this checksum, any transmission error which flips a single bit of the message, or an odd number of bits, will be detected as an incorrect checksum. However, an error which affects two bits will not be detected if those bits lie at the same position in two distinct words. Also swapping of two or more words will not be detected. If the affected bits are independently chosen at random, the probability of a two-bit error being undetected is 1/\"n\".\n\nA variant of the previous algorithm is to add all the \"words\" as unsigned binary numbers, discarding any overflow bits, and append the two's complement of the total as the checksum. To validate a message, the receiver adds all the words in the same manner, including the checksum; if the result is not a word full of zeros, an error must have occurred. This variant too detects any single-bit error, but the promodular sum is used in SAE J1708.\n\nThe simple checksums described above fail to detect some common errors which affect many bits at once, such as changing the order of data words, or inserting or deleting words with all bits set to zero. The checksum algorithms most used in practice, such as Fletcher's checksum, Adler-32, and cyclic redundancy checks (CRCs), address these weaknesses by considering not only the value of each word but also its position in the sequence. This feature generally increases the cost of computing the checksum.\n\nA message that is \"m\" bits long can be viewed as a corner of the \"m\"-dimensional hypercube. The effect of a checksum algorithm that yields an n-bit checksum is to map each \"m\"-bit message to a corner of a larger hypercube, with dimension . The 2 corners of this hypercube represent all possible received messages. The valid received messages (those that have the correct checksum) comprise a smaller set, with only 2 corners.\n\nA single-bit transmission error then corresponds to a displacement from a valid corner (the correct message and checksum) to one of the \"m\" adjacent corners. An error which affects \"k\" bits moves the message to a corner which is \"k\" steps removed from its correct corner. The goal of a good checksum algorithm is to spread the valid corners as far from each other as possible, so as to increase the likelihood \"typical\" transmission errors will end up in an invalid corner.\n\nGeneral topic\n\nError correction\n\nHash functions\n\nRelated concepts\n\n", "id": "7538", "title": "Checksum"}
{"url": "https://en.wikipedia.org/wiki?curid=7540", "text": "Cultural evolution (disambiguation)\n\nCultural evolution is cultural change viewed from an evolutionary perspective.\n\nIt may also refer to:\n\n", "id": "7540", "title": "Cultural evolution (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=7541", "text": "City University of New York\n\nThe City University of New York (CUNY; pron.: ) is the public university system of New York City, and the largest urban university in the United States. CUNY and the State University of New York (SUNY) are separate and independent university systems, although both are public institutions that receive funding from New York State. CUNY, however, is additionally funded by the City of New York.\n\nCUNY is the third-largest university system in the United States, in terms of enrollment, behind the State University of New York (SUNY), and the California State University system. More than 270,000-degree-credit students and 273,000 continuing and professional education students are enrolled at campuses located in all five New York City boroughs.\n\nThe university has one of the most diverse student bodies in the United States, with students hailing from 208 countries. The black, white and Hispanic undergraduate populations each comprise more than a quarter of the student body, and Asian undergraduates make up 18 percent. Fifty-eight percent are female, and 28 percent are 25 or older.\n\n\"The following table is 'sortable'; click on a column heading to re-sort the table by values of that column.\"\n\n\nCUNY was created in 1961, by New York State legislation, signed into law by Governor Nelson Rockefeller. The legislation\nintegrated existing institutions and a new graduate school into a coordinated system of higher education for the city, under the control of the \"Board of Higher Education of the City of New York\", which had been created by New York State legislation in 1926. By 1979, the Board of Higher Education had become the \"Board of Trustees of the CUNY\".\n\nThe institutions that were merged in order to create CUNY were:\n\n\nCUNY has served a diverse student body, especially those excluded from or unable to afford private universities. Its four-year colleges offered a high quality, tuition-free education to the poor, the working class and the immigrants of New York City who met the grade requirements for matriculated status. During the post-World War I era, when some Ivy League universities, such as Yale University, discriminated against Jews, many Jewish academics and intellectuals studied and taught at CUNY. The City College of New York developed a reputation of being \"the Harvard of the proletariat.\"\n\nAs the city's population—and public college enrollment—grew during the early 20th century and the city struggled for resources, the municipal colleges slowly began adopting selective tuition, also known as instructional fees, for a handful of courses and programs. During the Great Depression, with funding for the public colleges severely constrained, limits were imposed on the size of the colleges' free Day Session, and tuition was imposed upon students deemed \"competent\" but not academically qualified for the day program. Most of these \"limited matriculation\" students enrolled in the Evening Session, and paid tuition.\n\nDemand in the United States for higher education rapidly grew after World War II, and during the mid-1940s a movement began to create community colleges to provide accessible education and training. In New York City, however, the community-college movement was constrained by many factors including \"financial problems, narrow perceptions of responsibility, organizational weaknesses, adverse political factors, and another competing priorities.\"\n\nCommunity colleges would have drawn from the same city coffers that were funding the senior colleges, and city higher education officials were of the view that the state should finance them. It wasn't until 1955, under a shared-funding arrangement with New York State, that New York City established its first community college, on Staten Island. Unlike the day college students attending the city's public baccalaureate colleges for free, the community college students had to pay tuition fees under the state-city funding formula. Community college students paid tuition fees for approximately 10 years.\n\nOver time, tuition fees for limited-matriculated students became an important source of system revenues. In fall 1957, for example, nearly 36,000 attended Hunter, Brooklyn, Queens and City Colleges for free, but another 24,000 paid tuition fees of up to $300 a year – the equivalent of $2,413 in 2011. Undergraduate tuition and other student fees in 1957 comprised 17 percent of the colleges' $46.8 million in revenues, about $7.74 million — a figure equivalent to $62.4 million in 2011 buying power.\n\nThree community colleges had been established by early 1961, when the city's public colleges were codified by the state as a single university with a chancellor at the helm and an infusion of state funds. But the city's slowness in creating the community colleges as demand for college seats was intensifying, had resulted in mounting frustration, particularly on the part of minorities, that college opportunities were not available to them.\n\nIn 1964, as the city's Board of Higher Education moved to take full responsibility for the community colleges, city officials extended the senior colleges' free tuition policy to them, a change that was included by Mayor Robert Wagner in his budget plans and took effect with the 1964–65 academic year.\n\nIn 1969, a group of Black and Puerto Rican students occupied City College demanding the racial integration of CUNY, which at the time had an overwhelmingly white student body.\n\nStudents at some campuses became increasingly frustrated with the university's and Board of Higher Education's handling of university administration. At Baruch College in 1967, over a thousand students protested the plan to make the college an upper-division school limited to junior, senior, and graduate students. At Brooklyn College in 1968, students attempted a sit-in to demand the admission of more black and Puerto Rican students and additional black studies curriculum. Students at Hunter College also demanded a Black studies program. Members of the SEEK program, which provided academic support for underprepared and underprivileged students, staged a building takeover at Queens College in 1969 to protest the decisions of the program's director, who would later be replaced by a black professor. Puerto Rican students at Bronx Community College filed a report with the New York State Division of Human Rights in 1970, contending that the intellectual level of the college was inferior and discriminatory. Hunter College was crippled for several days by a protest of 2,000 students who had a list of demands focusing on more student representation in college administration. Across CUNY, students boycotted their campuses in 1970 to protest a rise in student fees and other issues, including the proposed (and later implemented) open admissions plan.\n\nLike many college campuses in 1970, CUNY faced a number of protests and demonstrations after the Kent State shootings and Cambodian Campaign. The Administrative Council of the City University of New York sent U.S. President Richard Nixon a telegram in 1970 stating, \"No nation can long endure the alienation of the best of its young people.\" Some colleges, including John Jay College of Criminal Justice, historically the \"college for cops,\" held teach-ins in addition to student and faculty protests.\n\nIn 1969, the Board of Trustees implemented a new admissions policy. The doors to CUNY were opened wide to all those demanding entrance, assuring all high school graduates entrance to the university without having to fulfill traditional requirements such as exams or grades. This policy was known as open admissions and nearly doubled the number of students enrolling in the CUNY system to 35,000 (compared to 20,000 the year before). With greater numbers came more diversity: Black and Hispanic student enrollment increased threefold. Remedial education, to supplement the training of under-prepared students, became a significant part of CUNY's offerings.\n\nIn fall 1976, during New York City's fiscal crisis, the free tuition policy was discontinued under pressure from the federal government, the financial community that had a role in rescuing the city from bankruptcy, and New York State, which would take over the funding of CUNY's senior colleges. Tuition, which had been in place in the State University of New York system since 1963, was instituted at all CUNY colleges.\n\nMeanwhile, CUNY students were added to the state's need-based Tuition Assistance Program (TAP), which had been created to help private colleges. Full-time students who met the income eligibility criteria were permitted to receive TAP, ensuring for the first time that financial hardship would deprive no CUNY student of a college education. Within a few years, the federal government would create its own need-based program, known as Pell Grants, providing the neediest students with a tuition-free college education. By 2011, nearly six of ten full- time undergraduates qualified for a tuition-free education at CUNY due in large measure to state, federal and CUNY financial aid programs. CUNY's enrollment dipped after tuition was re-established, and there were further enrollment declines through the 1980s and into the 1990s.\n\nIn 1995, CUNY suffered another fiscal crisis when Governor George Pataki proposed a drastic cut in state financing. Faculty cancelled classes and students staged protests. By May, CUNY adopted deep cuts to college budgets and class offerings. By June, in order to save money spent on remedial programs, CUNY adopted a stricter admissions policy for its senior colleges: students deemed unprepared for college would not be admitted, this a departure from the 1970 Open Admissions program. That year's final state budget cut funding by $102 million, which CUNY absorbed by increasing tuition by $750 and offering a retirement incentive plan for faculty.\n\nIn 1999, a task force appointed by Mayor Rudolph Giuliani issued a report that described CUNY as \"an institution adrift\" and called for an improved, more cohesive university structure and management, as well as more consistent academic standards. Following the report, Matthew Goldstein, a mathematician and City College graduate who had led CUNY's Baruch College and briefly, Adelphi University, was appointed chancellor. CUNY ended its policy of open admissions to its four-year colleges, raised its admissions standards its most selective four-year colleges (Baruch, Brooklyn, City, Hunter and Queens), and required new-enrollees who needed remediation, to begin their studies at a CUNY open-admissions community colleges.\n\nCUNY's enrollment of degree-credit students reached 220,727 in 2005 and 262,321 in 2010 as the university broadened its academic offerings. The university added more than 2,000 full-time faculty positions, opened new schools and programs, and expanded the university's fundraising efforts to help pay for them. Fundraising increased from $35 million in 2000 to more than $200 million in 2012.\n\nAs of Autumn 2013, all CUNY undergraduates are required to take an administration-dictated common core of courses which have been claimed to meet specific \"learning outcomes\" or standards. Since the courses are accepted University wide, the administration claims it will be easier for students to transfer course credits between CUNY colleges. It also reduced the number of core courses some CUNY colleges had required, to a level below national norms, particularly in the sciences. The program is the target of several lawsuits by students and faculty, and was the subject of a \"no confidence\" vote by the faculty, who rejected it by an overwhelming 92% margin.\n\nChancellor Goldstein retired on July 1, 2013, and was replaced on June 1, 2014 by James Milliken, president of the University of Nebraska, and a graduate of University of Nebraska and New York University Law School.\n\nThe forerunner of today's City University of New York was governed by the Board of Education of New York City. Members of the Board of Education, chaired by the President of the board, served as \"ex officio\" trustees. For the next four decades, the board members continued to serve as \"ex officio\" trustees of the College of the City of New York and the city's other municipal college, the Normal College of the City of New York.\n\nIn 1900, the New York State Legislature created separate boards of trustees for the College of the City of New York and the Normal College, which became Hunter College in 1914. In 1926, the Legislature established the Board of Higher Education of the City of New York, which assumed supervision of both municipal colleges.\n\nIn 1961, the New York State Legislature established the City University of New York, uniting what had become seven municipal colleges at the time: the City College of New York, Hunter College, Brooklyn College, Queens College, Staten Island Community College, Bronx Community College and Queensborough Community College. In 1979, the CUNY Financing and Governance Act was adopted by the State and the Board of Higher Education became the City University of New York Board of Trustees.\n\nToday, the City University is governed by the Board of Trustees composed of 17 members, ten of whom are appointed by the Governor of New York \"with the advice and consent of the senate,\" and five by the Mayor of New York City \"with the advice and consent of the senate.\" The final two trustees are \"ex officio\" members. One is the chair of the university's student senate, and the other is non-voting and is the chair of the university's faculty senate. Both the mayoral and gubernatorial appointments to the CUNY Board are required to include at least one resident of each of New York City's five boroughs. Trustees serve seven-year terms, which are renewable for another seven years. The Chancellor is elected by the Board of Trustees, and is the \"chief educational and administrative officer\" of the City University.\n\nThe administrative offices are in mid-town Manhattan.\n\nCUNY has its own public safety force whose duties are to protect and serve all students and faculty members, and enforce all state and city laws at all of CUNY's universities. The force has more than 1000 officers, making it one of the largest public safety forces in New York City.\n\nThe Public Safety Department came under heavy criticism, from student groups, after several students protesting tuition increases tried to occupy the lobby of the Baruch College. The occupiers were forcibly removed from the area and several were arrested on November 21, 2011.\n\nCUNY also has a broadcast TV service, CUNY TV (channel 75 on Time Warner cable, digital HD broadcast channel 25.3), which airs telecourses, classic and foreign films, magazine shows and panel discussions in foreign languages.\nCUFF is CUNY's official film festival. The festival was founded in 2009 by Hunter College student Daniel Cowen.\n\nCUNY graduates include 13 Nobel laureates, a Fields Medalist, a U.S. Secretary of State, a Supreme Court Justice, several New York City mayors, members of Congress, state legislators, scientists and artists.\n\n\n", "id": "7541", "title": "City University of New York"}
{"url": "https://en.wikipedia.org/wiki?curid=7543", "text": "Computational complexity theory\n\nComputational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.\n\nA problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.\n\nClosely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.\n\nA computational problem can be viewed as an infinite collection of \"instances\" together with a \"solution\" for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g. 15) and the solution is \"yes\" if the number is prime and \"no\" otherwise (in this case \"no\"). Stated another way, the \"instance\" is a particular input to the problem, and the \"solution\" is the output corresponding to the given input.\n\nTo further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.\n\nWhen considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.\n\nEven though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.\n\nDecision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either \"yes\" or \"no\", or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer \"yes\", the algorithm is said to accept the input string, otherwise it is said to reject the input.\n\nAn example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected, or not. The formal language associated with this decision problem is then the set of all connected graphs—of course, to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.\n\nA function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem, that is, it isn't just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.\n\nIt is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples (\"a\", \"b\", \"c\") such that the relation \"a\" × \"b\" = \"c\" holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.\n\nTo measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2\"n\" vertices compared to the time taken for a graph with \"n\" vertices?\n\nIf the input size is \"n\", the time taken can be expressed as a function of \"n\". Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(\"n\") is defined to be the maximum time taken over all inputs of size \"n\". If T(\"n\") is a polynomial in \"n\", then the algorithm is said to be a polynomial time algorithm. Cobham's thesis says that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.\n\nA Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a thought experiment representing a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.\n\nMany types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.\n\nA deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.\n\nMany machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.\n\nHowever, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.\n\nFor a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The \"time required\" by a deterministic Turing machine \"M\" on input \"x\" is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (\"yes\" or \"no\"). A Turing machine \"M\" is said to operate within time \"f\"(\"n\"), if the time required by \"M\" on each input of length \"n\" is at most \"f\"(\"n\"). A decision problem \"A\" can be solved in time \"f\"(\"n\") if there exists a Turing machine operating in time \"f\"(\"n\") that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time \"f\"(\"n\") on a deterministic Turing machine is then denoted by DTIME(\"f\"(\"n\")).\n\nAnalogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.\n\nThe complexity of an algorithm is often expressed using big O notation.\n\nThe best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size \"n\" may be faster to solve than others, we define the following complexities:\n\nFor example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(\"n\") for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(\"n\" log \"n\"). The best case occurs when each pivoting divides the list in half, also needing O(\"n\" log \"n\") time.\n\nTo classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the minimum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound \"T\"(\"n\") on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most \"T\"(\"n\"). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase \"all possible algorithms\" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of \"T\"(\"n\") for a problem requires showing that no algorithm can have time complexity lower than \"T\"(\"n\").\n\nUpper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if \"T\"(\"n\") = 7\"n\" + 15\"n\" + 40, in big O notation one would write \"T\"(\"n\") = O(\"n\").\n\nA complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:\n\nOf course, some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:\n\nBut bounding the computation time above by some concrete function \"f\"(\"n\") often yields complexity classes that depend on the chosen machine model. For instance, the language {\"xx\" | \"x\" is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that \"the time complexities in any two reasonable and general models of computation are polynomially related\" . This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.\n\nMany important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:\nThe logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.\n\nIt turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem.\n\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.\n\nFor the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(\"n\") is contained in DTIME(\"n\"), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.\n\nMore precisely, the time hierarchy theorem states that\n\nThe space hierarchy theorem states that\n\nThe time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.\n\nMany complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at least as difficult as another problem. For instance, if a problem \"X\" can be solved using an algorithm for \"Y\", \"X\" is no more difficult than \"Y\", and we say that \"X\" \"reduces\" to \"Y\". There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.\n\nThe most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.\n\nThis motivates the concept of a problem being hard for a complexity class. A problem \"X\" is \"hard\" for a class of problems \"C\" if every problem in \"C\" can be reduced to \"X\". Thus no problem in \"C\" is harder than \"X\", since an algorithm for \"X\" allows us to solve any problem in \"C\". Of course, the notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.\n\nIf a problem \"X\" is in \"C\" and hard for \"C\", then \"X\" is said to be \"complete\" for \"C\". This means that \"X\" is the hardest problem in \"C\". (Since many problems could be equally hard, one might say that \"X\" is one of the hardest problems in \"C\".) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π, to another problem, Π, would indicate that there is no known polynomial-time solution for Π. This is because a polynomial-time solution to Π would yield a polynomial-time solution to Π. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.\n\nThe complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.\n\nThe question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.\n\nIt was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.\n\nThe graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to Laszlo Babai and Eugene Luks has run time formula_3 for graphs with \"n\" vertices.\n\nThe integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than \"k\". No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time formula_4 to factor an \"n\"-bit integer. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.\n\nMany known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.\n\nAlong the same lines, co-NP is the class containing the complement problems (i.e. problems with the \"yes\"/\"no\" answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then P is not equal to NP, since if P=NP we would also have P=co-NP, since problems in NP are dual to those in co-NP.\n\nSimilarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.\n\nIt is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.\n\nProblems that can be solved in theory (e.g., given large but finite time), but which in practice take too long for their solutions to be useful, are known as \"intractable\" problems. In complexity theory, problems that lack polynomial-time solutions are considered to be intractable for more than the smallest inputs. In fact, the Cobham–Edmonds thesis states that only those problems that can be solved in polynomial time can be feasibly computed on some computational device. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then the NP-complete problems are also intractable in this sense. To see why exponential-time algorithms might be unusable in practice, consider a program that makes 2 operations before halting. For small \"n\", say 100, and assuming for the sake of example that the computer does 10 operations each second, the program would run for about 4 × 10 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. Nevertheless, a polynomial time algorithm is not always practical. If its running time is, say, \"n\", it is unreasonable to consider it efficient and it is still useless except on small instances.\n\nWhat intractability means in practice is open to debate. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.\n\nAn early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.\n\nBefore the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.\n\nThe beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a \"good\" algorithm to be one with running time bounded by a polynomial of the input size.\n\nEarlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:\n\nIn 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.\n\n\n\n", "id": "7543", "title": "Computational complexity theory"}
{"url": "https://en.wikipedia.org/wiki?curid=7544", "text": "Cadence\n\nCadence may refer to:\n\n\n\n\n", "id": "7544", "title": "Cadence"}
{"url": "https://en.wikipedia.org/wiki?curid=7546", "text": "Camelot\n\nCamelot is a castle and court associated with the legendary King Arthur. Absent in the early Arthurian material, Camelot first appeared in 12th-century French romances and, after the Lancelot-Grail cycle, eventually came to be described as the fantastic capital of Arthur's realm and a symbol of the Arthurian world. The stories locate it somewhere in Great Britain and sometimes associate it with real cities, though more usually its precise location is not revealed. Most scholars regard it as being entirely fictional, its geography being perfect for romance writers; Arthurian scholar Norris J. Lacy commented that \"Camelot, located no where in particular, can be anywhere\". Nevertheless, arguments about the location of the \"real Camelot\" have occurred since the 15th century and continue to rage today in popular works and for tourism purposes.\n\nThe castle is mentioned for the first time in Chrétien de Troyes' poem \"Lancelot, the Knight of the Cart\", dating to the 1170s, though it does not appear in all the manuscripts. It is mentioned in passing, and is not described:\n\nNothing in Chrétien's poem suggests the level of importance Camelot would have in later romances. For Chrétien, Arthur's chief court was in Caerleon in Wales; this was the king's primary base in Geoffrey of Monmouth's \"Historia Regum Britanniae\" and subsequent literature. Chrétien depicts Arthur, like a typical medieval monarch, holding court at a number of cities and castles. It is not until the 13th-century French prose romances, including the Lancelot-Grail and the Post-Vulgate Cycle, that Camelot began to supersede Caerleon, and even then, many descriptive details applied to Camelot derive from Geoffrey's earlier grand depiction of the Welsh town. Most Arthurian romances of this period produced in English or Welsh did not follow this trend; Camelot was referred to infrequently, and usually in translations from French. One exception is \"Sir Gawain and the Green Knight\", which locates Arthur's court at \"Camelot\"; however, in Britain, Arthur's court was generally located at Caerleon, or at Carlisle, which is usually identified with the \"Carduel\" of the French romances. However, in the late 15th century, Thomas Malory created the image of Camelot most familiar to English speakers today in his \"Le Morte d'Arthur\", a work based mostly on the French romances. He firmly identifies Camelot with Winchester, an identification that remained popular over the centuries, though it was rejected by Malory's own editor, William Caxton, who preferred a Welsh location.\n\nThe name's derivation is uncertain. It has numerous different spellings in medieval French Arthurian romance, including: \"Camaalot\", \"Camalot\", \"Chamalot\", \"Camehelot\" (sometimes read as \"Camchilot\"), \"Camaaloth\", \"Caamalot\", \"Camahaloth\", \"Camaelot\", \"Kamaalot\", \"Kamaaloth\", \"Kaamalot\", \"Kamahaloth\", \"Kameloth\", \"Kamaelot\", \"Kamelot\", \"Kaamelot\", \"Cameloth\", \"Camelot\" and \"Gamalaot\". Renowned Arthurian scholar Ernst Brugger suggested that it was a corruption of the site of Arthur's final battle, the Battle of Camlann, in Welsh tradition. Roger Sherman Loomis believed it was derived from \"Cavalon\", a place name that he suggested was a corruption of Avalon (under the influence of the Breton place name \"Cavallon\"). He further suggested that \"Cavalon/Camelot\" became Arthur's capital due to confusion with Arthur's other traditional court at \"Carlion\" (\"Caer Lleon\" in Welsh).\n\nOthers have suggested a derivation from the British Iron Age and Romano-British place name \"Camulodunum\", one of the first capitals of Roman Britain and which would have significance in Romano-British culture. Indeed, John Morris, the English historian who specialized in the study of the institutions of the Roman Empire and the history of Sub-Roman Britain, suggested in his book \"The Age of Arthur\" that as the descendants of Romanized Britons looked back to a golden age of peace and prosperity under Rome, the name \"Camelot\" of Arthurian legend may have referred to the capital of Britannia (\"Camulodunum\", modern Colchester) in Roman times. It is unclear, however, where Chrétien would have encountered the name \"Camulodunum\", or why he would render it as \"Camaalot\", though Urban T. Holmes argued in 1929 that Chretien had access to Book 2 of Pliny's \"Natural History\", where it is rendered as \"Camaloduno\". Given Chrétien's known tendency to create new stories and characters, being the first to mention the hero Lancelot's love affair with Queen Guinevere for example, the name might also be entirely invented.\n\nThe Lancelot-Grail Cycle and the texts it influenced depict the city of Camelot as standing along a river, downstream from Astolat. It is surrounded by plains and forests, and its magnificent cathedral, St. Stephen's, is the religious centre for Arthur's Knights of the Round Table. There, Arthur and Guinevere are married and there are the tombs of many kings and knights. In a mighty castle stands the Round Table; it is here that Galahad conquers the Siege Perilous, and where the knights see a vision of the Holy Grail and swear to find it. Jousts are held in a meadow outside the city. In the \"Palamedes\" and other works, the castle is eventually destroyed by King Mark of Cornwall after the loss of Arthur at the Battle of Camlann. However maddening to later scholars searching for Camelot's location, its imprecise geography serves the romances well, as Camelot becomes less a literal place than a powerful symbol of Arthur's court and universe. It should be noted, too, that there is a Kamaalot featured as the home of Perceval's mother in the romance \"Perlesvaus\".\n\nThe romancers' versions of Camelot drew on earlier descriptions of Arthur's fabulous court. From Geoffrey's grand description of Caerleon, Camelot gains its impressive architecture, its many churches and the chivalry and courtesy of its inhabitants. Geoffrey's description in turn drew on an already established tradition in Welsh oral tradition of the grandeur of Arthur's court. The tale \"Culhwch and Olwen\", associated with the \"Mabinogion\" and perhaps written in the 11th century, draws a dramatic picture of Arthur's hall and his many powerful warriors who go from there on great adventures, placing it in Celliwig, an uncertain locale in Cornwall. Although the court at Celliwig is the most prominent in remaining early Welsh manuscripts, the various versions of the \"Welsh Triads\" agree in giving Arthur multiple courts, one in each of the areas inhabited by the Celtic Britons: Cornwall, Wales and the Hen Ogledd. This perhaps reflects the influence of widespread oral traditions common by 800 which are recorded in various place names and features such as Arthur's Seat, indicating Arthur was a hero known and associated with many locations across Brittonic areas of Britain as well as Brittany. Even at this stage Arthur could not be tied to one location. Many other places are listed as a location where Arthur holds court in the later romances, Carlisle and London perhaps being the most prominent.\n\nThe romancers' versions of Camelot draw on earlier traditions of Arthur's fabulous court. The Celliwig of \"Culhwch and Olwen\" appears in the Welsh Triads as well; this early Welsh material places Wales' greatest leader outside its national boundaries. Geoffrey's description of Caerleon is probably based on his personal familiarity with the town and its impressive Roman ruins; it is less clear that Caerleon was associated with Arthur before Geoffrey. Several French romances (\"Perlesvaus\", the Didot \"Perceval\" attributed to Robert de Boron, and even the early romances of Chrétien de Troyes such as \"Erec and Enide\" and \"Yvain, the Knight of the Lion\") have Arthur hold court at \"Carduel in Wales,\" a northern city based on the real Carlisle.\n\nMalory's identification of Camelot as Winchester was probably partially inspired by the latter city's history. It had been the capital of Wessex under Alfred the Great, and boasted the Winchester Round Table, an artifact constructed in the 13th century but widely believed to be the original by Malory's time. Malory's editor Caxton rejects the association, saying Camelot was in Wales and that its ruins could still be seen; this is a likely reference to the Roman ruins at Caerwent. Malory associated other Arthurian locations with modern places, for instance locating Astolat at Guildford.\n\nIn 1542, John Leland reported the locals around Cadbury Castle, formerly known as Camalet, in Somerset considered it to be the original Camelot. This theory, which was repeated by later antiquaries, is bolstered, or may have derived from, Cadbury's proximity to the River Cam (Somerset) and the villages of Queen Camel and West Camel, and remained popular enough to help inspire a large-scale archaeological dig in the 20th century. These excavations, led by archaeologist Leslie Alcock from 1966–70, were titled \"Cadbury-Camelot,\" and won much media attention, even being mentioned in the film of the musical \"Camelot\". The dig revealed that the site seems to have been occupied as early as the 4th millennium and to have been refortified and occupied by a major Brittonic ruler and his war band from . This early medieval settlement continued until around 580. The works were by far the largest known fortification of the period, double the size of comparative \"caers\" and with Mediterranean artifacts representing extensive trade and Saxon ones showing possible conquest. The use of the name Camelot and the support of Geoffrey Ashe helped ensure much publicity for the finds, but Alcock himself later grew embarrassed by the supposed Arthurian connection to the site. Following the arguments of David Dumville, Alcock felt the site was too late and too uncertain to be a tenable Camelot. Modern archaeologists follow him in rejecting the name, calling it instead Cadbury Castle hill fort. Despite this, Cadbury remains widely associated with Camelot.\n\nThere were two towns in Roman Britain named Camulodunum, Colchester in Essex and Outlane in West Yorkshire, derived from the Celtic god Camulus, and this has led to the suggestion that they originated the name. However, the Essex Camulodunum was located well within territory usually thought to have been conquered early in the 5th century by Saxons, so it is unlikely to have been the location of any \"true\" Camelot. The town was definitely known as Colchester as early as the \"Anglo-Saxon Chronicle\" in 917. Even Colchester Museum argues strongly regarding the historical Arthur: \"It would be impossible and inconceivable to link him to the Colchester area, or to Essex more generally\", pointing out that the connection between the name Camulodunum and Colchester was unknown until the 18th century. Other places in Britain with names related to \"Camel\" have also been suggested, such as Camelford in Cornwall, located down the River Camel from where Geoffrey places Camlann, the scene of Arthur's final battle. The area's connections with Camelot and Camlann are merely speculative.\n\nRecently, Professor Peter Fields, formerly of Bangor University, has suggested that the Camulodunum in West Yorkshire is the likely location of King Arthur's Camelot.. He suggests that \"Slack, on the outskirts of Huddersfield in West Yorkshire,\" is where Arthur would have held court. This is because of the name, and also regarding its strategic location: it is but a few miles from the extreme South-West of Hen Ogledd (also making close to North Wales), and would have been a great flagship point in starving off attacks to the Celtic Kingdoms from both the Angles and other attackers.\n\nCamelot has become a permanent fixture in interpretations of the Arthurian legend. The symbolism of Camelot so impressed Alfred, Lord Tennyson that he wrote up a prose sketch on the castle as one of his earliest attempts to treat the Arthurian legend. \"A Connecticut Yankee in King Arthur's Court\", a novel by Mark Twain in 1889, takes place in Camelot. Recent versions typically retain Camelot's lack of precise location and its status as a symbol of the Arthurian world, though they typically transform the castle itself into romantically lavish visions of a High Middle Ages palace. It lends its name to the 1960 musical \"Camelot\" by Alan Jay Lerner and Frederick Loewe, which is based on T. H. White's literary version of the legend, \"The Once and Future King\". The musical was adapted into a 1967 film of the same name, which starred Richard Harris as Arthur, and which featured the Castle of Coca, Segovia as a fittingly opulent Camelot. Some writers of the \"realist\" strain of modern Arthurian fiction have attempted a more sensible Camelot; inspired by Alcock's Cadbury-Camelot excavation, writers Marion Zimmer Bradley, Mary Stewart, and Catherine Christian place their Camelots in that place and describe it accordingly.\n\nCamelot makes only a brief appearance in the 1975 parody \"Monty Python and the Holy Grail\": Having recruited several knights, King Arthur (Graham Chapman) invites them to reside with him at Camelot. The camera pans to a castle on a hill, then cuts to the knights as each in succession joyfully exclaims, \"Camelot!\" (Arthur's servant Patsy, played by co-director Terry Gilliam, grumbles, \"It's only a model.\") Then, after an interior sequence, in which the resident knights sing, \"We're Knights of the Round Table\" while engaging in madcap antics, the camera cuts back to Arthur, who decides, \"On second thought, let's not go to Camelot. It is a silly place.\"\n\nIn American contexts, the word \"Camelot\" is sometimes used to refer admiringly to the presidency of John F. Kennedy. The Lerner and Loewe musical was still quite recent at the time and his widow Jackie quoted its lines in a 1963 \"Life\" interview following JFK's assassination. She said the lines, \"Don't let it be forgot, that once there was a spot, for one brief shining moment, that was known as Camelot\" were Kennedy's favorite in the score, adding that \"there'll be great Presidents again, but there'll never be another Camelot again… It will never be that way again.\"\n\nRick Pitino referred to University of Kentucky Basketball as \"Camelot\", wishing he had never left []\n\nCamelot Theme Park was a resort and theme park located in the English county of Lancashire, UK.\n\n\"Kaamelott\" is a French television series that presents a humorous alternative version of the Arthurian legend.\n\nThe Canadian-American TV series Stargate SG-1 episode \"Camelot\" is set in a fanciful reproduction of the town.\n\nCamelot is featured in \"Once Upon a Time\". In \"Heroes and Villains,\" Rumplestiltskin once visited Camelot to obtain a magic gauntlet. In \"The Dark Swan,\" Camelot is fully seen. Brocéliande is somewhere near Camelot. According to an interview with Adam Horowitz, Camelot is a few days ride away from the Enchanted Forest and Arendelle.\n\nCamelot is the name of a story chapter in the mobile game \"Fate/Grand Order\". It features the Knights of the Round Table being summoned into the times of the Crusades, where they swiftly conquer Jerusalem and build a new Camelot over its ruins.\n\n\n", "id": "7546", "title": "Camelot"}
{"url": "https://en.wikipedia.org/wiki?curid=7548", "text": "Contras\n\nThe contras (some references use the capitalized form, \"Contras\") is a label given to the various U.S.-backed and funded right-wing militant groups that were active from 1979 to the early 1990s in opposition to the left-wing, socialist Sandinista Junta of National Reconstruction government in Nicaragua. Among the separate contra groups, the Nicaraguan Democratic Force (FDN) emerged as the largest by far. In 1987, virtually all contra organizations were united, at least nominally, into the Nicaraguan Resistance.\n\nFrom an early stage, the rebels received financial and military support from the United States government, and their military significance decisively depended on it. After US support was banned by Congress, the Reagan administration covertly continued it. These covert activities culminated in the Iran–Contra affair.\n\nThe term \"contra\" comes from the Spanish \"contra,\" which means \"against\" but in this case is short for , in English \"the counter-revolution\". Some rebels disliked being called contras, feeling that it defined their cause only in negative terms, or implied a desire to restore the old order. Rebel fighters usually referred to themselves as (\"commandos\"); peasant sympathizers also called the rebels (\"the cousins\"). From the mid-1980s, as the Reagan administration and the rebels sought to portray the movement as the \"democratic resistance,\" members started describing themselves as .\n\nDuring their war against the Nicaraguan government, the Contras committed a large number of human rights violations and used terrorist tactics, carrying out more than 1300 terrorist attacks. These actions were frequently carried out systematically as a part of the strategy of the Contras. Supporters of the Contras tried to minimize these violations, particularly the Reagan administration in the US, which engaged in a campaign of white propaganda to alter public opinion in favor of the contras.\n\nThe Contras were not a monolithic group, but a combination of three distinct elements of Nicaraguan society:\n\n\nThe CIA and Argentine intelligence, seeking to unify the anti-Sandinista cause before initiating large-scale aid, persuaded 15 September Legion, the UDN and several former smaller groups to merge in September 1981 as the Nicaraguan Democratic Force (\"Fuerza Democrática Nicaragüense\", FDN). Although the FDN had its roots in two groups made up of former National Guardsmen (of the Somoza regime), its joint political directorate was led by businessman and former anti-Somoza activist Adolfo Calero Portocarrero. Edgar Chamorro later stated that there was strong opposition within the UDN against working with the Guardsmen and that the merging only took place because of insistence by the CIA.\n\nBased in Honduras, Nicaragua's northern neighbor, under the command of former National Guard Colonel Enrique Bermúdez, the new FDN commenced to draw in other smaller insurgent forces in the north. Largely financed, trained, equipped, armed and organized by the U.S., it emerged as the largest and most active contra group.\n\nIn April 1982, Edén Pastora (\"Comandante Cero\"), one of the heroes in the fight against Somoza, organized the Sandinista Revolutionary Front (FRS) – embedded in the Democratic Revolutionary Alliance (ARDE) – and declared war on the Sandinista government. Himself a former Sandinista who had held several high posts in the government, he had resigned abruptly in 1981 and defected, believing that the newly found power had corrupted the Sandinista's original ideas. A popular and charismatic leader, Pastora initially saw his group develop quickly. He confined himself to operate in the southern part of Nicaragua; after a press conference he was holding on 30 May 1984 was bombed, he \"voluntarily withdrew\" from the contra struggle.\n\nA third force, Misurasata, appeared among the Miskito, Sumo and Rama Amerindian peoples of Nicaragua's Atlantic coast, who in December 1981 found themselves in conflict with the authorities following the government's efforts to nationalize Indian land. In the course of this conflict, forced removal of at least 10,000 Indians to relocation centers in the interior of the country and subsequent burning of some villages took place. The Misurasata movement split in 1983, with the breakaway Misura group of Stedman Fagoth Muller allying itself more closely with the FDN, and the rest accommodating themselves with the Sandinistas: On 8 December 1984 a ceasefire agreement known as the Bogota Accord was signed by Misurasata and the Nicaraguan government. A subsequent autonomy statute in September 1987 largely defused Miskito resistance.\n\nU.S. officials were active in attempting to unite the Contra groups. In June 1985 most of the groups reorganized as the United Nicaraguan Opposition (UNO), under the leadership of Adolfo Calero, Arturo Cruz and Alfonso Robelo, all originally supporters of the anti-Somoza revolution. After UNO's dissolution early in 1987, the Nicaraguan Resistance (RN) was organized along similar lines in May.\n\nIn front of the International Court of Justice, Nicaragua claimed that the contras were altogether a creation of the U.S. This claim was rejected. However, the evidence of a very close relationship between the contras and the United States was considered overwhelming and incontrovertible. The U.S. played a very large role in financing, training, arming, and advising the contras over a long period, and the contras only became capable of carrying out significant military operations as a result of this support.\n\nThe US government viewed the leftist Sandinistas as a threat to economic interests of American corporations in Nicaragua and to national security. US President Ronald Reagan stated in 1983 that “The defense of [the USA's] southern frontier” was at stake. \"In spite of the Sandinista victory being declared fair, the United States continued to oppose the left-wing Nicaraguan government.\" and opposed its ties to Cuba and the Soviet Union. Ronald Reagan, who had assumed the American presidency in January 1981, accused the Sandinistas of importing Cuban-style socialism and aiding leftist guerrillas in El Salvador. The Reagan administration continued to view the Sandinistas as undemocratic despite the 1984 Nicaraguan elections being generally declared fair by foreign observers. Throughout the 1980s the Sandinista government was regarded as \"Partly Free\" by Freedom House, an organization financed by the U.S. government. \nOn 4 January 1982, Reagan signed the top secret National Security Decision Directive 17 (NSDD-17), giving the CIA the authority to recruit and support the contras with $19 million in military aid. The effort to support the contras was one component of the Reagan Doctrine, which called for providing military support to movements opposing Soviet-supported, communist governments.\n\nBy December 1981, however, the United States had already begun to support armed opponents of the Sandinista government. From the beginning, the CIA was in charge. The arming, clothing, feeding and supervision of the contras became the most ambitious paramilitary and political action operation mounted by the agency in nearly a decade.\n\nIn the fiscal year 1984, the U.S. Congress approved $24 million in contra aid. However, since the contras failed to win widespread popular support or military victories within Nicaragua, opinion polls indicated that a majority of the U.S. public was not supportive of the contras, the Reagan administration lost much of its support regarding its contra policy within Congress after disclosure of CIA mining of Nicaraguan ports, and a report of the Bureau of Intelligence and Research commissioned by the State Department found Reagan's allegations about Soviet influence in Nicaragua \"exaggerated\", Congress cut off all funds for the contras in 1985 by the third Boland Amendment. The Boland Amendment had first been passed by Congress in December 1982. At this time, it only outlawed U.S. assistance to the contras \"for the purpose of overthrowing the Nicaraguan government\", while allowing assistance for other purposes. In October 1984, it was amended to forbid action by not only the Defense Department and the Central Intelligence Agency but all U.S. government agencies.\n\nNevertheless, the case for support of the contras continued to be made in Washington, D.C., by both the Reagan administration and The Heritage Foundation, which argued that support for the contras would counter Soviet influence in Nicaragua.\n\nOn 1 May 1985 President Reagan announced that his administration perceived Nicaragua to be \"an unusual and extraordinary threat to the national security and foreign policy of the United States\", and declared a \"national emergency\" and a trade embargo against Nicaragua to \"deal with that threat\". It \"is now a given; it is true\", the Washington Post declared in 1986, \"the Sandinistas are communists of the Cuban or Soviet school\"; that \"The Reagan administration is right to take Nicaragua as a serious menace—to civil peace and democracy in Nicaragua and to the stability and security of the region\"; that we must \"fit Nicaragua back into a Central American mode\" and \"turn Nicaragua back toward democracy,\" and with the \"Latin American democracies\" \"demand reasonable conduct by regional standard.\" \n\nSoon after the embargo was established, Managua re-declared \"a policy of nonalignment\" and sought the aid of Western Europe, who were opposed to U.S. policy, to escape dependency on the Soviet Union. Since 1981 U.S. pressures had curtailed Western credit to and trade with Nicaragua, forcing the government to rely almost totally on the Eastern bloc for credit, other aid, and trade by 1985. In his 1997 study on U.S. low intensity warfare, Kermit D. Johnson, a former Chief of the U.S. Army Chaplains, contends that U.S. hostility toward the revolutionary government was motivated not by any concern for \"national security\", but rather by what the world relief organization Oxfam termed \"the threat of a good example\": \n\nIt was alarming that in just a few months after the Sandinista revolution, Nicaragua received international acclaim for its rapid progress in the fields of literacy and health. It was alarming that a socialist-mixed-economy state could do in a few short months what the Somoza dynasty, a U.S. client state, could not do in 45 years! It was truly alarming that the Sandinistas were intent on providing the very services that establish a government's political and moral legitimacy.\n\nThe government's program included increased wages, subsidized food prices, and expanded health, welfare, and education services. And though it nationalized Somoza's former properties, it preserved a private sector that accounted for between 50 and 60 percent of GDP.\n\nThe United States began to support Contra activities against the Sandinista government by December 1981, with the CIA at the forefront of operations. The CIA supplied the funds and the equipment, coordinated training programs, and provided intelligence and target lists. While the Contras had little military successes, they did prove adept at carrying out CIA orders to attack \"soft targets\" — especially schools, health clinics, and cooperatives — and to \"neutralize\" pro-Sandinista civilians. The agency added to the Contras' sabotage efforts by blowing up refineries and pipelines, and mining ports. Finally, according to former Contra leader Edgar Chamorro, CIA trainers also gave Contra soldiers large knives. \"A commando knife [was given], and our people, everybody wanted to have a knife like that, to kill people, to cut their throats\". In 1985 Newsweek published a series of photos taken by Frank Wohl, a conservative student admirer traveling with the Contras, entitled \"Execution in the Jungle\": \n\nThe victim dug his own grave, scooping the dirt out with his hands... He crossed himself. Then a contra executioner knelt and rammed a k-bar knife into his throat. A second enforcer stabbed at his jugular, then his abdomen. When the corpse was finally still, the contras threw dirt over the shallow grave — and walked away.\n\nThe CIA officer in charge of the covert war, Duane \"Dewey\" Clarridge, admitted to the House Intelligence Committee staff in a secret briefing in 1984 that the Contras were routinely murdering \"civilians and Sandinista officials in the provinces as; well as heads of cooperatives, nurses, doctors and judges\". But he claimed that this did not violate President Reagan's executive order prohibiting assassinations because the agency defined it as just 'killing'. \"After all, this is war—a paramilitary operation,\" Clarridge said in conclusion. Edgar Chamorro explained the rationale behind this to a reporter. \"Sometimes terror is very productive. This is the policy, keep putting pressure until the people cry 'uncle'\". After the signing of the Central American Peace Accord in August 1987, the year war related deaths and economic destruction reached its peak, the Contras eventually entered negotiations with the Sandinista government (1988), and the war began to deescalate.\n\nBy 1989 the U.S.-orchestrated paramilitary war and economic isolation inflicted severe economic suffering on Nicaraguans. The supervisors knew that, in any democratic system, when the economy declines voters usually vote the incumbents out. They also knew that Nicaraguans had been exhausted from the war's carnage, including 30,865 people who had been killed by the end of 1989. Thus by the late 1980s the internal conditions in Nicaragua had deteriorated so drastically that Washington's approach towards the 1990 elections differed greatly from 1984. Instead of managing another boycott, the Bush administration decided to promote an opposition victory. The United States, through the National Endowment for Democracy, organized a united opposition, the National Opposition Union (Unión Nacional Opositora, UNO), out of fourteen dissimilar microparties. It then promoted UNO candidates-in particular presidential nominee Violeta Barrios de Chamorro. Washington thus \"micromanaged the opposition\" and exerted massive external pressure on the electorate. Contra attacks escalated from voter registration in October 1989 until election day. President Bush received Violeta Chamorro at the White House and promised to end the war and the economic embargo should she win.\n\nDespite respected pre-election polling showing the opposite, the UNO scored a decisive victory on February 25, 1990. Chamorro won with 55 percent of the presidential vote as compared to Ortega's 41 percent. Of 92 seats in the National Assembly, UNO gained 51, and the FSLN won 39. On April 25, 1990, Daniel Ortega slipped the sash of presidential office over Chamorro's shoulders.\n\nWith Congress blocking further contra aid, the Reagan administration sought to arrange funding and military supplies by means of third countries and private sources. Between 1984 and 1986, $34 million from third countries and $2.7 million from private sources were raised this way. The secret contra assistance was run by the National Security Council, with officer Lt. Col. Oliver North in charge. With the third-party funds, North created an organization called \"The Enterprise\", which served as the secret arm of the NSC staff and had its own airplanes, pilots, airfield, ship, operatives, and secret Swiss bank accounts. It also received assistance from personnel from other government agencies, especially from CIA personnel in Central America. This operation functioned, however, without any of the accountability required of U.S. government activities. The Enterprise's efforts culminated in the Iran–Contra Affair of 1986–1987, which facilitated contra funding through the proceeds of arms sales to Iran. \n\nAccording to the London Spectator, U.S. journalists in Central America had long known that the CIA was flying in supplies to the Contras inside Nicaragua before the scandal broke. No journalist payed it any attention until the alleged CIA supply man, Eugene Hasenfus, was shot down and captured by the Nicaraguan army. Similarly, reporters neglected to investigate many leads indicating that Oliver North was running the Contra operation from his office in the National Security Council. \n\nAccording to the National Security Archive, Oliver North had been in contact with Manuel Noriega, the military leader of Panama later convicted on drug charges, whom he personally met. The issue of drug money and its importance in funding the Nicaraguan conflict was the subject of various reports and publications. The contras were funded by drug trafficking, of which the United States was aware. Senator John Kerry's 1988 Committee on Foreign Relations report on Contra drug links concluded that \"senior U.S. policy makers were not immune to the idea that drug money was a perfect solution to the Contras' funding problems\".\n\nThe Reagan administration's support for the Contras continued to stir controversy well into the 1990s. In August 1996, \"San Jose Mercury News\" reporter Gary Webb published a series titled \"Dark Alliance\", alleging that the contras contributed to the rise of crack cocaine in California. In his subsequent 1999 book, also titled \"Dark Alliance\", Webb asserted that the Reagan administration helped harbor known drug traffickers, offering political asylum to some, to help raise funds for the rebel effort.\n\nGary Webb's career as a journalist was subsequently destroyed by the leading U.S. papers, the New York Times, the Washington Post, and the LA Times, at the behest of the Central Intelligence Agency. An internal CIA report, entitled, \"Managing a Nightmare\", shows the agency used \"a ground base of already productive relations with journalists\" to counter what it called \"a genuine public relations crisis.\" In the 1980s, Douglas Farah worked as a journalist, covering the civil wars in Central America for the Washington Post. According to Farah, while it was common knowledge that the Contras were involved in cocaine trafficking, the editors of the Washington Post refused to take it seriously: \n\nIf you’re talking about our intelligence community tolerating — if not promoting — drugs to pay for black ops, it’s rather an uncomfortable thing to do when you’re an establishment paper like the Post. If you were going to be directly rubbing up against the government, they wanted it more solid than it could probably ever be done.\n\nDuring the time the US Congress blocked funding for the contras, the Reagan government engaged in a campaign to alter public opinion and change the vote in Congress on contra aid. For this purpose, the NSC established an interagency working group, which in turn coordinated the Office of Public Diplomacy for Latin America and the Caribbean (managed by Otto Reich), which conducted the campaign. The S/LPD produced and widely disseminated a variety of pro-contra publications, arranged speeches and press conferences. It also disseminated \"white propaganda\"—pro-contra newspaper articles by paid consultants who did not disclose their connection to the Reagan administration.\n\nOn top of that, Oliver North helped Carl Channell's tax-exempt organization, the \"National Endowment for the Preservation of Liberty\", to raise $10 million, by arranging numerous briefings for groups of potential contributors at the premises of the White House and by facilitating private visits and photo sessions with President Reagan for major contributors. Channell in turn, used part of that money to run a series of television advertisements directed at home districts of Congressmen considered swing votes on contra aid. Out of the $10 million raised, more than $1 million was spent on pro-contra publicity.\n\nIn 1984 the Sandinista government filed a suit in the International Court of Justice (ICJ) against the United States (\"Nicaragua v. United States\"), which resulted in a 1986 judgment against the United States. The ICJ held that the U.S. had violated international law by supporting the contras in their rebellion against the Nicaraguan government and by mining Nicaragua's harbors. Regarding the alleged human rights violations by the contras, however, the ICJ took the view that the United States could only be held accountable for them if it would have been proven that the U.S. had effective control of the contra operations resulting in these alleged violations. Nevertheless, the ICJ found that the U.S. encouraged acts contrary to general principles of humanitarian law by producing the manual \"Psychological Operations in Guerrilla Warfare (Operaciones sicológicas en guerra de guerrillas\") and disseminating it to the contras. The manual, amongst other things, advised on how to rationalize killings of civilians and recommended to hire professional killers for specific selective tasks.\n\nThe United States, which did not participate in the merits phase of the proceedings, maintained that the ICJ's power did not supersede the Constitution of the United States and argued that the court did not seriously consider the Nicaraguan role in El Salvador, while it accused Nicaragua of actively supporting armed groups there, specifically in the form of supply of arms. The ICJ had found that evidence of a responsibility of the Nicaraguan government in this matter was insufficient. The U.S. argument was affirmed, however, by the dissenting opinion of ICJ member U.S. Judge Schwebel, who concluded that in supporting the contras, the United States acted lawfully in collective self-defence in El Salvador's support. The U.S. blocked enforcement of the ICJ judgment by the United Nations Security Council and thereby prevented Nicaragua from obtaining any actual compensation. The Nicaraguan government finally withdrew the complaint from the court in September 1992 (under the later, post-FSLN, government of Violeta Chamorro), following a repeal of the law requiring the country to seek compensation.\n\nAmericas Watch – which subsequently became part of Human Rights Watch – accused the Contras of:\n\nHuman Rights Watch released a report on the situation in 1989, which stated: \"[The] contras were major and systematic violators of the most basic standards of the laws of armed conflict, including by launching indiscriminate attacks on civilians, selectively murdering non-combatants, and mistreating prisoners.\"\n\nIn his affidavit to the World Court, former contra Edgar Chamorro testified that \"The CIA did not discourage such tactics. To the contrary, the Agency severely criticized me when I admitted to the press that the FDN had regularly kidnapped and executed agrarian reform workers and civilians. We were told that the only way to defeat the Sandinistas was to...kill, kidnap, rob and torture...\"\n\nContra leader Adolfo Calero denied that his forces deliberately targeted civilians: \"What they call a cooperative is also a troop concentration full of armed people. We are not killing civilians. We are fighting armed people and returning fire when fire is directed at us.\"\n\nU.S. news media published several articles accusing Americas Watch and other bodies of ideological bias and unreliable reporting. It alleged that Americas Watch gave too much credence to alleged Contra abuses and systematically tried to discredit Nicaraguan human rights groups such as the Permanent Commission on Human Rights, which blamed the major human rights abuses on the Contras.\n\nIn 1985, the \"Wall Street Journal\" reported:\nHuman Rights Watch, the umbrella organization of Americas Watch, replied to these allegations: \"Almost invariably, U.S. pronouncements on human rights exaggerated and distorted the real human rights violations of the Sandinista regime, and exculpated those of the U.S.-supported insurgents, known as the contras...The Bush administration is responsible for these abuses, not only because the contras are, for all practical purposes, a U.S. force, but also because the Bush administration has continued to minimize and deny these violations, and has refused to investigate them seriously.\"\n\nU.S. conservative political scientist Rudolph Rummel estimated that by 1987, the contras had murdered about 500 people while the Sandinistas had murdered 4,000 to 7,000 people in democide. In contrast, Witness for Peace and the Sandinista government claimed at least 736 civilians were murdered by the Contras between March 1987 and October 1988 alone.\n\nBy 1986 the contras were besieged by charges of corruption, human-rights abuses, and military ineptitude. A much-vaunted early 1986 offensive never materialized, and Contra forces were largely reduced to isolated acts of terrorism. In October 1987, however, the contras staged a successful attack in southern Nicaragua. Then on 21 December 1987, the FDN launched attacks at La Bonanza, La Siuna, and La Rosita in Zelaya province, resulting in heavy fighting. ARDE Frente Sur attacked at El Almendro and along the Rama road. These large-scale raids mainly became possible as the contras were able to use U.S.-provided Redeye missiles against Sandinista Mi-24 helicopter gunships, which had been supplied by the Soviets. Nevertheless, the Contras remained tenuously encamped within Honduras and weren't able to hold Nicaraguan territory.\n\nThere were isolated protests among the population against the draft implemented by the Sandinista government, which even resulted in full-blown street clashes in Masaya in 1988. However, polls showed the Sandinista government still enjoyed strong support from Nicaraguans. Political opposition groups were splintered and the Contras began to experience defections, although United States aid maintained them as a viable military force.\n\nAfter a cutoff in U.S. military support, and with both sides facing international pressure to bring an end to the conflict, the contras agreed to negotiations with the FSLN. With the help of five Central American Presidents, including Ortega, the sides agreed that a voluntary demobilization of the contras should start in early December 1989. They chose this date to facilitate free and fair elections in Nicaragua in February 1990 (even though the Reagan administration had pushed for a delay of contra disbandment).\n\nIn the resulting February 1990 elections, Violeta Chamorro and her party the UNO won an upset victory of 55% to 41% over Daniel Ortega, even though polls leading up to the election had clearly indicated an FSLN victory.\n\nPossible explanations include that the Nicaraguan people were disenchanted with the Ortega regime as well as the fact that already in November 1989, the White House had announced that the economic embargo against Nicaragua would continue unless Violeta Chamorro won. Also, there had been reports of intimidation from the side of the contras, with a Canadian observer mission confirming 42 people killed by the contras in \"election violence\" in October 1989. This led many commentators to assume that Nicaraguans voted against the Sandinistas out of fear of a continuation of the contra war and economic deprivation.\n\n\n\n", "id": "7548", "title": "Contras"}
{"url": "https://en.wikipedia.org/wiki?curid=7550", "text": "Craig Venter\n\nJohn Craig Venter (born October 14, 1946) is an American biotechnologist, biochemist, geneticist, and businessman. He is known for being involved with sequencing the second human genome and assembled the first team to transfect a cell with a synthetic chromosome. Venter founded Celera Genomics, The Institute for Genomic Research (TIGR) and the J. Craig Venter Institute (JCVI). He was the co-founder of Human Longevity Inc., served as its CEO until 2017, and is executive chairman of the board of directors. He was listed on \"Time\" magazine's 2007 and 2008 Time 100 list of the most influential people in the world. In 2010, the British magazine \"New Statesman\" listed Craig Venter at 14th in the list of \"The World's 50 Most Influential Figures 2010\". He is a member of the USA Science and Engineering Festival's Advisory Board.\n\nVenter was born in Salt Lake City, Utah, the son of Elizabeth and John Venter. In his youth, he did not take his education seriously, preferring to spend his time on the water in boats or surfing.\n\nAlthough he was against the Vietnam War, Venter was drafted and enlisted in the United States Navy where he worked in the intensive-care ward of a field hospital. While in Vietnam, he attempted suicide by swimming out to sea, but as he got deeper into the sea and was approaching the circling of a shark, he changed his mind more than a mile out.\nBeing confronted with wounded, maimed, and dying [marines] on a daily basis instilled in him a desire to study medicine — although he later switched to biomedical research.\n\nVenter began his college education at a community college, College of San Mateo in California, and later transferred to the University of California, San Diego, where he studied under biochemist Nathan O. Kaplan. He received a BS in biochemistry in 1972, and a PhD in physiology and pharmacology in 1975, both from UCSD. After working as an associate professor, and later as full professor, at the State University of New York at Buffalo, he joined the National Institutes of Health in 1984.\n\nAfter a brief marriage to Barbara Rae-Venter, with whom he had a son, Christopher, he married his student, Claire M. Fraser, remaining married to her until 2005. In late 2008 he married Heather Kowalski. They live in La Jolla outside San Diego, California where Venter gut-renovated a $6 million home.\n\nVenter is an atheist.\n\nVenter himself recognized his own ADHD behavior in his adolescence, and later found ADHD-linked genes in his own DNA.\n\nWhile an employee of the NIH, Venter learned how to identify mRNA and began to learn more about those expressed in the human brain. The short cDNA sequence fragments he was interested in are called expressed sequence tags, or ESTs. The NIH Office of Technology Transfer and Venter decided to take the ESTs discovered by others in an attempt to patent the genes identified based on studies of mRNA expression in the human brain. When Venter disclosed this strategy during a Congressional hearing, a firestorm of controversy erupted. The NIH later stopped the effort and abandoned the patent applications it had filed, following public outcry.\n\nVenter was passionate about the power of genomics to radically transform healthcare. Venter believed that shotgun sequencing was the fastest and most effective way to get useful human genome data. The method was rejected by the Human Genome Project however, since some geneticists felt it would not be accurate enough for a genome as complicated as that of humans, that it would be logistically more difficult, and that it would cost significantly more.\n\nVenter viewed the slow pace of progress in the Human Genome project as an opportunity to continue his interest in patenting genes, so he sought funding from the private sector to birth Celera Genomics. The company planned to profit from their work by creating genomic data to which users could subscribe for a fee. The goal consequently put pressure on the public genome program and spurred several groups to redouble their efforts to produce the full sequence. Venter's effort to publish a draft genome of his own DNA won him renown as the second person to sequence the human genome after the public effort, thus making it impossible to patent any of it.\n\nIn 2000, Venter and Francis Collins of the National Institutes of Health and U.S. Public Genome Project jointly made the announcement of the mapping of the human genome, a full three years ahead of the expected end of the Public Genome Program. The announcement was made along with U.S. President Bill Clinton, and UK Prime Minister Tony Blair. Venter and Collins thus shared an award for \"Biography of the Year\" from A&E Network.\nOn the 15 February 2001, the Human Genome Project consortium published the first Human Genome in the journal Nature, and was followed, one day later, by a Celera publication in Science. Despite some claims that shotgun sequencing was in some ways less accurate than the clone-by-clone method chosen by the Human Genome Project, the technique became widely accepted by the scientific community.\n\nVenter was fired by Celera in early 2002. According to his biography, Venter was fired due to a conflict with the main investor, Tony White, specifically barring him from attending the White House ceremony celebrating the achievement of sequencing the human genome.\n\nThe Global Ocean Sampling Expedition (GOS) is an ocean exploration genome project with the goal of assessing the genetic diversity in marine microbial communities and to understand their role in nature's fundamental processes. Begun as a Sargasso Sea pilot sampling project in August 2003, Venter announced the full Expedition on 4 March 2004. The project, which used Venter's personal yacht, \"Sorcerer II\", started in Halifax, Canada, circumnavigated the globe and returned to the U.S. in January 2006.\n\nVenter is currently the president of the J. Craig Venter Institute, which conducts research in synthetic biology. In June 2005, he co-founded Synthetic Genomics, a firm dedicated to using modified microorganisms to produce clean fuels and biochemicals. In July 2009, ExxonMobil announced a $600 million collaboration with Synthetic Genomics to research and develop next-generation biofuels. \nVenter continues to work on the creation of engineered diatomic microalgae for the production of biofuels.\n\nVenter is seeking to patent the first partially synthetic species possibly to be named \"Mycoplasma laboratorium\". There is speculation that this line of research could lead to producing bacteria that have been engineered to perform specific reactions, for example, produce fuels, make medicines, combat global warming, and so on.\n\nIn May 2010, a team of scientists led by Venter became the first to successfully create what was described as \"synthetic life\". This was done by synthesizing a very long DNA molecule containing an entire bacterium genome, and introducing this into another cell, analogous to the accomplishment of Eckard Wimmer's group, who synthesized and ligated an RNA virus genome and \"booted\" it in cell lysate. The single-celled organism contains four \"watermarks\"\nwritten into its DNA to identify it as synthetic and to help trace its descendants. The watermarks include \n\nOn March 25, 2016 Venter reported the creation of Syn 3.0, a synthetic genome having the fewest genes of any freely living organism (473 genes). Their aim was to strip away all nonessential genes, leaving only the minimal set necessary to support life.\nThis stripped-down, fast reproducing cell is expected to be a valuable tool for researchers in the field.\n\nOn September 4, 2007, a team led by Sam Levy published the first complete (six-billion-letter) genome of an individual human—Venter's own DNA sequence. Some of the sequences in Venter's genome are associated with wet earwax, increased risk of antisocial behavior, Alzheimer's and cardiovascular diseases. This publication was especially interesting since it contained a diploid instead of a haploid genome and shows promise for personalized medicine via genotyping. This genome, dubbed HuRef by Levy and others, was a landmark accomplishment.\n\nThe Human Reference Genome Browser is a web application for the navigation and analysis of Venter's recently published genome. The HuRef database consists of approximately 32 million DNA reads sequenced using microfluidic Sanger sequencing, assembled into 4,528 scaffolds and 4.1 million DNA variations identified by genome analysis. These variants include single-nucleotide polymorphisms (SNPs), block substitutions, short and large indels, and structural variations like insertions, deletions, inversions and copy number changes.\n\nThe browser enables scientists to navigate the HuRef genome assembly and sequence variations, and to compare it with the NCBI human build 36 assembly in the context of the NCBI and Ensembl annotations. The browser provides a comparative view between NCBI and HuRef consensus sequences, the sequence multi-alignment of the HuRef assembly, Ensembl and dbSNP annotations, HuRef variants, and the underlying variant evidence and functional analysis. The interface also represents the haplotype blocks from which diploid genome sequence can be inferred and the relation of variants to gene annotations. The display of variants and gene annotations are linked to external public resources including dbSNP, Ensembl, Online Mendelian Inheritance in Man (OMIM) and Gene Ontology (GO).\n\nUsers can search the HuRef genome using HUGO gene names, Ensembl and dbSNP identifiers, HuRef contig or scaffold locations, or NCBI chromosome locations. Users can then easily and quickly browse any genomic region via the simple and intuitive pan and zoom controls; furthermore, data relevant to specific loci can be exported for further analysis.\n\nOn March 4, 2014 Venter and co-founders Peter Diamandis and Robert Hariri announced the formation of Human Longevity, Inc., a company focused on extending the healthy, \"high performance\" human lifespan. At the time of the announcement the company had already raised $70 million in venture financing, which was expected to last 18 months. Venter served as the chairman and chief executive officer (CEO) until January 2017, when he stepped aside to become executive chairman of the board of directors. The company said that it plans to sequence 40,000 genomes per year, with an initial focus on cancer genomes and the genomes of cancer patients.\n\nHuman Longevity's mission is to extend healthy human lifespan by the use of high-resolution big data diagnostics from genomics, metabolomics, microbiomics, and proteomics, and the use of stem cell therapy.\n\nVenter is the author of two books, the first of which was ostensibly an autobiography titled \"A Life Decoded\". Venter's second book was titled \"Life at the Speed of Light\" in which he announced his theory that this is the generation in which there appears to be a dovetailing of the two previously diverse fields of science represented by computer programming and the genetic programming of life by DNA sequencing. He was applauded for his position on this by futurist Ray Kurzweil.\n\nVenter appeared in the \"Evolution\" episode of the documentary television series \"Understanding\" in 2002.\n\nVenter was featured on \"The Colbert Report\" on February 27, 2007, and again on October 30, 2007.\n\nOn December 4, 2007, Venter gave the Dimbleby lecture for the BBC in London.\n\nIn February 2008, he gave a speech about his current work at the TED conference.\n\nVenter was interviewed on his boat by BBC One for the first episode of the TV show \"Bang Goes the Theory\", which aired on July 27, 2009.\n\nOn November 21, 2010 Steve Kroft profiled Venter and his research on \"60 Minutes\".\n\nVenter appears in the 2-hour 2001 \"NOVA\" special, \"Cracking the code of life\".\n\nVenter has been the subject of articles in several magazines, including \"Wired\", \"The Economist\", Australian science magazine \"Cosmos\", and \"The Atlantic\". \n\nOn May 16, 2004, Venter gave the commencement speech at Boston University.\n\nIn a 2007 interview with \"New Scientist\" when asked \"Assuming you can make synthetic bacteria, what will you do with them?\", Venter replied: \"Over the next 20 years, synthetic genomics is going to become the standard for making anything. The chemical industry will depend on it. Hopefully, a large part of the energy industry will depend on it. We really need to find an alternative to taking carbon out of the ground, burning it, and putting it into the atmosphere. That is the single biggest contribution I could make.\"\n\nHe was on the 2007 Time 100 most influential people in the world list made by \"Time\" magazine. In 2007 he also received the Golden Eurydice Award for contributions to biophilosophy.\n\nVenter delivered the 2008 convocation speech for Faculty of Science honours and specialization students at the University of Alberta. A transcription of the speech is available here.\n\nVenter was featured in \"Time\" magazine's \"The Top 10 Everything of 2008\" article. Number three in 2008's Top 10 Scientific Discoveries was a piece outlining his work stitching together the 582,000 base pairs necessary to invent the genetic information for a whole new bacterium.\n\nOn May 20, 2010, Venter announced the creation of first self-replicating semi-synthetic bacterial cell.\n\nIn the June 2011 issue of \"Men's Journal\", Venter was featured as the \"Survival Skills\" celebrity of the month. He shared various anecdotes, and advice, including stories of his time in Vietnam, as well as mentioning a bout with melanoma upon his back, which subsequently resulted in \"giving a pound of flesh\" to surgery.\n\nVenter is mentioned, in the season finale of the first season of the science fiction series \"Orphan Black\", a joint production of Space and BBC America. In \nVenter has been a keynote speaker at the Congress of Future Medical Leaders (2014, 2015, 2016) and the Congress of Future Science and Technology Leaders (2015)\n\n\nVenter has authored over 200 publications in scientific journals.\n\n\n\n\n", "id": "7550", "title": "Craig Venter"}
{"url": "https://en.wikipedia.org/wiki?curid=7552", "text": "Chemical evolution\n\nChemical evolution may refer to:\n", "id": "7552", "title": "Chemical evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=7554", "text": "Carl Rogers\n\nCarl Ransom Rogers (January 8, 1902 – February 4, 1987) was an American psychologist and among the founders of the humanistic approach (or client-centered approach) to psychology. Rogers is widely considered to be one of the founding fathers of psychotherapy research and was honored for his pioneering research with the Award for Distinguished Scientific Contributions by the American Psychological Association (APA) in 1956.\n\nThe person-centered approach, his own unique approach to understanding personality and human relationships, found wide application in various domains such as psychotherapy and counseling (client-centered therapy), education (student-centered learning), organizations, and other group settings. For his professional work he was bestowed the Award for Distinguished Professional Contributions to Psychology by the APA in 1972. In a study by Haggbloom et al. (2002) using six criteria such as citations and recognition, Rogers was found to be the sixth most eminent psychologist of the 20th century and second, among clinicians, only to Sigmund Freud.\n\nRogers was born on January 8, 1902, in Oak Park, Illinois, a suburb of Chicago. His father, Walter A. Rogers, was a civil engineer and his mother, Julia M. Cushing, was a homemaker and devout Pentecostal Christian. Carl was the fourth of their six children.\n\nRogers was intelligent and could read well before kindergarten. Following an education in a strict religious and ethical environment as an altar boy at the vicarage of Jimpley, he became a rather isolated, independent and disciplined person, and acquired a knowledge and an appreciation for the scientific method in a practical world. His first career choice was agriculture, at the University of Wisconsin–Madison, where he was a part of the fraternity of Alpha Kappa Lambda, followed by history and then religion. At age 20, following his 1922 trip to Peking, China, for an international Christian conference, he started to doubt his religious convictions. To help him clarify his career choice, he attended a seminar entitled \"Why am I entering the Ministry?\", after which he decided to change his career. In 1924, he graduated from University of Wisconsin and enrolled at Union Theological Seminary. He later became an atheist.\n\nAfter two years he left the seminary to attend Teachers College, Columbia University, obtaining an MA in 1928 and a PhD in 1931. While completing his doctoral work, he engaged in child study. In 1930, Rogers served as director of the Society for the Prevention of Cruelty to Children in Rochester, New York. From 1935 to 1940 he lectured at the University of Rochester and wrote \"The Clinical Treatment of the Problem Child\" (1939), based on his experience in working with troubled children. He was strongly influenced in constructing his client-centered approach by the post-Freudian psychotherapeutic practice of Otto Rank. In 1940 Rogers became professor of clinical psychology at Ohio State University, where he wrote his second book, \"Counseling and Psychotherapy\" (1942). In it, Rogers suggested that the client, by establishing a relationship with an understanding, accepting therapist, can resolve difficulties and gain the insight necessary to restructure their life.\n\nIn 1945, he was invited to set up a counseling center at the University of Chicago. In 1947 he was elected President of the American Psychological Association. While a professor of psychology at the University of Chicago (1945–57), Rogers helped to establish a counseling center connected with the university and there conducted studies to determine the effectiveness of his methods. His findings and theories appeared in \"Client-Centered Therapy\" (1951) and \"Psychotherapy and Personality Change\" (1954). One of his graduate students at the University of Chicago, Thomas Gordon, established the Parent Effectiveness Training (P.E.T.) movement. Another student, Eugene T. Gendlin, who was getting his Ph.D. in philosophy, developed the practice of Focusing based on Rogerian listening. In 1956, Rogers became the first President of the American Academy of Psychotherapists. He taught psychology at the University of Wisconsin, Madison (1957–63), during which time he wrote one of his best-known books, \"On Becoming a Person\" (1961). Carl Rogers and Abraham Maslow (1908–70) pioneered a movement called humanistic psychology which reached its peak in the 1960s. In 1961, he was elected a Fellow of the American Academy of Arts and Sciences. Carl Rogers was also one of the people who questioned the rise of McCarthyism in 1950s. Through articles, he criticized society for its backward-looking affinities.\n\nRogers continued teaching at University of Wisconsin until 1963, when he became a resident at the new Western Behavioral Sciences Institute (WBSI) in La Jolla, California. Rogers left the WBSI to help found the Center for Studies of the Person in 1968. His later books include \"Carl Rogers on Personal Power\" (1977) and \"Freedom to Learn for the 80's\" (1983). He remained a resident of La Jolla for the rest of his life, doing therapy, giving speeches and writing until his sudden death in 1987. In 1987, Rogers suffered a fall that resulted in a fractured pelvis: he had life alert and was able to contact paramedics. He had a successful operation, but his pancreas failed the next night and he died a few days later.\n\nRogers's last years were devoted to applying his theories in situations of political oppression and national social conflict, traveling worldwide to do so. In Belfast, Northern Ireland, he brought together influential Protestants and Catholics; in South Africa, blacks and whites; in Brazil people emerging from dictatorship to democracy; in the United States, consumers and providers in the health field. His last trip, at age 85, was to the Soviet Union, where he lectured and facilitated intensive experiential workshops fostering communication and creativity. He was astonished at the numbers of Russians who knew of his work.\n\nTogether with his daughter, Natalie Rogers, and psychologists Maria Bowen, Maureen O'Hara, and John K. Wood, between 1974 and 1984, Rogers convened a series of residential programs in the US, Europe, Brazil and Japan, the Person-Centered Approach Workshops, which focused on cross-cultural communications, personal growth, self-empowerment, and learning for social change.\n\nRogers' theory of the self is considered to be humanistic, existential, and phenomenological. His theory is based directly on the \"phenomenal field\" personality theory of Combs and Snygg (1949). Rogers' elaboration of his own theory is extensive. He wrote 16 books and many more journal articles describing it. Prochaska and Norcross (2003) states Rogers \"consistently stood for an empirical evaluation of psychotherapy. He and his followers have demonstrated a humanistic approach to conducting therapy and a scientific approach to evaluating therapy need not be incompatible.\"\n\nHis theory (as of 1953) was based on 19 propositions:\n\nIn relation to No. 17, Rogers is known for practicing \"unconditional positive regard,\" which is defined as accepting a person \"without negative judgment of ... [a person's] basic worth.\"\n\nWith regard to development, Rogers described principles rather than stages. The main issue is the development of a self-concept and the progress from an undifferentiated self to being fully differentiated.\n\nIn the development of the self-concept, he saw conditional and unconditional positive regard as key. Those raised in an environment of unconditional positive regard have the opportunity to fully actualize themselves. Those raised in an environment of conditional positive regard feel worthy only if they match conditions (what Rogers describes as \"conditions of worth\") that have been laid down for them by others.\n\nOptimal development, as referred to in proposition 14, results in a certain process rather than static state. He describes this as \"the good life\", where the organism continually aims to fulfill its full potential. He listed the characteristics of a fully functioning person (Rogers 1961):\n\nRogers identified the \"real self\" as the aspect of one's being that is founded in the actualizing tendency, follows organismic valuing, needs and receives positive regard and self-regard. It is the \"you\" that, if all goes well, you will become. On the other hand, to the extent that our society is out of sync with the actualizing tendency, and we are forced to live with conditions of worth that are out of step with organismic valuing, and receive only conditional positive regard and self-regard, we develop instead an \"ideal self\". By ideal, Rogers is suggesting something not real, something that is always out of our reach, the standard we cannot meet. This gap between the real self and the ideal self, the \"I am\" and the \"I should\" is called \"incongruity\".\n\nRogers described the concepts of \"congruence\" and \"incongruence\" as important ideas in his theory. In proposition #6, he refers to the actualizing tendency. At the same time, he recognized the need for \"positive regard\". In a fully congruent person realizing their potential is not at the expense of experiencing positive regard. They are able to lead lives that are authentic and genuine. Incongruent individuals, in their pursuit of positive regard, lead lives that include falseness and do not realize their potential. Conditions put on them by those around them make it necessary for them to forgo their genuine, authentic lives to meet with the approval of others. They live lives that are not true to themselves, to who they are on the inside out.\n\nRogers suggested that the incongruent individual, who is always on the defensive and cannot be open to all experiences, is not functioning ideally and may even be malfunctioning. They work hard at maintaining/protecting their self-concept. Because their lives are not authentic this is a difficult task and they are under constant threat. They deploy \"defense mechanisms\" to achieve this. He describes two mechanisms: \"distortion\" and \"denial\". Distortion occurs when the individual perceives a threat to their self-concept. They distort the perception until it fits their self-concept.\n\nThis defensive behavior reduces the consciousness of the threat but not the threat itself. And so, as the threats mount, the work of protecting the self-concept becomes more difficult and the individual becomes more defensive and rigid in their self structure. If the incongruence is immoderate this process may lead the individual to a state that would typically be described as neurotic. Their functioning becomes precarious and psychologically vulnerable. If the situation worsens it is possible that the defenses cease to function altogether and the individual becomes aware of the incongruence of their situation. Their personality becomes disorganised and bizarre; irrational behavior, associated with earlier denied aspects of self, may erupt uncontrollably.\n\nRogers originally developed his theory to be the foundation for a system of therapy. He initially called this \"non-directive therapy\" but later replaced the term \"non-directive\" with the term \"client-centered\" and then later used the term \"person-centered\". Even before the publication of \"Client-Centered Therapy\" in 1951, Rogers believed that the principles he was describing could be applied in a variety of contexts and not just in the therapy situation. As a result, he started to use the term \"person-centered approach\" later in his life to describe his overall theory. Person-centered therapy is the application of the person-centered approach to the therapy situation. Other applications include a theory of personality, interpersonal relations, education, nursing, cross-cultural relations and other \"helping\" professions and situations. In 1946 Rogers co-authored \"Counseling with Returned Servicemen,\" with John L. Wallen (the creator of the behavioral model known as \"The Interpersonal Gap\"), documenting the application of person-centered approach to counseling military personnel returning from the second world war.\n\nThe first empirical evidence of the effectiveness of the client-centered approach was published in 1941 at the Ohio State University by Elias Porter, using the recordings of therapeutic sessions between Carl Rogers and his clients. Porter used Rogers' transcripts to devise a system to measure the degree of directiveness or non-directiveness a counselor employed. The attitude and orientation of the counselor were demonstrated to be instrumental in the decisions made by the client.\n\nThe application to education has a large robust research tradition similar to that of therapy with studies having begun in the late 1930s and continuing today (Cornelius-White, 2007). Rogers described the approach to education in \"Client-Centered Therapy\" and wrote \"Freedom to Learn\" devoted exclusively to the subject in 1969. \"Freedom to Learn\" was revised two times. The new Learner-Centered Model is similar in many regards to this classical person-centered approach to education.\nRogers and Harold Lyon began a book prior to Rogers death, entitled \"On Becoming an Effective Teacher -- Person-centered Teaching, Psychology, Philosophy, and Dialogues with Carl R. Rogers and Harold Lyon\", which was completed by Lyon and Reinhard Tausch and published in 2013 containing Rogers last unpublished writings on person-centered teaching. Rogers had the following five hypotheses regarding learner-centered education:\n\nIn 1970, Richard Young, Alton L. Becker, and Kenneth Pike published \"Rhetoric: Discovery and Change\", a widely influential college writing textbook that used a Rogerian approach to communication to revise the traditional Aristotelian framework for rhetoric. The Rogerian method of argument involves each side restating the other's position to the satisfaction of the other. In a paper, it can be expressed by carefully acknowledging and understanding the opposition, rather than dismissing them.\n\nThe application to cross-cultural relations has involved workshops in highly stressful situations and global locations including conflicts and challenges in South Africa, Central America, and Ireland. Along with Alberto Zucconi and Charles Devonshire, he co-founded the Istituto dell'Approccio Centrato sulla Persona (Person-Centered Approach Institute) in Rome, Italy.\n\nHis international work for peace culminated in the Rust Peace Workshop which took place in November 1985 in Rust, Austria. Leaders from 17 nations convened to discuss the topic \"The Central America Challenge\". The meeting was notable for several reasons: it brought national figures together as people (not as their positions), it was a private event, and was an overwhelming positive experience where members heard one another and established real personal ties, as opposed to stiffly formal and regulated diplomatic meetings.\n\nSome scholars believe there is a politics implicit in Rogers's approach to psychotherapy. Toward the end of his life, Rogers came to that view himself. The central tenet of a Rogerian, person-centered politics is that public life does not have to consist of an endless series of winner-take-all battles among sworn opponents; rather, it can and should consist of an ongoing dialogue among all parties. Such dialogue would be characterized by respect among the parties, authentic speaking by each party, and – ultimately – empathic understanding among all parties. Out of such understanding, mutually acceptable solutions would (or at least could) flow.\nDuring his last decade, Rogers facilitated or participated in a wide variety of dialogic activities among politicians, activists, and other social leaders, often outside the U.S. In addition, he lent his support to several non-traditional U.S. political initiatives, including the \"12-Hour Political Party\" of the Association for Humanistic Psychology and the founding of a \"transformational\" political organization, the New World Alliance. By the 21st century, interest in dialogic approaches to political engagement and change had become widespread, especially among academics and activists. Theorists of a specifically Rogerian, person-centered approach to politics as dialogue have made substantial contributions to that project. \n\nCarl Rogers served on the board of the Human Ecology Fund from the late 50s into the 60s, which was a CIA-funded organization that provided grants to researchers looking into personality. In addition, he and other people in the field of personality and psychotherapy were given a lot of information about Khrushchev. 'We were asked to figure out what we thought of him and what would be the best way of dealing with him. And that seemed to be an entirely principled and legitimate aspect. I don't think we contributed very much, but, anyway, we tried.' \".\n\nMore on the Human Ecology Fund and Carl Rogers: , , \n\n\n\n\n\n\n \n", "id": "7554", "title": "Carl Rogers"}
{"url": "https://en.wikipedia.org/wiki?curid=7555", "text": "Casimir effect\n\nIn quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. They are named after the Dutch physicist Hendrik Casimir who predicted them in 1948.\n\nThe typical example is of the two uncharged conductive plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field means that there is no field between the plates, and no force would be measured between them. When this field is instead studied using the quantum electrodynamic vacuum, it is seen that the plates do affect the virtual photons which constitute the field, and generate a net force—either an attraction or a repulsion depending on the specific arrangement of the two plates. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured and is a striking example of an effect captured formally by second quantization. However, the treatment of boundary conditions in these calculations has led to some controversy. In fact, \"Casimir's original goal was to compute the van der Waals force between polarizable molecules\" of the conductive plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields.\n\nDutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947, and, after a conversation with Niels Bohr who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948; the former is called the Casimir–Polder force while the latter is the Casimir effect in the narrow sense. Predictions of the force were later extended to finite-conductivity metals and dielectrics, and recent calculations have considered more general geometries. It was not until 1997, however, that a direct experiment, by S. Lamoreaux, described above, quantitatively measured the force to within 15% of the value predicted by the theory, although previous work had observed the force qualitatively, and indirect validation of the predicted Casimir energy had been made by measuring the thickness of liquid helium films. Subsequent experiments approach an accuracy of a few percent.\n\nBecause the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is extremely small. On a submicron scale, this force becomes so strong that it becomes the dominant force between uncharged conductors. In fact, at separations of 10 nm—about 100 times the typical size of an atom—the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depending on surface geometry and other factors).\n\nIn modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; in applied physics, it is significant in some aspects of emerging microtechnologies and nanotechnologies.\n\nAny medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in noisy water or gas illustrate the Casimir force.\n\nThe Casimir effect can be understood by the idea that the presence of conducting metals and dielectrics alters the vacuum expectation value of the energy of the second quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the conductors and dielectrics, the Casimir effect manifests itself as a force between such objects.\n\nThe causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a \"field\" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum.\n\nThe vacuum has, implicitly, all of the properties that a particle may have: spin, or polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, \"empty\" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is\n\nSumming over all possible oscillators at all points in space gives an infinite quantity. Since only \"differences\" in energy are physically measurable (with the notable exception of gravitation, which remains beyond the scope of quantum field theory), this infinity may be considered a feature of the mathematics rather than of the physics. This argument is the underpinning of the theory of renormalization. Dealing with infinite quantities in this way was a cause of widespread unease among quantum field theorists before the development in the 1970s of the renormalization group, a mathematical formalism for scale transformations that provides a natural basis for the process.\n\nWhen the scope of the physics is widened to include gravity, the interpretation of this formally infinite quantity remains problematic. There is currently no compelling explanation as to why it should not result in a cosmological constant that is many orders of magnitude larger than observed. However, since we do not yet have any fully coherent quantum theory of gravity, there is likewise no compelling reason as to why it should.\n\nAlternatively, a 2005 paper by Robert Jaffe of MIT states that \"Casimir effects\ncan be formulated and Casimir forces can be computed without reference to zero-point energies.\nThey are relativistic, quantum forces between charges and currents. The Casimir force (per unit\narea) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha approaching infinity limit,\" and that \"The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates.\" Casimir and Polder's original paper used this method to derive the Casimir-Polder force. In 1978, Schwinger, DeRadd, and Milton published a similar derivation for the Casimir Effect between two parallel plates.\n\nA third way of understanding Casimir forces has been suggested, based on canonical macroscopic quantum electrodynamics. In this interpretation, there exists a ground (vacuum) state of the \"coupled\" system of matter and fields, which determines the ground-state properties of the electromagnetic field, giving rise to a force. The Casimir force is fundamentally a property of the coupled system of matter and fields, in which the interaction between the plates is mediated by the zero-point fields. In more traditional interpretations, however, the emphasis has fallen either on the electromagnetic field or the fluctuating material in the plates.\n\nCasimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric.\n\nConsider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the \"n\"th standing wave is formula_2. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then\n\nwith the sum running over all possible values of \"n\" enumerating the standing waves. The factor of 1/2 is present because the zero-point energy of the n'th mode is formula_4, where formula_2 is the energy increment for the n'th mode. (It is the same 1/2 as appears in the equation formula_6). Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions.\n\nIn particular, one may ask how the zero-point energy depends on the shape \"s\" of the cavity. Each energy level formula_2 depends on the shape, and so one should write formula_8 for the energy level, and formula_9 for the vacuum expectation value. At this point comes an important observation: the force at point \"p\" on the wall of the cavity is equal to the change in the vacuum energy if the shape \"s\" of the wall is perturbed a little bit, say by formula_10, at point \"p\". That is, one has\n\nThis value is finite in many practical calculations.\n\nAttraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). With \"a\" « \"L\", the states within the slot of width \"a\" are highly constrained so that the energy \"E\" of any one mode is widely separated from that of the next. This is not the case in the large region \"L\", where there is a large number (numbering about \"L\" / \"a\") of states with energy evenly spaced between \"E\" and the next mode in the narrow slot—in other words, all slightly larger than \"E\". Now on shortening \"a\" by d\"a\" (< 0), the mode in the narrow slot shrinks in wavelength and therefore increases in energy proportional to −d\"a\"/\"a\", whereas all the \"L\" /\"a\" states that lie in the large region lengthen and correspondingly decrease their energy by an amount proportional to d\"a\"/\"L\" (note the denominator). The two effects nearly cancel, but the net change is slightly negative, because the energy of all the \"L\" /\"a\" modes in the large region are slightly larger than the single mode in the slot. Thus the force is attractive: it tends to make \"a\" slightly smaller, the plates attracting each other across the thin slot.\n\n\nIn the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance formula_12 apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the plates lie parallel to the \"xy\"-plane, the standing waves are\n\nwhere formula_14 stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, formula_15 and formula_16 are the wave numbers in directions parallel to the plates, and\n\nis the wave-number perpendicular to the plates. Here, \"n\" is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is\n\nwhere \"c\" is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in \"k\"-space. The assumption of periodic boundary conditions yields,\n\nwhere \"A\" is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is\n\nIn the end, the limit formula_21 is to be taken. Here \"s\" is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for \"s\" real and larger than 3. The sum has a pole at \"s\"=3, but may be analytically continued to \"s\"=0, where the expression is finite. The above expression simplifies to:\n\nwhere polar coordinates formula_23 were introduced to turn the double integral into a single integral. The formula_24 in front is the Jacobian, and the formula_25 comes from the angular integration. The integral converges if Re[\"s\"] > 3, resulting in\n\nThe sum diverges at \"s\" in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to \"s\"=0 is assumed to make sense physically in some way, then one has\n\nBut\n\nand so one obtains\n\nThe analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area formula_30 for idealized, perfectly conducting plates with vacuum between them is\n\nwhere\n\nThe force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of formula_32 shows that the Casimir force per unit area formula_30 is very small, and that furthermore, the force is inherently of quantum-mechanical origin.\n\nNOTE: In Casimir's original derivation , a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). The 0-point energy on \"both\" sides of the plate is considered. Instead of the above \"ad hoc\" analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as formula_37 in the above.\n\nCasimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz's theory for two metal plates reduces to Casimir's idealized 1/\"a\" force law for large separations \"a\" much greater than the skin depth of the metal, and conversely reduces to the 1/\"a\" force law of the London dispersion force (with a coefficient called a Hamaker constant) for small \"a\", with a more complicated dependence on \"a\" for intermediate separations determined by the dispersion of the materials.\n\nLifshitz' result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius \"R\" is much larger than the separation \"a\", in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate \"R\"/\"a\" force (neglecting both skin-depth and higher-order curvature effects). However, in the 2000s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes.\n\nOne of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven (Netherlands), in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. Some of the experimental details as well as some background information on how Casimir, Polder and Sparnaay arrived at this point are highlighted in a 2007 interview with Marcus Sparnaay.\n\nThe Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California, Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a large radius.\n\nIn 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators.\n\nIn order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator.\n\nThe heat kernel or exponentially regulated sum is\n\nwhere the limit formula_39 is taken in the end. The divergence of the sum is typically manifested as\n\nfor three-dimensional cavities. The infinite part of the sum is associated with the bulk constant \"C\" which \"does not\" depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator\n\nis better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator\n\nis completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex \"s\" plane, with the bulk divergence at \"s\"=4. This sum may be analytically continued past this pole, to obtain a finite part at \"s\"=0.\n\nNot every cavity configuration necessarily leads to a finite part (the lack of a pole at \"s\"=0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in \"Landau and Lifshitz\", \"Theory of Continuous Media\".)\n\nThe Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called \"virtual particles\".\n\nMore interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects.\n\nIn the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon.\n\nThe dynamical Casimir effect is the production of particles and energy from an accelerated \"moving mirror\". This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect.\nA similar analysis can be used to explain Hawking radiation that causes the slow \"evaporation\" of black holes (although this is generally visualized as the escape of one particle from a virtual particle-antiparticle pair, the other particle having been captured by the black hole).\n\nConstructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect has been used to better understand acceleration radiation such as the Unruh effect.\n\nThere are few instances wherein the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was recently carried out by Munday et al. Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers–Kronig relations). Casimir and Casimir-Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al.\n\nIt has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, silicon array propulsion for space drives, and so-called Casimir oscillators.\n\nThe Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where the energy can be \"arbitrarily\" negative at a given point, Many physicists such as Stephen Hawking, Kip Thorne, and others therefore argue that such effects might make it possible to stabilize a traversable wormhole. Miguel Alcubierre has suggested using the effect to obtain the negative energy density required for his Alcubierre Drive.\n\nOn 4 June 2013 it was reported that a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory have for the first time demonstrated a compact integrated silicon chip that can measure the Casimir force.\n\n\n\n\n\n", "id": "7555", "title": "Casimir effect"}
{"url": "https://en.wikipedia.org/wiki?curid=7558", "text": "Coin\n\nA coin is a small, flat, round piece of metal or plastic used primarily as a medium of exchange or legal tender. They are standardized in weight, and produced in large quantities at a mint in order to facilitate trade. They are most often issued by a government.\n\nCoins are usually metal or alloy, or sometimes made of synthetic materials. They are usually disc shaped. Coins made of valuable metal are stored in large quantities as bullion coins. Other coins are used as money in everyday transactions, circulating alongside banknotes. Usually the highest value coin in circulation (i.e. excluding bullion coins) is worth less than the lowest-value note. In the last hundred years, the face value of circulation coins has occasionally been lower than the value of the metal they contain, for example due to inflation. If the difference becomes significant, the issuing authority may decide to withdraw these coins from circulation, possibly issuing new equivalents with a different composition, or the public may decide to melt the coins down or hoard them (see Gresham's law).\n\nExceptions to the rule of face value being higher than content value also occur for some bullion coins made of copper, silver, or gold (and, rarely, other metals, such as platinum or palladium), intended for collectors or investors in precious metals. Examples of modern gold collector/investor coins include the British sovereign minted by the United Kingdom, the American Gold Eagle minted by the United States, the Canadian Gold Maple Leaf minted by Canada, and the Krugerrand, minted by South Africa. While the Eagle, Maple Leaf, and Sovereign coins have nominal (purely symbolic) face values; the Krugerrand does not.\n\nHistorically, a great quantity of coinage metals (including alloys) and other materials (e.g. porcelain) have been used to produce coins for circulation, collection, and metal investment: bullion coins often serve as more convenient stores of assured metal quantity and purity than other bullion.\n\nThe first coins were developed independently in Iron Age Anatolia and Archaic Greece, India and China around the 7th and 6th centuries BCE. Coins spread rapidly in the 6th and 5th centuries BCE, throughout Greece and Persia, and further to the Balkans.\n\nStandardized Roman currency was used throughout the Roman Empire. Important Roman gold and silver coins were continued into the Middle Ages (see Gold dinar, Solidus, Aureus, Denarius). Ancient and early medieval coins in theory had the value of their metal content, although there have been many instances throughout history of the metal content of coins being debased, so that the inferior coins were worth less in metal than their face value. Fiat money first arose in medieval China, with the jiaozi paper money. Early paper money was introduced in Europe in the later Middle Ages, but some coins continued to have the value of the gold or silver they contained throughout the Early Modern period. The penny was minted as a silver coin until the 17th century.\n\nThe first circulating United States coins were cents (pennies), produced in 1793, and made entirely from copper. Silver content was reduced in many coins in the 19th century (use of billon), and the first coins made entirely of base metal (e.g. nickel, cupronickel, aluminium bronze), representing values higher than the value of their metal, were minted in the mid 19th century.\nCoins were an evolution of \"currency\" systems of the Late Bronze Age, where standard-sized ingots, and tokens such as knife money, were used to store and transfer value. In the late Chinese Bronze Age, standardized cast tokens were made, such as those discovered in a tomb near Anyang. These were replicas in bronze of earlier Chinese currency, cowrie shells, so they were named Bronze Shell.\n\nAccording to Aristotle (fr. 611,37,ed. V. Rose) and Pollux (Onamastikon IX.83), the first issuer of coins was Hermodike of Kyme\n\nThe earliest coins are mostly associated with Iron Age Anatolia, especially with the kingdom of Lydia.\nEarly electrum coins were not standardized in weight, and in their earliest stage may have been ritual objects, such as badges or medals, issued by priests.\nMany early Lydian and Greek coins were minted under the authority of private individuals and are thus more akin to tokens or badges than to modern coins, though due to their numbers it is evident that some were official state issues, with King Alyattes of Lydia being a frequently mentioned originator of coinage.\n\nThe first Lydian coins were made of electrum, a naturally occurring alloy of silver and gold that was further alloyed with added silver and copper.\nMost of the early Lydian coins include no writing (\"legend\" or \"inscription\"), only an image of a symbolic animal. Therefore, the dating of these coins relies primarily on archaeological evidence, with the most commonly cited evidence coming from excavations at the Temple of Artemis at Ephesus, also called the Ephesian Artemision (which would later evolve into one of the Seven Wonders of the Ancient World). Because the oldest lion head \"coins\" were discovered in that temple, and they do not appear to have been used in commerce, these objects may not have been coins but badges or medals issued by the priests of that temple. Anatolian Artemis was the \"Πὀτνια Θηρῶν\" (Potnia Thêrôn, \"Mistress of Animals\"), whose symbol was the stag.\n\nA small percentage of early Lydian/Greek coins have a legend. A famous early electrum coin, the most ancient inscribed coin at present known, is from nearby Caria. This coin has a Greek legend reading \"phaenos emi sema\" interpreted variously as \"I am the badge of Phanes\", or \"I am the sign of light\", or \"I am the tomb of light\", or \"I am the tomb of Phanes\".\nThe coins of Phanes are known to be amongst the earliest of Greek coins, a hemihekte of the issue was found in the foundation deposit of the temple of Artemis at Ephesos (the oldest deposit of electrum coins discovered). One assumption is that Phanes was a wealthy merchant, another that this coin is associated with Apollo-Phanes and, due to the Deer, with Artemis (twin sister of the god of light Apollo-Phaneos). Although only seven Phanes type coins were discovered, it is also notable that 20% of all early electrum coins also have the lion of Artemis and the sun burst of Apollo-Phaneos.\n\nAlternatively, Phanes may have been the Halicarnassian mercenary of Amasis mentioned by Herodotus, who escaped to the court of Cambyses, and became his guide in the invasion of Egypt in 527 or 525 BCE. According to Herodotus, this Phanes was buried alive by a sandstorm, together with 50,000 Persian soldiers, while trying to conquer the temple of Amun–Zeus in Egypt. The fact that the Greek word \"Phanes\" also means light (or lamp), and the word \"sema\" also means tomb makes this coin a famous and controversial one.\n\nAnother candidate for the site of the earliest coins is Aegina, where Chelone (\"turtle\") coins were first minted on 700 BCE, either by the local Aegina people or by Pheidon king of Argos (who first set the standards of weights and measures). In the Bibliothèque Nationale, Paris, there is a unique electrum stater of Aegina.\n\nCoins from Athens and Corinth appeared shortly thereafter, known to exist at least since the late 6th century BCE.\n\nCoinage followed Greek colonization and influence first around the Mediterranean and soon after to North Africa (including Egypt), Syria, Persia, and the Balkans.\n\nCoins were minted in the Achaemenid Empire, including the gold \"darics\" and silver \"sigloi\". With the Achemenid conquest of Gandhara under Darius the Great c. 520 BCE, the practice spread to the Indo-Gangetic Plain. The coins of this period were called \"Puranas\", \"Karshapanas\" or \"Pana\". These earliest Indian coins, however, are unlike those circulated in Persia, which were derived from the Greek/Anatolian type; they not disk-shaped but rather stamped bars of metal, suggesting that the innovation of stamped currency was added to a pre-existing form of token currency which had already been present in the Mahajanapada kingdoms of the Indian Iron Age. Mahajanapadas that minted their own coins included Gandhara, Kuntala, Kuru, Panchala, Shakya, Surasena and Surashtra.\n\nIn China, early round coins appeared in the 4th century BCE.\nThe first Roman coins, which were crude, heavy cast bronzes, were issued c. 289 BCE.\n\nIn the Philippines, gold, which was plentiful in many parts of the islands, invariably found its way into these objects that included the Piloncitos, small bead-like gold bits considered by the local numismatists as the earliest coin of ancient Filipinos, and gold barter rings.\n\nThe first European coin to use Arabic numerals to date the year in which the coin was minted was the St. Gall silver \"Plappart\" of 1424.\n\nMost coins presently are made of a base metal, and their value comes from their status as fiat money. This means that the value of the coin is decreed by government fiat (law), and thus is determined by the free market only in as much as national currencies are used in domestic trade and also traded internationally on foreign exchange markets. Thus, these coins are monetary tokens, just as paper currency is: they are usually not backed by metal, but rather by some form of government guarantee. Some have suggested that such coins not be considered to be \"true coins\" (see below). Thus, there is very little economic difference between notes and coins of equivalent face value.\n\nCoins may be in circulation with fiat values lower than the value of their component metals, but they are never initially issued with such value, and the shortfall only arises over time due to inflation, as market values for the metal overtake the fiat declared face value of the coin. Examples are the pre-1965 US dime, quarter, half dollar, and dollar (nominally containing slightly less than a tenth, quarter, half, and full ounce of silver, respectively), US nickel, and pre-1982 US penny. As a result of the increase in the value of copper, the United States greatly reduced the amount of copper in each penny. Since mid-1982, United States pennies are made of 97.5% zinc, with the remaining 2.5% being a coating of copper. Extreme differences between fiat values and metal values of coins cause coins to be hoarded or removed from circulation by illicit smelters in order to realise the value of their metal content. This is an example of Gresham's law. The United States Mint, in an attempt to avoid this, implemented new interim rules on December 14, 2006, subject to public comment for 30 days, which criminalized the melting and export of pennies and nickels. Violators can be fined up to $10,000 and/or imprisoned for up to five years.\n\nA coin's value as a collector's item or as an investment generally depends on its condition, specific historical significance, rarity, quality, beauty of the design and general popularity with collectors. If a coin is greatly lacking in all of these, it is unlikely to be worth much. The value of bullion coins is also influenced to some extent by those factors, but is largely based on the value of their gold, silver, or platinum content. Sometimes non-monetized bullion coins such as the Canadian Maple Leaf and the American Gold Eagle are minted with nominal face values less than the value of the metal in them, but as such coins are never intended for circulation, these face values have no relevance.\n\nCoins can be used as creative medium of expression – from fine art sculpture to the penny machines that can be found in most amusement parks. In the Code of Federal Regulations (CFR) in the United States there are some regulations specific to nickels and pennies that are informative on this topic. 31 CFR § 82.1 forbids unauthorized persons from exporting, melting, or treating any 5 or 1 cent coins.\n\nThis has been a particular problem with nickels and dimes (and with some comparable coins in other currencies) because of their relatively low face value and unstable commodity prices. For a while the copper in US pennies was worth more than one cent, so people would hoard pennies and then melt them down for their metal value. It cost more than face value to manufacture pennies or nickels, so any widespread loss of the coins in circulation could be expensive for the US Treasury. This was more of a problem when coins were still made of precious metals like silver and gold, so strict laws against alteration make more sense historically.\n\n31 CFR § 82.2 goes on to state that: \"(b) The prohibition contained in § 82.1 against the treatment of 5-cent coins and one-cent coins shall not apply to the treatment of these coins for educational, amusement, novelty, jewelry, and similar purposes as long as the volumes treated and the nature of the treatment makes it clear that such treatment is not intended as a means by which to profit solely from the value of the metal content of the coins.\"\n\nThroughout history, monarchs and governments have often created more coinage than their supply of precious metals would allow if the coins were pure metal. By replacing some fraction of a coin's precious metal content with a base metal (often copper or nickel), the intrinsic value of each individual coin was reduced (thereby \"debasing\" the money), allowing the coining authority to produce more coins than would otherwise be possible. Debasement occasionally occurs in order to make the coin physically harder and therefore less likely to be worn down as quickly, but the more usual reason is to profit from the difference between face value and metal value. Debasement of money almost always leads to price inflation. Sometimes price controls are at the same time also instituted by the governing authority, but historically these have generally proved unworkable.\n\nThe United States is unusual in that it has only slightly modified its coinage system (except for the images and symbols on the coins, which have changed a number of times) to accommodate two centuries of inflation. The one-cent coin has changed little since 1856 (though its composition was changed in 1982 to remove virtually all copper from the coin) and still remains in circulation, despite a greatly reduced purchasing power. On the other end of the spectrum, the largest coin in common circulation is valued at 25 cents, a very low value for the largest denomination coin compared to many other countries. Increases in the prices of copper, nickel, and zinc meant that both the US one- and five-cent coins became worth more for their raw metal content than their face (fiat) value. In particular, copper one-cent pieces (those dated prior to 1982 and some 1982-dated coins) contained about two cents' worth of copper.\n\nSome denominations of circulating coins that were formerly minted in the United States are no longer made. These include coins with a face value of a half cent, two cents, three cents, and twenty cents. (The half dollar and dollar coins are still produced, but mostly for vending machines and collectors.) In the past, the US also coined the following denominations for circulation in gold: One dollar, $2.50, three dollars, five dollars, ten dollars, and twenty dollars. In addition, cents were originally slightly larger than the modern quarter and weighed nearly half an ounce, while five-cent coins (known then as \"half dimes\") were smaller than a dime and made of a silver alloy. Dollar coins were also much larger, and weighed approximately an ounce. One-dollar gold coins are no longer produced and rarely used. The US also issues bullion and commemorative coins with the following denominations: 50¢, $1, $5, $10, $25, $50, and $100.\n\nSome convicted criminals from the British Isles who were sentenced to transportation to Australia in the 18th and 19th centuries used coins to leave messages of remembrance to loved ones left behind in Britain. The coins were defaced, smoothed and inscribed, either by stippling or engraving, with sometimes touching words of loss. These coins were called \"convict love tokens\" or \"leaden hearts\". A number of these tokens are in the collection of the National Museum of Australia.\n\nCirculating coins commonly suffered from \"shaving\" or \"clipping\": the public would cut off small amounts of precious metal from their edges to sell it and then pass on the mutilated coins at full value. Unmilled British sterling silver coins were sometimes reduced to almost half their minted weight. This form of debasement in Tudor England was commented on by Sir Thomas Gresham, whose name was later attached to Gresham's law. The monarch would have to periodically recall circulating coins, paying only the bullion value of the silver, and reminting them. This, also known as recoinage, is a long and difficult process that was done only occasionally. Many coins have milled or reeded edges, originally designed to make it easier to detect clipping.\n\nThe side of a coin carrying an image of a monarch or other authority, or a national emblem, is usually called the \"obverse\", or colloquially, \"heads\"; \"see also List of people on coins\". The other side, which may carry the denomination, is usually called the \"reverse\", or colloquially, \"tails\". The year of minting is usually shown on the obverse, although some Chinese coins, most Canadian coins, the pre-2008 British 20p coin, the post-1999 American quarter, and all Japanese coins, are exceptions.\n\nIn cases where a correctly oriented coin is flipped about its horizontal axis to show the other side correctly oriented, the coin is said to have coin orientation. In cases where a coin is flipped about its vertical axis to show the other side correctly oriented, it is said to have medallic orientation. While coins of the United States dollar display coin orientation, those of the Euro and pound sterling have medallic orientation.\n\nBimetallic coins are sometimes used for higher values and for commemorative purposes. In the 1990s, France used a tri-metallic coin. Common circulating bimetallic examples include the €1, €2, British £2 and Canadian $2 and several Peso coins in Mexico.\n\nThe \"exergue\" is the space on a coin beneath the main design, often used to show the coin's date, although it is sometimes left blank or containing a mint mark, privy mark, or some other decorative or informative design feature. Many coins do not have an exergue at all, especially those with few or no legends, such as the Victorian bun penny.\nNot all coins are round. The Australian 50 cent coin, for example, has twelve flat sides. Some coins have wavy edges, e.g. the $2 and 20-cent coins of Hong Kong and the 10 cent coins of Bahamas. Some are square-shaped, such as the 15 cent coin of the Bahamas and the 50 cent coin from Aruba. During the 1970s, Swazi coins were minted in several shapes, including squares, polygons, and wavy edged circles with 8 and 12 waves.\n\nSome other coins, like the British 20 and 50 pence coins and the Canadian Loonie, have an odd number of sides, with the edges rounded off. This way the coin has a constant diameter, recognisable by vending machines whichever direction it is inserted.\n\nA triangular coin with a face value of £5 (produced to commemorate the 2007/2008 Tutankhamun exhibition at The O2 Arena) was commissioned by the Isle of Man: it became legal tender on 6 December 2007. Other triangular coins issued earlier include: Cabinda coin, Bermuda coin, 2 Dollar Cook Islands 1992 triangular coin, Uganda Millennium Coin and Polish Sterling-Silver 10-Zloty Coin.\n\nSome mediaeval coins, called bracteates, were so thin they were struck on only one side.\n\nMany coins over the years have been manufactured with integrated holes such as Chinese \"cash\" coins, Japanese coins, Colonial French coins, etc. This may have been done to permit their being strung on cords, to facilitate storage and being carried, etc.\n\nThe Royal Canadian Mint is now able to produce holographic-effect gold and silver coinage. However, this procedure is not limited to only bullion or commemorative coinage. The 500 yen coin from Japan was subject to a massive amount of counterfeiting. The Japanese government in response produced a circulatory coin with a holographic image.\n\nThe Royal Canadian Mint has also released several coins that are coloured, the first of which was in commemoration of Remembrance Day. The subject was a coloured poppy on the reverse of a 25 cent piece.\n\nAn example of non-metallic composite coins (sometimes incorrectly called plastic coins) was introduced into circulation in Transnistria on 22 August 2014. Most of these coins are also non-circular, with different shapes corresponding to different coin values.\n\nFor a list of many pure metallic elements and their alloys which have been used in actual circulation coins and for trial experiments, see coinage metals.\n\nCoins are popularly used as a sort of two-sided dice; in order to choose between two options with a random possibility, one choice will be labeled \"heads\" and the other \"tails\", and a coin will be flipped or tossed to see whether the heads or tails side comes up on top – see coin flipping. Mathematically, this is known as a Bernoulli trial: a fair coin is defined to have the probability of heads (in the parlance of Bernoulli trials, a \"success\") of exactly 0.5.\n\nCoins can also be spun on a flat surface such as a table. This results in the following phenomenon: as the coin falls over and rolls on its edge, it spins faster and faster (formally, the precession rate of the symmetry axis of the coin, i.e., the axis passing from one face of the coin to the other) before coming to an abrupt stop. This is mathematically modeled as a finite-time singularity – the precession rate is accelerating to infinity, before it suddenly stops, and has been studied using high speed photography and devices such as Euler's Disk. The slowing down is predominantly caused by rolling friction (air resistance is minor), and the singularity (divergence of the precession rate) can be modeled as a power law with exponent approximately −1/3.\n\nIron and copper coins have a characteristic metallic smell that is produced upon contact with oils in the skin. Perspiration is chemically reduced upon contact with these metals, which causes the skin oils to decompose, forming with iron the volatile molecule 1-octen-3-one.\n\n", "id": "7558", "title": "Coin"}
{"url": "https://en.wikipedia.org/wiki?curid=7560", "text": "College of the City of New York\n\nCollege of the City of New York may refer to:\n\n\n", "id": "7560", "title": "College of the City of New York"}
{"url": "https://en.wikipedia.org/wiki?curid=7561", "text": "Classical Kuiper belt object\n\nA classical Kuiper belt object, also called a cubewano ( \"QB1-o\"), is a low-eccentricity Kuiper belt object (KBO) that orbits beyond Neptune and is not controlled by an orbital resonance with Neptune. Cubewanos have orbits with semi-major axes in the 40–50 AU range and, unlike Pluto, do not cross Neptune’s orbit. That is, they have low-eccentricity and sometimes low-inclination orbits like the classical planets.\n\nThe name \"cubewano\" derives from the first trans-Neptunian object (TNO) found after Pluto and Charon, . Similar objects found later were often called \"QB1-o's\", or \"cubewanos\", after this object, though the term \"classical\" is much more frequently used in the scientific literature.\n\nObjects identified as cubewanos include:\n\nHaumea was provisionally listed as a cubewano by the Minor Planet Center in 2006, but turned out to be resonant.\n\nMost cubewanos are found between the 2:3 orbital resonance with Neptune (populated by plutinos) and the 1:2 resonance. 50000 Quaoar, for example, has a near-circular orbit close to the ecliptic. Plutinos, on the other hand, have more eccentric orbits bringing some of them closer to the Sun than Neptune.\n\nThe majority of objects (the so-called 'cold population'), have low inclinations and near-circular orbits. A smaller population (the 'hot population') is characterised by highly inclined, more eccentric orbits.\n\nThe Deep Ecliptic Survey reports the distributions of the two populations; one with the inclination centered at 4.6° (named \"Core\") and another with inclinations extending beyond 30° (\"Halo\").\n\nThe vast majority of KBOs (more than two-thirds) have inclinations of less than 5° and eccentricities of less than 0.1. Their semi-major axes show a preference for the middle of the main belt; arguably, smaller objects close to the limiting resonances have been either captured into resonance or have their orbits modified by Neptune.\n\nThe 'hot' and 'cold' populations are strikingly different: more than 30% of all cubewanos are in low inclination, near-circular orbits. The parameters of the plutinos’ orbits are more evenly distributed, with a local maximum in moderate eccentricities in 0.15–0.2 range and low inclinations 5–10°.\nSee also the comparison with scattered disk objects.\n\nWhen the orbital eccentricities of cubewanos and plutinos are compared, it can be seen that the cubewanos form a clear 'belt' outside Neptune's orbit, whereas the plutinos approach, or even cross Neptune's orbit. When orbital inclinations are compared, 'hot' cubewanos can be easily distinguished by their higher inclinations, as the plutinos typically keep orbits below 20°. (No clear explanation currently exists for the inclinations of 'hot' cubewanos.)\n\nIn addition to the distinct orbital characteristics, the two populations display different physical characteristics.\n\nThe difference in colour between the red cold population and more heterogeneous hot population was observed as early as in 2002.\nRecent studies, based on a larger data set, indicate the cut-off inclination of 12° (instead of 5°) between the cold and hot populations and confirm the distinction between the homogenous red cold population and the bluish hot population.\n\nAnother difference between the low-inclination (cold) and high-inclination (hot) classical objects is the observed number of binary objects. Binaries are quite common on low-inclination orbits and are typically similar-brightness systems. Binaries are less common on high-inclination orbits and their components typically differ in brightness. This correlation, together with the differences in colour, support further the suggestion that the currently observed classical objects belong to at least two different overlapping populations, with different physical properties and orbital history.\n\nThere is no official definition of 'cubewano' or 'classical KBO'. However, the terms are normally used to refer to objects free from significant perturbation from Neptune, thereby excluding KBOs in orbital resonance with Neptune (resonant trans-Neptunian objects). The Minor Planet Center (MPC) and the Deep Ecliptic Survey (DES) do not list cubewanos (classical objects) using the same criteria. Many TNOs classified as cubewanos by the MPC are classified as ScatNear (possibly scattered by Neptune) by the DES. Dwarf planet Makemake is such a borderline classical cubewano/scatnear object. may be an inner cubewano near the plutinos. Furthermore, there is evidence that the Kuiper belt has an 'edge', in that an apparent lack of low-inclination objects beyond 47–49 AU was suspected as early as 1998 and shown with more data in 2001. Consequently, the traditional usage of the terms is based on the orbit’s semi-major axis, and includes objects situated between the 2:3 and 1:2 resonances, that is between 39.4 and 47.8 AU (with exclusion of these resonances and the minor ones in-between).\n\nThese definitions lack precision: in particular the boundary between the classical objects and the scattered disk remains blurred. As of 2010, there are 377 objects with perihelion (q) > 40 AU and aphelion (Q) < 47 AU.\n\nIntroduced by the report from the Deep Ecliptic Survey by J. L. Elliott et al. in 2005 uses formal criteria based on the mean orbital parameters. Put informally, the definition includes the objects that have never crossed the orbit of Neptune. According to this definition, an object qualifies as a classical KBO if:\n\nAn alternative classification, introduced by B. Gladman, B. Marsden and C. van Laerhoven in 2007, uses a 10-million-year orbit integration instead of the Tisserand's parameter. Classical objects are defined as not resonant and not being currently scattered by Neptune.\n\nFormally, this definition includes as \"classical\" all objects with their \"current\" orbits that\nUnlike other schemes, this definition includes the objects with major semi-axis less than 39.4 AU (2:3 resonance)—termed inner classical belt, or more than 48.7 (1:2 resonance) – termed outer classical belt, and reserves the term main classical belt for the orbits between these two resonances.\n\nThe first known collisional family in the classical Kuiper belt—a group of objects thought to be remnants from the breakup of a single body—is the Haumea family. It includes Haumea, its moons, and seven smaller bodies. The objects not only follow similar orbits but also share similar physical characteristics. Unlike many other KBO their surface contains large amounts of ice (HO) and no or very little tholins. The surface composition is inferred from their neutral (as opposed to red) colour and deep absorption at 1.5 and 2. μm in infrared spectrum.\n\nHere is a very generic list of classical Kuiper belt objects. As of 2014, there are about 473 objects with q > 40 (AU) and Q < 48 (AU).\n\n", "id": "7561", "title": "Classical Kuiper belt object"}
{"url": "https://en.wikipedia.org/wiki?curid=7564", "text": "Foreign policy of the United States\n\nThe foreign policy of the United States is the way in which it interacts with foreign nations and sets standards of interaction for its organizations, corporations and individual citizens of the United States.\n\nThe officially stated goals of the foreign policy of the United States, including all the Bureaus and Offices in the United States Department of State, as mentioned in the \"Foreign Policy Agenda\" of the Department of State, are \"to build and sustain a more democratic, secure, and prosperous world for the benefit of the American people and the international community.\" In addition, the United States House Committee on Foreign Affairs states as some of its jurisdictional goals: \"export controls, including nonproliferation of nuclear technology and nuclear hardware; measures to foster commercial interaction with foreign nations and to safeguard American business abroad; international commodity agreements; international education; and protection of American citizens abroad and expatriation.\" U.S. foreign policy and foreign aid have been the subject of much debate, praise and criticism, both domestically and abroad.\n\nSubject to the advice and consent role of the U.S. Senate, the President of the United States negotiates treaties with foreign nations, but treaties enter into force if ratified by two-thirds of the Senate. The President is also Commander in Chief of the United States Armed Forces, and as such has broad authority over the armed forces; however only Congress has authority to declare war, and the civilian and military budget is written by the Congress. The United States Secretary of State is the foreign minister of the United States and is the primary conductor of state-to-state diplomacy. Both the Secretary of State and ambassadors are appointed by the President, with the advice and consent of the Senate. Congress also has power to regulate commerce with foreign nations.\n\nThe main trend regarding the history of U.S. foreign policy since the American Revolution is the shift from non-interventionism before and after World War I, to its growth as a world power and global hegemony during and since World War II and the end of the Cold War in the 20th century. Since the 19th century, U.S. foreign policy also has been characterized by a shift from the realist school to the idealistic or Wilsonian school of international relations.\n\nForeign policy themes were expressed considerably in George Washington's farewell address; these included among other things, observing good faith and justice towards all nations and cultivating peace and harmony with all, excluding both \"inveterate antipathies against particular nations, and passionate attachments for others\", \"steer[ing] clear of permanent alliances with any portion of the foreign world\", and advocating trade with all nations. These policies became the basis of the Federalist Party in the 1790s. But the rival Jeffersonian feared Britain and favored France in the 1790s, declaring the War of 1812 on Britain. After the 1778 alliance with France, the U.S. did not sign another permanent treaty until the North Atlantic Treaty in 1949. Over time, other themes, key goals, attitudes, or stances have been variously expressed by Presidential 'doctrines', named for them. Initially these were uncommon events, but since WWII, these have been made by most presidents.\n\nJeffersonians vigorously opposed a large standing army and any navy until attacks against American shipping by Barbary corsairs spurred the country into developing a naval force projection capability, resulting in the First Barbary War in 1801.\n\nDespite two wars with European Powers—the War of 1812 and the 1898 Spanish–American War—American foreign policy was peaceful and marked by steady expansion of its foreign trade during the 19th century. The 1803 Louisiana Purchase doubled the nation's geographical area; Spain ceded the territory of Florida in 1819; annexation brought in the independent Texas Republic in 1845; a war with Mexico in 1848 added California, Arizona, Utah, Nevada, and New Mexico. The U.S. bought Alaska from the Russian Empire in 1867, and it annexed the independent Republic of Hawaii in 1898. Victory over Spain in 1898 brought the Philippines, and Puerto Rico, as well as oversight of Cuba. The short experiment in imperialism ended by 1908, as the U.S. turned its attention to the Panama Canal and the stabilization of regions to its south, including Mexico.\n\nThe 20th century was marked by two world wars in which the United States, along with allied powers, defeated its enemies and increased its international reputation. President Wilson's Fourteen Points was developed from his idealistic Wilsonianism program of spreading democracy and fighting militarism so as to end any wars. It became the basis of the German Armistice (really a surrender) and the 1919 Paris Peace Conference. The resulting Treaty of Versailles, due to European allies' punitive and territorial designs, showed insufficient conformity with these points and the U.S. signed separate treaties with each of its adversaries; due to Senate objections also, the U.S. never joined the League of Nations, which was established as a result of Wilson's initiative. In the 1920s, the United States followed an independent course, and succeeded in a program of naval disarmament, and refunding the German economy. Operating outside the League it became a dominant player in diplomatic affairs. New York became the financial capital of the world, but the Wall Street Crash of 1929 hurled the Western industrialized world into the Great Depression. American trade policy relied on high tariffs under the Republicans, and reciprocal trade agreements under the Democrats, but in any case exports were at very low levels in the 1930s.\n\nThe United States adopted a non-interventionist foreign policy from 1932 to 1938, but then President Franklin D. Roosevelt moved toward strong support of the Allies in their wars against Germany and Japan. As a result of intense internal debate, the national policy was one of becoming the Arsenal of Democracy, that is financing and equipping the Allied armies without sending American combat soldiers. Roosevelt mentioned four fundamental freedoms, which ought to be enjoyed by people \"everywhere in the world\"; these included the freedom of speech and religion, as well as freedom from want and fear. Roosevelt helped establish terms for a post-war world among potential allies at the Atlantic Conference; specific points were included to correct earlier failures, which became a step toward the United Nations. American policy was to threaten Japan, to force it out of China, and to prevent its attacking the Soviet Union. However, Japan reacted by an attack on Pearl Harbor in December 1941, and the United States was at war with Japan, Germany, and Italy. Instead of the loans given to allies in World War I, the United States provided Lend-Lease grants of $50,000,000,000. Working closely with Winston Churchill of Britain, and Joseph Stalin of the Soviet Union, Roosevelt sent his forces into the Pacific against Japan, then into North Africa against Italy and Germany, and finally into Europe starting with France and Italy in 1944 against the Germans. The American economy roared forward, doubling industrial production, and building vast quantities of airplanes, ships, tanks, munitions, and, finally, the atomic bomb. Much of the American war effort went to strategic bombers, which flattened the cities of Japan and Germany.\n\nAfter the war, the U.S. rose to become the dominant non-colonial economic power with broad influence in much of the world, with the key policies of the Marshall Plan and the Truman Doctrine. Almost immediately however, the world witnessed division into broad two camps during the Cold War; one side was led by the U.S., and the other by the Soviet Union, but this situation also led to the establishment of the Non-Aligned Movement. This period lasted until almost the end of the 20th century, and is thought to be both an ideological and power struggle between the two superpowers. A policy of containment was adopted to limit Soviet expansion, and a series of proxy wars were fought with mixed results. In 1991, the Soviet Union dissolved into separate nations, and the Cold War formally ended as the United States gave separate diplomatic recognition to the Russian Federation and other former Soviet states.\n\nIn domestic politics, foreign policy is not usually a central issue. In 1945-1970 the Democratic Party took a strong anti-Communist line and supported wars in Korea and Vietnam. Then the party split with a strong dovish element (typified by 1972 presidential candidate George McGovern). Many \"hawks\" joined the Neoconservative movement and started supporting the Republicans—especially Reagan—based on foreign policy. Meanwhile, down to 1952 the Republican Party was split between an isolationist wing, based in the Midwest and led by Senator Robert A. Taft, and an internationalist wing based in the East and led by Dwight D. Eisenhower. Eisenhower defeated Taft for the 1952 nomination largely on foreign policy grounds. Since then the Republicans have been characterized by a hawkish and intense American nationalism, and strong opposition to Communism, and strong support for Israel.\n\nIn the 21st century, U.S. influence remains strong but, in relative terms, is declining in terms of economic output compared to rising nations such as China, India, Russia, Brazil, and the newly consolidated European Union. Substantial problems remain, such as climate change, nuclear proliferation, and the specter of nuclear terrorism. Foreign policy analysts Hachigian and Sutphen in their book \"The Next American Century\" suggest all six powers have similar vested interests in stability and terrorism prevention and trade; if they can find common ground, then the next decades may be marked by peaceful growth and prosperity.\n\nIn the United States, there are three types of treaty-related law:\n\nInternational law in most nations considers all three of the above agreements as \"treaties\". In most nations, treaty laws supersede domestic law. So if there is a conflict between a treaty obligation and a domestic law, then the treaty usually prevails.\n\nIn contrast to most other nations, the United States considers the three types of agreements as distinct. Further, the United States incorporates treaty law into the body of U.S. federal law. As a result, Congress can modify or repeal treaties afterwards. It can overrule an agreed-upon treaty obligation even if this is seen as a violation of the treaty under international law. Several U.S. court rulings confirmed this understanding, including the 1900 Supreme Court decision in \"Paquete Habana\", a late 1950s decision in \"Reid v. Covert\", and a lower court ruling in 1986 in \"Garcia-Mir v. Meese\". Further, the Supreme Court has declared itself as having the power to rule a treaty as void by declaring it \"unconstitutional\", although as of 2011, it has never exercised this power.\n\nThe State Department has taken the position that the Vienna Convention on the Law of Treaties represents established law. Generally when the U.S. signs a treaty, it is binding. However, because of the \"Reid v. Covert\" decision, the U.S. adds a reservation to the text of every treaty that says, in effect, that the U.S. intends to abide by the treaty, but if the treaty is found to be in violation of the Constitution, then the U.S. legally can't abide by the treaty since the U.S. signature would be \"ultra vires\".\n\nThe United States has ratified and participates in many other multilateral treaties, including arms control treaties (especially with the Soviet Union), human rights treaties, environmental protocols, and free trade agreements.\n\nThe United States is a founding member of the United Nations and most of its specialized agencies, notably including the World Bank Group and International Monetary Fund. The U.S. has at times has withheld payment of dues due to disagreements with the UN.\n\nThe United States is also member of:\n\nAfter it captured the islands from Japan during World War II, the United States administered the Trust Territory of the Pacific Islands from 1947 to 1986 (1994 for Palau). The Northern Mariana Islands became a U.S. territory (part of the United States), while Federated States of Micronesia, the Marshall Islands, and Palau became independent countries. Each has signed a Compact of Free Association that gives the United States exclusive military access in return for U.S. defense protection and conduct of military foreign affairs (except declaration of war) and a few billion dollars of aid. These agreements also generally allow citizens of these countries to live and work in the United States with their spouses (and vice versa), and provide for largely free trade. The federal government also grants access to services from domestic agencies, including the Federal Emergency Management Agency, National Weather Service, the United States Postal Service, the Federal Aviation Administration, the Federal Communications Commission, and U.S. representation to the International Frequency Registration Board of the International Telecommunication Union.\n\nThe United States notably does not participate in various international agreements adhered to by almost all other industrialized countries, by almost all the countries of the Americas, or by almost all other countries in the world. With a large population and economy, on a practical level this can undermine the effect of certain agreements, or give other countries a precedent to cite for non-participation in various agreements.\n\nIn some cases the arguments against participation include that the United States should maximize its sovereignty and freedom of action, or that ratification would create a basis for lawsuits that would treat American citizens unfairly. In other cases, the debate became involved in domestic political issues, such as gun control, climate change, and the death penalty.\n\nExamples include:\n\nWhile America's relationships with Europe have tended to be in terms of multilateral frameworks, such as NATO, America's relations with Asia have tended to be based on a \"hub and spoke\" model using a series of bilateral relationships where states coordinate with the United States and do not collaborate with each other. On May 30, 2009, at the Shangri-La Dialogue Defense Secretary Robert M. Gates urged the nations of Asia to build on this hub and spoke model as they established and grew multilateral institutions such as ASEAN, APEC and the ad hoc arrangements in the area. However, in 2011 Gates said that the United States must serve as the \"indispensable nation,\" for building multilateral cooperation.\n\nAs of 2014, the U.S. currently produces about 66% of the oil that it consumes. While its imports have exceeded domestic production since the early 1990s, new hydraulic fracturing techniques and discovery of shale oil deposits in Canada and the American Dakotas offer the potential for increased energy independence from oil exporting countries such as OPEC. Former U.S. President George W. Bush identified dependence on imported oil as an urgent \"\"national security concern\".\"\n\nTwo-thirds of the world's proven oil reserves are estimated to be found in the\nPersian Gulf. Despite its distance, the Persian Gulf region was first proclaimed to be of national interest to the United States during World War II. Petroleum is of central importance to modern armies, and the United States—as the world's leading oil producer at that time—supplied most of the oil for the Allied armies. Many U.S. strategists were concerned that the war would dangerously reduce the U.S. oil supply, and so they sought to establish good relations with Saudi Arabia, a kingdom with large oil reserves.\n\nThe Persian Gulf region continued to be regarded as an area of vital importance to the United States during the Cold War. Three Cold War United States Presidential doctrines—the Truman Doctrine, the Eisenhower Doctrine, and the Nixon Doctrine—played roles in the formulation of the Carter Doctrine, which stated that the United States would use military force if necessary to defend its \"national interests\" in the Persian Gulf region. Carter's successor, President Ronald Reagan, extended the policy in October 1981 with what is sometimes called the \"\"Reagan Corollary to the Carter Doctrine\"\", which proclaimed that the United States would intervene to protect Saudi Arabia, whose security was threatened after the outbreak of the Iran–Iraq War. Some analysts have argued that the implementation of the Carter Doctrine and the Reagan Corollary also played a role in the outbreak of the 2003 Iraq War.\n\nAlmost all of Canada's energy exports go to the United States, making it the largest foreign source of U.S. energy imports: Canada is consistently among the top sources for U.S. oil imports, and it is the largest source of U.S. natural gas and electricity imports.\n\nIn 2007 the U.S. was Sub-Saharan Africa's largest single export market accounting for 28% of exports (second in total to the EU at 31%). 81% of U.S. imports from this region were petroleum products.\n\nForeign assistance is a core component of the State Department's international affairs budget, which is $49 billion in all for 2014. Aid is considered an essential instrument of U.S. foreign policy. There are four major categories of non-military foreign assistance: bilateral development aid, economic assistance supporting U.S. political and security goals, humanitarian aid, and multilateral economic contributions (for example, contributions to the World Bank and International Monetary Fund).\n\nIn absolute dollar terms, the United States government is the largest international aid donor ($23 billion in 2014). The U.S. Agency for International Development (USAID) manages the bulk of bilateral economic assistance; the Treasury Department handles most multilateral aid. In addition many private agencies, churches and philanthropies provide aid.\n\nAlthough the United States is the largest donor in absolute dollar terms, it is actually ranked 19 out of 27 countries on the Commitment to Development Index. The CDI ranks the 27 richest donor countries on their policies that affect the developing world. In the aid component the United States is penalized for low net aid volume as a share of the economy, a large share of tied or partially tied aid, and a large share of aid given to less poor and relatively undemocratic governments.\n\nForeign aid is a highly partisan issue in the United States, with liberals, on average, supporting foreign aid much more than conservatives do.\n\nAs of 2016, the United States is actively conducting military operations against the Islamic State of Iraq and the Levant and Al-Qaeda under the Authorization for Use of Military Force Against Terrorists, including in areas of fighting in the Syrian Civil War and Yemeni Civil War. The Guantanamo Bay Naval Base holds what the federal government considers unlawful combatants from these ongoing activities, and has been a controversial issue in foreign relations, domestic politics, and Cuba–United States relations. Other major U.S. military concerns include stability in Afghanistan and Iraq after the recent invasions of those countries, and Russian military activity in Ukraine.\n\nThe United States is a founding member of NATO, an alliance of 28 North American and European nations formed to defend Western Europe against the Soviet Union during the Cold War. Under the NATO charter, the United States is compelled to defend any NATO state that is attacked by a foreign power. The United States itself was the first country to invoke the mutual defense provisions of the alliance, in response to the September 11 attacks.\n\nThe United States also has mutual military defense treaties with:\n\nThe United States has responsibility for the defense of the three Compact of Free Association states: Federated States of Micronesia, the Marshall Islands, and Palau.\n\nIn 1989, the United States also granted five nations the major non-NATO ally status (MNNA), and additions by later presidents have brought the list to 28 nations. Each such state has a unique relationship with the United States, involving various military and economic partnerships and alliances.\n\nand lesser agreements with:\n\nThe U.S. participates in various military-related multi-lateral organizations, including:\n\nThe U.S. also operates hundreds of military bases around the world.\n\nThe United States has undertaken unilateral and multilateral military operations throughout its history (see Timeline of United States military operations). In the post-World War II era, the country has had permanent membership and veto power in the United Nations Security Council, allowing it to undertake any military action without formal Security Council opposition. With vast military expenditures, the United States is known as the sole remaining superpower after the collapse of the Soviet Union. The U.S. contributes a relatively small number of personnel for United Nations peacekeeping operations. It sometimes acts though NATO, as with the NATO intervention in Bosnia and Herzegovina, NATO bombing of Yugoslavia, and ISAF in Afghanistan, but often acts unilaterally or in ad-hoc coalitions as with the 2003 invasion of Iraq.\n\nThe United Nations Charter requires that military operations be either for self-defense or affirmatively approved by the Security Council. Though many of their operations have followed these rules, the United States and NATO have been accused of committing crimes against peace in international law, for example in the 1999 Yugoslavia and 2003 Iraq operations.\n\nThe U.S. provides military aid through many different channels. Counting the items that appear in the budget as 'Foreign Military Financing' and 'Plan Colombia', the U.S. spent approximately $4.5 billion in military aid in 2001, of which $2 billion went to Israel, $1.3 billion went to Egypt, and $1 billion went to Colombia. Since 9/11, Pakistan has received approximately $11.5 billion in direct military aid.\n\nAs of 2004, according to Fox News, the U.S. had more than 700 military bases in 130 different countries.\n\nEstimated U.S. foreign military financing and aid by recipient for 2010:\n\nAccording to a 2016 report by the Congressional Research Service, the U.S. topped the market in global weapon sales for 2015, with $40 billion sold. The largest buyers were Qatar, Egypt, Saudi Arabia, South Korea, Pakistan, Israel, the United Arab Emirates and Iraq.\n\nThe Strategic Defense Initiative (SDI) was a proposal by U.S. President Ronald Reagan on March 23, 1983 to use ground and space-based systems to protect the United States from attack by strategic nuclear ballistic missiles, later dubbed \"\"Star Wars\"\". The initiative focused on strategic defense rather than the prior strategic offense doctrine of mutual assured destruction (MAD). Though it was never fully developed or deployed, the research and technologies of SDI paved the way for some anti-ballistic missile systems of today.\n\nIn February 2007, the U.S. started formal negotiations with Poland and Czech Republic concerning construction of missile shield installations in those countries for a Ground-Based Midcourse Defense system (in April 2007, 57% of Poles opposed the plan). According to press reports the government of the Czech Republic agreed (while 67% Czechs disagree) to host a missile defense radar on its territory while a base of missile interceptors is supposed to be built in Poland.\n\nRussia threatened to place short-range nuclear missiles on the Russia's border with NATO if the United States refuses to abandon plans to deploy 10 interceptor missiles and a radar in Poland and the Czech Republic. In April 2007, Putin warned of a new Cold War if the Americans deployed the shield in Central Europe. Putin also said that Russia is prepared to abandon its obligations under an Intermediate-Range Nuclear Forces Treaty of 1987 with the United States.\n\nOn August 14, 2008, the United States and Poland announced a deal to implement the missile defense system in Polish territory, with a tracking system placed in the Czech Republic. \"The fact that this was signed in a period of very difficult crisis in the relations between Russia and the United States over the situation in Georgia shows that, of course, the missile defense system will be deployed not against Iran but against the strategic potential of Russia\", Dmitry Rogozin, Russia's NATO envoy, said.\n\nKeir A. Lieber and Daryl G. Press, argue in Foreign Affairs that U.S. missile defenses are designed to secure Washington's nuclear primacy and are chiefly directed at potential rivals, such as Russia and China. The authors note that Washington continues to eschew nuclear first strike and contend that deploying missile defenses \"would be valuable primarily in an offensive context, not a defensive one; as an adjunct to a US First Strike capability, not as a stand-alone shield\":\n\nIf the United States launched a nuclear attack against Russia (or China), the targeted country would be left with only a tiny surviving arsenal, if any at all. At that point, even a relatively modest or inefficient missile defense system might well be enough to protect against any retaliatory strikes.\n\nThis analysis is corroborated by the Pentagon's 1992 Defense Planning Guidance (DPG), prepared by then Secretary of Defense Richard Cheney and his deputies. The DPG declares that the United States should use its power to \"prevent the reemergence of a new rival\" either on former Soviet territory or elsewhere. The authors of the Guidance determined that the United States had to \"Field a missile defense system as a shield against accidental missile launches or limited missile strikes by 'international outlaws'\" and also must \"Find ways to integrate the 'new democracies' of the former Soviet bloc into the U.S.-led system\". The National Archive notes that Document 10 of the DPG includes wording about \"disarming capabilities to destroy\" which is followed by several blacked out words. \"This suggests that some of the heavily excised pages in the still-classified DPG drafts may include some discussion of preventive action against threatening nuclear and other WMD programs.\" \n\nFinally, Robert David English, writing in Foreign Affairs, observes that in addition to the deployment U.S. missile defenses, the DPG's second recommendation has also been proceeding on course. \"Washington has pursued policies that have ignored Russian interests (and sometimes international law as well) in order to encircle Moscow with military alliances and trade blocs conducive to U.S. interests.\"\n\nIn United States history, critics have charged that presidents have used democracy to justify military intervention abroad. Critics have also charged that the U.S. helped local militaries overthrow democratically elected governments in Iran, Guatemala, and in other instances. Studies have been devoted to the historical success rate of the U.S. in exporting democracy abroad. Some studies of American intervention have been pessimistic about the overall effectiveness of U.S. efforts to encourage democracy in foreign nations. Until recently, scholars have generally agreed with international relations professor Abraham Lowenthal that U.S. attempts to export democracy have been \"negligible, often counterproductive, and only occasionally positive.\" Other studies find U.S. intervention has had mixed results, and another by Hermann and Kegley has found that military interventions have improved democracy in other countries.\n\nProfessor Paul W. Drake argued that the U.S. first attempted to export democracy in Latin America through intervention from 1912 to 1932. Drake argued that this was contradictory because international law defines intervention as \"dictatorial interference in the affairs of another state for the purpose of altering the condition of things.\" The study suggested that efforts to promote democracy failed because democracy needs to develop out of internal conditions, and can not be forcibly imposed. There was disagreement about what constituted \"democracy\"; Drake suggested American leaders sometimes defined democracy in a narrow sense of a nation having elections; Drake suggested a broader understanding was needed. Further, there was disagreement about what constituted a \"rebellion\"; Drake saw a pattern in which the U.S. State Department disapproved of any type of rebellion, even so-called \"revolutions\", and in some instances rebellions against dictatorships. Historian Walter LaFeber stated, \"The world's leading revolutionary nation (the U.S.) in the eighteenth century became the leading protector of the status quo in the twentieth century.\"\n\nMesquita and Downs evaluated 35 U.S. interventions from 1945 to 2004 and concluded that in only one case, Colombia, did a \"full fledged, stable democracy\" develop within ten years following the intervention. Samia Amin Pei argued that nation building in developed countries usually unravelled four to six years after American intervention ended. Pei, based on study of a database on worldwide democracies called \"Polity\", agreed with Mesquita and Downs that U.S. intervention efforts usually don't produce real democracies, and that most cases result in greater authoritarianism after ten years.\n\nProfessor Joshua Muravchik argued U.S. occupation was critical for Axis power democratization after World War II, but America's failure to encourage democracy in the third world \"prove ... that U.S. military occupation is not a sufficient condition to make a country democratic.\" The success of democracy in former Axis countries such as Italy were seen as a result of high national per-capita income, although U.S. protection was seen as a key to stabilization and important for encouraging the transition to democracy. Steven Krasner agreed that there was a link between wealth and democracy; when per-capita incomes of $6,000 were achieved in a democracy, there was little chance of that country ever reverting to an autocracy, according to an analysis of his research in the \"Los Angeles Times\".\n\nTures examined 228 cases of American intervention from 1973 to 2005, using Freedom House data. A plurality of interventions, 96, caused no change in the country's democracy. In 69 instances, the country became less democratic after the intervention. In the remaining 63 cases, a country became more democratic. However this does not take into account the direction the country would have gone with no U.S. intervention.\n\nHermann and Kegley found that American military interventions designed to protect or promote democracy increased freedom in those countries. Peceny argued that the democracies created after military intervention are still closer to an autocracy than a democracy, quoting Przeworski \"while some democracies are more democratic than others, unless offices are contested, no regime should be considered democratic.\" Therefore, Peceny concludes, it is difficult to know from the Hermann and Kegley study whether U.S. intervention has only produced less repressive autocratic governments or genuine democracies.\n\nPeceny stated that the United States attempted to export democracy in 33 of its 93 20th-century military interventions. Peceny argued that proliberal policies after military intervention had a positive impact on democracy.\n\nA global survey done by Pewglobal indicated that at (as of 2014) least 33 surveyed countries have a positive view (50% or above) of the United States. With the top ten most positive countries being Philippines (92%), Israel (84%), South Korea (82%), Kenya (80%), El Salvador (80%), Italy (78%), Ghana (77%), Vietnam (76%), Bangladesh (76%), and Tanzania (75%). While 10 surveyed countries have the most negative view (Below 50%) of the United States. With the countries being Egypt (10%), Jordan (12%), Pakistan (14%), Turkey (19%), Russia (23%), Palestinian Territories (30%), Greece (34%), Argentina (36%), Lebanon (41%), Tunisia (42%). American's own view of the United States was viewed at 84%.\n\nUnited States foreign policy also includes covert actions to topple foreign governments that have been opposed to the United States. According to J. Dana Stuster, writing in \"Foreign Policy\", there are seven \"confirmed cases\" where the U.S.—acting principally through the Central Intelligence Agency (CIA), but sometimes with the support of other parts of the U.S. government, including the Navy and State Department—covertly assisted in the overthrow of a foreign government: Iran in 1953, Guatemala in 1954, Congo in 1960, the Dominican Republic in 1961, South Vietnam in 1963, Brazil in 1964, and Chile in 1973. Stuster states that this list excludes \"U.S.-supported insurgencies and failed assassination attempts\" such as those directed against Cuba's Fidel Castro, as well as instances where U.S. involvement has been alleged but not proven (such as Syria in 1949).\n\nIn 1953 the CIA, working with the British government, initiated \"Operation Ajax\" against the Prime Minister of Iran Mohammad Mossadegh who had attempted to nationalize Iran's oil, threatening the interests of the Anglo-Persian Oil Company. This had the effect of restoring and strengthening the authoritarian monarchical reign of Shah Mohammad Reza Pahlavi. In 1957, the CIA and Israeli Mossad aided the Iranian government in establishing its intelligence service, SAVAK, later blamed for the torture and execution of the regime's opponents.\n\nA year later, in \"Operation PBSUCCESS\", the CIA assisted the local military in toppling the democratically elected left-wing government of Jacobo Árbenz in Guatemala and installing the military dictator Carlos Castillo Armas. The United Fruit Company lobbied for Árbenz overthrow as his land reforms jeopardized their land holdings in Guatemala, and painted these reforms as a communist threat. The coup triggered a decades long civil war which claimed the lives of an estimated 200,000 people (42,275 individual cases have been documented), mostly through 626 massacres against the Maya population perpetrated by the U.S.-backed Guatemalan military. An independent Historical Clarification Commission found that U.S. corporations and government officials \"exercised pressure to maintain the country's archaic and unjust socio-economic structure,\" and that the CIA backed illegal counterinsurgency operations.\n\nDuring the massacre of alleged communists in 1960s Indonesia, the U.S. government provided assistance to the Indonesian military that helped facilitate the mass killings. This included the U.S. Embassy in Jakarta supplying Indonesian forces with lists of up to 5,000 names of suspected members of the Communist Party of Indonesia (PKI), who were subsequently killed in the massacres. In 2001, the CIA attempted to prevent the publication of the State Department volume \"Foreign Relations of the United States, 1964–1968\", which documents the U.S. role in providing covert assistance to the Indonesian military for the express purpose of the extirpation of the PKI.\n\nIn 1970, the CIA worked with coup-plotters in Chile in the attempted kidnapping of General René Schneider, who was targeted for refusing to participate in a military coup upon the election of Salvador Allende. Schneider was shot in the botched attempt and died three days later. The CIA later paid the group $35,000 for the failed kidnapping.\n\nAccording to one peer-reviewed study, the U.S. intervened in 81 foreign elections between 1946 and 2000, while the Soviet Union or Russia intervened in 36. In 2016 the Huffington Post reported that U.S. meddling in elections was still a widespread and frequent phenomenon, and noted that there’s even a running joke in Latin America about it:\n\n\"Q: Why has there never been a coup in the United States?\n\nA: Because there’s no U.S. embassy in Washington.\"\n\nThe reporters, Ryan Grim and Arthur Delaney, point to a number of cases between 1953 and 2009. The latest cited in the article took place in Haiti and Honduras in 2009, according to memos, emails, and other documentation written by senior U.S. officials and published by WikiLeaks. Both episodes were carried out by Hillary Clinton's State Department. In Haiti, the Obama administration, through the U.S. embassy, and in collaboration with local sweatshop owners, helped suppress a minimum wage increase in the Caribbean nation. The State Department, as well as the Organization of American States (OAS), also actively subverted the 2010 presidential election, thus bringing U.S. favorite Michel Martelly to power. Secretary Clinton reportedly pressured then-Haitian President René Préval with the loss of U.S. and international aid unless the election came out \"the right way.\"\n\nIn Honduras 2009, there was a moderately reformist government of then-President Manuel Zelaya. The tiny elite of super rich were opposed to his policies. He was thus ousted in a military coup, supported by the United States. Mrs. Clinton's State Department worked behind the scenes to stall efforts by neighboring countries through the Organization of American States to restore Manuel Zelaya to office. \"The OAS meeting today turned into a non-event ― just as we hoped,\" wrote one senior State Department official, celebrating their success in slow-walking a restoration. In Hillary Clinton's Memoirs, \"Hard Choices\", she admits that she also organized elections that would, in her words, \"render the question of Zelaya moot\".\n\nSince the 1970s, issues of human rights have become increasingly important in American foreign policy. Congress took the lead in the 1970s. Following the Vietnam War, the feeling that U.S. foreign policy had grown apart from traditional American values was seized upon by Senator Donald M. Fraser (D, MI), leading the Subcommittee on International Organizations and Movements, in criticizing Republican Foreign Policy under the Nixon administration. In the early 1970s, Congress concluded the Vietnam War and passed the War Powers Act. As \"part of a growing assertiveness by Congress about many aspects of Foreign Policy,\" Human Rights concerns became a battleground between the Legislative and the Executive branches in the formulation of foreign policy. David Forsythe points to three specific, early examples of Congress interjecting its own thoughts on foreign policy:\n\nThese measures were repeatedly used by Congress, with varying success, to affect U.S. foreign policy towards the inclusion of Human Rights concerns. Specific examples include El Salvador, Nicaragua, Guatemala and South Africa. The Executive (from Nixon to Reagan) argued that the Cold War required placing regional security in favor of U.S. interests over any behavioral concerns of national allies. Congress argued the opposite, in favor of distancing the United States from oppressive regimes. Nevertheless, according to historian Daniel Goldhagen, during the last two decades of the Cold War, the number of American client states practicing mass murder outnumbered those of the Soviet Union. John Henry Coatsworth, a historian of Latin America and the provost of Columbia University, suggests the number of repression victims in Latin America alone far surpassed that of the USSR and its East European satellites during the period 1960 to 1990. W. John Green contends that the United States was an \"essential enabler\" of \"Latin America's political murder habit, bringing out and allowing to flourish some of the region's worst tendencies.\"\n\nOn December 6, 2011, Obama instructed agencies to consider LGBT rights when issuing financial aid to foreign countries. He also criticized Russia's law discriminating against gays, joining other western leaders in the boycott of the 2014 Winter Olympics in Russia.\n\nIn June 2014, a Chilean court ruled that the United States played a key role in the murders of Charles Horman and Frank Teruggi, both American citizens, shortly after the 1973 Chilean coup d'état.\n\nUnited States foreign policy is influenced by the efforts of the U.S. government to control imports of illicit drugs, including cocaine, heroin, methamphetamine, and cannabis. This is especially true in Latin America, a focus for the U.S. War on Drugs. Those efforts date back to at least 1880, when the U.S. and China completed an agreement that prohibited the shipment of opium between the two countries.\n\nOver a century later, the Foreign Relations Authorization Act requires the President to identify the major drug transit or major illicit drug-producing countries. In September 2005, the following countries were identified: Bahamas, Bolivia, Brazil, Burma, Colombia, Dominican Republic, Ecuador, Guatemala, Haiti, India, Jamaica, Laos, Mexico, Nigeria, Pakistan, Panama, Paraguay, Peru and Venezuela. Two of these, Burma and Venezuela are countries that the U.S. considers to have failed to adhere to their obligations under international counternarcotics agreements during the previous 12 months. Notably absent from the 2005 list were Afghanistan, the People's Republic of China and Vietnam; Canada was also omitted in spite of evidence that criminal groups there are increasingly involved in the production of MDMA destined for the United States and that large-scale cross-border trafficking of Canadian-grown cannabis continues. The U.S. believes that the Netherlands are successfully countering the production and flow of MDMA to the U.S.\n\nCritics from the left cite episodes that undercut leftist governments or showed support for Israel. Others cite human rights abuses and violations of international law. Critics have charged that the U.S. presidents have used democracy to justify military intervention abroad. Critics also point to declassified records which indicate that the CIA under Allen Dulles and the FBI under J. Edgar Hoover aggressively recruited more than 1,000 Nazis, including those responsible for war crimes, to use as spies and informants against the Soviet Union in the Cold War.\n\nThe U.S. has faced criticism for backing right-wing dictators that systematically violated human rights, such as Augusto Pinochet of Chile, Alfredo Stroessner of Paraguay, Efraín Ríos Montt of Guatemala, Jorge Rafael Videla of Argentina, Hissène Habré of Chad Yahya Khan of Pakistan and Suharto of Indonesia. Critics have also accused the United States of supporting Operation \"Condor\", an international campaign of political assassination and state terror organized by right-wing military dictatorships in the Southern Cone of South America.\n\nJournalists and human rights organizations have been critical of US-led airstrikes and targeted killings by drones which have in some cases resulted in collateral damage of civilian populations. In early 2017, the U.S. faced criticism from some scholars, activists and media outlets for dropping 26,171 bombs on seven different countries throughout 2016: Syria, Iraq, Afghanistan, Libya, Yemen, Somalia and Pakistan.\n\nStudies have been devoted to the historical success rate of the U.S. in exporting democracy abroad. Some studies of American intervention have been pessimistic about the overall effectiveness of U.S. efforts to encourage democracy in foreign nations. Some scholars have generally agreed with international relations professor Abraham Lowenthal that U.S. attempts to export democracy have been \"negligible, often counterproductive, and only occasionally positive.\" Other studies find U.S. intervention has had mixed results, and another by Hermann and Kegley has found that military interventions have improved democracy in other countries. A 2013 global poll in 68 countries with 66,000 respondents by Win/Gallup found that the U.S. is perceived as the biggest threat to world peace.\n\nRegarding support for certain anti-Communist dictatorships during the Cold War, a response is that they were seen as a necessary evil, with the alternatives even worse Communist or fundamentalist dictatorships. David Schmitz says this policy did not serve U.S. interests. Friendly tyrants resisted necessary reforms and destroyed the political center (though not in South Korea), while the 'realist' policy of coddling dictators brought a backlash among foreign populations with long memories.\n\nMany democracies have voluntary military ties with United States. See NATO, ANZUS, Treaty of Mutual Cooperation and Security between the United States and Japan, Mutual Defense Treaty with South Korea, and Major non-NATO ally. Those nations with military alliances with the U.S. can spend less on the military since they can count on U.S. protection. This may give a false impression that the U.S. is less peaceful than those nations.\n\nResearch on the democratic peace theory has generally found that democracies, including the United States, have not made war on one another. There have been U.S. support for coups against some democracies, but for example Spencer R. Weart argues that part of the explanation was the perception, correct or not, that these states were turning into Communist dictatorships. Also important was the role of rarely transparent United States government agencies, who sometimes mislead or did not fully implement the decisions of elected civilian leaders.\n\nEmpirical studies (see democide) have found that democracies, including the United States, have killed much fewer civilians than dictatorships. Media may be biased against the U.S. regarding reporting human rights violations. Studies have found that \"The New York Times\" coverage of worldwide human rights violations predominantly focuses on the human rights violations in nations where there is clear U.S. involvement, while having relatively little coverage of the human rights violations in other nations. For example, the bloodiest war in recent time, involving eight nations and killing millions of civilians, was the Second Congo War, which was almost completely ignored by the media.\n\nNiall Ferguson argues that the U.S. is incorrectly blamed for all the human rights violations in nations they have supported. He writes that it is generally agreed that Guatemala was the worst of the US-backed regimes during the Cold War. However, the U.S. cannot credibly be blamed for all the 200,000 deaths during the long Guatemalan Civil War. The U.S. Intelligence Oversight Board writes that military aid was cut for long periods because of such violations, that the U.S. helped stop a coup in 1993, and that efforts were made to improve the conduct of the security services.\nToday the U.S. states that democratic nations best support U.S. national interests. According to the U.S. State Department, \"Democracy is the one national interest that helps to secure all the others. Democratically governed nations are more likely to secure the peace, deter aggression, expand open markets, promote economic development, protect American citizens, combat international terrorism and crime, uphold human and worker rights, avoid humanitarian crises and refugee flows, improve the global environment, and protect human health.\" According to former U.S. President Bill Clinton, \"Ultimately, the best strategy to ensure our security and to build a durable peace is to support the advance of democracy elsewhere. Democracies don't attack each other.\" In one view mentioned by the U.S. State Department, democracy is also good for business. Countries that embrace political reforms are also more likely to pursue economic reforms that improve the productivity of businesses. Accordingly, since the mid-1980s, under President Ronald Reagan, there has been an increase in levels of foreign direct investment going to emerging market democracies relative to countries that have not undertaken political reforms. Leaked cables in 2010 suggested that the \"dark shadow of terrorism still dominates the United States' relations with the world\".\n\nThe United States officially maintains that it supports democracy and human rights through several tools Examples of these tools are as follows:\n\nUnited States foreign policy affirms its alliance with the United Kingdom as its most important bilateral relationship in the world, evidenced by aligned political affairs between the White House and 10 Downing Street, as well as joint military operations carried out between the two nations. While both the United States and the United Kingdom maintain close relationships with many other nations around the world, the level of cooperation in military planning, execution of military operations, nuclear weapons technology, and intelligence sharing with each other has been described as \"unparalleled\" among major powers throughout the 20th and early 21st century.\n\nThe United States and Britain share the world's largest foreign direct investment partnership. American investment in the United Kingdom reached $255.4 billion in 2002, while British direct investment in the United States totaled $283.3 billion.\n\nThe bilateral relationship between Canada and the United States is of notable importance to both countries. About 75–85% of Canadian trade is with the United States, and Canada is the United States' largest trading partner and chief supplier of oil. While there are disputed issues between the two nations, relations are close and the two countries share the \"world's longest undefended border.\" The border was demilitarized after the War of 1812 and, apart from minor raids, has remained peaceful. Military collaboration began during World War II and continued throughout the Cold War on both a bilateral basis and a multilateral relationship through NATO. A high volume of trade and migration between the United States and Canada since the 1850s has generated closer ties, despite continued Canadian fears of being culturally overwhelmed by its neighbor, which is nine times larger in terms of population and eleven times larger in terms of economy. The two economies have increasingly merged since the North American Free Trade Agreement (NAFTA) of 1994, which also includes Mexico.\n\nThe United States shares a unique and often complex relationship with Mexico. A history of armed conflict goes back to the Texas Revolution in the 1830s, the Mexican–American War in the 1840s, and an American invasion in the 1910s. Important treaties include the Gadsden Purchase, and multilaterally with Canada, the North American Free Trade Agreement. The central issue in recent years has been illegal immigration, followed by illegal gun sales (from the U.S.), drug smuggling (to the U.S.) and escalating drug cartel violence just south of the U.S.-Mexico border.\n\nThe United States' relationship with Australia is a very close one, with Secretary of State Hillary Clinton stating that \"America doesn't have a better friend in the world than Australia\". The relationship is formalized by the ANZUS treaty and the Australia–United States Free Trade Agreement.\nThe two countries have a shared history, both have previously been British Colonies and many Americans flocked to the Australian goldfields in the 19th century. At a strategic level, the relationship really came to prominence in World War II, when the two nations worked extremely closely in the Pacific War against Japan, with General Douglas MacArthur undertaking his role as Supreme Allied Commander based in Australia, effectively having Australian troops and resources under his command. During this period, the cultural interaction between Australia and the U.S. were elevated to a higher level as over 1 million U.S. military personnel moved through Australia during the course of the war. The relationship continued to evolve throughout the second half of the 20th Century, and today now involves strong relationships at the executive and mid levels of government and the military, leading Assistant Secretary of State for East Asian and Pacific Affairs, Kurt M. Campbell to declare that \"in the last ten years, [Australia] has ascended to one of the closest one or two allies [of the U.S.] on the planet\".\n\nThe United States has many important allies in the Greater Middle East region. These allies are Turkey, Saudi Arabia, Morocco, Jordan, Afghanistan, Israel, Egypt, Kuwait, Bahrain and Qatar. Israel and Egypt are leading recipients of United States foreign aid, receiving $2.775 billion and 1.75 billion in 2010. Turkey is an ally of the United States through its membership in NATO, while all of the other countries except Saudi Arabia and Qatar are major non-NATO allies.\n\nThe United States toppled the government of Saddam Hussein during the 2003 invasion of Iraq. Turkey is host to approximately 90 B61 nuclear bombs at Incirlik Air Base. Other allies include Qatar, where 3,500 U.S. troops are based, and Bahrain, where the United States Navy maintains NSA Bahrain, home of NAVCENT and the Fifth Fleet.\n\nThe relationship began in the 1850s as the U.S. was a major factor in forcing Japan to resume contacts with the outer world beyond a very restricted role. In the late 19th century the Japanese sent many delegations to Europe, and some to the U.S., to discover and copy the latest technology and thereby modernize Japan very rapidly and allow it to build its own empire. There was some friction over control of Hawaii and the Philippines, but Japan stood aside as the U.S. annexed those lands in 1898. Likewise the U.S. did not object when Japan took control of Korea. The two nations cooperated with the European powers in suppressing the Boxer Rebellion in China in 1900, but the U.S. was increasingly troubled about Japan's denial of the Open Door Policy that would ensure that all nations could do business with China on an equal basis.\n\nPresident Theodore Roosevelt admired Japan's strength as it defeated a major European power, Russia. He brokered an end to the war between Russia and Japan in 1905–6. Anti-Japanese sentiment (especially on the West Coast) soured relations in the 1907–24 era. In the 1930s the U.S. protested vehemently against Japan's seizure of Manchuria (1931), its war against China (1937–45), and its seizure of Indochina (Vietnam) 1940–41. American sympathies were with China and Japan rejected increasingly angry American demands that Japan pull out of China. The two nations fought an all-out war 1941–45; the U.S. won a total victory, with heavy bombing (including two atomic bombs on Hiroshima and Nagasaki) that devastated Japan's 50 largest industrial cities. The American army under Douglas MacArthur occupied and ruled Japan, 1945–51, with the successful goal of sponsoring a peaceful, prosperous and democratic nation.\n\nIn 1951, the U.S. and Japan signed Treaty of San Francisco and Security Treaty Between the United States and Japan, subsequently revised as Treaty of Mutual Cooperation and Security between the United States and Japan in 1960, relations since then have been excellent. The United States considers Japan to be one of its closest allies, and it is both a Major Non-NATO ally and NATO contact country. The United States has several military bases in Japan including Yokosuka, which harbors the U.S. 7th Fleet. The JSDF, or Japanese Self Defense Force, cross train with the U.S. Military, often providing auxiliary security and conducting war games. When the U.S.President Barack Obama met with Japanese Prime Minister Taro Aso in 2009, he said the relationship with Japan as the \"cornerstone of security in East Asia\". After the several years of critical moment during Japan's Democratic Party administration, President Obama and Prime Minister Shinzo Abe reconfirmed the importance of its alliance and currently the U.S. and Japan negotiating to participate Trans-Pacific Strategic Economic Partnership.\n\nSouth Korea–United States relations have been most extensive since 1945, when the United States helped establish capitalism in South Korea and led the UN-sponsored Korean War against North Korea and China (1950–53). Stimulated by heavy American aid, South Korea's rapid economic growth, democratization and modernization greatly reduced its U.S. dependency. Large numbers of U.S. forces remain in Korea. At the 2009 G-20 London summit, U.S. President Barack Obama called South Korea \"one of America's closest allies and greatest friends.\" \n\nAmerican relations with the People's Republic of China are quite strong, yet complex. A great amount of trade between the two countries necessitates positive political relations, although occasional disagreements over tariffs, currency exchange rates and the Political status of Taiwan do occur. Nevertheless, the United States and China have an extremely extensive partnership. The U.S. criticizes China on human rights issues and in recent years, China has criticized the United States on human rights in return.\n\nTaiwan (officially the Republic of China), does not have official diplomatic relations with America and no longer receives diplomatic recognition from the State Department of the United States, but it conducts unofficial diplomatic relations through its de facto embassy, commonly known as the \"American Institute in Taiwan (AIT)\", and is considered to be a strong Asian ally and supporter of the United States.\n\nAssociation of Southeast Asian Nations (ASEAN) is an important partner for United States in both economic and geostrategic aspects. ASEAN's geostrategic importance stems from many factors, including: the strategic location of member countries, the large shares of global trade that pass through regional waters, and the alliances and partnerships which the United States shares with ASEAN member states. In July 2009, the United States signed ASEAN's Treaty of Amity and Cooperation, which establishes guiding principles intended to build confidence among its signatories with the aim of maintaining regional peace and stability. Trade flows are robust and increasing between America and the ASEAN region.\n\nAs the largest ASEAN member, Indonesia has played an active and prominent role in developing the organization. For United States, Indonesia is important for dealing with certain issues; such as terrorism, democracy, and how United States project its relations with Islamic world, since Indonesia has the world's largest Islamic population, and one that honors and respects religious diversity. The U.S. eyes Indonesia as a potential strategic ally in Southeast Asia. During his stately visit to Indonesia, U.S. President Barack Obama has held up Indonesia as an example of how a developing nation can embrace democracy and diversity.\n\nDespite increasingly strained relations under the Mahathir Mohamad government, ties have been thawed under Najib Razak's administration. Economic ties are particularly robust, with the United States being Malaysia's largest trading partner and Malaysia is the tenth-largest trading partner of the U.S. Annual two-way trade amounts to $49 billion. The United States and Malaysia launched negotiations for a bilateral free trade agreement (FTA) in June 2006.\n\nThe United States and Malaysia enjoy strong security cooperation. Malaysia hosts the Southeast Asia Regional Center for Counterterrorism (SEARCCT), where over 2000 officials from various countries have received training. The United States is among the foreign countries that has collaborated with the center in conducting capacity building programmes. The U.S. and Malaysia share a strong military-to-military relationship with numerous exchanges, training, joint exercises, and visits.\n\nBilateral ties have generally been strained but are slowly improving. The United States has placed broad sanctions on Burma because of the military crackdown in 1988 and the military regime's refusal to honour the election results of the 1990 People's Assembly election. Similarly, the European Union has placed embargoes on Burma, including an arms embargo, cessation of trade preferences, and suspension of all aid with the exception of humanitarian aid.\n\nUS and European government sanctions against the military government, alongside boycotts and other types direct pressure on corporations by western supporters of the Burmese democracy movement, have resulted in the withdrawal from Burma of most U.S. and many European companies. However, several Western companies remain due to loopholes in the sanctions. Asian corporations have generally remained willing to continue investing in Myanmar and to initiate new investments, particularly in natural resource extraction.\n\nOngoing reforms have improved relations between Burma and the United States.\n\nThe United States ruled the Philippines from 1898 to 1946. The Spanish government ceded the Philippines to the United States in the 1898 Treaty of Paris that ended the Spanish–American War. The United States finally recognized Philippine independence on July 4, 1946 in the Treaty of Manila. July 4 was observed in the Philippines as \"Independence Day\" until August 4, 1964 when, upon the advice of historians and the urging of nationalists, President Diosdado Macapagal signed into law Republic Act No. 4166 designating June 12 as the country's \"Independence Day\". Since 2003 the U.S. has designated the Philippines as a Major Non-NATO Ally.\n\nThailand and the U.S. are both former Southeast Asia Treaty Organization (SEATO) members, being close partners throughout the Cold War, and are still close allies. Since 2003, the U.S. has designated Thailand as a Major Non-NATO Ally.\n\nUnited States involved in Vietnam War in 1955 to 1975. In 1995, President Bill Clinton announced the formal normalization of diplomatic relations with Vietnam. Today, the U.S. eyes Vietnam as a potential strategic ally in Southeast Asia.\n\nAmerican relations with Eastern Europe are influenced by the legacy of the Cold War. Since the collapse of the Soviet Union, former Communist-bloc states in Europe have gradually transitioned to democracy and capitalism. Many have also joined the European Union and NATO, strengthening economic ties with the broader Western world and gaining the military protection of the United States via the North Atlantic Treaty.\n\nThe UN Security Council divided on the question of Kosovo's declaration of independence. Kosovo declared its independence on February 17, 2008, whilst Serbia objected that Kosovo is part of its territory. Of the five members with veto power in the UN Security Council, the US, UK, and France recognized the declaration of independence, and China has expressed concern, while Russia considers it illegal. \"In its declaration of independence, Kosovo committed itself to the highest standards of democracy, including freedom and tolerance and justice for citizens of all ethnic backgrounds\", President George W. Bush said on February 19, 2008.\n\n\n\n\n\n\n\n", "id": "7564", "title": "Foreign policy of the United States"}
{"url": "https://en.wikipedia.org/wiki?curid=7565", "text": "Christmas in Poland\n\nChristmas in Poland is a major annual celebration, as in most countries of the Christian world. The observance of Christmas developed gradually over the centuries, beginning in ancient times; combining old pagan customs with the religious ones introduced after the Christianization of Poland by the Catholic Church. Later influences include mutual permeating of local traditions and various folk cultures. Christmas trees are decorated and lit in family rooms on the day of Christmas Eve. Other trees are placed in most public areas and outside churches. Christmas is called \"Boże Narodzenie\" in Polish (literally 'God's Birth').\n\nAmong the special tasks carried out in private homes during Advent (a time of waiting for the celebration of the Nativity of Jesus) is the baking of the Christmas piernik (gingerbread), and the making of Christmas decorations. Pierniki (plural of \"piernik\") are made in a variety of shapes, including hearts, animals, and St. Nicholas figures. St. Nicholas does not play a major role on Christmas Day, but is celebrated on his Saint feast day of December 6. He visits good children in secret and leaves presents for them.\n\nTraditionally, the Christmas trees are decorated with glass baubles, garlands and many homemade ornaments including painted eggshells, shiny red apples, walnuts, wrapped chocolate shapes, candles, etc. They are lit on Christmas Eve before Wigilia. At the top of each tree there is a star or a glittering tree topper. In many homes, sparklers are hung on the branches of the trees for wintery ambiance. Sometimes the trees are left standing until February 2, the feast day of St. Mary of the Candle of Lighting.\n\nDuring Advent and all the way until Epiphany, or the baptism of Jesus (day of January 6), the \"gwiazdory\", or the star carriers walk through the villages. Some of them sing carols; others recite verses or put on \"szopki\", or \"herody\" (nativity scenes). The last two customs are inspired by the traditional manger scenes or \"Jaselka\" (crib). One tradition unique to Poland is the sharing of the \"opłatek\", a thin wafer into which a holy picture is pressed. In the old days people carried these wafers from house to house wishing their neighbors a Merry Christmas. Nowadays, opłatek is mostly shared with members of the family and immediate neighbors before the Christmas Eve supper (Wigilia in the Polish language). As each person shares pieces of the wafer with another, they are supposed to forgive each other any hurts that have occurred over the past year and wish them happiness in the coming year.\n\nIn Poland, Christmas Eve is a day first of fasting, then of feasting. The Wigilia feast begins at the appearance of the first star. There is no red meat served but fish, usually carp. The supper, which includes many traditional dishes and desserts, can sometimes last for over two hours. It is followed by the exchange of gifts. The next day, the Christmas Day, is often spent visiting friends. In Polish tradition, people combine religion and family closeness at Christmas. Although gift-giving plays a major role in the rituals, emphasis is placed more on the making of special foods and decorations.\n\nOn the night of Christmas Eve, so important is the appearance of the first star in remembrance of the Star of Bethlehem, that it has been given an affectionate name of \"the little star\" or Gwiazdka (the female counterpart of St. Nicholas). On that evening, children watch the sky anxiously hoping to be the first to cry out, \"The star has come!\" Only after it appears, the family members sit down to a dinner table.\n\nAccording to tradition, bits of hay are spread beneath the tablecloth as a reminder that Christ was born in a manger. Others partake in the practice of placing money under the table cloth for each guest, in order to wish for prosperity in the coming year. Some practice the superstition that an even number of people must be seated around the table. In many homes an empty place setting is symbolically left at the table for the Baby Jesus or, for a lonely wanderer who may be in need of food, or if a deceased relative should come and would like to share in the meal.\n\nThe supper begins with the breaking of the opłatek. Everyone at the table breaks off a piece and eats it as a symbol of their unity with Christ. They then share a piece with each family member. A tradition exists among some families to serve twelve different dishes at Wigilia symbolizing the Twelve Apostles, or perhaps, an odd number of dishes for good luck (usually five, seven, or nine).\n\nA traditional Wigilia supper in Poland includes fried carp and barszcz (beetroot soup) with uszka (ravioli). Carp provides a main component of the Christmas Eve meal across Poland; carp fillet, carp in aspic etc. Universal Polish Christmas foods are pierogi as well as some herring dishes, and for dessert, makowiec or noodles with poppy seed. Often, there is a compote of dry fruits for a drink.\n\nThe remainder of the evening is given to stories and songs around the Christmas tree. In some areas of the country, children are taught that \"The Little Star\" brings the gifts. As presents are unwrapped, carollers may walk from house to house receiving treats along the way.\n\nChristmas Eve ends with Pasterka, the Midnight Mass at the local church. The tradition commemorates the arrival of the shepards to Bethlehem and their paying of respect and bearing witness to the new born Messiah. The custom of Christmas night liturgy was introduced in the Christian churches after the second half of the 5th century. In Poland that custom arrived together with the coming of Christianity. The next day (December 25) begins with the early morning mass followed by daytime masses. According to scripture, the Christmas Day masses are interchangeable allowing for greater flexibility in choosing the religious services by individual parishioners.\n\nChristmas carols are not celebrated in Poland until during-and-after the Christmas Vigil Mass called Pasterka held between 24 and 25 of December. The Christmas season often runs until February 2. The early hymns sung in Catholic church were brought to Poland by the Franciscan Brothers in the Middle Ages. The early Christmas music was Latin in origin. When the Polish words and melodies started to become popular, including many new secular pastorals (pastoralka, or shepherd's songs), they were not written down originally, but rather taught among people by heart. Notably, the song \"Bóg się rodzi\" (\"God Is Born\") with lyrics written by Franciszek Karpiński in 1792 became the Christmas hymn of Poland already in the court of King Stefan Batory. Many of the early Polish carols were collected in 1838 by in a book called \"Pastorałki i Kolędy z Melodiami\" (Pastorals and Carols with Melodies).\n\nPoland produces some of the finest hand-blown glass Christmas ornaments in Europe. Families and collectors value these ornaments for high quality, traditional artwork, and unique decorations.\n\nPolish blown-glass Christmas ornaments are generally manufactured only in winter season. The modern glass workshops and manufacturers tend to be localized in the southern regions of Poland.\n\n\n", "id": "7565", "title": "Christmas in Poland"}
{"url": "https://en.wikipedia.org/wiki?curid=7566", "text": "Carousel (musical)\n\nCarousel is the second musical by the team of Richard Rodgers (music) and Oscar Hammerstein II (book and lyrics). The 1945 work was adapted from Ferenc Molnár's 1909 play \"Liliom\", transplanting its Budapest setting to the Maine coastline. The story revolves around carousel barker Billy Bigelow, whose romance with millworker Julie Jordan comes at the price of both their jobs. He attempts a robbery to provide for Julie and their unborn child; after it goes wrong, he is given a chance to make things right. A secondary plot line deals with millworker Carrie Pipperidge and her romance with ambitious fisherman Enoch Snow. The show includes the well-known songs \"If I Loved You\", \"June Is Bustin' Out All Over\" and \"You'll Never Walk Alone\". Richard Rodgers later wrote that \"Carousel\" was his favorite of all his musicals.\n\nFollowing the spectacular success of the first Rodgers and Hammerstein musical, \"Oklahoma!\" (1943), the pair sought to collaborate on another piece, knowing that any resulting work would be compared with \"Oklahoma!\", most likely unfavorably. They were initially reluctant to seek the rights to \"Liliom\"; Molnár had refused permission for the work to be adapted in the past, and the original ending was considered too depressing for the musical theatre. After acquiring the rights, the team created a work with lengthy sequences of music and made the ending more hopeful.\n\nThe musical required considerable modification during out-of-town tryouts, but once it opened on Broadway on April 19, 1945, it was an immediate hit with both critics and audiences. \"Carousel\" initially ran for 890 performances and duplicated its success in the West End in 1950. Though it has never achieved as much commercial success as \"Oklahoma!\", the piece has been repeatedly revived, and has been recorded several times. A production by Nicholas Hytner enjoyed success in 1992 in London, in 1994 in New York and on tour. In 1999, \"Time\" magazine named \"Carousel\" the best musical of the 20th century.\n\nFerenc Molnár's Hungarian-language drama, \"Liliom\", premiered in Budapest in 1909. The audience was puzzled by the work, and it lasted only thirty-odd performances before being withdrawn, the first shadow on Molnár's successful career as a playwright. \"Liliom\" was not presented again until after World War I. When it reappeared on the Budapest stage, it was a tremendous hit.\nExcept for the ending, the plots of \"Liliom\" and \"Carousel\" are very similar. Andreas Zavocky (nicknamed Liliom, the Hungarian word for \"lily\", a slang term for \"tough guy\"), a carnival barker, falls in love with Julie Zeller, a servant girl, and they begin living together. With both discharged from their jobs, Liliom is discontented and contemplates leaving Julie, but decides not to do so on learning that she is pregnant. A subplot involves Julie's friend Marie, who has fallen in love with Wolf Biefeld, a hotel porter—after the two marry, he becomes the owner of the hotel. Desperate to make money so that he, Julie and their child can escape to America and a better life, Liliom conspires with lowlife Ficsur to commit a robbery, but it goes badly, and Liliom stabs himself. He dies, and his spirit is taken to heaven's police court. As Ficsur suggested while the two waited to commit the crime, would-be robbers like them do not come before God Himself. Liliom is told by the magistrate that he may go back to Earth for one day to attempt to redeem the wrongs he has done to his family, but must first spend sixteen years in a fiery purgatory.\n\nOn his return to Earth, Liliom encounters his daughter, Louise, who like her mother is now a factory worker. Saying that he knew her father, he tries to give her a star he stole from the heavens. When Louise refuses to take it, he strikes her. Not realizing who he is, Julie confronts him, but finds herself unable to be angry with him. Liliom is ushered off to his fate, presumably Hell, and Louise asks her mother if it is possible to feel a hard slap as if it was a kiss. Julie reminiscently tells her daughter that it is very possible for that to happen.\n\nAn English translation of \"Liliom\" was credited to Benjamin \"Barney\" Glazer, though there is a story that the actual translator, uncredited, was Rodgers' first major partner Lorenz Hart. The Theatre Guild presented it in New York City in 1921, with Joseph Schildkraut as Liliom, and the play was a success, running 300 performances. A 1940 revival, with Burgess Meredith and Ingrid Bergman was seen by both Hammerstein and Rodgers. Glazer, in introducing the English translation of \"Liliom\", wrote of the play's appeal:\n\nAnd where in modern dramatic literature can such pearls be matched—Julie incoherently confessing to her dead lover the love she had always been ashamed to tell; Liliom crying out to the distant carousel the glad news that he is to be a father; the two thieves gambling for the spoils of their prospective robbery; Marie and Wolf posing for their portrait while the broken-hearted Julie stands looking after the vanishing Liliom, the thieves' song ringing in her ears; the two policemen grousing about pay and pensions while Liliom lies bleeding to death; Liliom furtively proffering his daughter the star he has stolen for her in heaven. ... The temptation to count the whole scintillating string is difficult to resist.\n\nIn the 1920s and 1930s, Rodgers and Hammerstein both became well known for creating Broadway hits with other partners. Rodgers, with Lorenz Hart, had produced a string of over two dozen musicals, including such popular successes as \"Babes in Arms\" (1937), \"The Boys from Syracuse\" (1938) and \"Pal Joey\" (1940). Some of Rodgers' work with Hart broke new ground in musical theatre: \"On Your Toes\" was the first use of ballet to sustain the plot (in the \"Slaughter on Tenth Avenue\" scene), while \"Pal Joey\" flouted Broadway tradition by presenting a knave as its hero. Hammerstein had written or co-written the words for such hits as \"Rose-Marie\" (1924), \"The Desert Song\" (1926), \"The New Moon\" (1927) and \"Show Boat\" (1927). Though less productive in the 1930s, he wrote material for musicals and films, sharing an Academy Award for his song with Jerome Kern, \"The Last Time I Saw Paris\", which was included in the 1941 film \"Lady Be Good\".\n\nBy the early 1940s, Hart had sunk into alcoholism and emotional turmoil, becoming unreliable and prompting Rodgers to approach Hammerstein to ask if he would consider working with him. Hammerstein was eager to do so, and their first collaboration was \"Oklahoma!\" (1943). Thomas Hischak states, in his \"The Rodgers and Hammerstein Encyclopedia\", that \"Oklahoma!\" is \"the single most influential work in the American musical theatre. In fact, the history of the Broadway musical can accurately be divided into what came before \"Oklahoma!\" and what came after it.\" An innovation for its time in integrating song, character, plot and dance, \"Oklahoma!\" would serve, according to Hischak, as \"the model for Broadway shows for decades\", and proved a huge popular and financial success. Once it was well-launched, what to do as an encore was a daunting challenge for the pair. Movie producer Sam Goldwyn saw \"Oklahoma!\" and advised Rodgers to shoot himself, which according to Rodgers \"was Sam's blunt but funny way of telling me that I'd never create another show as good as \"Oklahoma!\"\" As they considered new projects, Hammerstein wrote, \"We're such fools. No matter what we do, everyone is bound to say, 'This is not another \"Oklahoma!\"' \"\n\n\"Oklahoma!\" had been a struggle to finance and produce. Hammerstein and Rodgers met weekly in 1943 with Theresa Helburn and Lawrence Langner of the Theatre Guild, producers of the blockbuster musical, who together formed what they termed \"the Gloat Club\". At one such luncheon, Helburn and Langner proposed to Rodgers and Hammerstein that they turn Molnár's \"Liliom\" into a musical. Both men refused—they had no feeling for the Budapest setting and thought that the unhappy ending was unsuitable for musical theatre. In addition, given the unstable wartime political situation, they might need to change the setting from Hungary while in rehearsal. At the next luncheon, Helburn and Langner again proposed \"Liliom\", suggesting that they move the setting to Louisiana and make Liliom a Creole. Rodgers and Hammerstein played with the idea over the next few weeks, but decided that Creole dialect, filled with \"zis\" and \"zose\" would sound corny and would make it difficult to write effective lyrics.\n\nA breakthrough came when Rodgers, who owned a house in Connecticut, proposed a New England setting. Hammerstein wrote of this suggestion in 1945,\n\nI began to see an attractive ensemble—sailors, whalers, girls who worked in the mills up the river, clambakes on near-by islands, an amusement park on the seaboard, things people could do in crowds, people who were strong and alive and lusty, people who had always been depicted on the stage as thin-lipped puritans—a libel I was anxious to refute ... as for the two leading characters, Julie with her courage and inner strength and outward simplicity seemed more indigenous to Maine than to Budapest. Liliom is, of course, an international character, indigenous to nowhere.\nRodgers and Hammerstein were also concerned about what they termed \"the tunnel\" of Molnár's second act—a series of gloomy scenes leading up to Liliom's suicide—followed by a dark ending. They also felt it would be difficult to set Liliom's motivation for the robbery to music. Molnár's opposition to having his works adapted was also an issue; he had famously turned down Giacomo Puccini when the great composer wished to transform \"Liliom\" into an opera, stating that he wanted the piece to be remembered as his, not Puccini's. In 1937, Molnár, who had recently emigrated to the United States, had declined another offer from Kurt Weill to adapt the play into a musical.\n\nThe pair continued to work on the preliminary ideas for a \"Liliom\" adaptation while pursuing other projects in late 1943 and early 1944—writing the film musical \"State Fair\" and producing \"I Remember Mama\" on Broadway. Meanwhile, the Theatre Guild took Molnár to see \"Oklahoma!\" Molnár stated that if Rodgers and Hammerstein could adapt \"Liliom\" as beautifully as they had modified \"Green Grow the Lilacs\" into \"Oklahoma!\", he would be pleased to have them do it. The Guild obtained the rights from Molnár in October 1943. The playwright received one percent of the gross and $2,500 for \"personal services\". The duo insisted, as part of the contract, that Molnár permit them to make changes in the plot. At first, the playwright refused, but eventually yielded. Hammerstein later stated that if this point had not been won, \"we could never have made \"Carousel\".\"\n\nIn seeking to establish through song Liliom's motivation for the robbery, Rodgers remembered that he and Hart had a similar problem in \"Pal Joey\". Rodgers and Hart had overcome the problem with a song that Joey sings to himself, \"I'm Talking to My Pal\". This inspired \"Soliloquy\". Both partners later told a story that \"Soliloquy\" was only intended to be a song about Liliom's dreams of a son, but that Rodgers, who had two daughters, insisted that Liliom consider that Julie might have a girl. However, the notes taken at their meeting of December 7, 1943 state: \"Mr. Rodgers suggested a fine musical number for the end of the scene where Liliom discovers he is to be a father, in which he sings first with pride of the growth of a boy, and then suddenly realizes it might be a girl and changes completely.\"\nHammerstein and Rodgers returned to the \"Liliom\" project in mid-1944. Hammerstein was uneasy as he worked, fearing that no matter what they did, Molnár would disapprove of the results. \"Green Grow the Lilacs\" had been a little-known work; \"Liliom\" was a theatrical standard. Molnár's text also contained considerable commentary on the Hungarian politics of 1909 and the rigidity of that society. A dismissed carnival barker who hits his wife, attempts a robbery and commits suicide seemed an unlikely central character for a musical comedy. Hammerstein decided to use the words and story to make the audience sympathize with the lovers. He also built up the secondary couple, who are incidental to the plot in \"Liliom\"; they became Enoch Snow and Carrie Pipperidge. \"This Was a Real Nice Clambake\" was repurposed from a song, \"A Real Nice Hayride\", written for \"Oklahoma!\" but not used.\n\nMolnár's ending was unsuitable, and after a couple of false starts, Hammerstein conceived the graduation scene that ends the musical. According to Frederick Nolan in his book on the team's works: \"From that scene the song \"You'll Never Walk Alone\" sprang almost naturally.\" In spite of Hammerstein's simple lyrics for \"You'll Never Walk Alone\", Rodgers had great difficulty in setting it to music. Rodgers explained his rationale for the changed ending,\n\n\"Liliom\" was a tragedy about a man who cannot learn to live with other people. The way Molnár wrote it, the man ends up hitting his daughter and then having to go back to purgatory, leaving his daughter helpless and hopeless. We couldn't accept that. The way we ended \"Carousel\" it may still be a tragedy but it's a hopeful one because in the final scene it is clear that the child has at last learned how to express herself and communicate with others.\nWhen the pair decided to make \"This Was a Real Nice Clambake\" into an ensemble number, Hammerstein realized he had no idea what a clambake was like, and researched the matter. Based on his initial findings, he wrote the line, \"First came codfish chowder\". However, further research convinced him the proper term was \"codhead chowder\", a term unfamiliar to many playgoers. He decided to keep it as \"codfish\". When the song proceeded to discuss the lobsters consumed at the feast, Hammerstein wrote the line \"We slit 'em down the back/And peppered 'em good\". He was grieved to hear from a friend that lobsters are always slit down the front. The lyricist sent a researcher to a seafood restaurant and heard back that lobsters are always slit down the back. Hammerstein concluded that there is disagreement about which side of a lobster is the back. One error not caught involved the song \"June Is Bustin' Out All Over\", in which sheep are depicted as seeking to mate in late spring—they actually do so in the winter. Whenever this was brought to Hammerstein's attention, he told his informant that 1873 was a special year, in which sheep mated in the spring.\n\nRodgers early decided to dispense with an overture, feeling that the music was hard to hear over the banging of seats as latecomers settled themselves. In his autobiography, Rodgers complained that only the brass section can be heard during an overture because there are never enough strings in a musical's small orchestra. He determined to force the audience to concentrate from the beginning by opening with a pantomime scene accompanied by what became known as \"The Carousel Waltz\". The pantomime paralleled one in the Molnár play, which was also used to introduce the characters and situation to the audience. Author Ethan Mordden described the effectiveness of this opening:\n\nOther characters catch our notice—Mr. Bascombe, the pompous mill owner, Mrs. Mullin, the widow who runs the carousel and, apparently, Billy; a dancing bear; an acrobat. But what draws us in is the intensity with which Julie regards Billy—the way she stands frozen, staring at him, while everyone else at the fair is swaying to the rhythm of Billy's spiel. And as Julie and Billy ride together on the swirling carousel, and the stage picture surges with the excitement of the crowd, and the orchestra storms to a climax, and the curtain falls, we realize that R & H have not only skipped the overture \"and\" the opening number but the exposition as well. They have plunged into the story, right into the middle of it, in the most intense first scene any musical ever had.\nThe casting for \"Carousel\" began when \"Oklahoma!\"'s production team, including Rodgers and Hammerstein, was seeking a replacement for the part of Curly (the male lead in \"Oklahoma!\"). Lawrence Langner had heard, through a relative, of a California singer named John Raitt, who might be suitable for the part. Langner went to hear Raitt, then urged the others to bring Raitt to New York for an audition. Raitt asked to sing \"Largo al factotum\", Figaro's aria from \"The Barber of Seville\", to warm up. The warmup was sufficient to convince the producers that not only had they found a Curly, they had found a Liliom (or Billy Bigelow, as the part was renamed). Theresa Helburn made another California discovery, Jan Clayton, a singer/actress who had made a few minor films for MGM. She was brought east and successfully auditioned for the part of Julie.\n\nThe producers sought to cast unknowns. Though many had played in previous Hammerstein or Rodgers works, only one, Jean Casto (cast as carousel owner Mrs. Mullin, and a veteran of \"Pal Joey\"), had ever played on Broadway before. It proved harder to cast the ensemble than the leads, due to the war—Rodgers told his casting director, John Fearnley, that the sole qualification for a dancing boy was that he be alive. Rodgers and Hammerstein reassembled much of the creative team that had made \"Oklahoma!\" a success, including director Rouben Mamoulian and choreographer Agnes de Mille. Miles White was the costume designer while Jo Mielziner (who had not worked on \"Oklahoma!\") was the scenic and lighting designer. Even though \"Oklahoma!\" orchestrator Russell Bennett had informed Rodgers that he was unavailable to work on \"Carousel\" due to a radio contract, Rodgers insisted he do the work in his spare time. He orchestrated \"The Carousel Waltz\" and \"(When I Marry) Mister Snow\" before finally being replaced by Don Walker. A new member of the creative team was Trude Rittmann, who arranged the dance music. Rittmann initially felt that Rodgers mistrusted her because she was a woman, and found him difficult to work with, but the two worked together on Rodgers' shows until the 1970s.\n\nRehearsals began in January 1945; either Rodgers or Hammerstein was always present. Raitt was presented with the lyrics for \"Soliloquy\" on a five-foot long sheet of paper—the piece ran nearly eight minutes. Staging such a long solo number presented problems, and Raitt later stated that he felt that they were never fully addressed. At some point during rehearsals, Molnár came to see what they had done to his play. There are a number of variations on the story. As Rodgers told it, while watching rehearsals with Hammerstein, the composer spotted Molnár in the rear of the theatre and whispered the news to his partner. Both sweated through an afternoon of rehearsal in which nothing seemed to go right. At the end, the two walked to the back of the theatre, expecting an angry reaction from Molnár. Instead, the playwright said enthusiastically, \"What you have done is so beautiful. And you know what I like best? The ending!\" Hammerstein wrote that Molnár became a regular attendee at rehearsals after that.\n\nLike most of the pair's works, \"Carousel\" contains a lengthy ballet, \"Billy Makes a Journey\", in the second act, as Billy looks down to the Earth from \"Up There\" and observes his daughter. In the original production the ballet was choreographed by de Mille. It began with Billy looking down from heaven at his wife in labor, with the village women gathered for a \"birthing\". The ballet involved every character in the play, some of whom spoke lines of dialogue, and contained a number of subplots. The focus was on Louise, played by Bambi Linn, who at first almost soars in her dance, expressing the innocence of childhood. She is teased and mocked by her schoolmates, and Louise becomes attracted to the rough carnival people, who symbolize Billy's world. A youth from the carnival attempts to seduce Louise, as she discovers her own sexuality, but he decides she is more girl than woman, and he leaves her. After Julie comforts her, Louise goes to a children's party, where she is shunned. The carnival people reappear and form a ring around the children's party, with Louise lost between the two groups. At the end, the performers form a huge carousel with their bodies.\n\nThe play opened for tryouts in New Haven, Connecticut on March 22, 1945. The first act was well-received; the second act was not. Casto recalled that the second act finished about 1:30 a.m. The staff immediately sat down for a two-hour conference. Five scenes, half the ballet, and two songs were cut from the show as the result. John Fearnley commented, \"Now I see why these people have hits. I never witnessed anything so brisk and brave in my life.\" De Mille said of this conference, \"not three minutes had been wasted pleading for something cherished. Nor was there any idle joking. ... We cut and cut and cut and then we went to bed.\" By the time the company left New Haven, de Mille's ballet was down to forty minutes.\n\nA major concern with the second act was the effectiveness of the characters He and She (later called by Rodgers \"Mr. and Mrs. God\"), before whom Billy appeared after his death. Mr. and Mrs. God were depicted as a New England minister and his wife, seen in their parlor. The couple was still part of the show at the Boston opening. Rodgers said to Hammerstein, \"We've got to get God out of that parlor\". When Hammerstein inquired where he should put the deity, Rodgers replied, \"I don't care where you put Him. Put Him on a ladder for all I care, only get Him out of that parlor!\" Hammerstein duly put Mr. God (renamed the Starkeeper) atop a ladder, and Mrs. God was removed from the show. Rodgers biographer Meryle Secrest terms this change a mistake, leading to a more fantastic afterlife, which was later criticized by \"The New Republic\" as \"a Rotarian atmosphere congenial to audiences who seek not reality but escape from reality, not truth but escape from truth\".\n\nHammerstein wrote that Molnár's advice, to combine two scenes into one, was key to pulling together the second act and represented \"a more radical departure from the original than any change we had made\". A reprise of \"If I Loved You\" was added in the second act, which Rodgers felt needed more music. Three weeks of tryouts in Boston followed the brief New Haven run, and the audience there gave the musical a warm reception. An even shorter version of the ballet was presented the final two weeks in Boston, but on the final night there, de Mille expanded it back to forty minutes, and it brought the house down, causing both Rodgers and Hammerstein to embrace her.\n\nTwo young female millworkers in 1873 Maine visit the town's carousel after work. One of them, Julie Jordan, attracts the attention of the barker, Billy Bigelow (\"The Carousel Waltz\"). When Julie lets Billy put his arm around her during the ride, Mrs. Mullin, the widowed owner of the carousel, tells Julie never to return. Julie and her friend, Carrie Pipperidge, argue with Mrs. Mullin. Billy arrives and, seeing that Mrs. Mullin is jealous, mocks her; he is fired from his job. Billy, unconcerned, invites Julie to join him for a drink. As he goes to get his belongings, Carrie presses Julie about her feelings toward him, but Julie is evasive (\"You're a Queer One, Julie Jordan\"). Carrie has a beau too, fisherman Enoch Snow (\"(When I Marry) Mister Snow\"), to whom she is newly engaged. Billy returns for Julie as the departing Carrie warns that staying out late means the loss of Julie's job. Mr. Bascombe, owner of the mill, happens by along with a policeman, and offers to escort Julie to her home, but she refuses and is fired. Left alone, she and Billy talk about what life might be like if they were in love, but neither quite confesses to the growing attraction they feel for each other (\"If I Loved You\").\nOver a month passes, and preparations for the summer clambake are under way (\"June Is Bustin' Out All Over\"). Julie and Billy, now married, live at Julie's cousin Nettie's spa. Julie confides in Carrie that Billy, frustrated over being unemployed, hit her. Carrie has happier news—she is engaged to Enoch, who enters as she discusses him (\"(When I Marry) Mister Snow (reprise))\". Billy arrives with his ne'er-do-well whaler friend, Jigger. The former barker is openly rude to Enoch and Julie, then leaves with Jigger, followed by a distraught Julie. Enoch tells Carrie that he expects to become rich selling herring and to have a large family, larger perhaps than Carrie is comfortable having (\"When the Children Are Asleep\").\n\nJigger and his shipmates, joined by Billy, then sing about life on the sea (\"Blow High, Blow Low\"). The whaler tries to recruit Billy to help with a robbery, but Billy declines, as the victim—Julie's former boss, Mr. Bascombe—might have to be killed. Mrs. Mullin enters and tries to tempt Billy back to the carousel (and to her). He would have to abandon Julie; a married barker cannot evoke the same sexual tension as one who is single. Billy reluctantly mulls it over as Julie arrives and the others leave. She tells him that she is pregnant, and Billy is overwhelmed with happiness, ending all thoughts of returning to the carousel. Once alone, Billy imagines the fun he will have with Bill Jr.—until he realizes that his child might be a girl, and reflects soberly that \"you've got to be a \"father\" to a girl\" (\"Soliloquy\"). Determined to provide financially for his future child, whatever the means, Billy decides to be Jigger's accomplice.\n\nThe whole town leaves for the clambake. Billy, who had earlier refused to go, agrees to join in, to Julie's delight, as he realizes that being seen at the clambake is integral to his and Jigger's alibi (\"Act I Finale\").\n\nEveryone reminisces about the huge meal and much fun (\"This Was a Real Nice Clambake\"). Jigger tries to seduce Carrie; Enoch walks in at the wrong moment, and declares that he is finished with her (\"Geraniums In the Winder\"), as Jigger jeers (\"There's Nothin' So Bad for a Woman\"). The girls try to comfort Carrie, but for Julie all that matters is that \"he's your feller and you love him\" (\"What's the Use of Wond'rin'?\"). Julie sees Billy trying to sneak away with Jigger and, trying to stop him, feels the knife hidden in his shirt. She begs him to give it to her, but he refuses and leaves to commit the robbery.\n\nAs they wait, Jigger and Billy gamble with cards. They stake their shares of the anticipated robbery spoils. Billy loses: his participation is now pointless. Unknown to Billy and Jigger, Mr. Bascombe, the intended victim, has already deposited the mill's money. The robbery fails: Bascombe pulls a gun on Billy while Jigger escapes. Billy stabs himself with his knife; Julie arrives just in time for him to say his last words to her and die. Julie strokes his hair, finally able to tell him that she loved him. Carrie and Enoch, reunited by the crisis, attempt to console Julie; Nettie arrives and gives Julie the resolve to keep going despite her despair (\"You'll Never Walk Alone\").\n\nBilly's defiant spirit (\"The Highest Judge of All\") is taken Up There to see the Starkeeper, a heavenly official. The Starkeeper tells Billy that the good he did in life was not enough to get into heaven, but so long as there is a person alive who remembers him, he can return for a day to try to do good to redeem himself. He informs Billy that fifteen years have passed on Earth since the former barker's suicide, and suggests that Billy can get himself into heaven if he helps his daughter, Louise. He helps Billy look down from heaven to see her (instrumental ballet: \"Billy Makes a Journey\"). Louise has grown up to be lonely and bitter. The local children ostracize her because her father was a thief and a wife-beater. In the dance, a young ruffian, much like her father at that age, flirts with her and abandons her as too young. The dance concludes, and Billy is anxious to return to Earth and help his daughter. He steals a star to take with him, as the Starkeeper pretends not to notice.\n\nOutside Julie's cottage, Carrie describes her visit to New York with the now-wealthy Enoch. Carrie's husband and their many children enter to fetch her—the family must get ready for the high school graduation later that day. Enoch Jr., the oldest son, remains behind to talk with Louise, as Billy and the Heavenly Friend escorting him enter, invisible to the other characters. Louise confides in Enoch Jr. that she plans to run away from home with an acting troupe. He says that he will stop her by marrying her, but that his father will think her an unsuitable match. Louise is outraged: each insults the other's father, and Louise orders Enoch Jr. to go away. Billy, able to make himself visible at will, reveals himself to the sobbing Louise, pretending to be a friend of her father. He offers her a gift—the star he stole from heaven. She refuses it and, frustrated, he slaps her hand. He makes himself invisible, and Louise tells Julie what happened, stating that the slap miraculously felt like a kiss, not a blow—and Julie understands her perfectly. Louise retreats to the house, as Julie notices the star that Billy dropped; she picks it up and seems to feel Billy's presence (\"If I Loved You (Reprise)\").\n\nBilly invisibly attends Louise's graduation, hoping for one last chance to help his daughter and redeem himself. The beloved town physician, Dr. Seldon (who resembles the Starkeeper) advises the graduating class not to rely on their parents' success or be held back by their failure (words directed at Louise). Seldon prompts everyone to sing an old song, \"You'll Never Walk Alone\". Billy, still invisible, whispers to Louise, telling her to believe Seldon's words, and when she tentatively reaches out to another girl, she learns she does not have to be an outcast. Billy goes to Julie, telling her at last that he loved her. As his widow and daughter join in the singing, Billy is taken to his heavenly reward.\n\n° denotes original Broadway cast\n\nAct I\nAct II\n\nThe original Broadway production opened at the Majestic Theatre on April 19, 1945. The dress rehearsal the day before had gone badly, and the pair feared the new work would not be well received. One successful last-minute change was to have de Mille choreograph the pantomime. The movement of the carnival crowd in the pantomime had been entrusted to Mamoulian, and his version was not working. Rodgers had injured his back the previous week, and he watched the opening from a stretcher propped in a box behind the curtain. Sedated with morphine, he could see only part of the stage. As he could not hear the audience's applause and laughter, he assumed the show was a failure. It was not until friends congratulated him later that evening that he realized that the curtain had been met by wild applause. Bambi Linn, who played Louise, was so enthusiastically received by the audience during her ballet that she was forced to break character, when she next appeared, and bow. Rodgers' daughter Mary caught sight of her friend, Stephen Sondheim, both teenagers then, across several rows; both had eyes wet with tears.\n\nThe original production ran for 890 performances, closing on May 24, 1947. The original cast included John Raitt (Billy), Jan Clayton (Julie), Jean Darling (Carrie), Eric Mattson (Enoch Snow), Christine Johnson (Nettie Fowler), Murvyn Vye (Jigger), Bambi Linn (Louise) and Russell Collins (Starkeeper). In December 1945, Clayton left to star in the Broadway revival of \"Show Boat\" and was replaced by Iva Withers; Raitt was replaced by Henry Michel in January 1947; Darling was replaced by Margot Moser.\n\nAfter closing on Broadway, the show went on a national tour for two years. It played for five months in Chicago alone, visited twenty states and two Canadian cities, covered and played to nearly two million people. The touring company had a four-week run at New York City Center in January 1949.<ref name=\"NYT/Calta 1949-01-25\">Calta, Louis. \"'Carousel' opens tonight at City Center\". \"The New York Times\", January 25, 1949, p. 27. Retrieved on December 21, 2010.</ref> Following the City Center run, the show was moved back to the Majestic Theatre in the hopes of filling the theatre until \"South Pacific\" opened in early April. However, ticket sales were mediocre, and the show closed almost a month early.<ref name=\"NYT/Calta 1949-02-28\">Calta, Louis. \"'Carousel' to end run on Saturday\". \"The New York Times\", February 28, 1949, p. 15. Retrieved on December 21, 2010.</ref>\n\nThe musical premiered in the West End, London, at the Theatre Royal, Drury Lane on June 7, 1950. The production was restaged by Jerome Whyte, with a cast that included Stephen Douglass (Billy), Iva Withers (Julie) and Margot Moser (Carrie). \"Carousel\" ran in London for 566 performances, remaining there for over a year and a half.\n\n\"Carousel\" was revived in 1954 and 1957 at City Center, presented by the New York City Center Light Opera Company. Both times, the production featured Barbara Cook, though she played Carrie in 1954 and Julie in 1957 (playing alongside Howard Keel as Billy). The production was then taken to Belgium to be performed at the 1958 Brussels World's Fair, with David Atkinson as Billy, Ruth Kobart as Nettie, and Clayton reprising the role of Julie, which she had originated.\n\nIn August 1965, Rodgers and the Music Theater of Lincoln Center produced \"Carousel\" for 47 performances. John Raitt reprised the role of Billy, with Jerry Orbach as Jigger and Reid Shelton as Enoch Snow. The roles of the Starkeeper and Dr. Seldon were played by Edward Everett Horton in his final stage appearance. The following year, New York City Center Light Opera Company brought \"Carousel\" back to City Center for 22 performances, with Bruce Yarnell as Billy and Constance Towers as Julie.\n\nNicholas Hytner directed a new production of \"Carousel\" in 1992, at London's Royal National Theatre, with choreography by Sir Kenneth MacMillan and designs by Bob Crowley. In this staging, the story begins at the mill, where Julie and Carrie work, with the music slowed down to emphasize the drudgery. After work ends, they move to the shipyards and then to the carnival. As they proceed on a revolving stage, carnival characters appear, and at last the carousel is assembled onstage for the girls to ride. Louise is seduced by the ruffian boy during her Act 2 ballet, set around the ruins of a carousel. Michael Hayden played Billy not as a large, gruff man, but as a frustrated smaller one, a time bomb waiting to explode. Hayden, Joanna Riding (Julie) and Janie Dee (Carrie) all won Olivier Awards for their performances. Patricia Routledge played Nettie. Enoch and Carrie were cast as an interracial couple whose eight children, according to the review in \"The New York Times\", looked like \"a walking United Colors of Benetton ad\". Clive Rowe, as Enoch, was nominated for an Olivier Award. The production's limited run from December 1992 through March 1993 was a sellout. It re-opened at the Shaftesbury Theatre in London in September 1993, presented by Cameron Mackintosh, where it continued until May 1994.\n\nThe Hytner production moved to New York's Vivian Beaumont Theater, where it opened on March 24, 1994 and ran for 322 performances. This won five Tony Awards, including best musical revival, as well as awards for Hytner, MacMillan, Crowley and Audra McDonald (as Carrie). The cast also included Sally Murphy as Julie, Shirley Verrett as Nettie, Fisher Stevens as Jigger and Eddie Korbich as Enoch. One change made from the London to the New York production was to have Billy strike Louise across the face, rather than on the hand. According to Hayden, \"He does the one unpardonable thing, the thing we can't forgive. It's a challenge for the audience to like him after that.\" The Hytner \"Carousel\" was presented in Japan in May 1995. A U.S. national tour with a scaled-down production began in February 1996 in Houston and closed in May 1997 in Providence, Rhode Island. Producers sought to feature young talent on the tour, with Patrick Wilson as Billy and Sarah Uriarte Berry, and later Jennifer Laura Thompson, as Julie.\n\nA revival opened at London's Savoy Theatre on December 2, 2008, after a week of previews, starring Jeremiah James (Billy), Alexandra Silber (Julie) and Lesley Garrett (Nettie). The production received warm to mixed reviews. It closed in June 2009, a month early. Michael Coveney, writing in \"The Independent\", admired Rodgers' music but stated, \"Lindsay Posner's efficient revival doesn't hold a candle to the National Theatre 1992 version\".\n\nThe third Broadway revival is set to open in the spring of 2018, starring Jessie Mueller, Joshua Henry, and Renée Fleming. The production will be directed by Jack O'Brien and choreographed by Justin Peck.\n\nA film version of the musical was made in 1956, starring Gordon MacRae and Shirley Jones. It follows the musical's story fairly closely, although a prologue, set in the Starkeeper's heaven, was added. The film was released only a few months after the release of the film version of \"Oklahoma!\". It garnered some good reviews, and the soundtrack recording was a best seller. As the same stars appeared in both pictures, however, the two films were often compared, generally to the disadvantage of \"Carousel\". Thomas Hischak, in \"The Rodgers and Hammerstein Encyclopedia\", later wondered \"if the smaller number of \"Carousel\" stage revivals is the product of this often-lumbering<nowiki> [film] </nowiki> musical\".\n\nThere was also an abridged (100 minute) 1967 network television version that starred Robert Goulet, with choreography by Edward Villella.\n\nThe New York Philharmonic presented a staged concert version of the musical from February 28, 2013 to March 2, 2013 at Avery Fisher Hall. Kelli O'Hara played Julie, with Nathan Gunn as Billy, Stephanie Blythe as Nettie, Jessie Mueller as Carrie, Jason Danieley as Enoch, Shuler Hensley as Jigger, John Cullum as the Starkeeper, and Kate Burton as Mrs. Mullin. Tiler Peck danced the role of Louise to choreography by Warren Carlyle. The production was directed by John Rando. Charles Isherwood of \"The New York Times\" wrote, \"this is as gorgeously sung a production of this sublime 1945 Broadway musical as you are ever likely to hear.\" It was broadcast as part of the PBS \"Live from Lincoln Center\" series, premiering on April 26, 2013.\n\nRodgers designed \"Carousel\" to be an almost continuous stream of music, especially in Act 1. In later years, Rodgers was asked if he had considered writing an opera. He stated that he had been sorely tempted to, but saw \"Carousel\" in operatic terms. He remembered, \"We came very close to opera in the Majestic Theatre. ... There's much that is operatic in the music.\"\nRodgers uses music in \"Carousel\" in subtle ways to differentiate characters and tell the audience of their emotional state. In \"You're a Queer One, Julie Jordan\", the music for the placid Carrie is characterized by even eighth-note rhythms, whereas the emotionally restless Julie's music is marked by dotted eighths and sixteenths; this rhythm will characterize her throughout the show. When Billy whistles a snatch of the song, he selects Julie's dotted notes rather than Carrie's. Reflecting the close association in the music between Julie and the as-yet unborn Louise, when Billy sings in \"Soliloquy\" of his daughter, who \"gets hungry every night\", he uses Julie's dotted rhythms. Such rhythms also characterize Julie's Act 2 song, \"What's the Use of Wond'rin'\". The stable love between Enoch and Carrie is strengthened by her willingness to let Enoch not only plan his entire life, but hers as well. This is reflected in \"When the Children Are Asleep\", where the two sing in close harmony, but Enoch musically interrupts his intended's turn at the chorus with the words \"Dreams that won't be interrupted\". Rodgers biographer Geoffrey Block, in his book on the Broadway musical, points out that though Billy may strike his wife, he allows her musical themes to become a part of him and never interrupts her music. Block suggests that, as reprehensible as Billy may be for his actions, Enoch requiring Carrie to act as \"the little woman\", and his having nine children with her (more than she had found acceptable in \"When the Children are Asleep\") can be considered to be even more abusive.\n\nThe twelve-minute \"bench scene\", in which Billy and Julie get to know each other and which culminates with \"If I Loved You\", according to Hischak, \"is considered the most completely integrated piece of music-drama in the American musical theatre\". The scene is almost entirely drawn from Molnár and is one extended musical piece; Stephen Sondheim described it as \"probably the single most important moment in the revolution of contemporary musicals\". \"If I Loved You\" has been recorded many times, by such diverse artists as Frank Sinatra, Barbra Streisand, Sammy Davis Jr., Mario Lanza and Chad and Jeremy. The D-flat major theme that dominates the music for the second act ballet seems like a new melody to many audience members. It is, however, a greatly expanded development of a theme heard during \"Soliloquy\" at the line \"I guess he'll call me 'The old man' \".\n\nWhen the pair discussed the song that would become \"Soliloquy\", Rodgers improvised at the piano to give Hammerstein an idea of how he envisioned the song. When Hammerstein presented his collaborator with the lyrics after two weeks of work (Hammerstein always wrote the words first, then Rodgers would write the melodies), Rodgers wrote the music for the eight-minute song in two hours. \"What's the Use of Wond'rin' \", one of Julie's songs, worked well in the show but was never as popular on the radio or for recording, and Hammerstein believed that the lack of popularity was because he had concluded the final line, \"And all the rest is talk\" with a hard consonant, which does not allow the singer a vocal climax.\nIrving Berlin later stated that \"You'll Never Walk Alone\" had the same sort of effect on him as the 23rd Psalm. When singer Mel Tormé told Rodgers that \"You'll Never Walk Alone\" had made him cry, Rodgers nodded impatiently. \"You're supposed to.\" The frequently recorded song has become a universally accepted hymn. The cast recording of \"Carousel\" proved popular in Liverpool, like many Broadway albums, and in 1963, the Brian Epstein-managed band, Gerry and the Pacemakers had a number-one hit with the song. At the time, the top ten hits were played before Liverpool F.C. home matches; even after \"You'll Never Walk Alone\" dropped out of the top ten, fans continued to sing it, and it has become closely associated with the soccer team and the city of Liverpool. A BBC program, \"Soul Music\", ranked it alongside \"Silent Night\" and \"Abide With Me\" in terms of its emotional impact and iconic status.\n\nThe cast album of the 1945 Broadway production was issued on 78s, and the score was significantly cut—as was the 1950 London cast recording. Theatre historian John Kenrick notes of the 1945 recording that a number of songs had to be abridged to fit the 78 format, but that there is a small part of \"Soliloquy\" found on no other recording, as Rodgers cut it from the score immediately after the studio recording was made.\n\nA number of songs were cut for the 1956 film, but two of the deleted numbers had been recorded and were ultimately retained on the soundtrack album. The expanded CD version of the soundtrack, issued in 2001, contains all of the singing recorded for the film, including the cut portions, and nearly all of the dance music. The recording of the 1965 Lincoln Center revival featured Raitt reprising the role of Billy. Studio recordings of \"Carousel\"'s songs were released in 1956 (with Robert Merrill as Billy, Patrice Munsel as Julie, and Florence Henderson as Carrie), 1962 and 1987. The 1987 version featured a mix of opera and musical stars, including Samuel Ramey, Barbara Cook and Sarah Brightman. Kenrick recommends the 1962 studio recording for its outstanding cast, including Alfred Drake, Roberta Peters, Claramae Turner, Lee Venora, and Norman Treigle.\n\nBoth the London (1993) and New York (1994) cast albums of the Hytner production contain portions of dialogue that, according to Hischak, speak to the power of Michael Hayden's portrayal of Billy. Kenrick judges the 1994 recording the best all-around performance of \"Carousel\" on disc, despite uneven singing by Hayden, due to Sally Murphy's Julie and the strong supporting cast (calling Audra McDonald the best Carrie he has heard). The Stratford Festival issued a recording in 2015.\n\nThe musical received almost unanimous rave reviews after its opening in 1945. According to Hischak, reviews were not as exuberant as for \"Oklahoma!\" as the critics were not taken by surprise this time. John Chapman of the \"Daily News\" termed it \"one of the finest musical plays I have ever seen and I shall remember it always\". \"The New York Times\"'s reviewer, Lewis Nichols, stated that \"Richard Rodgers and Oscar Hammerstein 2d, who can do no wrong, have continued doing no wrong in adapting \"Liliom\" into a musical play. Their \"Carousel\" is on the whole delightful.\" Wilella Waldorf of the \"New York Post\", however, complained, \"\"Carousel\" seemed to us a rather long evening. The \"Oklahoma!\" formula is becoming a bit monotonous and so are Miss de Mille's ballets. All right, go ahead and shoot!\" \"Dance Magazine\" gave Linn plaudits for her role as Louise, stating, \"Bambi doesn't come on until twenty minutes before eleven, and for the next forty minutes, she practically holds the audience in her hand\". Howard Barnes in the \"New York Herald Tribune\" also applauded the dancing: \"It has waited for Miss de Mille to come through with peculiarly American dance patterns for a musical show to become as much a dance as a song show.\"\n\nWhen the musical returned to New York in 1949, \"The New York Times\" reviewer Brooks Atkinson described \"Carousel\" as \"a conspicuously superior musical play ... \"Carousel\", which was warmly appreciated when it opened, seems like nothing less than a masterpiece now.\" In 1954, when \"Carousel\" was revived at City Center, Atkinson discussed the musical in his review:\n\n\"Carousel\" has no comment to make on anything of topical importance. The theme is timeless and universal: the devotion of two people who love each other through thick and thin, complicated in this case by the wayward personality of the man, who cannot fulfill the responsibilities he has assumed.  ... Billy is a bum, but \"Carousel\" recognizes the decency of his motives and admires his independence. There are no slick solutions in \"Carousel\".\n\nStephen Sondheim noted the duo's ability to take the innovations of \"Oklahoma!\" and apply them to a serious setting: \"\"Oklahoma!\" is about a picnic, \"Carousel\" is about life and death.\" Critic Eric Bentley, on the other hand, wrote that \"the last scene of \"Carousel\" is an impertinence: I refuse to be lectured to by a musical comedy scriptwriter on the education of children, the nature of the good life, and the contribution of the American small town to the salvation of souls.\"\n\n\"New York Times\" critic Frank Rich said of the 1992 London production: \"What is remarkable about Mr. Hytner's direction, aside from its unorthodox faith in the virtues of simplicity and stillness, is its ability to make a 1992 audience believe in Hammerstein's vision of redemption, which has it that a dead sinner can return to Earth to do godly good.\" The Hytner production in New York was hailed by many critics as a grittier \"Carousel\", which they deemed more appropriate for the 1990s. Clive Barnes of the \"New York Post\" called it a \"defining \"Carousel\"—hard-nosed, imaginative, and exciting.\"\n\nCritic Michael Billington has commented that \"lyrically [\"Carousel\"] comes perilously close to acceptance of the inevitability of domestic violence.\" BroadwayWorld.com stated in 2013 that \"Carousel\" is now \"considered somewhat controversial in terms of its attitudes on domestic violence\" because Julie chooses to stay with Billy despite the abuse; actress Kelli O'Hara noted that the domestic violence that Julie \"chooses to deal with – is a real, existing and very complicated thing. And exploring it is an important part of healing it.\"\n\nRodgers considered \"Carousel\" his favorite of all his musicals and wrote, \"it affects me deeply every time I see it performed\". In 1999, \"Time\" magazine, in its \"Best of the Century\" list, named \"Carousel\" the Best Musical of the 20th century, writing that Rodgers and Hammerstein \"set the standards for the 20th century musical, and this show features their most beautiful score and the most skillful and affecting example of their musical storytelling\". Hammerstein's grandson, Oscar Andrew Hammerstein, in his book about his family, suggested that the wartime situation made \"Carousel\"'s ending especially poignant to its original viewers, \"Every American grieved the loss of a brother, son, father, or friend ... the audience empathized with <nowiki>[Billy's]</nowiki> all-too-human efforts to offer advice, to seek forgiveness, to complete an unfinished life, and to bid a proper good-bye from beyond the grave.\" Author and composer Ethan Mordden agreed with that perspective:\n\nIf \"Oklahoma!\" developed the moral argument for sending American boys overseas, \"Carousel\" offered consolation to those wives and mothers whose boys would only return in spirit. The meaning lay not in the tragedy of the present, but in the hope for a future where no one walks alone.\n\n\n\n\n\"Note: The Tony Awards were not established until 1947, and so \"Carousel\" was not eligible to win any Tonys at its premiere.\n\n\n\n\n\n", "id": "7566", "title": "Carousel (musical)"}
{"url": "https://en.wikipedia.org/wiki?curid=7572", "text": "Christian alternative rock\n\nChristian alternative rock is a form of alternative rock music that is lyrically grounded in a Christian worldview. Some critics have suggested that unlike CCM and older Christian rock, Christian alternative rock generally emphasizes musical style over lyrical content as a defining genre characteristic, though the degree to which the faith appears in the music varies from artist to artist.\n\nChristian alternative music has its roots in the early 1980s, as the earliest efforts at Christian punk and new wave were recorded by artists like Andy McCarroll and Moral Support, Undercover, The 77s, Steve Scott, Adam Again, Quickflight, Daniel Amos, Youth Choir (later renamed The Choir), Lifesavers Underground, Michael Knott, The Altar Boys, Breakfast with Amy, Steve Taylor, 4-4-1, David Edwards and Vector. Early labels, most now-defunct, included Blonde Vinyl, Frontline, Exit, and Refuge.\n\nBy the 1990s, many of these bands and artists had disbanded, were no longer performing, or were being carried by independent labels because their music tended to be more lyrically complex (and often more controversial) than mainstream Christian pop. The modern market is currently supported by labels such as Tooth & Nail, Gotee and Floodgate. These companies are often children of or partially owned by general market labels such as Warner, EMI, and Capitol Records, giving successful artists an opportunity to \"cross over\" into mainstream markets.\n\n\n", "id": "7572", "title": "Christian alternative rock"}
{"url": "https://en.wikipedia.org/wiki?curid=7573", "text": "Clive Barker\n\nClive Barker (born 5 October 1952) is an English writer, film director, and visual artist best known for his work in both fantasy and horror fiction. Barker came to prominence in the mid-1980s with a series of short stories, the \"Books of Blood\", which established him as a leading horror writer. He has since written many novels and other works, and his fiction has been adapted into films, notably the \"Hellraiser\" and \"Candyman\" series. He was the Executive Producer of the film \"Gods and Monsters\".\n\nBarker's paintings and illustrations have been featured in galleries in the United States as well as within his own books. He has created original characters and series for comic books, and some of his more popular horror stories have been adapted to comics.\n\nHis archives have been a source of material for biographies and non-fiction books containing his personal essays, discussions of his fringe theatre work, interviews, and other content.\n\nBarker was born in Liverpool, Merseyside, the son of Joan Ruby (née Revill), a painter and school welfare officer, and Leonard Barker, a personnel director for an industrial relations firm. He was educated at Dovedale Primary School, Quarry Bank High School and the University of Liverpool, where he studied English and Philosophy.\n\nWhen he was three years old, Barker witnessed the French skydiver Léo Valentin plummet to his death during a performance at an air show in Liverpool. Barker would later allude to Valentin in many of his stories.\n\nBarker is an author of contemporary horror/fantasy. He began writing horror early in his career, mostly in the form of short stories (collected in \"Books of Blood\" 1 – 6) and the Faustian novel \"The Damnation Game\" (1985). Later he moved towards modern-day fantasy and urban fantasy with horror elements in \"Weaveworld\" (1987), \"The Great and Secret Show\" (1989), the world-spanning \"Imajica\" (1991), and \"Sacrament\" (1996).\n\nWhen the \"Books of Blood\" were first published in the United States in paperback, Stephen King was quoted on the book covers: \"I have seen the future of horror and his name is Clive Barker.\" As influences on his writing, Barker lists Herman Melville, Edgar Allan Poe, Ray Bradbury, William S. Burroughs, William Blake and Jean Cocteau, among others.\n\nHe is the writer of the best-selling Abarat series, and plans on producing two more novels in the series.\n\nBarker has an interest in film production. He wrote the screenplays for \"Underworld\" and \"Rawhead Rex\" (1986), both directed by George Pavlou. Displeased by how his material was handled, he moved to directing with \"Hellraiser\" (1987), based on his novella \"The Hellbound Heart\". After his film \"Nightbreed\" (1990) flopped, Barker returned to write and direct \"Lord of Illusions\" (1995). The short story \"The Forbidden\", from Barker's \"Books of Blood\", provided the basis for the 1992 film \"Candyman\" and its two sequels. Barker was an executive producer of the film \"Gods and Monsters\" (1998), which received major critical acclaim. He had been working on a series of film adaptations of his \"The Abarat Quintet\" books under Disney's management, but because of creative differences, the project was cancelled.\n\nIn 2005, Barker and horror film producer Jorge Saralegui created the film production company Midnight Picture Show with the intent of producing two horror films per year.\n\nIn October 2006, Barker announced through his website that he will be writing the script to a forthcoming remake of the original \"Hellraiser\" film. He is developing a film based on his \"Tortured Souls\" line of toys from McFarlane Toys.\n\nBarker is a prolific visual artist, often illustrating his own books. His paintings have been seen first on the covers of his official fan club magazine, \"Dread\", published by Fantaco in the early '90s; on the covers of the collections of his plays, \"Incarnations\" (1995) and \"Forms of Heaven\" (1996); and on the second printing of the original British publications of his \"Books of Blood\" series. Barker also provided the artwork for his young adult novel \"The Thief of Always\" and for the \"Abarat\" series. His artwork has been exhibited at Bert Green Fine Art in Los Angeles and Chicago, at the Bess Cutler Gallery in New York and La Luz De Jesus in Los Angeles. Many of his sketches and paintings can be found in the collection \"Clive Barker, Illustrator\", published in 1990 by Arcane/Eclipse Books, and in \"Visions of Heaven and Hell\", published in 2005 by Rizzoli Books.\n\nHe worked on the horror video game \"Clive Barker's Undying\", providing the voice for the character Ambrose. \"Undying\" was developed by DreamWorks Interactive and released in 2001. He worked on \"Clive Barker's Jericho\" for Codemasters, which was released in late 2007.\n\nBarker created Halloween costume designs for Disguise Costumes.\n\nBarker published his Razorline imprint via Marvel Comics in 1993.\n\nBarker horror adaptations and spinoffs in comics include the Marvel/Epic Comics series \"Hellraiser\", \"Nightbreed\", \"Pinhead\", \"The Harrowers\", \"Book of the Damned\", and \"Jihad\"; Eclipse Books' series and graphic novels \"Tapping The Vein\", \"Dread\", \"Son of Celluloid\", \"Revelations\" \"The Life of Death\", \"Rawhead Rex\" and \"The Yattering and Jack\", and Dark Horse Comics' \"Primal\", among others. Barker served as a consultant and wrote issues of the Hellraiser anthology comic book.\n\nIn 2005, IDW published a three-issue adaptation of Barker's children's fantasy novel \"The Thief of Always\", written and painted by Kris Oprisko and Gabriel Hernandez. IDW is publishing a 12 issue adaptation of Barker's novel \"The Great and Secret Show\".\n\nIn December 2007, Chris Ryall and Clive Barker announced an upcoming collaboration of an original comic book series, \"Torakator\", to be published by IDW.\n\nIn October 2009, IDW published \"Seduth\", co-written by Barker. The work was released with three variant covers.\n\nIn 2011, Boom! Studios began publishing an original Hellraiser comic book series.\n\nIn 2013, Boom! Studios announced \"Next Testament\", the first original story by Barker to be published in comic book format.\n\n\n\n\n\nIn 2003, Barker received the Davidson/Valentini Award at the 15th GLAAD Media Awards.\n\nBarker has been critical of organised religion throughout his career, but he has stated that the Bible influences his work and spirituality. In 2003, Barker stated that he was a Christian during an episode of \"Real Time With Bill Maher\" when Ann Coulter implied he was not.\n\nBarker said in a December 2008 online interview (published in March 2009) that he had polyps in his throat which were so severe that a doctor told him he was taking in ten percent of the air he was supposed to have been getting. He has had two surgeries to remove them and believes his resultant voice is an improvement over how it was prior to the surgeries. He said he did not have cancer and has given up cigars.\n\nAs of 2015, he is a member of the board of advisers for the Hollywood Horror Museum.\n\nIn a 20 August 1996 appearance on the radio call-in show \"Loveline\", Barker stated that during his teens he had several relationships with older women, and came to identify himself as homosexual by 18 or 19 years old. Barker has been openly gay since the early 1990s. His relationship with John Gregson lasted from 1975 until 1986. It was during this period, with the support that Gregson provided, that Barker was able to write the \"Books of Blood\" series and \"The Damnation Game\".\n\nHe later spent thirteen years with photographer David Armstrong, described as his husband in the introduction to \"Coldheart Canyon\"; they separated in 2009.\n\nBarker lives in Beverly Hills with his partner, Johnny Ray Raymond Jr.\n\n\n\n", "id": "7573", "title": "Clive Barker"}
{"url": "https://en.wikipedia.org/wiki?curid=7574", "text": "Comic fantasy\n\nComic fantasy is a subgenre of fantasy that is primarily humorous in intent and tone. Usually set in imaginary worlds, comic fantasy often includes puns on and parodies of other works of fantasy. It is sometimes known as low fantasy in contrast to high fantasy, which is primarily serious in intent and tone. The term \"low fantasy\" is used to represent other types of fantasy, however, so while comic fantasies may also correctly be classified as low fantasy, many examples of low fantasy are not comic in nature.\n\nThe subgenre rose in the nineteenth century. Elements of comic fantasy can be found in such nineteenth century works\nas some of Hans Christian Andersen's fairy tales, Charles Dickens' \"Christmas Books\", and Lewis Carroll's Alice books. The first writer to specialize in the subgenre was \"F. Anstey\" in novels such as \"Vice Versa\" (1882), where magic disrupts Victorian society with humorous results. Anstey's work was popular enough to inspire several imitations, including E. Nesbit's light-hearted children's fantasies, \"The Phoenix and the Carpet\" (1904) and \"The Story of the Amulet\" (1906). The United States had several writers of comic fantasy, including James Branch Cabell, whose satirical fantasy \"Jurgen, A Comedy of Justice\" (1919) was the subject of an unsuccessful prosecution for obscenity. \nAnother American writer in a similar vein was Thorne Smith, whose works (such as \"Topper\" and \"The Night Life of the Gods\") were popular and influential, and often adapted for film and television.\nHumorous fantasies narrated in a \"gentleman's club\" setting are common; they include John Kendrick Bangs' \"A Houseboat on the Styx\" (1895), Lord Dunsany's \"Jorkens\" stories, and Maurice Richardson's \n\"The Exploits of Englebrecht\" (1950).\n\nAccording to Lin Carter, T. H. White's works exemplify comic fantasy, L. Sprague de Camp and Fletcher Pratt's Harold Shea stories are early exemplars. The overwhelming bulk of de Camp's fantasy was comic. Pratt and de Camp were among several contributors to \"Unknown Worlds\", a pulp magazine which emphasized fantasy with a comedic element. The work of Fritz Leiber also appeared in \"Unknown Worlds\", including his Fafhrd and the Gray Mouser stories, a jocose take on the sword and sorcery subgenre.\n\nIn more modern times, Terry Pratchett's \"Discworld\" books, Piers Anthony's \"Xanth\" books, Robert Asprin's \"MythAdventures\" of Skeeve and Aahz books, and Tom Holt's books provide good examples, as do many of the works by Christopher Moore. There are also comic-strips/graphic novels in the humorous fantasy genre, including Chuck Whelon's Pewfell series and the webcomics \"8-Bit Theater\" and \"The Order of the Stick\". Other recent authors in the genre include Toby Frost, Stuart Sharp, Nicholas Andrews, and DC Farmer, and the writing team of John P. Logsdon and Christopher P. Young.\n\nThe subgenre has also been represented in television, such as in the television series \"I Dream of Jeannie\", \"Kröd Mändoon\".\n\nExamples on radio are the BBC's \"Hordes of the Things\" and \"ElvenQuest\".\n\nComic fantasy films can either be parodies (\"Monty Python and the Holy Grail\"), comedies with fantastical elements (\"Being John Malkovich\") or animated (\"Shrek\").\n\n", "id": "7574", "title": "Comic fantasy"}
{"url": "https://en.wikipedia.org/wiki?curid=7575", "text": "CLU (programming language)\n\nCLU is a pioneering programming language created at the Massachusetts Institute of Technology (MIT) by Barbara Liskov and her students between 1974 and 1975. While it did not find extensive use, it introduced many features that are used widely now, and is seen as a step in the development of object-oriented programming (OOP). However, it is not object-oriented, instead being considered an object-based language, as it lacked many features of OOP.\n\nKey contributions include abstract data types, call-by-sharing, iterators, multiple return values (a form of parallel assignment), type-safe parameterized types, and type-safe variant types. It is also notable for its use of classes with constructors and methods, but without inheritance.\n\nThe syntax of CLU was based on ALGOL, then the starting point for most new language designs. The key addition was the concept of a \"cluster\", CLU's type extension system and the root of the language's name (CLUster). Clusters correspond generally to the concept of a \"class\" in an OO language, and have similar syntax. For instance, here is the CLU syntax for a cluster that implements complex numbers:\n\nA cluster is a module that encapsulates all of its components except for those explicitly named in the \"is\" clause. These correspond to the public components of a class in recent OO languages. A cluster also defines a type that can be named outside the cluster (in this case, \"complex_number\"), but its representation type (rep) is hidden from external clients.\n\nCluster names are global, and no namespace mechanism was provided to group clusters or allow them to be created \"locally\" inside other clusters.\n\nCLU does not perform implicit type conversions. In a cluster, the explicit type conversions \"up\" and \"down\" change between the abstract type and the representation. There is a universal type \"any\", and a procedure force[] to check that an object is a certain type. Objects may be mutable or immutable, the latter being \"base types\" such as integers, booleans, characters and strings.\n\nAnother key feature of the CLU type system are \"iterators\", which return objects from a collection serially, one after another. Iterators offer an identical application programming interface (API) no matter what data they are being used with. Thus the iterator for a collection of codice_1s can be used interchangeably with that for an array of codice_2s. A distinctive feature of CLU iterators is that they are implemented as coroutines, with each value being provided to the caller via a \"yield\" statement. Iterators like those in CLU are now a common feature of many modern languages, such as C#, Ruby, and Python, though recently they are often referred to as generators.\n\nCLU also includes exception handling, based on various attempts in other languages; exceptions are raised using codice_3 and handled with codice_4. Unlike most other languages with exception handling, exceptions are not implicitly resignaled up the calling chain. Exceptions that are neither caught nor resignaled explicitly are immediately converted into a special failure exception that typically terminates the program.\n\nCLU is often credited as being the first language with type-safe variant types, called \"oneofs\", before the language ML had them.\n\nA final distinctive feature in CLU is parallel assignment (multiple assignment), where more than one variable can appear on the left hand side of an assignment operator. For instance, writing codice_5 would exchange values of codice_6 and codice_7. In the same way, functions could return several values, like codice_8. Parallel assignment (though not multiple return values) predates CLU, appearing in CPL (1963), named \"simultaneous assignment\", but CLU popularized it and is often credited as the direct influence leading to parallel assignment in later languages.\n\nAll objects in a CLU program live in the heap, and memory management is automatic.\n\nCLU supported type parameterized user-defined data abstractions. It was the first language to offer type-safe bounded parameterized types, using structure \"where clauses\" to express constraints on actual type arguments.\n\nCLU has influenced many other languages in many ways. In approximate chronological order, these include:\n\nCLU and Ada were major inspirations for C++ templates.\n\nCLU's exception handling mechanisms influenced later languages like C++ and Java.\n\nC++, Sather, Python, and C# include iterators, which first appeared in CLU.\n\nPerl and Lua took multiple assignment and multiple returns from function calls from CLU.\n\nPython and Ruby borrowed several concepts from CLU, such as call by sharing, the \"yield\" statement, and multiple assignment\n\n", "id": "7575", "title": "CLU (programming language)"}
{"url": "https://en.wikipedia.org/wiki?curid=7577", "text": "History of the Soviet Union (1982–91)\n\nThe story of the Soviet Union from 1982 through 1991 spans the period from Leonid Brezhnev's death and funeral until the dissolution of the Soviet Union. Due to the years of Soviet military buildup at the expense of domestic development, economic growth stagnated . Failed attempts at reform, a standstill economy, and the success of the United States against the Soviet Union's forces in the war in Afghanistan led to a general feeling of discontent, especially in the Baltic republics and Eastern Europe.\n\nGreater political and social freedoms, instituted by the last Soviet leader, Mikhail Gorbachev, created an atmosphere of open criticism of the communist regime. The dramatic drop of the price of oil in 1985 and 1986 profoundly influenced actions of the Soviet leadership.\n\nNikolai Tikhonov, the Chairman of the Council of Ministers, was succeeded by Nikolai Ryzhkov, and Vasili Kuznetsov, the acting Chairman of the Presidium of the Supreme Soviet, was succeeded by Andrei Gromyko, the former Minister of Foreign Affairs.\n\nSeveral republics began resisting central control, and increasing democratization led to a weakening of the central government. The USSR's trade gap progressively emptied the coffers of the union, leading to eventual bankruptcy. The Soviet Union finally collapsed in 1991 when Boris Yeltsin seized power in the aftermath of a failed coup that had attempted to topple reform-minded Gorbachev.\n\nBy 1982, the stagnation of the Soviet economy was obvious, as evidenced by the fact that the Soviet Union had been importing grain from the U.S. throughout the 1970s, but the system was so firmly entrenched that any real change seemed impossible. A huge rate of defense spending consumed large parts of the economy. The transition period that separated the Brezhnev and Gorbachev eras resembled the former much more than the latter, although hints of reform emerged as early as 1983.\n\nBrezhnev died on 10 November 1982. Two days passed between his death and the announcement of the election of Yuri Andropov as the new General Secretary, suggesting to many outsiders that a power struggle had occurred in the Kremlin. Andropov maneuvered his way into power both through his KGB connections and by gaining the support of the military by promising not to cut defense spending. For comparison, some of his rivals such as Konstantin Chernenko were skeptical of a continued high military budget. Aged 69, he was the oldest person ever appointed as General Secretary and 11 years older than Brezhnev when he acquired that post. In June 1983, he assumed the post of chairman of the Presidium of the Supreme Soviet, thus becoming the ceremonial head of state. It had taken Brezhnev 13 years to acquire this post. Andropov began a thorough house-cleaning throughout the party and state bureaucracy, a decision made easy by the fact that the Central Committee had an average age of 69. He replaced more than one-fifth of the Soviet ministers and regional party first secretaries and more than one-third of the department heads within the Central Committee apparatus. As a result, he replaced the aging leadership with younger, more vigorous administrators. But Andropov's ability to reshape the top leadership was constrained by his own age and poor health and the influence of his rival (and longtime ally of Leonid Brezhnev) Konstantin Chernenko, who had previously supervised personnel matters in the Central Committee.\n\nThe transition of power from Brezhnev to Andropov was notably the first one in Soviet history to occur completely peacefully with no one being imprisoned, killed, or forced from office.\n\nAndropov's domestic policy leaned heavily towards restoring discipline and order to Soviet society. He eschewed radical political and economic reforms, promoting instead a small degree of candor in politics and mild economic experiments similar to those that had been associated with the late Premier Alexei Kosygin's initiatives in the mid-1960s. In tandem with such economic experiments, Andropov launched an anti-corruption drive that reached high into the government and party ranks. Unlike Brezhnev, who possessed several mansions and a fleet of luxury cars, he lived quite simply. While visiting Budapest in early 1983, he expressed interest in Hungary's Goulash Communism and that the sheer size of the Soviet economy made strict top-down planning impractical. Changes were needed in a hurry for 1982 had witnessed the country's worst economic performance since World War II, with real GDP growth at almost zero percent.\n\nIn foreign affairs, Andropov continued Brezhnev's policies. US−Soviet relations deteriorated rapidly beginning in March 1983, when US President Ronald Reagan dubbed the Soviet Union an \"evil empire\". The official press agency TASS accused Reagan of \"thinking only in terms of confrontation and bellicose, lunatic anti-communism\". Further deterioration occurred as a result of the 1 Sep 1983 Soviet shootdown of Korean Air Lines Flight 007 near Moneron Island carrying 269 people including a sitting US congressman, Larry McDonald, and over Reagan's stationing of intermediate-range nuclear missiles in Western Europe. In Afghanistan, Angola, Nicaragua and elsewhere, under the Reagan Doctrine, the US began undermining Soviet-supported governments by supplying arms to anti-communist resistance movements in these countries.\n\nPresident Reagan's decision to deploy medium-range Pershing II missiles in Western Europe met with mass protests in countries such as France and West Germany, sometimes numbering 1 million people at a time. A skillful KGB propaganda campaign had succeeded in convincing many Europeans that the US and not the Soviet Union was the real aggressive warmongering country, but much was also genuine terror over the prospect of a war, especially since there was a widespread conviction in Europe that the US, being separated from the Red Army by two oceans as opposed to a short land border, was insensitive to the people of Germany and other countries. Moreover, the memory of World War II was still strong and many Germans could not forget the destruction and mass rapes committed by Soviet troops in the closing days of that conflict. This attitude was helped along by the Reagan Administration's comments that a war between NATO and the Warsaw Pact would not necessarily result in the use of nuclear weapons.\n\nAndropov's health declined rapidly during the tense summer and fall of 1983, and he became the first Soviet leader to miss the anniversary celebrations of the 1917 revolution that November. He died in February 1984 of kidney failure after disappearing from public view for several months. His most significant legacy to the Soviet Union was his discovery and promotion of Mikhail Gorbachev. Beginning in 1978, Gorbachev advanced in two years through the Kremlin hierarchy to full membership in the Politburo. His responsibilities for the appointment of personnel allowed him to make the contacts and distribute the favors necessary for a future bid to become general secretary. At this point, Western experts believed that Andropov was grooming Gorbachev as his successor. However, although Gorbachev acted as a deputy to the general secretary throughout Andropov's illness, Gorbachev's time had not yet arrived when his patron died early in 1984.\n\nAt 71, Konstantin Chernenko was in poor health, suffering from emphysema, and unable to play an active role in policy making when he was chosen, after lengthy discussion, to succeed Andropov. But Chernenko's short time in office did bring some significant policy changes. The personnel changes and investigations into corruption undertaken under Andropov's tutelage came to an end. Chernenko advocated more investment in consumer goods and services and in agriculture. He also called for a reduction in the CPSU's micromanagement of the economy and greater attention to public opinion. However, KGB repression of Soviet dissidents also increased. In February 1983, Soviet representatives withdrew from the World Psychiatric Organization in protest of that group's continued complaints about the use of psychiatry to suppress dissent. This policy was underlined in June when Vladimir Danchev, a broadcaster for Radio Moscow, referred to the Soviet troops in Afghanistan as \"invaders\" while conducting English-language broadcasts. After refusing to retract this statement, he was sent to a mental institution for several months. Valery Senderov, a leader of an unofficial union of professional workers, was sentenced to seven years in a labor camp early in the year for speaking out on discrimination practiced against Jews in education and the professions.\n\nAlthough Chernenko had called for renewed \"détente\" with the West, little progress was made towards closing the rift in East−West relations during his rule. The Soviet Union boycotted the 1984 Summer Olympics in Los Angeles, retaliating for the United States-led boycott of the 1980 Summer Olympics in Moscow. In the late summer of 1984, the Soviet Union also prevented a visit to West Germany by East German leader Erich Honecker. Fighting in Afghanistan also intensified, but in the late autumn of 1984 the United States and the Soviet Union did agree to resume arms control talks in early 1985.\n\nThe war in Afghanistan, often referred to as the Soviet Union's \"Vietnam War\", led to increased public dissatisfaction with the Communist regime. Also, the Chernobyl disaster in 1986 added motive force to Gorbachev's glasnost and perestroika reforms, which eventually spiraled out of control and caused the Soviet system to collapse.\n\nAfter years of stagnation, the \"new thinking\" (Anatoli Cherniaev, 2008: 131) of younger Communist apparatchik began to emerge. Following the death of terminally ill Konstantin Chernenko, the Politburo elected Mikhail Gorbachev to the position of General Secretary of the Communist Party of the Soviet Union (CPSU) in March 1985. At 54, Gorbachev was the youngest person since Joseph Stalin to become General Secretary and the country's first head of state born a Soviet citizen instead of a subject of the tsar. During his official confirmation on March 11, Foreign Minister Andrei Gromyko spoke of how the new Soviet leader had filled in for Chernenko as CC Secretariat, and praised his intelligence and flexible, pragmatic ideas instead of rigid adherence to party ideology. Gorbachev was aided by a lack of serious competition in the Politburo. He immediately began appointing younger men of his generation to important party posts, including Nikolai Ryzhkov, Secretary of Economics, Viktor Cherbrikov, KGB Chief, Foreign Minister Eduard Shevardnadze (replacing the 75-year-old Gromyko), Secretary of Defense Industries Lev Zaikov, and Secretary of Construction Boris Yeltsin. Removed from the Politburo and Secretariat was Grigory Romanov, who had been Gorbachev's most significant rival for the position of General Secretary. Gromyko's removal as Foreign Minister was the most unexpected change given his decades of unflinching, faithful service compared to the unknown, inexperienced Shevardnadze.\n\nMore predictably, the 80-year-old Nikolai Tikhonov, the Chairman of the Council of Ministers, was succeeded by Nikolai Ryzhkov, and Vasili Kuznetsov, the acting Chairman of the Presidium of the Supreme Soviet, was succeeded by Andrei Gromyko, the former Minister of Foreign Affairs.\n\nFurther down the chain, up to 40% of the first secretaries of the \"oblasts\" (provinces) were replaced with younger, better educated, and more competent men. The defense establishment was also given a thorough shakeup with the commanders of all 16 military districts replaced along with all theaters of military operation, as well as the three Soviet fleets. Not since World War II had the Soviet military had such a rapid turnover of officers. Sixty-eight-year-old Marshal Nikolai Ogarkov was fully rehabilitated after having fallen from favor in 1983-84 due to his handling of the KAL 007 shootdown and his ideas about improving Soviet strategic and tactical doctrines were made into an official part of defense policy, although some of his other ambitions such as developing the military into a smaller, tighter force based on advanced technology were not considered feasible for the time being. Many, but not all, of the younger army officers appointed during 1985 were proteges of Ogarkov.\n\nGorbachev got off to an excellent start during his first months in power. He projected an aura of youth and dynamism compared to his aged predecessors and made frequent walks in the streets of the major cities answering questions from ordinary citizens. He became the first leader that spoke with the Soviet people in person. When he made public speeches, he made clear that he was interested in constructive exchanges of ideas instead of merely reciting lengthy platitudes about the excellence of the Soviet system. He also spoke candidly about the slackness and run-down condition of Soviet society in recent years, blaming alcohol abuse, poor workplace discipline, and other factors for these situations. Alcohol was a particular nag of Gorbachev's, especially as he himself did not drink, and he made one of his major policy aims curbing the consumption of it.\n\nIn terms of foreign policy, the most important one, relations with the United States, remained twitchy through 1985. In October, Gorbachev made his first visit to a non-communist country when he traveled to France and was warmly received. The fashion-conscious French were also captivated by his wife Raisa and political pundits widely believed that the comparatively young Soviet leader would have a PR advantage over President Reagan, who was 20 years his senior.\n\nReagan and Gorbachev met for the first time in Geneva in November. The three weeks preceding the summit meeting were marked by an unprecedented Soviet media campaign against the Strategic Defense Initiative (SDI), taking advantage of opposition at home in the US to the program. When it finally took place, the two superpower leaders established a solid rapport that boded well for the future despite Reagan's refusal to compromise on abandonment of SDI. A joint communique by both parties stated that they were in agreement that nuclear war could not be won by either side and must never be allowed to happen. It was also agreed that Reagan and Gorbachev would carry out two more summit meetings in 1986-87.\n\nJimmy Carter had officially ended the policy of détente, by financially aiding the Mujahideen movement in neighboring Afghanistan, which served as a pretext for the Soviet intervention in Afghanistan six months later, with the aims of supporting the Afghan government, controlled by the People's Democratic Party of Afghanistan. Tensions between the superpowers increased during this time, when Carter placed trade embargoes on the Soviet Union and stated that the Soviet invasion of Afghanistan was \"the most serious threat to the peace since the Second World War.\"\n\nEast-West tensions increased during the first term of U.S. President Ronald Reagan (1981–85), reaching levels not seen since the Cuban Missile Crisis as Reagan increased US military spending to 7% of the GDP. To match the USA's military buildup, the Soviet Union increased its own military spending to 27% of its GDP and froze production of civilian goods at 1980 levels, causing a sharp economic decline in the already failing Soviet economy. However, it is not clear where the number 27% of the GDP came from. This thesis is not confirmed by the extensive study on the causes of the dissolution of the Soviet Union by two prominent economists from the World Bank—William Easterly and Stanley Fischer from the Massachusetts Institute of Technology. “… the study concludes that the increased Soviet defense spending provoked by Mr. Reagan's policies was not the straw that broke the back of the Empire. The Afghan war and the Soviet response to Mr. Reagan's Star Wars program caused only a relatively small rise in defense costs. And the defense effort throughout the period from 1960 to 1987 contributed only marginally to economic decline.\"\n\nIf economic premises are taken into account, it is not clear why the Soviet leaders did not adopt the Chinese option—economic liberalization with preservation of political system. Instead Gorbachev chose political liberalization during the years leading to the collapse of the USSR, while not implementing any significant economic reforms.\n\nThe US financed the training for the Mujahideen warlords such as Jalaluddin Haqqani, Gulbudin Hekmatyar and Burhanuddin Rabbani eventually culminated to the fall of the Soviet satellite the Democratic Republic of Afghanistan. While the CIA and MI6 and the People's Liberation Army of China financed the operation along with the Pakistan government against the Soviet Union, eventually the Soviet Union began looking for a withdrawal route and in 1988 the Geneva Accords were signed between Communist-Afghanistan and the Islamic Republic of Pakistan; under the terms Soviet troops were to withdraw. Once the withdrawal was complete the Pakistan ISI continued to support the Mujahideen against the Communist Government and by 1992, the government collapsed. US President Reagan also actively hindered the Soviet Union's ability to sell natural gas to Europe whilst simultaneously actively working to keep gas prices low, which kept the price of Soviet oil low and further starved the Soviet Union of foreign capital. This \"long-term strategic offensive,\" which \"contrasts with the essentially reactive and defensive strategy of \"containment\", accelerated the fall of the Soviet Union by encouraging it to overextend its economic base. The proposition that special operations by the CIA in Saudi Arabia affected the prices of Soviet oil was refuted by Marshall Goldman—one of the leading experts on the economy of the Soviet Union—in his latest book. He pointed out that the Saudis decreased their production of oil in 1985 (it reached a 16-year low), whereas the peak of oil production was reached in 1980. They increased the production of oil in 1986, reduced it in 1987 with a subsequent increase in 1988, but not to the levels of 1980 when production reached its highest level. The real increase happened in 1990, by which time the Cold War was almost over. In his book he asked why, if Saudi Arabia had such an effect on Soviet oil prices, did prices not fall in 1980 when the production of oil by Saudi Arabia reached its highest level—three times as much oil as in the mid-eighties—and why did the Saudis wait till 1990 to increase their production, five years after the CIA's supposed intervention? Why didn't the Soviet Union collapse in 1980 then?\n\nHowever, this theory ignores the fact that the Soviet Union had already suffered several important setbacks during “reactive and defensive strategy” of “containment”. In 1972, Nixon normalized US relations with China, thus creating pressure on the Soviet Union. In 1979, Egyptian president Anwar Sadat severed military and economic relations with the USSR after signing the Camp David Accords (by that time the USSR provided a lot of assistance to Egypt and supported it in all its military operations against Israel).\n\nBy the time Gorbachev ushered in the process that would lead to the dismantling of the Soviet administrative planned economy through his programs of \"glasnost\" (political openness), \"uskoreniye\" (speed-up of economic development) and \"perestroika\" (political and economic restructuring) announced in 1986, the Soviet economy suffered from both hidden inflation and pervasive supply shortages aggravated by an increasingly open black market that undermined the official economy. Additionally, the costs of superpower status—the military, space program, subsidies to client states—were out of proportion to the Soviet economy. The new wave of industrialization based upon information technology had left the Soviet Union desperate for Western technology and credits in order to counter its increasing backwardness.\n\nThe Law on Cooperatives enacted in May 1989 was perhaps the most radical of the economic reforms during the early part of the Gorbachev era. For the first time since Vladimir Lenin's New Economic Policy, the law permitted private ownership of businesses in the services, manufacturing, and foreign-trade sectors. Under this provision, cooperative restaurants, shops, and manufacturers became part of the Soviet scene.\n\n\"Glasnost\" resulted in greater freedom of speech and the press becoming far less controlled. Thousands of political prisoners and many dissidents were also released. Soviet social science became free to explore and publish on many subjects that had previously been off limits, including conducting public opinion polls. The All−Union Center for Public Opinion Research (VCIOM)—the most prominent of several polling organizations that were started then— was opened. State archives became more accessible, and some social statistics that had been kept secret became open for research and publication on sensitive subjects such as income disparities, crime, suicide, abortion, and infant mortality. The first center for gender studies was opened within a newly formed Institute for the Socio−Economic Study of Human Population.\n\nIn January 1987, Gorbachev called for democratization: the infusion of democratic elements such as multi-candidate elections into the Soviet political process. A 1987 conference convened by Soviet economist and Gorbachev adviser Leonid Abalkin, concluded: \"Deep transformations in the management of the economy cannot be realized without corresponding changes in the political system.\"\n\nIn June 1988, at the CPSU's Nineteenth Party Conference, Gorbachev launched radical reforms meant to reduce party control of the government apparatus. On 1 December 1988, the Supreme Soviet amended the Soviet constitution to allow for the establishment of a Congress of People's Deputies as the Soviet Union's new supreme legislative body.\n\nElections to the new Congress of People's Deputies were held throughout the USSR in March and April 1989. Gorbachev, as General Secretary of the Communist Party, could be forced to resign at any moment if the communist elite became dissatisfied with him. To proceed with reforms opposed by the majority of the communist party, Gorbachev aimed to consolidate power in a new position, President of the Soviet Union, which was independent from the CPSU and the soviets (councils) and whose holder could be impeached only in case of direct violation of the law. On 15 March 1990, Gorbachev was elected as the first executive president. At the same time, Article 6 of the constitution was changed to deprive the CPSU of a monopoly on political power.\n\nGorbachev's efforts to streamline the Communist system offered promise, but ultimately proved uncontrollable and resulted in a cascade of events that eventually concluded with the dissolution of the Soviet Union. Initially intended as tools to bolster the Soviet economy, the policies of \"perestroika\" and \"glasnost\" soon led to unintended consequences.\n\nRelaxation under \"glasnost\" resulted in the Communist Party losing its absolute grip on the media. Before long, and much to the embarrassment of the authorities, the media began to expose severe social and economic problems the Soviet government had long denied and actively concealed. Problems receiving increased attention included poor housing, alcoholism, drug abuse, pollution, outdated Stalin-era factories, and petty to large-scale corruption, all of which the official media had ignored. Media reports also exposed crimes committed by Joseph Stalin and the Soviet regime, such as the gulags, his treaty with Adolf Hitler, and the Great Purges, which had been ignored by the official media. Moreover, the ongoing war in Afghanistan, and the mishandling of the 1986 Chernobyl disaster, which Gorbachev tried to cover up, further damaged the credibility of the Soviet government at a time when dissatisfaction was increasing.\n\nIn all, the positive view of Soviet life long presented to the public by the official media was rapidly fading, and the negative aspects of life in the Soviet Union were brought into the spotlight. This undermined the faith of the public in the Soviet system and eroded the Communist Party's social power base, threatening the identity and integrity of the Soviet Union itself.\n\nFraying amongst the members of the Warsaw Pact countries and instability of its western allies, first indicated by Lech Wałęsa's 1980 rise to leadership of the trade union Solidarity, accelerated, leaving the Soviet Union unable to depend upon its Eastern European satellite states for protection as a buffer zone. By 1989, Moscow had repudiated the Brezhnev Doctrine in favor of non-intervention in the internal affairs of its Warsaw Pact allies. Gradually, each of the Warsaw Pact countries saw their communist governments fall to popular elections and, in the case of Romania, a violent uprising. By 1990, the governments of Bulgaria, Czechoslovakia, East Germany, Hungary, Poland and Romania, all of which had been imposed after World War II, were brought down as revolutions swept Eastern Europe.\n\nThe Soviet Union also began experiencing upheaval as the political consequences of \"glasnost\" reverberated throughout the country. Despite efforts at containment, the upheaval in Eastern Europe inevitably spread to nationalities within the USSR. In elections to the regional assemblies of the Soviet Union's constituent republics, nationalists as well as radical reformers swept the board. As Gorbachev had weakened the system of internal political repression, the ability of the USSR's central Moscow government to impose its will on the USSR's constituent republics had been largely undermined. Massive peaceful protests in the Baltic republics such as the Baltic Way and the Singing Revolution drew international attention and bolstered independence movements in various other regions.\n\nThe rise of nationalism under \"freedom of speech\" soon re-awakened simmering ethnic tensions in various Soviet republics, further discrediting the ideal of a unified Soviet people. One instance occurred in February 1988, when the government in Nagorno-Karabakh, a predominantly ethnic Armenian region in the Azerbaijan SSR, passed a resolution calling for unification with the Armenian SSR. Violence against local Azerbaijanis was reported on Soviet television, provoking massacres of Armenians in the Azerbaijani city of Sumgait.\n\nEmboldened by the liberalized atmosphere of \"glasnost\", public dissatisfaction with economic conditions was much more overt than ever before in the Soviet period. Although \"perestroika\" was considered bold in the context of Soviet history, Gorbachev's attempts at economic reform were not radical enough to restart the country's chronically sluggish economy in the late 1980s. The reforms made some inroads in decentralization, but Gorbachev and his team left intact most of the fundamental elements of the Stalinist system, including price controls, inconvertibility of the ruble, exclusion of private property ownership, and the government monopoly over most means of production.\n\nThe value of all consumer goods manufactured in 1990 in retail prices was about 459 billion rubles ($2.1 trillion). Nevertheless, the Soviet government had lost control over economic conditions. Government spending increased sharply as an increasing number of unprofitable enterprises required state support and consumer price subsidies to continue. Tax revenues declined as republic and local governments withheld tax revenues from the central government under the growing spirit of regional autonomy. The anti−alcohol campaign reduced tax revenues as well, which in 1982 accounted for about 12% of all state revenue. The elimination of central control over production decisions, especially in the consumer goods sector, led to the breakdown in traditional supplier−producer relationships without contributing to the formation of new ones. Thus, instead of streamlining the system, Gorbachev's decentralization caused new production bottlenecks.\n\nThe dissolution of the Soviet Union was a process of systematic disintegration, which occurred in the economy, social structure and political structure. It resulted in the abolition of the Soviet Federal Government (\"the Union center\") and independence of the USSR's republics on 25 December 1991. The process was caused by a weakening of the Soviet government, which led to disintegration and took place from about 19 January 1990 to 31 December 1991. The process was characterized by many of the republics of the Soviet Union declaring their independence and being recognized as sovereign nation-states.\n\nAndrei Grachev, the Deputy Head of the Intelligence Department of the Central Committee, summed up the denouement of the downfall quite cogently:\n\n\"Gorbachev actually put the sort of final blow to the resistance of the Soviet Union by killing the fear of the people. It was still that this country was governed and kept together, as a structure, as a government structure, by the fear from Stalinist times.\"\n\nThe principal elements of the old Soviet political system were Communist Party dominance, the hierarchy of soviets, state socialism, and ethnic federalism. Gorbachev's programs of \"perestroika\" (restructuring) and \"glasnost\" (openness) produced radical unforeseen effects that brought that system down. As a means of reviving the Soviet state, Gorbachev repeatedly attempted to build a coalition of political leaders supportive of reform and created new arenas and bases of power. He implemented these measures because he wanted to resolve serious economic problems and political inertia that clearly threatened to put the Soviet Union into a state of long-term stagnation.\n\nBut by using structural reforms to widen opportunities for leaders and popular movements in the union republics to gain influence, Gorbachev also made it possible for nationalist, orthodox communist, and populist forces to oppose his attempts to liberalize and revitalize Soviet communism. Although some of the new movements aspired to replace the Soviet system altogether with a liberal democratic one, others demanded independence for the national republics. Still others insisted on the restoration of the old Soviet ways. Ultimately, Gorbachev could not forge a compromise among these forces and the consequence was the dissolution of the Soviet Union.\n\nTo restructure the Soviet administrative command system and implement a transition to a market economy, Yeltsin's shock program was employed within days of the dissolution of the Soviet Union. The subsidies to money-losing farms and industries were cut, price controls abolished, and the ruble moved towards convertibility. New opportunities for Yeltsin's circle and other entrepreneurs to seize former state property were created, thus restructuring the old state-owned economy within a few months.\n\nAfter obtaining power, the vast majority of \"idealistic\" reformers gained huge possessions of state property using their positions in the government and became business oligarchs in a manner that appeared antithetical to an emerging democracy. Existing institutions were conspicuously abandoned prior to the establishment of new legal structures of the market economy such as those governing private property, overseeing financial markets, and enforcing taxation.\n\nMarket economists believed that the dismantling of the administrative command system in Russia would raise GDP and living standards by allocating resources more efficiently. They also thought the collapse would create new production possibilities by eliminating central planning, substituting a decentralized market system, eliminating huge macroeconomic and structural distortions through liberalization, and providing incentives through privatization.\n\nSince the USSR's collapse, Russia faced many problems that free market proponents in 1992 did not expect. Among other things, 25% of the population lived below the poverty line, life expectancy had fallen, birthrates were low, and the GDP was halved. There was a sharp increase in economic inequality: between 1988/1989 and 1993/1995, the Gini ratio increased by an average of 9 points for all former socialist countries. These problems led to a series of crises in the 1990s, which nearly led to the election of Yeltsin's Communist challenger, Gennady Zyuganov, in the 1996 presidential election. In recent years, the economy of Russia has begun to improve greatly, due to major investments and business development and also due to high prices of natural resources.\n\n\n\n", "id": "7577", "title": "History of the Soviet Union (1982–91)"}
{"url": "https://en.wikipedia.org/wiki?curid=7578", "text": "Corsican language\n\nCorsican (\"corsu\" or \"lingua corsa\") is a Romance language within the Italo-Dalmatian subfamily. It is closely related to the Italian language and especially to its Tuscan branch. It is spoken and written on the islands of Corsica (France) and northern Sardinia (Italy). Corsican was long the vernacular alongside Italian, the official language in Corsica until 1859; afterwards Italian was replaced by French, owing to the acquisition of the island by France from the Republic of Genoa in 1768. Over the next two centuries, the use of French grew to the extent that, by the Liberation in 1945, all islanders had a working knowledge of French. The 20th century saw a wholesale language shift, with islanders changing their language practices to the extent that there were no monolingual Corsican speakers left by the 1960s. By 1995, an estimated 65 percent of islanders had some degree of proficiency in Corsican, and a small minority, perhaps 10 percent, used Corsican as a first language.\n\nOne of the main sources of confusion in popular classifications is the difference between a dialect and a language. Typically it is not possible to ascertain what an author means by these terms. For example, one might read that Corsican belongs to the \"centrosouthern Italian dialects\" along with Standard Italian, Neapolitan and others or that it is \"closely related to the Tuscan dialect of Italian\".\n\nOne of the characteristics of Italian is the retention of the -\"re\" infinitive ending as in Latin \"mittere\", \"send\". Such infinitival ending is lost in Corsican as well as in Tuscan, which has \"mette\" / \"metta\", \"to put\". The Latin relative pronouns, \"qui\", \"quae\" \"who\", and \"quod\" \"what\", are inflected in Latin while the relative pronoun in Italian for \"who\" is \"chi\" and \"what\" is \"che\" and \"(che) cosa\", and in Corsican, it is uninflected \"chì\".\n\nThe Corsican language has been influenced by the languages of the major powers taking an interest in Corsican affairs; earlier by those of the medieval Italian powers: Tuscany (828–1077), Pisa (1077–1282) and Genoa (1282–1768), more recently by France (1768–present), which, since 1789, has promulgated the official Parisian French. The term \"gallicised Corsican\" refers to Corsican up to about the year 1950. The term \"distanciated Corsican\" refers to an idealized Corsican from which various agents have removed French or other elements.\n\nIn 40 AD, the natives of Corsica did not speak Latin. The Roman exile, Seneca the Younger, reports that both coast and interior were occupied by natives whose language he did not understand (see the Ligurian hypothesis). Whatever language was spoken is still visible in the toponymy or in some words, for instance in the Gallurese dialect spoken in Sardinia \"zerru\" 'pig'. A similar situation is valid for Sardinian and Sicilian. The occupation of the island by the Vandals around the year 469 marked the end of authoritative influence by Latin speakers (see Medieval Corsica). If the natives of that time spoke Latin, they must have acquired it during the late empire.\n\nThe two most widely spoken forms of the Corsican language are \"Supranacciu\" or \"Cismonticu\", spoken in the Bastia and Corte area (generally throughout the northern half of the island, known as Haute-Corse or \"Corsica suprana\"), and \"Suttanacciu\" or \"Pumonticu\", spoken around Sartène and Porto-Vecchio (generally throughout the southern half of the island, known as Corse-du-Sud or \"Corsica suttana\"). The dialect of Ajaccio has been described as in transition. The dialects spoken at Calvi and Bonifacio are closer to the Genoese dialect, also known as Ligurian.\n\nThis division along the Girolata-Porto Vecchio line was due to the massive immigration from Tuscany which took place in Corsica during the lower Middle Ages: as a result, the northern Corsican dialects became very close to a central Italian lect like Tuscan, while the southern Corsican varieties could keep the original characteristics of the language which make it much more similar to Sicilian and, only to some extent, Sardinian.\n\nThe Gallurese variety is spoken in the extreme north of Sardinia, including the region of Gallura and the archipelago of La Maddalena, and Sassarese is spoken in Sassari and in its neighbourhood, in the northwest of Sardinia. Whether the two should be included either in Corsican or in Sardinian as dialects or considered independent languages is still subject of debate.\n\nOn Maddalena archipelago the local dialect (called \"Isulanu, Maddaleninu, Maddalenino\") was brought by fishermen and shepherds from Bonifacio during immigration in the 17th and 18th centuries. Though influenced by Gallurese, it has maintained the original characteristics of Corsican. There are also numerous words of Genoese and Ponzese origin.\n\nOn October 14, 1997, Article 2 Item 4 of Law Number 26 of the Autonomous Region of Sardinia granted \"\"al dialetto sassarese e a quello gallurese\"\" – equal legal status with the other languages on Sardinia. They are thus legally defined as different languages from Sardinian by the Sardinian government.\nThe January 2007 estimated population of Corsica was 281,000, whereas the figure for the March 1999 census, when most of the studies—though not the linguistic survey work referenced in this article—were performed, was about 261,000 (see under Corsica). Only a fraction of the population at either time spoke Corsican with any fluency.\n\nThe use of Corsican language over French language has been declining. In 1980 about 70 percent of the population of the island \"had some command of the Corsican language.\" In 1990 out of a total population of about 254,000 the percentage had declined to 50 percent, with only 10 percent using it as a first language. (These figures do not count varieties of Corsican spoken in Sardinia.) The language appeared to be in serious decline when the French government reversed its unsupportive stand and initiated some strong measures to save it.\n\nAccording to an official survey run on behalf of the \"Collectivité territoriale de Corse\" which took place in April 2013, in Corsica the Corsican language has a number of speakers situated between 86,800 and 130,200, out of a total population amounting to 309,693 inhabitants. The percentage of those who have a solid knowledge of the language varies between a minimum of 25 percent in the 25-34 age group and the maximum of 65 percent in the over-65 age group: almost a quarter of the former age group does not understand Corsican, while only a small minority of the older people do not understand it. While 32 percent of the population of northern Corsica speaks Corsican quite well, this percentage drops to 22 percent for South Corsica. Moreover, 10 percent of the population of Corsica speaks only French, while 62 percent speak both French and Corsican. However, only 8 percent of the Corsicans know how to write correctly in Corsican, while about 60 percent of the population does not know how to write in Corsican. While 90 percent of the population is in favor of a Corsican-French bilingualism, 3 percent would like to have Corsican as the only official language in the island, and 7 percent would prefer French in this role.\n\nUNESCO classifies Corsican as a \"definitely endangered language.\" The Corsican language is a key vehicle for Corsican culture, which is notably rich in proverbs and in polyphonic song.\n\nThe 1991 \"Joxe Statute\", in setting up the Collectivité Territoriale de Corse, also provided for the Corsican Assembly, and charged it with developing a plan for the optional teaching of Corsican. The University of Corsica Pasquale Paoli at Corte, Haute-Corse took a central role in the planning.\n\nAt the primary school level Corsican is taught up to a fixed number of hours per week (three in the year 2000) and is a voluntary subject at the secondary school level, but is required at the University of Corsica. It is available through adult education. It can be spoken in court or in the conduct of other government business if the officials concerned speak it. The Cultural Council of the Corsican Assembly advocates for its use; for example, on public signs.\n\nAccording to the anthropologist Dumenica Verdoni, writing new literature in modern Corsican, known as the \"Riacquistu\", is an integral part of affirming Corsican identity. Individuals who had a notable career in France returned to Corsica to write in Corsican, such as the musical producers, Dumenicu Togniotti, director of the Teatru Paisanu, which produced polyphonic musicals, 1973–1982, followed in 1980 by Michel Raffaelli's Teatru di a Testa Mora, and Saveriu Valentini's Teatru Cupabbia in 1984. The list of prose writers includes Alanu di Meglio, Ghjacumu Fusina, Lucia Santucci, Marcu Biancarelli,and many others.\n\nThroughout the 1700s and 1800s there was a steady stream of writers in Corsican, many of whom wrote also in other languages.\n\nFerdinand Gregorovius, 19th century traveller and enthusiast of Corsican culture, reported that the preferred form of the literary tradition of his time was the \"vocero\", a type of polyphonic ballad originating from funeral obsequies. These laments were similar in form to the chorales of Greek drama except that the leader could improvise. Some performers were noted at this, such as the 1700s Mariola della Piazzole and Clorinda Franseschi.\n\nThe trail of written popular literature of known date in Corsican currently goes no further back than the 17th century. An undated corpus of proverbs from communes may well precede it (see under \"External links\" below). Corsican has also left a trail of legal documents ending in the late 12th century. At that time the monasteries held considerable land on Corsica and many of the churchmen were notaries.\n\nBetween 1200 and 1425 the monastery of Gorgona, which belonged to the Order of Saint Benedict for much of that time and was in the territory of Pisa, acquired about 40 legal papers of various sorts written on Corsica. As the church was replacing Pisan prelates with Corsican ones there, the legal language shows a transition from entirely Latin through partially Latin and partially Corsican to entirely Corsican. The first known surviving document containing some Corsican is a bill of sale from Patrimonio dated to 1220. These documents were moved to Pisa before the monastery closed its doors and were published there. Research into earlier evidence of Corsican is ongoing.\n\nCorsican is written in the standard Latin script, using 21 of the letters for native words. The letters j, k, w, x, and y are found only in foreign names and French vocabulary. The digraphs and trigraphs \"chj\", \"ghj\", \"sc\" and \"sg\" are also defined as \"letters\" of the alphabet in its modern scholarly form (compare the presence of \"ch\" or \"ll\" in the Spanish alphabet) and appear respectively after \"c\", \"g\" and \"s\".\n\nThe primary diacritic used is the grave accent, indicating word stress when it is not penultimate. In scholarly contexts, disyllables may be distinguished from diphthongs by use of the diaeresis on the former vowel (as in Italian and distinct from French and English). In older writing, the acute accent is sometimes found on stressed , the circumflex on stressed , indicating respectively () and () phonemes.\n\nCorsican has been regarded as a dialect of Italian historically, similar to the regional Romance languages in Italy proper, and in writing, it often resembles Italian (with the substitution of -u for final -o and the articles \"u\" and \"a\" for \"il/lo\" and \"la\" respectively). However, the phonemes of the modern spoken forms of Corsican undergo complex and sometimes irregular phenomena depending on phonological context, so the pronunciation of the language for foreigners familiar with other Romance languages is not straightforward.\n\nAs in Italian, the grapheme appears in some digraphs and trigraphs in which it does not represent the phonemic vowel. All vowels are pronounced except in a few well-defined instances. is not pronounced between and : \"sciarpa\" ; or initially in some words: \"istu\" .\n\nVowels may be nasalized before (which is assimilated to before or ) and the palatal nasal consonant represented by . The nasal vowels are represented by the vowel plus , or . The combination is a digraph or trigraph indicating the nasalized vowel. The consonant is pronounced in weakened form. The same combination of letters might not be the digraph or trigraph but might be just the non-nasal vowel followed by the consonant at full weight. The speaker must know the difference. Example of nasal: is pronounced and not .\n\nThe vowel inventory, or collection of phonemic vowels (and the major allophones), transcribed in IPA symbols, is:\n\n\n", "id": "7578", "title": "Corsican language"}
{"url": "https://en.wikipedia.org/wiki?curid=7580", "text": "Commodore International\n\nCommodore International (or Commodore International Limited) was a North American home computer and electronics manufacturer. Commodore International (CI), along with its subsidiary Commodore Business Machines (CBM), participated in the development of the home–personal computer industry in the 1970s and 1980s. The company developed and marketed one of the world's best-selling desktop computers, the Commodore 64 (1982) and released its Amiga computer line in July 1985.\n\nThe company that would become Commodore Business Machines, Inc. was founded in 1954 in Toronto as the Commodore Portable Typewriter Company by Jewish immigrant and Auschwitz survivor Jack Tramiel. For a few years, he had been living in New York, driving a taxicab and running a small business repairing typewriters, when he managed to sign a deal with a Czechoslovakian company to manufacture their designs in Canada. He moved to Toronto to start production. By the late 1950s a wave of Japanese machines forced most North American typewriter companies to cease business, but Tramiel instead turned to adding machines.\n\nIn 1955, the company was formally incorporated as Commodore Business Machines, Inc. (CBM) in Canada. In 1962, Commodore went public on the New York Stock Exchange (NYSE), under the name of Commodore International Limited. In the late 1960s, history repeated itself when Japanese firms started producing and exporting adding machines. The company's main investor and chairman, Irving Gould, suggested that Tramiel travel to Japan to understand how to compete. Instead, he returned with the new idea to produce electronic calculators, which were just coming on the market.\n\nCommodore soon had a profitable calculator line and was one of the more popular brands in the early 1970s, producing both consumer as well as scientific/programmable calculators. However, in 1975, Texas Instruments, the main supplier of calculator parts, entered the market directly and put out a line of machines priced at less than Commodore's cost for the parts. Commodore obtained an infusion of cash from Gould, which Tramiel used beginning in 1976 to purchase several second-source chip suppliers, including MOS Technology, Inc., in order to assure his supply. He agreed to buy MOS, which was having troubles of its own, only on the condition that its chip designer Chuck Peddle join Commodore directly as head of engineering.\n\nThrough the 1970s, Commodore also produced numerous peripherals and consumer electronic products such as the Chessmate, a chess computer based around a MOS 6504 chip, released in 1978.\n\nIn December 2007 when Tramiel was visiting the Computer History Museum in Mountain View, California, for the 25th anniversary of the Commodore 64, he was asked why he called his company Commodore. He said: \"I wanted to call my company General, but there's so many Generals in the U.S.: General Electric, General Motors. Then I went to Admiral, but that was taken. So I wind up in Berlin, Germany, with my wife, and we were in a cab, and the cab made a short stop, and in front of us was an Opel Commodore.\" Tramiel gave this account in many interviews, but Opel's Commodore didn't debut until 1967, years after the company had been named.\n\nOnce Chuck Peddle had taken over engineering at Commodore, he convinced Jack Tramiel that calculators were already a dead end, and that they should turn their attention to home computers. Peddle packaged his single-board computer design in a metal case, initially with a keyboard using calculator keys, later with a full-travel QWERTY keyboard, monochrome monitor, and tape recorder for program and data storage, to produce the Commodore PET (Personal Electronic Transactor). From PET's 1977 debut, Commodore would be a computer company.\n\nCommodore had been reorganized the year before into Commodore International, Ltd., moving its financial headquarters to the Bahamas and its operational headquarters to West Chester, Pennsylvania, near to the MOS Technology site. The operational headquarters, where research and development of new products occurred, retained the name Commodore Business Machines, Inc. In 1980 Commodore launched production for the European market in Braunschweig (Germany).\n\nBy 1980 Commodore was one of the three largest microcomputer companies, and the largest in the Common Market. \"BYTE\" stated of the business computer market, however, that \"the lack of a marketing strategy by Commodore, as well as its past nonchalant attitude toward the encouragement and development of good software, has hurt its credibility, especially in comparison to the other systems on the market\".\n\nThe PET computer line was used primarily in schools, where its tough all-metal construction and ability to share printers and disk drives on a simple Local Area Network were advantages, but PETs did not compete well in the home setting where graphics and sound were important. This was addressed with the introduction of the VIC-20 in 1981, which was introduced at a cost of US$299 and sold in retail stores. Commodore took out aggressive ads featuring William Shatner asking consumers \"Why buy just a video game?\" The strategy worked and the VIC-20 became the first computer to ship more than one million units. A total of 2.5 million units were sold over the machine's lifetime and helped Commodore's sales to Canadian schools. In another promotion aimed at schools (and as a way of getting rid of old unsold inventory) some PET models labeled \"Teacher's PET\" were given away as part of a \"buy 2 get 1 free\" promotion.\n\nIn 1982, Commodore introduced the Commodore 64 as the successor to the VIC-20. Thanks to a well-designed set of chips designed by MOS Technology, the Commodore 64, (also referred to as C64), possessed remarkable sound and graphics for its time and is often credited with starting the computer demo scene. Its US$595 price was high compared with that of the VIC-20, but it was still much less expensive than any other 64K computer on the market. Early C64 ads boasted, \"You can't buy a better computer at twice the price.\" Australian adverts in the mid-1980s used a tune speaking the words \"Are you keeping up with the Commodore? Because the Commodore is keeping up with you.\"\n\nIn 1983, Tramiel decided to focus on market share and cut the price of the VIC-20 and C64 dramatically, starting what would be called the \"home computer war\". TI responded by cutting prices on its TI-99/4A, which had been introduced in 1981. Soon there was an all-out price war involving Commodore, TI, Atari, and practically every vendor other than Apple Computer. Commodore began selling the VIC-20 and C64 through mass-market retailers such as K-Mart, in addition to traditional computer stores. By the end of this conflict, Commodore had shipped somewhere around 22 million C64s, making the C64 the best selling computer of all time.\n\nAt the June 1983 Consumer Electronics Show, Commodore lowered the retail price of the 64 to $300, and stores sold it for as little as $199. At one point the company was selling as many computers as the rest of the industry combined. Its prices for the VIC-20 and 64 were $50 lower than Atari's prices for the 600XL and 800XL. Commodore's strategy was to, according to a spokesman, devote 50% of its efforts to the under-$500 market, 30% on the $500–1000 market, and 20% on the over-$1000 market. Its vertical integration and Tramiel's focus on cost control helped Commodore do well during the price war, with $1 billion in 1983 sales. Although the company and Tramiel's focus on cost cutting over product testing caused many hardware defects in the 64, by early 1984 Synapse Software—the largest provider of third-party Atari 8-bit software—received 65% of sales from the Commodore market, and Commodore sold almost three times as many computers as Atari that year.\n\nDespite its focus on the lower end of the market, Commodore's computers were also sold in upmarket department stores such as Harrod's. The company also attracted several high-profile customers. In 1984, the company's British branch became the first manufacturer to receive a royal warrant for computer business systems. NASA's Kennedy Space Center was another noted customer, with over 60 Commodore systems processing documentation, tracking equipment and employees, costing jobs, and ensuring the safety of hazardous waste.\n\nAlthough by early 1984 \"Creative Computing\" compared Commodore to \"a well-armed battleship [which] rules the micro waves\" and threatened to destroy rivals like Atari and Coleco, Commodore's board of directors were as impacted as anyone else by the price spiral and decided they wanted out. An internal power struggle resulted; in January 1984, Tramiel resigned due to intense disagreement with the chairman of the board, Irving Gould. Gould replaced Tramiel with Marshall F. Smith, a steel executive who had no experience with computers or consumer marketing. Tramiel founded a new company, Tramel Technology (spelled differently so people would pronounce it correctly), and hired away a number of Commodore engineers to begin work on a next-generation computer design.\n\nNow it was left to the remaining Commodore management to salvage the company's fortunes and plan for the future. It did so by buying a small startup company called Amiga Corporation in February 1983, for $25 million ($12.8 million in cash and 550,000 in common shares) which became a subsidiary of Commodore, called Commodore-Amiga, Inc. Commodore brought this new 32-bit computer design (initially codenamed \"Lorraine\", from 1979, and had been called High-Toro from 1980 -1981 then later dubbed the Amiga, under Amiga Inc.in early 1982. There were three unsuccessful attempts to release the Amiga by Jay Miner and company. These were: 1982, 1983 and one more after Commodore bought Amiga in 1984, after which it was released only to the local public. Then in 1985 Commodore re-released it to the world. Cost was $1000-$1300.\n\nBut Tramiel had beaten Commodore to the punch. His design was 95% completed by June (which fueled speculation that his engineers had taken technology with them from Commodore). In July 1984 he bought the consumer side of Atari Inc. from Warner Communications which allowed him to strike back and release the Atari ST earlier in 1985 for about $800. The Atari ST was technology was almost out, however the Amiga was out sooner.\n\nDuring development in 1981, Amiga had exhausted venture capital and was desperate for more financing. Jay Miner and company had approached former employer Atari, and the Warner-owned Atari had paid Amiga to continue development work. In return Atari was to get one-year exclusive use of the design as a video game console. After one year Atari would have the right to add a keyboard and market the complete Amiga computer. The Atari Museum has acquired the Atari-Amiga contract and Atari engineering logs revealing that the Atari Amiga was originally designated as the 1850XLD. As Atari was heavily involved with Disney at the time, it was later code-named \"Mickey\", and the 256K memory expansion board was codenamed \"Minnie\".\n\nThe following year, Tramiel discovered that Warner Communications wanted to sell Atari, which was rumored to be losing about $10,000 a day. Interested in Atari's overseas manufacturing and worldwide distribution network for his new computer, he approached Atari and entered negotiations. After several on-again/off-again talks with Atari in May and June 1984, Tramiel had secured his funding and bought Atari's Consumer Division (which included the console and home computer departments) in July.\n\nAs more execs and researchers left Commodore after the announcement to join up with Tramiel's new company Atari Corp., Commodore followed by filing lawsuits against four former engineers for theft of trade secrets in late July. This was intended, in effect, to bar Tramiel from releasing his new computer.\n\nOne of Tramiel's first acts after forming Atari Corp. was to fire most of Atari's remaining staff, and to cancel almost all ongoing projects, in order to review their continued viability. In late July/early August, Tramiel representatives discovered the original Amiga contract from the previous fall. Seeing a chance to gain some leverage, Tramiel immediately used the contract to counter-sue Commodore through its new subsidiary, Amiga, on August 13.\n\nThe Amiga crew, still suffering serious financial problems, had sought more monetary support from investors that entire spring. At around the same time that Tramiel was in negotiations with Atari, Amiga entered into discussions with Commodore. The discussions ultimately led to Commodore's intentions to purchase Amiga outright, which would (from Commodore's viewpoint) cancel any outstanding contracts - including Atari Inc.'s. This \"interpretation\" is what Tramiel used to counter-sue, and sought damages and an injunction to bar Amiga (and effectively Commodore) from producing any resembling technology. This was an attempt to render Commodore's new acquisition (and the source for its next generation of computers) useless. The resulting court case lasted for several years, with both companies releasing their respective products. In the end, the Amiga computer outlasted the Atari.\n\nThroughout the life of the ST and Amiga platforms, a ferocious Atari-Commodore rivalry raged. While this rivalry was in many ways a holdover from the days when the Commodore 64 had first challenged the Atari 800 (among others) in a series of scathing television commercials, the events leading to the launch of the ST and Amiga only served to further alienate fans of each computer, who fought vitriolic holy wars on the question of which platform was superior. This was reflected in sales numbers for the two platforms until the release of the Amiga 500 in 1987 which led the Amiga sales to exceed the ST by about 1.5 to 1, despite reaching the market later. However, the battle was in vain, as neither platform captured a significant share of the world computer market and only the Apple Macintosh would survive the industry-wide shift to Microsoft Windows running on PC clones.\n\nAdam Osborne stated in April 1981 that \"the microcomputer industry abounds with horror stories describing the way Commodore treats its dealers and its customers.\" Many in the industry believed rumors in late 1983 that Commodore would discontinue the 64 despite its great success because they disliked the company's business practices, including poor treatment of dealers and introducing new computers incompatible with existing ones. One dealer said \"It's too unsettling to be one of their dealers and not know where you stand with them.\" After Tramiel's departure, another journalist wrote that he \"had never been able to establish very good relations with computer dealers ... computer retailers have accused Commodore of treating them as harshly as if they were suppliers or competitors, and as a result, many have become disenchanted with Commodore and dropped the product line\". However, upon the 1987 introduction of the Amiga 2000, Commodore retreated from its earlier strategy of selling its computers to discount outlets and toy stores, and now favored authorized dealers. Software developers also disliked the company, with one stating that \"Dealing with Commodore was like dealing with Attila the Hun.\" At the 1987 Comdex, an informal \"InfoWorld\" survey found that none of the developers present planned to write for Commodore platforms. Although Comdex was oriented toward business computing, not Commodore's traditional consumer market, such a response did not bode well for Commodore's efforts to establish the Amiga as a business platform.\n\nCommodore faced the problem, when marketing the Amiga, of still being seen as the company that made cheap, disposable computers like the 64 and VIC were perceived to be. By the late 1980s, the personal computer market had become dominated by the IBM PC and Apple Macintosh platforms and Commodore's marketing efforts for the Amiga were less successful in breaking the new computer into this now-established market than its promotions for the 8-bit line had been in making Commodore the home computer leader. The company put effort into developing and promoting consumer products that would not be in demand for years, such as an Amiga 500-based HTPC called CDTV. As early as 1986, the mainstream press was predicting Commodore's demise, and in 1990 \"Computer Gaming World\" wrote of its \"abysmal record of customer and technical support in the past\". Nevertheless, as profits and the stock price began to slide, \"The Philadelphia Inquirer's\" Top 100 Businesses annual continued to list several Commodore executives among the highest-paid in the region and the paper documented the company's questionable hiring practices and large bonuses paid to executives amid shareholder discontent.\n\nCommodore failed to update the Amiga to keep pace as the PC platform advanced. CBM continued selling Amiga 2000s with 7.14 MHz 68000 CPUs, even though the Amiga 3000 with 25 MHz 68030 was on the market. Apple by this time was using the 68040 and had relegated the 68000 to its lowest end model, the black and white Macintosh Classic. The 68000 was used in the Sega Genesis, one of the leading game consoles of the era, PCs fitted with high-color VGA graphics cards and SoundBlaster (or compatible) sound cards had finally caught up with the Amiga's performance and Commodore began to fade from the consumer market. Although the Amiga was originally conceived as a gaming machine, Commodore had always emphasized the Amiga's potential for professional applications. But the Amiga's high-performance sound and graphics were irrelevant for most of the day's MS-DOS-based routine business word-processing and data-processing requirements, and the machine could not successfully compete with PCs in a business market that was rapidly undergoing commoditization. Commodore introduced a range of PC compatible systems designed by its German division, and while the Commodore name was better known in the US than some of its competition, the systems' price and specs were only average.\n\nIn 1992, the A600 replaced the A500. It removed the numeric keypad, Zorro expansion slot, and other functionality, but added IDE, PCMCIA and a theoretically cost-reduced design. Designed as the Amiga 300, a nonexpandable model to sell for less than the Amiga 500, the 600 was forced to become a replacement for the 500 due to the unexpected higher cost of manufacture. Productivity developers increasingly moved to PC and Macintosh, while the console wars took over the gaming market. David Pleasance, managing director of Commodore UK, described the A600 as a 'complete and utter screw-up'.\n\nIn 1992, Commodore released the Amiga CD32 and the Amiga 1200, and in 1992 Amiga 4000 computers, which featured an improved graphics chipset, the AGA. The custom-designed and custom-built AGA chipset cost Commodore more than the commodity chips used in IBM PCs, despite lagging them in performance. The advent of PC games using 3D graphics such as \"Doom\" and \"Wolfenstein 3D\" spelt the end of Amiga as a gaming platform, due to mismanagement.\n\nIn 1993, the 'make or break' system, according to Pleasance, was a 32-bit CD-ROM-based game console called the Amiga CD32, but it was not sufficiently profitable to put Commodore back in the black.\n\nIn 1992, all UK servicing and warranty repairs were outsourced to Wang Laboratories., who were replaced by ICL after failing to meet repair demand during the Christmas rush 1992. By 1994, only its operations in Germany and the United Kingdom were still profitable. Commodore declared bankruptcy on April 29, 1994 and ceased to exist, causing the board of directors to \"authorize the transfer of its assets to trustees for the benefit of its creditors\", according to an official statement.\n\nThe company's computer systems, especially the C64 and Amiga series, retained a cult following decades after its demise.\n\nFollowing its liquidation, Commodore's former assets went their separate ways, with none of the descendant companies repeating Commodore's early success. Both Commodore and Amiga product lines were produced in the 21st century, but separately with Amiga, Inc. being its own company and Commodore computers being produced by Commodore USA, an unrelated Florida-based company that had purchased the brand name. Other companies develop operating systems and manufacture computers for both Commodore and Amiga brands as well as software.\n\nCommodore UK was the only subsidiary to survive the bankruptcy and even placed a bid to buy out the rest of the operation, or at least the former parent company. For a time it was considered the front runner in the bid, and numerous reports surfaced during the 1994–1995 time frame that Commodore UK had made the purchase. Commodore UK stayed in business by selling old inventory and making computer speakers and some other types of computer peripherals. However, Commodore UK withdrew its bid at the start of the auction process after several larger companies, including Gateway Computers and Dell Inc., became interested, primarily for Commodore's 47 patents relating to the Amiga. Ultimately, the successful bidder was German PC conglomerate Escom, and Commodore UK went into liquidation on August 30, 1995.\n\nIn 1995 Escom paid US$14 million for the assets of Commodore International. It separated the Commodore and Amiga operations into separate divisions and quickly started using the Commodore brand name on a line of PCs sold in Europe. However, it soon started losing money due to over-expansion, went bankrupt on July 15, 1996, and was liquidated.\n\nIn September 1997, the Commodore brand name was acquired by Dutch computer maker Tulip Computers NV.\n\nIn July 2004, Tulip announced a new series of products using the Commodore name: fPET, a flash memory-based USB Flash drive; mPET, a flash-based MP3 Player and digital recorder; eVIC, a 20 GB music player. Also, it licensed the Commodore trademark and \"chicken lips\" logo to the producers of the C64 DTV.\n\nIn late 2004, Tulip sold the Commodore trademarks to Yeahronimo Media Ventures for €22 million. The sale was completed in March 2005 after months of negotiations. Yeahronimo Media Ventures soon renamed itself to \"Commodore International Corporation\" and started an operation intended to relaunch the Commodore brand. The company launched its \"Gravel\" line of products: personal multimedia players equipped with Wi-Fi, with the hope the Commodore brand would help them take off. The \"Gravel\" was never a success and was discontinued. On June 24, 2009, CIC renamed itself to Reunite Investments. CIC's founder, Ben van Wijhe, bought a Hong Kong-based company called Asiarim. The brand is now owned by C= Holdings (formerly Commodore International B.V.): Reunite became the sole owner of it in 2010, after buying the remaining shares from the bankrupt Nedfield, then sold it to Commodore Licensing BV, a subsidiary of Asiarim, later in 2010. It was sold again on 7 November 2011: this transaction became the basis of a legal dispute between Asiarim (which, even after that date, made commercial use of the Commodore trademark, among others by advertising for sale Commodore-branded computers, and dealing licensing agreements for the trademarks) and the new owners, that was resolved by the United States District Court for the Southern District of New York on 16 December 2013 in favour of the new owners.\n\nThe Commodore Semiconductor Group (formerly MOS Technology, Inc.) was bought by its former management and in 1995, resumed operations under the name GMT Microelectronics, utilizing a troubled facility in Norristown, Pennsylvania that Commodore had closed in 1992. By 1999 it had $21 million in revenues and 183 employees. However, in 2001 the United States Environmental Protection Agency shut the plant down. GMT ceased operations and was liquidated.\n\nOwnership of the remaining assets of Commodore International, including the copyrights and patents, and the Amiga trademarks, passed from Escom to U.S. PC clone maker Gateway 2000 in 1997, who retained the patents and sold the copyrights and trademarks, together with a license to use the patents, to Amiga, Inc., a Washington company founded, among others, by former Gateway subcontractors Bill McEwen and Fleecy Moss in 2000. On March 15, 2004, Amiga, Inc. announced that on April 23, 2003 it had transferred its rights over past and future versions of the Amiga OS (but not yet over other intellectual property) to Itec, LLC, later acquired by KMOS, Inc., a Delaware company. Shortly afterwards, on the basis of some loans and security agreements between Amiga, Inc. and Itec, LLC, the remaining intellectual property assets were also transferred from Amiga, Inc. to KMOS, Inc. On March 16, 2005, KMOS, Inc. announced that it had completed all registrations with the State of Delaware to change its corporate name to Amiga, Inc. The Commodore/Amiga copyrights were later sold to Cloanto. AmigaOS (as well as spin-offs MorphOS and AROS) is still maintained and updated. Several companies produce related hardware and software today.\n\nCommodore's former US headquarters is currently the headquarters to QVC.\n\nIn February 2017 an exhibition room for about 200 Commodore products was opened in Braunschweig, commemorating the European production site of Commodore which had up to 2000 employees.\n\nThis product line consists of original Commodore products.\n\n774D, 9R23, C108, C110, F4146R, F4902, MM3, Minuteman 6, P50, PR100, SR1800, SR4120D, SR4120R, SR4148D, SR4148R, SR4190R, SR4212, SR4912, SR4921RPN, SR5120D, SR5120R, SR5148D, SR5148R, SR5190R, SR59, SR7919, SR7949, SR9150R, SR9190R, US*3, and The Specialist series: M55 (The Mathematician), N60 (The Navigator), S61 (The Statistician).\n\n\"(listed chronologically)\"\n\n\n1000, 1024, 1070, 1080, 1081, 1083S, 1084, 1084S, 1084ST, 1085S, 1201, 1402, 1403, 1404, 1405, 1407, 1428, 1428x, 1432D, 1432V, 1701, 1702, 1703, 1801, 1802, 1803, 1900M/DM602, 1901/75BM13/M1, 1902, 1902A, 1930, 1930-II, 1930-III, 1934, 1935, 1936, 1936ALR, 1940, 1942, 1950, 1960, 1962, 2002, A2024, 2080, 76M13, CM-141, DM-14, DM602 \n\nCommodore's own software had a poor reputation; \"InfoWorld\" in 1984, for example, stated that \"so far, the normal standard for Commodore software is mediocrity\". Third parties developed the vast majority of software for Commodore computers.\n\n\n", "id": "7580", "title": "Commodore International"}
{"url": "https://en.wikipedia.org/wiki?curid=7581", "text": "Commodore (rank)\n\nCommodore is a naval rank used in many navies that is superior to a navy captain, but below a rear admiral. Non-English-speaking nations often use the rank of flotilla admiral or counter admiral or senior captain as an equivalent, although counter admiral may also correspond to rear admiral.\n\nTraditionally, \"commodore\" is the title for any officer assigned to command more than one ship at a time, even temporarily, much as \"captain\" is the traditional title for the commanding officer of a single ship even if the officer's official title in the service is a lower rank. As an official rank, a commodore typically commands a flotilla or squadron of ships as part of a larger task force or naval fleet commanded by an admiral. A commodore's ship is typically designated by the flying of a Broad pennant, as opposed to an admiral's flag. \n\nIt is often regarded as a one-star rank with a NATO code of OF-6 (which is known in the U.S. as \"rear admiral (lower half)\"), but whether it is regarded as a flag rank varies between countries.\n\nIt is sometimes abbreviated: as \"Cdre\" in British Royal Navy, \"CDRE\" in the US Navy, \"Cmdre\" in the Royal Canadian Navy, \"COMO\" in the Spanish Navy and in some navies speaking the Spanish language, or \"CMDE\" as used in the Indian Navy or in some other Navies.\n\nThe rank of commodore derives from the French \"commandeur\", which was one of the highest ranks in orders of knighthood, and in military orders the title of the knight in charge of a \"commenda\" (a local part of the order's territorial possessions).\n\nThe Dutch Navy also used the rank of \"commandeur\" from the end of the 16th century for a variety of temporary positions, until it became a conventional permanent rank in 1955. The Royal Netherlands Air Force has adopted the English spelling of \"commodore\" for an equivalent rank.\n\nThe rank of commodore was at first a position created as a temporary title to be bestowed upon captains who commanded squadrons of more than one vessel. In many navies, the rank of commodore was merely viewed as a senior captain position, whereas other naval services bestowed upon the rank of commodore the prestige of flag officer status.\n\nCommodore is the highest rank in the Irish Naval Service held by the Chief of Naval Operations. This is because Ireland, despite having the largest part of EU waters to patrol, has among the smallest navies (always fewer than 10 ships) and thus the rank of admiral for flag officers seemed inappropriate. \n\nIn the Royal Navy, the position was introduced to combat the cost of appointing more admirals—a costly business with a fleet as large as the Royal Navy's at that time.\n\nIn 1899, the substantive rank of commodore was discontinued in the United States Navy, but revived during World War II. It was discontinued as a rank in these services during the postwar period, but as an appointment, the title \"commodore\" was then used to identify senior U.S. Navy captains who commanded squadrons of more than one vessel or functional air wings or air groups that were not part of a carrier air wing or air group. Concurrently, until the early 1980s, U.S. Navy and U.S. Coast Guard captains selected for promotion to the rank of rear admiral (lower half), would wear the same insignia as rear admiral (upper half), i.e., two silver stars for collar insignia or sleeve braid of one wide and one narrow gold stripe, even though they were actually only equivalent to one-star officers and paid at the one-star rate.\n\nTo correct this inequity, the rank of commodore as a single star flag officer was reinstated by both services in the early 1980s. This immediately caused confusion with those senior U.S. Navy captains commanding destroyer squadrons, submarine squadrons, functional air wings and air groups, and so on, who held the temporary \"title\" of commodore while in their major command billet. As a result of this confusion, the services soon renamed the new one-star rank as commodore admiral (CADM) within the first six months following the rank's reintroduction. However, this was considered an awkward title and the one-star flag rank was renamed a few months later to its current title of rear admiral (lower half), later abbreviated by the U.S. Navy and U.S. Coast Guard as RDML.\n\nThe \"title\" of commodore continues to be used in the U.S. Navy and Coast Guard for those senior captains in command of organizations consisting of groups of ships or submarines organized into squadrons; air wings or air groups of multiple aviation squadrons other than carrier air wings (the latter whose commanders still use the title \"CAG\"); explosive ordnance disposal (EOD), mine warfare and special warfare (SEAL) groups; and construction battalion (SeaBee) regiments. Although not flag officers, modern day commodores in the U.S. Navy rate a blue and white command pennant, also known as a broad pennant, that is normally flown at their headquarters facilities ashore or from ships that they are embarked aboard when they are the senior officer present afloat (SOPA).\n\nIn the Argentine Navy, the position of commodore was created in the late 1990s, and is usually, but not always, issued to senior captains holding rear-admirals' positions. It is not a rank but a distinction and, as such, can be issued by the chief of staff without congressional approval. Its equivalents are colonel-major in the Army and commodore-major in the Air Force. It is usually—but incorrectly—referred to as \"navy commodore\", to avoid confusion with the \"air force commodore\", which is equivalent to the navy's captain and army's colonel. The sleeve lace is identical to that of the Royal Navy, and wears one star on the epaulette.\n\nThe following articles deal with the rank of commodore (or its equivalent) as it is employed OF-6 one-star flag officer rank in various countries.\n\nCommodore, in Spanish \"comodoro\", is a rank in the Argentine Air Force. This rank is the equivalent of a colonel in the Argentine Army, and a colonel or group captain in other air forces of the world. The Argentine rank below commodore is the rank of vice-commodore (Spanish \"vicecomodoro\") equivalent to a lieutenant-colonel in the Argentine Army, and a lieutenant-colonel or wing commander in other air forces.\n\nCommodore is a rank in the Royal Netherlands Air Force. It is a one-star rank and has essentially the same rank insignia as the British air commodore.\n\nMany air forces use the rank of air commodore. This rank was first used by the Royal Air Force and is now used in many countries such as Australia, Bangladesh, Greece, India, New Zealand, Nigeria, Pakistan, Thailand and Zimbabwe. It is the equivalent rank to the navy rank of \"commodore\", and the army ranks of brigadier and brigadier general.\n\nThe German air force used the concept of a unit commodore, although this was a unit command appointment rather than a rank.\n\nCommodore is also a title held by many captains with a recognised very high grade of navigation and seagoing seniority in the Merchant Marine, and by the directors of few yacht clubs and boating associations. Commodores in command as Master aboard merchant marine ships, wear rank ensignia and particular golden cap ensignia.\n\nDuring wartime, a shipping convoy will have a ranking officer—sometimes an active-duty naval officer, at other times a civilian master or retired naval officer—designated as the \"convoy commodore\". This title is not related to the individuals military rank (if any), but instead is the title of the senior individual responsible for the overall operation of the merchant ships and naval auxiliary ships that make up the convoy. The convoy commodore does not command the convoy escort forces (if any), which are commanded by a naval officer who serves as escort commander.\n\nThe U.S. Coast Guard Auxiliary also employs variants of the \"title\" of commodore. Members of the Auxiliary are civilian volunteers who do not have military rank, but who do wear modified U.S. Coast Guard uniforms and U.S. military-style officer rank insignia to indicate office. Auxiliary members who have been elected to positions in the highest levels of the organization, similar in nature to active and reserve rear admirals and vice admirals use the term commodore (e.g., District Commodore, Assistant National Commodore, Deputy National Commodore, National Commodore, etc.). These Coast Guard Auxiliarists may permanently append the title commodore, sometimes abbreviated COMO, to their names (e.g., Commodore James A. Smith, National Commodore; or COMO Jim Smith, (NACO)).\n\nIn the Philippine Coast Guard Auxiliary—PCGA—each of the directors in command of the ten Coast Guard Auxiliary districts are commodores, as well as most of the Deputy National Directors (some may be rear admirals). Commodore is appreviated to COMMO in the PCGA.\n\nVanderbilt University's intercollegiate athletics teams are nicknamed the \"Commodores\", a reference to Cornelius Vanderbilt's self-appointed title (he was the master of a large shipping fleet).\n\nIn the U.S. Sea Scouting program (which is part of the Boy Scouts of America), all National, Regional, Area, and Council committee chairs are titled as commodore, while senior committee members are addressed as vice commodore. Ship committee chairs do not hold this recognition.\n\n\n", "id": "7581", "title": "Commodore (rank)"}
{"url": "https://en.wikipedia.org/wiki?curid=7583", "text": "Cauchy–Riemann equations\n\nIn the field of complex analysis in mathematics, the Cauchy–Riemann equations, named after Augustin Cauchy and Bernhard Riemann, consist of a system of two partial differential equations which, together with certain continuity and differentiability criteria, form a necessary and sufficient condition for a complex function to be complex differentiable, that is, holomorphic. This system of equations first appeared in the work of Jean le Rond d'Alembert . Later, Leonhard Euler connected this system to the analytic functions . then used these equations to construct his theory of functions. Riemann's dissertation on the theory of functions appeared in 1851.\n\nThe Cauchy–Riemann equations on a pair of real-valued functions of two real variables \"u\"(\"x\",\"y\") and \"v\"(\"x\",\"y\") are the two equations:\n\nTypically \"u\" and \"v\" are taken to be the real and imaginary parts respectively of a complex-valued function of a single complex variable , . Suppose that \"u\" and \"v\" are real-differentiable at a point in an open subset of C (C is the set of complex numbers), which can be considered as functions from R to R. This implies that the partial derivatives of \"u\" and \"v\" exist (although they need not be continuous) and we can approximate small variations of \"f\" linearly. Then is complex-differentiable at that point if and only if the partial derivatives of \"u\" and \"v\" satisfy the Cauchy–Riemann equations (1a) and (1b) at that point. The sole existence of partial derivatives satisfying the Cauchy–Riemann equations is not enough to ensure complex differentiability at that point. It is necessary that u and v be real differentiable, which is a stronger condition than the existence of the partial derivatives, but it is not necessary that these partial derivatives be continuous.\n\nHolomorphy is the property of a complex function of being differentiable at every point of an open and connected subset of C (this is called a domain in C). Consequently, we can assert that a complex function \"f\", whose real and imaginary parts \"u\" and \"v\" are real-differentiable functions, is holomorphic if and only if, equations (1a) and (1b) are satisfied throughout the domain we are dealing with. Holomorphic functions are analytic and vice versa. This means that, in complex analysis, a function that is complex-differentiable in a whole domain (holomorphic) is the same as an analytic function. This is not true for real differentiable functions.\n\nSuppose that \"z\" = \"x\" + \"iy\". A complex-valued function \"f\"(\"z\") = \"z\" is differentiable at any point \"z\" in the complex plane. \nThe real part \"u\"(\"x\", \"y\") and the imaginary part \"v\"(\"x\", \"y\") of \"f\"(\"z\") are\nrespectively. The partial derivatives of these are \nThese partial derivatives have following relationships: \nThus this complex-valued function \"f\"(\"z\") satisfies the Cauchy–Riemann equations.\n\nThe equations are one way of looking at the condition on a function to be differentiable in the sense of complex analysis: in other words they encapsulate the notion of function of a complex variable by means of conventional differential calculus. In the theory there are several other major ways of looking at this notion, and the translation of the condition into other language is often needed.\n\nFirst, the Cauchy–Riemann equations may be written in complex form\n\nIn this form, the equations correspond structurally to the condition that the Jacobian matrix is of the form\n\nwhere formula_14 and formula_15. A matrix of this form is the matrix representation of a complex number. Geometrically, such a matrix is always the composition of a rotation with a scaling, and in particular preserves angles. The Jacobian of a function \"f\"(\"z\") takes infinitesimal line segments at the intersection of two curves in z and rotates them to the corresponding segments in \"f\"(\"z\"). Consequently, a function satisfying the Cauchy–Riemann equations, with a nonzero derivative, preserves the angle between curves in the plane. That is, the Cauchy–Riemann equations are the conditions for a function to be conformal.\n\nMoreover, because the composition of a conformal transformation with another conformal transformation is also conformal, the composition of a solution of the Cauchy–Riemann equations with a conformal map must itself solve the Cauchy–Riemann equations. Thus the Cauchy–Riemann equations are conformally invariant.\n\nSuppose that\n\nis a function of a complex number \"z\". Then the complex derivative of \"f\" at a point \"z\" is defined by\n\nprovided this limit exists.\n\nIf this limit exists, then it may be computed by taking the limit as \"h\" → 0 along the real axis or imaginary axis; in either case it should give the same result. Approaching along the real axis, one finds\n\nOn the other hand, approaching along the imaginary axis,\n\nThe equality of the derivative of \"f\" taken along the two axis is\n\nwhich are the Cauchy–Riemann equations (2) at the point \"z\".\n\nConversely, if \"f\" : C → C is a function which is differentiable when regarded as a function on R, then \"f\" is complex differentiable if and only if the Cauchy–Riemann equations hold. In other words, if u and v are real-differentiable functions of two real variables, obviously \"u\" + \"iv\" is a (complex-valued) real-differentiable function, but \"u\" + \"iv\" is complex-differentiable if and only if the Cauchy–Riemann equations hold.\n\nIndeed, following , suppose \"f\" is a complex function defined in an open set Ω ⊂ C. Then, writing for every \"z\" ∈ Ω, one can also regard Ω as an open subset of R, and \"f\" as a function of two real variables \"x\" and \"y\", which maps Ω ⊂ R to C. We consider the Cauchy–Riemann equations at \"z\" = \"z\". So assume \"f\" is differentiable at \"z\", as a function of two real variables from Ω to C. This is equivalent to the existence of the following linear approximation\n\nwhere \"z\" = \"x\" + \"iy\" and \"η\"(Δ\"z\") → 0 as Δ\"z\" → 0. Since formula_22 and formula_23, the above can be re-written as\n\nDefining the two Wirtinger derivatives as\n\nin the limit formula_26 the above equality can be written as\n\nFor real values of \"z\", we have formula_28 and for purely imaginary \"z\" we have formula_29. Similarly, when approaching \"z\" from different directions in the complex plane, the value of formula_30 is different. But since for complex differentiability the derivative should be the same, approaching from any direction, hence \"f\" is complex differentiable at \"z\" if and only if formula_31 at formula_32. But this is exactly the Cauchy–Riemann equations, thus \"f\" is differentiable at \"z\" if and only if the Cauchy–Riemann equations hold at \"z\".\n\nThe above proof suggests another interpretation of the Cauchy–Riemann equations. The complex conjugate of \"z\", denoted formula_33, is defined by\n\nfor real \"x\" and \"y\". The Cauchy–Riemann equations can then be written as a single equation\n\nby using the Wirtinger derivative with respect to the conjugate variable. In this form, the Cauchy–Riemann equations can be interpreted as the statement that \"f\" is independent of the variable formula_33. As such, we can view analytic functions as true functions of \"one\" complex variable as opposed to complex functions of \"two\" real variables.\n\nA standard physical interpretation of the Cauchy–Riemann equations going back to Riemann's work on function theory (see ) is that \"u\" represents a velocity potential of an incompressible steady fluid flow in the plane, and \"v\" is its stream function. Suppose that the pair of (twice continuously differentiable) functions formula_37 satisfies the Cauchy–Riemann equations. We will take \"u\" to be a velocity potential, meaning that we imagine a flow of fluid in the plane such that the velocity vector of the fluid at each point of the plane is equal to the gradient of \"u\", defined by\n\nBy differentiating the Cauchy–Riemann equations a second time, one shows that \"u\" solves Laplace's equation:\nThat is, \"u\" is a harmonic function. This means that the divergence of the gradient is zero, and so the fluid is incompressible.\n\nThe function \"v\" also satisfies the Laplace equation, by a similar analysis. Also, the Cauchy–Riemann equations imply that the dot product formula_40. This implies that the gradient of \"u\" must point along the formula_41 curves; so these are the streamlines of the flow. The formula_42 curves are the equipotential curves of the flow.\n\nA holomorphic function can therefore be visualized by plotting the two families of level curves formula_42 and formula_41. Near points where the gradient of \"u\" (or, equivalently, \"v\") is not zero, these families form an orthogonal family of curves. At the points where formula_45, the stationary points of the flow, the equipotential curves of formula_42 intersect. The streamlines also intersect at the same point, bisecting the angles formed by the equipotential curves.\n\nAnother interpretation of the Cauchy–Riemann equations can be found in . Suppose that \"u\" and \"v\" satisfy the Cauchy–Riemann equations in an open subset of R, and consider the vector field\n\nregarded as a (real) two-component vector. Then the second Cauchy–Riemann equation (1b) asserts that formula_48 is irrotational (its curl is 0):\n\nThe first Cauchy–Riemann equation (1a) asserts that the vector field is solenoidal (or divergence-free):\n\nOwing respectively to Green's theorem and the divergence theorem, such a field is necessarily a conservative one, and it is free from sources or sinks, having net flux equal to zero through any open domain without holes. (These two observations combine as real and imaginary parts in Cauchy's integral theorem.) In fluid dynamics, such a vector field is a potential flow . In magnetostatics, such vector fields model static magnetic fields on a region of the plane containing no current. In electrostatics, they model static electric fields in a region of the plane containing no electric charge.\n\nThis interpretation can equivalently be restated in the language of differential forms. The pair \"u\",\"v\" satisfy the Cauchy–Riemann equations if and only if the one-form formula_51 is both closed and coclosed (a harmonic differential form).\n\nAnother formulation of the Cauchy–Riemann equations involves the complex structure in the plane, given by\nThis is a complex structure in the sense that the square of \"J\" is the negative of the 2×2 identity matrix: formula_53. As above, if \"u\"(\"x\",\"y\"),\"v\"(\"x\",\"y\") are two functions in the plane, put\n\nThe Jacobian matrix of \"f\" is the matrix of partial derivatives\n\nThen the pair of functions \"u\", \"v\" satisfies the Cauchy–Riemann equations if and only if the 2×2 matrix \"Df\" commutes with \"J\" \n\nThis interpretation is useful in symplectic geometry, where it is the starting point for the study of pseudoholomorphic curves.\n\nOther representations of the Cauchy–Riemann equations occasionally arise in other coordinate systems. If (1a) and (1b) hold for a differentiable pair of functions \"u\" and \"v\", then so do\n\nfor any coordinate system such that the pair (∇\"n\", ∇\"s\") is orthonormal and positively oriented. As a consequence, in particular, in the system of coordinates given by the polar representation , the equations then take the form\n\nCombining these into one equation for \"f\" gives\n\nThe inhomogeneous Cauchy–Riemann equations consist of the two equations for a pair of unknown functions \"u\"(\"x\",\"y\") and \"v\"(\"x\",\"y\") of two real variables\n\nfor some given functions α(\"x\",\"y\") and β(\"x\",\"y\") defined in an open subset of R. These equations are usually combined into a single equation\n\nwhere \"f\" = \"u\" + i\"v\" and \"φ\" = (\"α\" + i\"β\")/2.\n\nIf \"φ\" is \"C\", then the inhomogeneous equation is explicitly solvable in any bounded domain \"D\", provided \"φ\" is continuous on the closure of \"D\". Indeed, by the Cauchy integral formula,\n\nfor all ζ ∈ \"D\".\n\nSuppose that is a complex-valued function which is differentiable as a function . Then Goursat's theorem asserts that \"f\" is analytic in an open complex domain Ω if and only if it satisfies the Cauchy–Riemann equation in the domain . In particular, continuous differentiability of \"f\" need not be assumed .\n\nThe hypotheses of Goursat's theorem can be weakened significantly. If is continuous in an open set Ω and the partial derivatives of \"f\" with respect to \"x\" and \"y\" exist in Ω, and satisfy the Cauchy–Riemann equations throughout Ω, then \"f\" is holomorphic (and thus analytic). This result is the Looman–Menchoff theorem.\n\nThe hypothesis that \"f\" obey the Cauchy–Riemann equations throughout the domain Ω is essential. It is possible to construct a continuous function satisfying the Cauchy–Riemann equations at a point, but which is not analytic at the point (e.g., \"f\"(\"z\") = . Similarly, some additional assumption is needed besides the Cauchy–Riemann equations (such as continuity), as the following example illustrates \n\nwhich satisfies the Cauchy–Riemann equations everywhere, but fails to be continuous at \"z\" = 0.\n\nNevertheless, if a function satisfies the Cauchy–Riemann equations in an open set in a weak sense, then the function is analytic. More precisely :\nThis is in fact a special case of a more general result on the regularity of solutions of hypoelliptic partial differential equations.\n\nThere are Cauchy–Riemann equations, appropriately generalized, in the theory of several complex variables. They form a significant overdetermined system of PDEs. As often formulated, the \"d-bar operator\" \n\nannihilates holomorphic functions. This generalizes most directly the formulation\n\nwhere\n\nViewed as conjugate harmonic functions, the Cauchy–Riemann equations are a simple example of a Bäcklund transform. More complicated, generally non-linear Bäcklund transforms, such as in the sine-Gordon equation, are of great interest in the theory of solitons and integrable systems.\n\nIn Clifford algebra the complex number formula_66 is represented as formula_67 where formula_68. The fundamental derivative operator in Clifford algebra of Complex numbers is defined as formula_69. The function formula_70 is considered analytic if and only if formula_71, which can be calculated in following way:\n\nGrouping by formula_73 and formula_74:\n\nHenceforth in traditional notation:\n\nLet Ω be an open set in the Euclidean space R. The equation for an orientation-preserving mapping formula_77 to be a conformal mapping (that is, angle-preserving) is that\nwhere \"Df\" is the Jacobian matrix, with transpose formula_79, and \"I\" denotes the identity matrix . For , this system is equivalent to the standard Cauchy–Riemann equations of complex variables, and the solutions are holomorphic functions. In dimension , this is still sometimes called the Cauchy–Riemann system, and Liouville's theorem implies, under suitable smoothness assumptions, that any such mapping is a Möbius transformation.\n\n\n\n", "id": "7583", "title": "Cauchy–Riemann equations"}
{"url": "https://en.wikipedia.org/wiki?curid=7585", "text": "Chaim Topol\n\nChaim Topol (; born September 9, 1935), mononymously known as Topol, is an Israeli theatrical and film performer, singer, actor, comedian, voice artist, writer and producer. He is best known for his role as Tevye the dairyman in the production of \"Fiddler on the Roof\" on both stage and film. He has been nominated for an Academy Award and a Tony Award, and has won two Golden Globe Awards.\n\nTopol was born in Tel Aviv in 1935 in what was then British mandated Palestine, to Rel (née Goldman) and Jacob Topol. He first practiced acting in amateur theatrical plays staged by the Israeli Army. Subsequently he established his own theatre troupe in Tel Aviv, and in 1961 he significantly contributed to the foundation of the Haifa Municipal Theatre.\n\nAmong Topol's earliest film appearances was the lead role in the 1964 film \"Sallah Shabati\" by Ephraim Kishon—a play, later adapted for film, depicting the hardships of a Mizrahi Jewish immigrant family in Israel of the early 1960s. The film was nominated for the Academy Award for Best Foreign Language Film and earned the actor the Golden Globe Award for New Star of the Year – Actor. In 1966, Topol made his first English-language screen appearance as Abou Ibn Kaqden in the big-budget Mickey Marcus biopic \"Cast a Giant Shadow\".\n\nHe came to greatest prominence in the role of Tevye the milkman in the long-running musical show \"Fiddler on the Roof\", at Her Majesty's Theatre. After a major success on the West End stage (which began on 16 February 1967 and ran for 2,030 performances), he later starred in the 1971 film version. In 1972, Topol won a Golden Globe Award and was nominated for a Best Actor Oscar for his performance in the film. He was on active service with the Israeli Army at the time, but was granted permission to attend the awards ceremonies.\n\nIn 1976, Topol originated the leading role of the baker, Amiable, in the new musical \"The Baker's Wife\", but was fired after eight months by producer David Merrick. In her autobiography, \"\", his co-star in the production relayed that Topol behaved unprofessionally in front of paying audiences, sometimes speaking gibberish instead of his lines, and other times responding to the director's instructions by grossly overacting on purpose. Lupone's account was echoed by the show's composer, Stephen Schwartz, in the book \"\", in which he claimed that Topol's behavior greatly disturbed the cast and directors and resulted in the production not reaching Broadway as planned.\n\nSome of Topol's other notable film appearances were the title role in \"Galileo\" (1975), directed by Joseph Losey, Dr. Hans Zarkov in \"Flash Gordon\" (1980), and as Milos Columbo in the James Bond movie \"For Your Eyes Only\" (1981).\n\nIn 1983, he reprised the role of Tevye in a London revival of \"Fiddler on the Roof\". In the late 1980s, he played the role in a touring United States production. He was by then the approximate age of the character. Also, the actress playing his wife, Golde, in that production – Rosalind Harris – had played his eldest daughter, Tzeitel, in the film. In 1990, he again played the part in a Broadway revival of \"Fiddler\", and was nominated in 1991 for a Tony Award for Best Performance by a Leading Actor in a Musical, losing to Jonathan Pryce in \"Miss Saigon\". Topol again played Tevye in a 1994 London revival, which became a touring production. He has since played the part in various productions including stages in Europe, Australia, and Japan. His most recent film roles were in \"Left Luggage\" (1998) in the role of Mr. Apfelschnitt, and \"Time Elevator\" (1998) as Shalem.\n\nIn November 2005, Topol reprised Tevye in an Australian production at the Capitol Theatre in Sydney. In April 2006, he followed Sydney with a Brisbane production at the Lyric Theatre. Finally, in June of that year, he brought his Tevye to Her Majesty's Theatre in Melbourne. In April 2007, he played the role in Wellington, New Zealand.\n\nIn September 2008, Topol played the part of Honore in \"Gigi\" at the Open Air Theatre in Regent's Park, London.\n\nOn January 20, 2009, Topol began a farewell tour of \"Fiddler on the Roof\" as Tevye, opening in Wilmington, Delaware, USA. He was forced to withdraw from the tour owing to a shoulder injury, and made his last appearance as Tevye in Boston, Massachusetts on November 15, 2009. (Theodore Bikel and Harvey Fierstein, both of whom have portrayed Tevye on Broadway, replaced him in scheduled appearances.)\n\nHis autobiography, \"Chaim Topol on Topol\" (aka \"Topol by Topol\"), was published in London (September 1981) and Israel (1983).\n\nTopol is also an illustrator, responsible for drawings in several books, including \"A Treasury of Jewish Humour\".\n\nTopol serves as chairman of the board of Jordan River Village.\n\n", "id": "7585", "title": "Chaim Topol"}
{"url": "https://en.wikipedia.org/wiki?curid=7586", "text": "Christadelphians\n\nThe Christadelphians () are a millenarian Christian group who hold a view of Biblical Unitarianism. There are approximately 50,000 Christadelphians in around 120 countries. The movement developed in the United Kingdom and North America in the 19th century around the teachings of John Thomas, who coined the name \"Christadelphian\" from the Greek for \"Brethren in Christ\".\n\nBasing their beliefs solely on the Bible, Christadelphians differ from mainstream Christianity in a number of doctrinal areas. For example, they reject the Trinity and the immortality of the soul, believing these to be corruptions of original Christian teaching. They were initially found predominantly in the developed English-speaking world, but expanded in developing countries after the Second World War. Congregations are traditionally referred to as 'ecclesias' and would not use the word 'church' due to its association with mainstream Christianity, although today it is more acceptable.\n\nThe Christadelphian religious group traces its origins to John Thomas (1805–1871), who emigrated to North America from England in 1832. Following a near shipwreck he vowed to find out the truth about life and God through personal Biblical study. Initially he sought to avoid the kind of sectarianism he had seen in England. In this he found sympathy with the rapidly emerging Restoration Movement in the US at the time. This movement sought a reform based upon the Bible alone as a sufficient guide and rejected all creeds. However, this liberality eventually led to dissent as John Thomas developed his personal beliefs and began to question mainstream orthodox Christian beliefs. Whilst the Restoration Movement accepted Thomas's right to have his own beliefs, when he started preaching that they were essential to salvation, it led to a fierce series of debates with a notable leader of the movement, Alexander Campbell. John Thomas believed that scripture, as God's word, did not support a multiplicity of differing beliefs, and challenged the leaders to continue with the process of restoring 1st-century Christian beliefs and correct interpretation through a process of debate. The history of this process appears in the book \"Dr. Thomas, His Life and Work\" (1873) by a Christadelphian, Robert Roberts.\n\nDuring this period of formulating his ideas John Thomas was baptised twice, the second time after renouncing the beliefs he previously held. He based his new position on a new appreciation for the reign of Christ on David's throne. The abjuration of his former beliefs eventually led to the Restoration Movement disfellowshipping him when he toured England and they became aware of his abjuration in the United States of America.\n\nThe Christadelphian community in Britain effectively dates from Thomas's first lecturing tour (May 1848 – October 1850). His message was particularly welcomed in Scotland, and Campbellite, Unitarian and Adventist friends separated to form groups of \"Baptised Believers\". Two thirds of ecclesias, and members, in Britain before 1864 were in Scotland. In 1849, during his tour of Britain, he completed (a decade and a half before the name \"Christadelphian\" was conceived) \"Elpis Israel\" in which he laid out his understanding of the main doctrines of the Bible. Since his medium for bringing change was print and debate, it was natural for the origins of the Christadelphian body to be associated with books and journals, such as Thomas's \"Herald of the Kingdom\".\n\nIn his desire to seek to establish Biblical truth and test orthodox Christian beliefs through independent scriptural study he was not alone. Among other churches, he had links with Adventist movement and with Benjamin Wilson (who later set up the Church of God of the Abrahamic Faith in the 1860s). In terms of his rejection of the trinity, Thomas' views had certain similarities with Unitarianism which had developed in a formal way in Europe in the 16th century (although he formally described both Unitarianism and Socinianism as \"works of the devil\" for their failure to develop his doctrine of God-manifestation). See History of Unitarianism\n\nAlthough the Christadelphian movement originated through the activities of John Thomas, he never saw himself as making his own disciples. He believed rather that he had rediscovered 1st century beliefs from the Bible alone, and sought to prove that through a process of challenge and debate and writing journals. Through that process a number of people became convinced and set up various fellowships that had sympathy with that position. Groups associated with John Thomas met under various names, including Believers, Baptised Believers, the Royal Association of Believers, Baptised Believers in the Kingdom of God, Nazarines (or Nazarenes) and The Antipas until the time of the American Civil War (1861–1865). At that time, church affiliation was required in the United States and in the Confederacy in order to register for conscientious objector status, and in 1864 Thomas chose for registration purposes the name \"Christadelphian\".\n\nThrough the teaching of John Thomas and the need in the American Civil War for a name, the Christadelphians emerged as a denomination, but they were formed into a lasting structure through a passionate follower of Thomas's interpretation of the Bible, Robert Roberts. In 1864 he began to publish \"The Ambassador of the Coming Age\" magazine. This was renamed \"The Christadelphian\" in 1869 and continues to be published under that name. Roberts was prominent in the period following the death of John Thomas in 1871, and helped craft the structures of the Christadelphian body.\n\nRobert Roberts was certain that John Thomas had rediscovered the truth. Robert Robert's life was characterised by debates over issues that arose within the fledgling organisation; some of these debates can be found in the book \"Robert Roberts—A study of his life and character\" by Islip Collyer.\n\nInitially the denomination grew in the English-speaking world, particularly in the English Midlands and in parts of North America. In the early days after the death of John Thomas the group could have moved in a number of directions. Doctrinal issues arose, debates took place and statements of faith were created and amended as other issues arose. These attempts were felt necessary by many to both settle and define a doctrinal stance for the newly emerging denomination and to keep out error. As a result of these debates, several groups separated from the main body of Christadelphians, most notably the Suffolk Street fellowship and the Unamended fellowship.\n\nThe Christadelphian position on conscientious objection came to the fore with the introduction of conscription during the First World War. Varying degrees of exemption from military service were granted to Christadelphians in the United Kingdom, Canada, Australia, New Zealand and the United States. In the Second World War, this frequently required the person seeking exemption to undertake civilian work under the direction of the authorities.\n\nDuring the Second World War the Christadelphians in Britain assisted in the Kindertransport, helping to relocate several hundred Jewish children away from Nazi persecution and founding a hostel Elpis Lodge. In Germany the small Christadelphian community founded by Albert Maier went underground from 1940–1945, and a leading brother, Albert Merz, was imprisoned as a conscientious objector and later executed.\n\nAfter the Second World War, moves were taken to try to reunite various of the earlier divisions. By the end of the 1950s, most Christadelphians had united into one community, but there are still a number of small groups who remain separate.\n\nThe post-war, and post-reunions, period saw an increase in co-operation and interaction between ecclesias, resulting in the establishment of a number of week-long Bible schools and the formation of national and international organisations such as the Christadelphian Bible Mission (for preaching and pastoral support overseas), the Christadelphian Support Network (for counselling), and the Christadelphian Meal-A-Day Fund (for charity and humanitarian work).\n\nThe period following the reunions was accompanied by expansion in the developing world, which now accounts for around 40% of Christadelphians.\n\nIn the absence of centralised organization, some differences exist amongst Christadelphians on matters of belief and practice. This is because each congregation (commonly styled 'ecclesias') is organized autonomously, typically following common practices which have altered little since the 19th century. Most ecclesias have a constitution, which includes a 'Statement of Faith', a list of 'Doctrines to be Rejected' and a formalized list of 'The Commandments of Christ'. With no central authority, individual congregations are responsible for maintaining orthodoxy in belief and practice, and the statement of faith is seen by many as useful to this end. The statement of faith acts as the official standard of most ecclesias to determine fellowship within and between ecclesias, and as the basis for co-operation between ecclesias. Congregational discipline and conflict resolution are applied using various forms of consultation, mediation, and discussion, with disfellowship (similar to excommunication) being the final response to those with unorthodox practices or beliefs.\n\nThe relative uniformity of organization and practice is undoubtedly due to the influence of a booklet, written early in Christadelphian history by Robert Roberts, called \"A Guide to the Formation and Conduct of Christadelphian Ecclesias\". It recommends a basically democratic arrangement by which congregational members elect 'brothers' to arranging and serving duties, and includes guidelines for the organization of committees, as well as conflict resolution between congregational members and between congregations. Christadelphians do not have paid ministers. Male members are assessed by the congregation for their eligibility to teach and perform other duties, which are usually assigned on a rotation basis, as opposed to having a permanently appointed preacher. Congregational governance typically follows a democratic model, with an elected arranging committee for each individual ecclesia. This unpaid committee is responsible for the day-to-day running of the ecclesia and is answerable to the rest of the ecclesia's members.\n\nInter-ecclesial organizations co-ordinate the running of, among other things, Christadelphian schools and elderly care homes, the Christadelphian Isolation League (which cares for those prevented by distance or infirmity from attending an ecclesia regularly) and the publication of .\n\nNo official membership figures are published, but the \"Columbia Encyclopedia\" gives an estimated figure of 50,000 Christadelphians. They are spread across approximately 120 countries; there are established churches (often referred to as \"ecclesias\") in many of those countries, along with isolated members. Estimates for the main centers of Christadelphian population are as follows: United Kingdom (18,000), Australia (10,653), Mozambique (7,500), Malawi (7,000), United States (6,500), Canada (3,375), New Zealand (1,785), India (1,750), Kenya (1,700), Tanzania (1,000). and Pakistan (900). Combining the estimates from the Christadelphian Bible Mission with the figures above, the numbers for each continent are as follows: Africa (21,400), Americas (10,500), Asia (4,150), Australasia (12,600), Europe (18,950). This puts the total figure at around 67,000.\n\nThe Christadelphian body consists of a number of \"fellowships\" – groups of ecclesias which associate with one another, often to the exclusion of ecclesias outside their group. They are to some degree localised. The Unamended Fellowship, for example, exists only in North America. Christadelphian fellowships have often been named after ecclesias or magazines who took a lead in developing a particular stance.\n\nThe majority of Christadelphians (around 60,000) belong to \"Central fellowship\", named after the Birmingham Central ecclesia. This was formed in 1957–1958 as a result of a reunion between the Temperance Hall and Suffolk Street fellowships in the UK. The \"Suffolk Street fellowship\" had formed in 1885 over the inspiration of the Bible. Robert Ashcroft, a leading member, wrote an article which challenged Christadelphian belief in inspiration and which, although he himself left, led to a division in the main body. One group formed a new ecclesia which later met in Suffolk Street, Birmingham. Other ecclesias throughout the world which supported them became known as the \"Suffolk Street fellowship\" to distinguish them from the group they had separated from, which became known as the \"Temperance Hall fellowship\". The main magazine of this group from 1884–1957 was \"The Fraternal Visitor\", whose editors included J.J. Bishop and J.J. Hadley (d. 1912), then Thomas Turner, and finally Cyril Cooper (till reunion in 1957). Reunion in the United Kingdom (Temperance Hall-Suffolk Street) was closely followed by reunion in Australia in 1958 between the Shield fellowship (aligned with the Suffolk Street fellowship) and the Central fellowship on the basis of an understanding of the atonement expressed in a document called the Cooper-Carter Addendum (to the BASF). The Central fellowship in North America is often referred to as the \"Amended fellowship\".\n\nThe \"Unamended fellowship\", consisting of around 1,850 members, is found in East Coast and Midwest USA and Ontario, Canada. This group separated in 1898 as a result of differing views on who would be raised to judgment at the return of Christ. The majority of Christadelphians believe that the judgment will include anyone who had sufficient knowledge of the gospel message, and is not limited to baptized believers. The majority in Britain, Australia and North America amended their statement of faith accordingly. Those who opposed the amendment became known as the \"Unamended fellowship\" and allowed the teaching that God either could not or would not raise those who had no covenant relationship with him. Opinions vary as to what the established position was on this subject prior to the controversy. Prominent in the formation of the Unamended fellowship was Thomas Williams, editor of the Christadelphian Advocate magazine. The majority of the Unamended Fellowship outside North America joined the Suffolk Street fellowship before its eventual incorporation into Central fellowship. There is also some co-operation between the Central (Amended) and Unamended Fellowships in North America – most recently in the Great Lakes region, where numerous Amended and Unamended ecclesias have opened fellowship to one another despite the failure of wider attempts at re-union under the North American Statement of Understanding (NASU).\n\nThe \"Berean Fellowship\" was formed in 1923 as a result of varying views on military service in Britain, and on the atonement in North America. The majority of the North American Bereans re-joined the main body of Christadelphians in 1952. A number continue as a separate community, numbering around 200 in Texas, 100 in Kenya and 30 in Wales. Most of the divisions still in existence within the Christadelphian community today stem from further divisions of the Berean fellowship.\n\nIn 1942 the Berean fellowship divided over marriage and divorce with the stricter party forming the \"Dawn fellowship\". Following union with the Lightstand fellowship in Australia in November 2007, there are now 800 members in the UK, Australia, Canada, India, Jamaica, Poland, the Philippines and Russia.\n\nThe \"Old Paths fellowship\" was formed in the 1957 by those in the Temperance Hall fellowship who held that the reasons for separation from the Suffolk Street fellowship remained and opposed the re-union. There are around 250 members in the UK, and 150 in Australasia.\n\nOther Christadelphian Fellowships,with various numbers of members, include the \"Watchman fellowship\", the \"Companion fellowship\" and the \"Pioneer Fellowship\".\n\nThe Church of God of the Abrahamic Faith (CGAF) has common origins with Christadelphians and shares Christadelphian beliefs. Numbering around 400 (primarily Ohio and Florida, USA), they are welcomed into fellowship by some Christadelphians and are currently involved in unity talks.\n\nAccording to Bryan Wilson, functionally the definition of a \"fellowship\" within Christadelphian history has been mutual or unilateral exclusion of groupings of ecclesias from the breaking of bread. This functional definition still holds true in North America, where the Unamended fellowship and the Church of God of the Abrahamic Faith are not received by most North American Amended ecclesias. But outside North America this functional definition no longer holds. Many articles and books on the doctrine and practice of fellowship now reject the notion itself of separate \"fellowships\" among those who recognise the same baptism, viewing such separations as schismatic. Many ecclesias in the Central fellowship would not refuse a baptised Christadelphian from a minority fellowship from breaking bread; the exclusion is more usually the other way.\n\nThey tend to operate organisationally fairly similarly, although there are different emphases. Despite their differences, the Central, Old Paths, Dawn and Berean fellowships generally subscribe to the \"Birmingham Amended Statement of Faith\" (BASF), though the latter two have additional clauses or supporting documents to explain their position. Most Unamended ecclesias use the \"Birmingham Unamended Statement of Faith\" (BUSF) with one clause being different. Within the Central fellowship individual ecclesias also may have their own statement of faith, whilst still accepting the statement of faith of the larger community. Some ecclesias have statements around their positions, especially on divorce and re-marriage, making clear that offence would be caused by anyone in that position seeking to join them at the 'Breaking of Bread' service. Others tolerate a degree of divergence from commonly held Christadelphian views.\n\nFor each fellowship, anyone who publicly assents to the doctrines described in the statement and is in good standing in their \"home ecclesia\" is generally welcome to participate in the activities of any other ecclesia.\n\nDue to the way the Christadelphian body is organised there is no central authority to establish and maintain a standardised set of beliefs and it depends what statement of faith is adhered to and how liberal the ecclesia is, but there are core doctrines most Christadelphians would accept. In the formal statements of faith a more complete list is found. For instance in the Central fellowship, the BASF, the standard statement of faith, has 30 doctrines to be accepted and 35 to be rejected.\n\nChristadelphians state that their beliefs are based wholly on the Bible, and they do not see other works as inspired by God. They regard the Bible as inspired by God and, therefore, believe that, in its original form, it is error free and errors in later copies are due to errors of transcription or translation. Based on this, Christadelphians teach what they believe as true Bible teaching.\n\nChristadelphians believe that God is the creator of all things and the father of true believers, that he is a separate being from his son, Jesus Christ, and that the Holy Spirit is the power of God used in creation and for salvation. They also believe that the phrase \"Holy Spirit\" sometimes refers to God's character/mind, depending on the context in which the phrase appears, but reject the view that we need strength, guidance and power from the Holy Spirit to live the Christian life, believing instead that the spirit a believer needs within themselves is the mind/character of God, which is developed in a believer by their reading of the Bible (which, they believe, contains words God gave by his Spirit) and trying to live by what it says during the events of their lives which God uses to help shape their character.\n\nChristadelphians believe that Jesus is the promised Jewish Messiah, in whom the prophecies and promises of the Old Testament find their fulfilment. They believe he is the Son of Man, in that he inherited human nature (with its inclination to sin) from his mother, and the Son of God by virtue of his miraculous conception by the power of God. Although he was tempted, Jesus committed no sin, and was therefore a perfect representative sacrifice to bring salvation to sinful humankind. They believe that God raised Jesus from death and gave him immortality, and he ascended to Heaven, God's dwelling place. Christadelphians believe that he will return to the earth in person to set up the Kingdom of God in fulfilment of the promises made to Abraham and David. This includes the belief that the coming Kingdom will be the restoration of God's first Kingdom of Israel, which was under David and Solomon. For Christadelphians, this is the focal point of the gospel taught by Jesus and the apostles.\n\nChristadelphians believe that the Satan or Devil is not an independent spiritual being or fallen angel. \"Devil\" is viewed as the general principle of evil and inclination to sin which resides in humankind. They are convinced that, dependent on the context, the term \"Satan\" in Hebrew merely means \"opponent\" or \"adversary\" and is frequently applied to human beings. Accordingly, they do not define hell as a place of eternal torment for sinners, but as a state of eternal death respectively non-existence due to annihilation of body and mind.\n\nChristadelphians believe that people are separated from God because of their sins but that humankind can be reconciled to him by becoming disciples of Jesus Christ. This is by belief in the gospel, through repentance, and through baptism by total immersion in water. They do not believe we can be sure of being saved, believing instead that salvation comes as a result of remaining \"in Christ\". Christadelphians do not believe in salvation by works, but that \"by grace you have been saved through faith. And this is not your own doing; it is the gift of God\" (Ephesians 2:8). After death, believers are in a state of non-existence, knowing nothing until the Resurrection at the return of Christ. Following the judgement at that time, the accepted receive the gift of immortality, and live with Christ on a restored Earth, assisting him to establish the Kingdom of God and to rule over the mortal population for a thousand years (the Millennium). Christadelphians believe that the Kingdom will be centred upon Israel, but Jesus Christ will also reign over all the other nations on the earth. Some believe that the Kingdom itself is not worldwide but limited to the land of Israel promised to Abraham and ruled over in the past by David, with a worldwide empire.\n\nThe historic \"\" demonstrates the community's recognition of the importance of Biblical teaching on morality. Marriage and family life are important. Christadelphians believe that sexual relationships should be limited to heterosexual marriage, ideally between baptised believers.\n\nChristadelphians reject a number of doctrines held by many other Christians, notably the immortality of the soul (see also mortalism; conditionalism), trinitarianism, the personal pre-existence of Christ, the baptism of infants, the personhood of the Holy Spirit the divinity of Jesus and the present-day possession of the gifts of the Holy Spirit (see cessationism). They believe that the word \"devil\" is a reference in the scriptures to sin and human nature in opposition to God, while the word \"satan\" is merely a reference to an adversary (be it good or bad). According to Christadelphians, these terms are used in reference to specific political systems or individuals in opposition or conflict. \"Hell\" (Hebrew: Sheol; Greek: Hades, Gehenna) is understood to refer exclusively to death and the grave, rather than being a place of everlasting torment (see also annihilationism). Christadelphians do not believe that anyone will \"go to Heaven\" upon death. Instead, they believe that only Christ Jesus went to Heaven, and when he comes back to the earth there will be a resurrection and God's kingdom will be established on earth, starting in the land of Israel. Christadelphians believe the doctrines they reject were introduced into Christendom after the 1st century in large part through exposure to pagan Greek philosophy, and cannot be substantiated from the Biblical texts.\n\nOne criticism of the Christadelphian movement has been over the claim of John Thomas and Robert Roberts to have \"rediscovered\" scriptural truth. However one might argue that \"all\" Protestant groups make the same claims to some extent. Although both men believed that they had \"recovered\" the true doctrines for themselves and contemporaries, they also believed there had always existed a group of true believers throughout the ages, albeit marred by the apostasy.\n\nThe most notable Christadelphian attempts to find a continuity of those with doctrinal similarities since that point have been geographer Alan Eyre's two books \"The Protesters\" (1975) and \"Brethren in Christ\" (1982) in which he shows that many individual Christadelphian doctrines had been previously believed. Eyre focused in particular on the Radical Reformation, and also among the Socinians and other early Unitarians and the English Dissenters. In this way, Eyre was able to demonstrate substantial historical precedents for individual Christadelphian teachings and practices, and believed that the Christadelphian community was the 'inheritor of a noble tradition, by which elements of the Truth were from century to century hammered out on the anvil of controversy, affliction and even anguish'. Although noting in the introduction to 'The Protestors' that 'Some recorded herein perhaps did not have \"all the truth\" — so the writer has been reminded', Eyre nevertheless claimed that the purpose of the work was to 'tell how a number of little-known individuals, groups and religious communities strove to preserve or revive the original Christianity of apostolic times', and that 'In faith and outlook they were far closer to the early springing shoots of 1st-century Christianity and the penetrating spiritual challenge of Jesus himself than much that has passed for the religion of the Nazarene in the last nineteen centuries'.\n\nEyre's research has been criticized by some of his Christadelphian peers, and as a result Christadelphian commentary on the subject has subsequently been more cautious and circumspect, with caveats being issued concerning Eyre's claims, and the two books less used and publicized than in previous years.\n\nNevertheless, even with most source writings of those later considered \"heretics\" destroyed, evidence can be provided that since the 1st century CE there have been various groups and individuals who have held certain individual Christadelphian beliefs or similar ones. For example, all the distinctive Christadelphian doctrines (with the exception of the non-literal devil), down to interpretations of specific verses, can be found particularly among 16th century Socinian writers (e.g. the rejection of the doctrines of the trinity, pre-existence of Christ, immortal souls, a literal hell of fire, original sin). Early English Unitarian writings also correspond closely to those of Christadelphians. Also, recent discoveries and research have shown a large similarity between Christadelphian beliefs and those held by Isaac Newton who, among other things, rejected the doctrines of the trinity, immortal souls, a personal devil and literal demons. Further examples are as follows:\n\nOrganised worship in England for those whose beliefs anticipated those of Christadelphians only truly became possible in 1779 when the Act of Toleration 1689 was amended to permit denial of the Trinity, and only fully when property penalties were removed in the Doctrine of the Trinity Act 1813. This is only 35 years before John Thomas' 1849 lecture tour in Britain which attracted significant support from an existing non-Trinitarian Adventist base, particularly, initially, in Scotland where Arian, Socinian, and unitarian (with a small 'u' as distinct from the Unitarian Church of Theophilus Lindsey) views were prevalent.\n\nChristadelphians are organised into local congregations, that commonly call themselves \"ecclesias\", which is taken from usage in the New Testament and is Greek for \"gathering of those summoned\". Congregational worship, which usually takes place on Sunday, centres on the remembrance of the death and celebration of the resurrection of Jesus Christ by the taking part in the \"memorial service\". Additional meetings are often organised for worship, prayer, preaching and Bible study.\n\nEcclesias are typically involved in preaching the gospel (evangelism) in the form of public lectures on Bible teaching, college-style seminars on reading the Bible, and Bible Reading Groups. Correspondence courses are also used widely, particularly in areas where there is no established Christadelphian presence. Some ecclesias, organisations or individuals also preach through other media like video, podcasts and internet forums. There are also a number of Bible Education/Learning Centres around the world.\n\nOnly baptised (by complete immersion in water) believers are considered members of the ecclesia. Ordinarily, baptism follows someone making a \"good confession\" (cf. 1 Tim. 6:12) of their faith before two or three nominated elders of the ecclesia they are seeking to join. The good confession has to demonstrate a basic understanding of the main elements – \"first principles\" – of the faith of the community. The children of members are encouraged to attend Christadelphian Sunday Schools and youth groups. Interaction between youth from different ecclesias is encouraged through regional and national youth gatherings. Many ecclesias organise holidays for young people, the most popular form in the UK being camping holidays and Youth Weekends such as Swanwick and others locally organised by different ecclesias.\n\nChristadelphians understand the Bible to teach that male and female believers are equal in God's sight, and also that there is a distinction between the roles of male and female members. Women are typically not eligible to teach in formal gatherings of the ecclesia when male believers are present, are expected to cover their heads (using hat or scarf, etc.) during formal services, and do not sit on the main ecclesial arranging (organising) committees. They do, however: participate in other ecclesial and inter-ecclesial committees; participate in discussions; teach children in Sunday Schools as well as at home, teach other women and non-members; perform music; discuss and vote on business matters; and engage in the majority of other activities. Generally, at formal ecclesial and inter-ecclesial meetings the women wear head coverings when there are acts of worship and prayer.\n\nThere are ecclesially-accountable committees for co-ordinated preaching, youth and Sunday School work, conscientious objection issues, care of the elderly, and humanitarian work. These do not have any legislative authority, and are wholly dependent upon ecclesial support. Ecclesias in an area may regularly hold joint activities combining youth groups, fellowship, preaching, and Bible study.\n\nChristadelphians refuse to participate in any military (and police forces) because they are conscientious objectors.\n\nMost Christadelphians do not vote in political elections, as they take direction from Romans 13:1–4, which they interpret as meaning that God puts into power those leaders He deems worthy. To vote for a candidate that does not win an election would be considered to vote against God's will. To avoid the risk of such conflict, Christadelphians abstain from voting.\n\nThere is a strong emphasis on personal Bible reading and study and many Christadelphians use the Bible Companion to help them systematically read the Bible each year.\n\nChristadelphians are a non-liturgical denomination. Christadelphian ecclesias are autonomous and free to adopt whatever pattern of worship they choose. However, in the English-speaking world, there tends to be a great deal of uniformity in order of service and hymnody.\n\nChristadelphian hymnody makes considerable use of the hymns of the Anglican and British Protestant traditions (even in US ecclesias the hymnody is typically more British than American). In many Christadelphian hymn books a sizeable proportion of hymns are drawn from the Scottish Psalter and non-Christadelphian hymn-writers including Isaac Watts, Charles Wesley, William Cowper and John Newton. Despite incorporating non-Christadelphian hymns however, Christadelphian hymnody preserves the essential teachings of the community.\n\nThe earliest hymn book published was the \"Sacred Melodist\" which was published by Benjamin Wilson in Geneva, Illinois in 1860. The next was the hymn book published for the use of \"Baptised Believers in the Kingdom of God\" (an early name for Christadelphians) by George Dowie in Edinburgh in 1864. In 1865 Robert Roberts published a collection of Scottish psalms and hymns called \"The Golden Harp\" (which was subtitled \"Psalms, Hymns, and Spiritual Songs, compiled for the use of Immersed Believers in 'The Things concerning the Kingdom of God and the Name of Jesus Christ'\"). This was replaced only five years later by the first \"Christadelphian Hymn Book\" (1869), compiled by J. J. and A. Andrew, and this was revised and expanded in 1874, 1932 and 1964. A thorough revision by the Christadelphian Magazine and Publishing Association resulted in the latest (2002) edition which is almost universally used by English-speaking Christadelphian ecclesias. In addition some Christadelphian fellowships have published their own hymn books.\n\nSome ecclesias use the \"Praise the Lord\" songbook. It was produced with the aim of making contemporary songs which are consistent with Christadelphian theology more widely available. Another publication, the \"Worship\" book is a compilation of songs and hymns that have been composed only by members of the Christadelphian community. This book was produced with the aim of providing extra music for non-congregational music items within services (e.g. voluntaries, meditations, etc) but has been adopted by congregations worldwide and is now used to supplement congregational repertoire.\n\nIn the English-speaking world, worship is typically accompanied by organ or piano, though in recent years a few ecclesias have promoted the use of other instruments (e.g. strings, wind and brass as mentioned in the Psalms). This trend has also seen the emergence of some Christadelphian bands and the establishment of the Christadelphian Art Trust to support performing, visual and dramatic arts within the Christadelphian community.\n\nIn other countries, hymn books have been produced in local languages, sometimes resulting in styles of worship which reflect the local culture. It has been noted that Christadelphian hymnody has historically been a consistent witness to Christadelphian beliefs, and that hymnody occupies a significant role in the community.\n\n", "id": "7586", "title": "Christadelphians"}
{"url": "https://en.wikipedia.org/wiki?curid=7587", "text": "Cable television\n\nCable television is a system of delivering television programming to paying subscribers via radio frequency (RF) signals transmitted through coaxial cables, or in more recent systems, light pulses through fiber-optic cables. This contrasts with broadcast television, in which the television signal is transmitted over the air by radio waves and received by a television antenna attached to the television. FM radio programming, high-speed Internet, telephone services, and similar non-television services may also be provided through these cables.\nAnalog television was standard in the 20th century, but since the 2000s, cable systems have been upgraded to digital cable operation.\n\nA \"cable channel\" (sometimes known as a \"cable network\") is a television network available via cable television. When available through satellite television, including direct broadcast satellite providers such as DirecTV, Dish Network and BSkyB, as well as via IPTV providers such as Verizon FIOS and AT&T U-verse is referred to as a \"satellite channel\". Alternative terms include \"non-broadcast channel\" or \"programming service\", the latter being mainly used in legal contexts. Examples of cable/satellite channels/cable networks available in many countries are HBO, MTV, Cartoon Network, E!, Eurosport and CNN International.\n\nThe abbreviation CATV is often used for cable television. It originally stood for \"Community Access Television\" or \"Community Antenna Television\", from cable television's origins in 1948. In areas where over-the-air TV reception was limited by distance from transmitters or mountainous terrain, large \"community antennas\" were constructed, and cable was run from them to individual homes. The origins of cable \"broadcasting\" for radio are even older as radio programming was distributed by cable in some European cities as far back as 1924.\n\nCable television has gone through a series of steps of evolution in the United States and Canada. From its founding, it primarily served small communities without access to a television station, or ones with an adverse location that prevented reception of outside signals even if they were not far away, such as being in a valley. Particularly in Canada, communities with their own signals were fertile cable markets, as viewers wanted to receive American signals. Early systems carried only a maximum of seven channels, using 2, 4, 5 or 6, 7, 9, 11 and 13, as the equipment was unable to confine the signal discreetly within the assigned channel bandwidth. \n\nThe reason 4 and 5 along with 6 and 7 could be used together was because of the 4 MHz gap between 4 and 5 and the nearly 90 MHz gap between 6 and 7. Even though eight channels are listed, in early systems that maximized 7 channels, either Channel 5 or Channel 6 was eliminated from the lineup.\n\nAs equipment improved, all twelve channels could be utilized, except where a local VHF television station broadcast. Local broadcast channels were not usable for signals deemed to be priority, but technology allowed low-priority signals to be placed on such channels by synchronizing their blanking intervals. Similarly, a local VHF station could not be carried on its broadcast channel as the signals would arrive at the TV set slightly separated in time, causing \"ghosting\"\n\nThe bandwidth of the amplifiers also was limited, meaning frequencies over 250 MHz were difficult to transmit to distant portions of the coaxial network, and UHF channels could not be used at all. To expand beyond 12 channels, non-standard \"midband\" channels had to be used, located between the FM band and Channel 7, or \"superband\" beyond Channel 13 up to about 300 MHz; these channels initially were only accessible using separate tuner boxes that sent the chosen channel into the TV set on Channel 2, 3 or 4. Later, the cable operators began to carry FM radio stations, and encouraged subscribers to connect their FM stereo sets to cable. Before stereo and bilingual TV sound became common, Pay-TV channel sound was added to the FM stereo cable line-ups. About this time, operators expanded beyond the 12-channel dial to use the \"midband\" and \"superband\" VHF channels adjacent to the \"high band\" 7-13 of North American television frequencies. Some operators as in Cornwall, Ontario, used a dual distribution network with Channels 2-13 on each of the two cables.\n\nDuring the 1980s, United States regulations not unlike public, educational, and government access (PEG) created the beginning of cable-originated live television programming. As cable penetration increased, numerous cable-only TV stations were launched, many with their own news bureaus that could provide more immediate and more localized content than that provided by the nearest network newscast.\n\nSuch stations may use similar on-air branding as that used by the nearby broadcast network affiliate, but the fact that these stations do not broadcast over the air and are not regulated by the FCC, their call signs are meaningless. These stations evolved partially into today's over-the-air digital subchannels, where a main broadcast TV station e.g. NBS 37* would – in the case of no local CNB or ABS station being available – rebroadcast the programming from a nearby affiliate but fill in with its own news and other community programming to suit its own locale. Many live local programs with local interests were subsequently created all over the United States in most major television markets in the early 1980s.\n\nThis evolved into today's many cable-only broadcasts of diverse programming, including cable-only produced television movies and miniseries. Cable specialty channels, starting with channels oriented to show movies and large sporting or performance events, diversified further, and \"narrowcasting\" became common. By the late 1980s, cable-only signals outnumbered broadcast signals on cable systems, some of which by this time had expanded beyond 35 channels. By the mid-1980s in Canada, cable operators were allowed by the regulator to enter into distribution contracts with cable networks on their own. \n\nBy the 1990s, tiers became common, with customers able to subscribe to different tiers to obtain different selections of additional channels above the basic selection. By subscribing to additional tiers, customers could get specialty channels, movie channels, and foreign channels. Large cable companies used addressable descramblers to limit access to premium channels for customers not subscribing to higher tiers.\n\nDuring the 1990s, the pressure to accommodate the growing array of offerings resulted in digital transmission that made more efficient use of the VHF signal capacity; fibre optics was common to carry signals into areas near the home, where coax could carry higher frequencies over the short remaining distance. Although for a time in the 1980s and 1990s, television receivers and VCRs were equipped to receive the mid-band and super-band channels. Due to the fact that the descrambling circuitry was for a time present in these tuners, depriving the cable operator of much of their revenue, such cable-ready tuners are rarely used now - requiring a return to the set-top boxes used from the 1970s onward. \n\nThe conversion to digital broadcasting has put all signals - broadcast and cable - into digital form, rendering analog cable television service mostly obsolete, functional in an ever-dwindling supply of select markets. Analog television sets are still accommodated, but their tuners are mostly obsolete; oftentimes, dependent entirely on the set-top box.\n\nTo receive cable television at a given location, cable distribution lines must be available on the local utility poles or underground utility lines. Coaxial cable brings the signal to the customer's building through a \"service drop\", an overhead or underground cable. If the subscriber's building does not have a cable service drop, the cable company will install one. The standard cable used in the U.S. is RG-6, which has a 75 ohm impedance, and connects with a type F connector. The cable company's portion of the wiring usually ends at a distribution box on the building exterior, and built-in cable wiring in the walls usually distributes the signal to jacks in different rooms to which televisions are connected. Multiple cables to different rooms are split off the incoming cable with a small device called a splitter. There are two standards for cable television; older analog cable, and newer digital cable which can carry data signals used by digital television receivers such as HDTV equipment. All cable companies in the United States have switched to or are in the course of switching to digital cable television since it was first introduced in the late 1990s.\n\nMost cable companies require a set-top box to view their cable channels, even on newer televisions with digital cable QAM tuners, because most digital cable channels are now encrypted, or \"scrambled\", to reduce cable service theft. A cable from the jack in the wall is attached to the input of the box, and an output cable from the box is attached to the television, usually the RF-IN or composite input on older TVs. Since the set-top box only decodes the single channel that is being watched, each television in the house requires a separate box. Some unencrypted channels, usually traditional over-the-air broadcast networks, can be displayed without a receiver box. The cable company will provide set top boxes based on the level of service a customer purchases, from basic set top boxes with a standard definition picture connected through the standard coaxial connection on the TV, to high-definition wireless DVR receivers connected via HDMI or component. Older analog television sets are \"cable ready\" and can receive the old analog cable without a set-top box. To receive digital cable channels on an analog television set, even unencrypted ones, requires a different type of box, a digital television adapter supplied by the cable company. A new distribution method that takes advantage of the low cost high quality DVB distribution to residential areas, uses TV gateways to convert the DVB-C, DVB-C2 stream to IP for distribution of TV over IP network in the home.\n\nIn the most common system, multiple television channels (as many as 500, although this varies depending on the provider's available channel capacity) are distributed to subscriber residences through a coaxial cable, which comes from a trunkline supported on utility poles originating at the cable company's local distribution facility, called the \"headend\". Many channels can be transmitted through one coaxial cable by a technique called frequency division multiplexing. At the headend, each television channel is translated to a different frequency. By giving each channel a different frequency \"slot\" on the cable, the separate television signals do not interfere with each other. The main cable meets lines from the subscriber's residence in an outdoor cable box, and either the subscriber's television or a set-top box provided by the cable company translates the desired channel back to its original frequency (baseband), and it is displayed onscreen. Due to widespread cable theft in earlier analog systems, the signals are typically encrypted on modern digital cable systems, and the set-top box must be activated by an activation code sent by the cable company before it will function, which is only sent after the subscriber signs up. \n\nThere are also usually \"upstream\" channels on the cable to send data from the customer box to the cable headend, for advanced features such as requesting pay-per-view shows or movies, cable internet access, and cable telephone service. The \"downstream\" channels occupy a band of frequencies from approximately 50 MHz to 1 GHz, while the \"upstream\" channels occupy frequencies of 5 to 42 MHz. Subscribers pay with a monthly fee. Subscribers can choose from several levels of service, with \"premium\" packages including more channels but costing a higher rate. At the local headend, the feed signals from the individual television channels are received by dish antennas from communication satellites. Additional local channels, such as local broadcast television stations, educational channels from local colleges, and community access channels devoted to local governments (PEG channels) are usually included on the cable service. Commercial advertisements for local business are also inserted in the programming at the headend (the individual channels, which are distributed nationally, also have their own nationally oriented commercials).\n\nModern cable systems are large, with a single network and headend often serving an entire metropolitan area. Most systems use hybrid fiber-coaxial (HFC) distribution; this means the trunklines that carry the signal from the headend to local neighborhoods are optical fiber to provide greater bandwidth and also extra capacity for future expansion. At the headend, the radio frequency electrical signal carrying all the channels is modulated on a light beam and sent through the fiber. The fiber trunkline goes to several \"distribution hubs\", from which multiple fibers fan out to carry the signal to boxes called \"optical nodes\" in local communities. At the optical node, the light beam from the fiber is translated back to an electrical signal and carried by coaxial cable distribution lines on utility poles, from which cables branch out to subscriber residences.\n\nCable television is mostly available in North America, Europe, Australia and East Asia, and less so in South America and the Middle East. Cable television has had little success in Africa, as it is not cost-effective to lay cables in sparsely populated areas. So-called \"wireless cable\" or microwave-based systems are used instead.\n\nCoaxial cables are capable of bi-directional carriage of signals as well as the transmission of large amounts of data. Cable television signals use only a portion of the bandwidth available over coaxial lines. This leaves plenty of space available for other digital services such as cable internet, cable telephony and wireless services, using both unlicensed and licensed spectrum. Broadband internet access is achieved over coaxial cable by using cable modems to convert the network data into a type of digital signal that can be transferred over coaxial cable. One problem with some cable systems is the older amplifiers placed along the cable routes are unidirectional thus in order to allow for uploading of data the customer would need to use an analog telephone modem to provide for the upstream connection. This limited the upstream speed to 31.2k and prevented the always-on convenience broadband internet typically provides. Many large cable systems have upgraded or are upgrading their equipment to allow for bi-directional signals, thus allowing for greater upload speed and always-on convenience, though these upgrades are expensive.\n\nIn North America, Australia and Europe, many cable operators have already introduced cable telephone service, which operates just like existing fixed line operators. This service involves installing a special telephone interface at the customer's premises that converts the analog signals from the customer's in-home wiring into a digital signal, which is then sent on the local loop (replacing the analog last mile, or plain old telephone service (POTS)) to the company's switching center, where it is connected to the public switched telephone network (PSTN). The biggest obstacle to cable telephone service is the need for nearly 100% reliable service for emergency calls. One of the standards available for digital cable telephony, PacketCable, seems to be the most promising and able to work with the quality of service (QOS) demands of traditional analog plain old telephone service (POTS) service. The biggest advantage to digital cable telephone service is similar to the advantage of digital cable, namely that data can be compressed, resulting in much less bandwidth used than a dedicated analog circuit-switched service. Other advantages include better voice quality and integration to a Voice over Internet Protocol (VoIP) network providing cheap or unlimited nationwide and international calling. In many cases, digital cable telephone service is separate from cable modem service being offered by many cable companies and does not rely on Internet Protocol (IP) traffic or the Internet.\n\nTraditional cable television providers and traditional telecommunication companies increasingly compete in providing voice, video and data services to residences. The combination of television, telephone and Internet access is commonly called \"triple play\", regardless of whether CATV or telcos offer it.\n\n", "id": "7587", "title": "Cable television"}
{"url": "https://en.wikipedia.org/wiki?curid=7591", "text": "Cholera\n\nCholera is an infection of the small intestine by some strains of the bacterium \"Vibrio cholerae\". Symptoms may range from none, to mild, to severe. The classic symptom is large amounts of watery diarrhea that lasts a few days. Vomiting and muscle cramps may also occur. Diarrhea can be so severe that it leads within hours to severe dehydration and electrolyte imbalance. This may result in sunken eyes, cold skin, decreased skin elasticity, and wrinkling of the hands and feet. The dehydration may result in the skin turning bluish. Symptoms start two hours to five days after exposure.\nCholera is caused by a number of types of \"Vibrio cholerae\", with some types producing more severe disease than others. It is spread mostly by unsafe water and unsafe food that has been contaminated with human feces containing the bacteria. Undercooked seafood is a common source. Humans are the only animal affected. Risk factors for the disease include poor sanitation, not enough clean drinking water, and poverty. There are concerns that rising sea levels will increase rates of disease. Cholera can be diagnosed by a stool test. A rapid dipstick test is available but is not as accurate.\nPrevention involves improved sanitation and access to clean water. Cholera vaccines that are given by mouth provide reasonable protection for about six months. They have the added benefit of protecting against another type of diarrhea caused by \"E. coli\". The primary treatment is oral rehydration therapy—the replacement of fluids with slightly sweet and salty solutions. Rice-based solutions are preferred. Zinc supplementation is useful in children. In severe cases, intravenous fluids, such as Ringer's lactate, may be required, and antibiotics may be beneficial. Testing to see which antibiotic the cholera is susceptible to can help guide the choice.\nCholera affects an estimated 3–5 million people worldwide and causes 58,000–130,000 deaths a year as of 2010. While it is currently classified as a pandemic, it is rare in the developed world. Children are mostly affected. Cholera occurs as both outbreaks and chronically in certain areas. Areas with an ongoing risk of disease include Africa and south-east Asia. While the risk of death among those affected is usually less than 5%, it may be as high as 50% among some groups who do not have access to treatment. Historical descriptions of cholera are found as early as the 5th century BC in Sanskrit. The study of cholera by John Snow between 1849 and 1854 led to significant advances in the field of epidemiology.\n\nThe primary symptoms of cholera are profuse diarrhea and vomiting of clear fluid. These symptoms usually start suddenly, half a day to five days after ingestion of the bacteria. The diarrhea is frequently described as \"rice water\" in nature and may have a fishy odor. An untreated person with cholera may produce of diarrhea a day. Severe cholera, without treatment, kills about half of affected individuals. If the severe diarrhea is not treated, it can result in life-threatening dehydration and electrolyte imbalances. Estimates of the ratio of asymptomatic to symptomatic infections have ranged from 3 to 100. Cholera has been nicknamed the \"blue death\" because a person's skin may turn bluish-gray from extreme loss of fluids.\n\nFever is rare and should raise suspicion for secondary infection. Patients can be lethargic, and might have sunken eyes, dry mouth, cold clammy skin, decreased skin turgor, or wrinkled hands and feet. Kussmaul breathing, a deep and labored breathing pattern, can occur because of acidosis from stool bicarbonate losses and lactic acidosis associated with poor perfusion. Blood pressure drops due to dehydration, peripheral pulse is rapid and thready, and urine output decreases with time. Muscle cramping and weakness, altered consciousness, seizures, or even coma due to electrolyte losses and ion shifts are common, especially in children.\n\nCholera has been found in two animal populations: shellfish and plankton.\n\nTransmission is usually through the fecal-oral route of contaminated food or water caused by poor sanitation. Most cholera cases in developed countries are a result of transmission by food, while in the developing world it is more often water. Food transmission can occur when people harvest seafood such as oysters in waters infected with sewage, as \"Vibrio cholerae\" accumulates in planktonic crustaceans and the oysters eat the zooplankton.\n\nPeople infected with cholera often have diarrhea, and disease transmission may occur if this highly liquid stool, colloquially referred to as \"rice-water\", contaminates water used by others. The source of the contamination is typically other cholera sufferers when their untreated diarrheal discharge is allowed to get into waterways, groundwater or drinking water supplies. Drinking any infected water and eating any foods washed in the water, as well as shellfish living in the affected waterway, can cause a person to contract an infection. Cholera is rarely spread directly from person to person.\n\nBoth toxic and non-toxic strains exist. Non-toxic strains can acquire toxicity through a temperate bacteriophage.\n\nAbout 100 million bacteria must typically be ingested to cause cholera in a normal healthy adult. This dose, however, is less in those with lowered gastric acidity (for instance those using proton pump inhibitors). Children are also more susceptible, with two- to four-year-olds having the highest rates of infection. Individuals' susceptibility to cholera is also affected by their blood type, with those with type O blood being the most susceptible. Persons with lowered immunity, such as persons with AIDS or children who are malnourished, are more likely to experience a severe case if they become infected. Any individual, even a healthy adult in middle age, can experience a severe case, and each person's case should be measured by the loss of fluids, preferably in consultation with a professional health care provider.\n\nThe cystic fibrosis genetic mutation known as delta-F508 in humans has been said to maintain a selective heterozygous advantage: heterozygous carriers of the mutation (who are thus not affected by cystic fibrosis) are more resistant to \"V. cholerae\" infections. In this model, the genetic deficiency in the cystic fibrosis transmembrane conductance regulator channel proteins interferes with bacteria binding to the gastrointestinal epithelium, thus reducing the effects of an infection.\n\nWhen consumed, most bacteria do not survive the acidic conditions of the human stomach. The few surviving bacteria conserve their energy and stored nutrients during the passage through the stomach by shutting down much protein production. When the surviving bacteria exit the stomach and reach the small intestine, they must propel themselves through the thick mucus that lines the small intestine to reach the intestinal walls where they can attach and thrive.\n\nOnce the cholera bacteria reach the intestinal wall they no longer need the flagella to move. The bacteria stop producing the protein flagellin to conserve energy and nutrients by changing the mix of proteins which they express in response to the changed chemical surroundings. On reaching the intestinal wall, \"V. cholerae\" start producing the toxic proteins that give the infected person a watery diarrhea. This carries the multiplying new generations of \"V. cholerae\" bacteria out into the drinking water of the next host if proper sanitation measures are not in place.\n\nThe cholera toxin (CTX or CT) is an oligomeric complex made up of six protein subunits: a single copy of the A subunit (part A), and five copies of the B subunit (part B), connected by a disulfide bond. The five B subunits form a five-membered ring that binds to GM1 gangliosides on the surface of the intestinal epithelium cells. The A1 portion of the A subunit is an enzyme that ADP-ribosylates G proteins, while the A2 chain fits into the central pore of the B subunit ring. Upon binding, the complex is taken into the cell via receptor-mediated endocytosis. Once inside the cell, the disulfide bond is reduced, and the A1 subunit is freed to bind with a human partner protein called ADP-ribosylation factor 6 (Arf6). Binding exposes its active site, allowing it to permanently ribosylate the Gs alpha subunit of the heterotrimeric G protein. This results in constitutive cAMP production, which in turn leads to secretion of HO, Na, K, Cl, and HCO into the lumen of the small intestine and rapid dehydration. The gene encoding the cholera toxin was introduced into \"V. cholerae\" by horizontal gene transfer. Virulent strains of \"V. cholerae\" carry a variant of a temperate bacteriophage called CTXφ.\n\nMicrobiologists have studied the genetic mechanisms by which the \"V. cholerae\" bacteria turn off the production of some proteins and turn on the production of other proteins as they respond to the series of chemical environments they encounter, passing through the stomach, through the mucous layer of the small intestine, and on to the intestinal wall. Of particular interest have been the genetic mechanisms by which cholera bacteria turn on the protein production of the toxins that interact with host cell mechanisms to pump chloride ions into the small intestine, creating an ionic pressure which prevents sodium ions from entering the cell. The chloride and sodium ions create a salt-water environment in the small intestines, which through osmosis can pull up to six litres of water per day through the intestinal cells, creating the massive amounts of diarrhea. The host can become rapidly dehydrated unless an appropriate mixture of dilute salt water and sugar is taken to replace the blood's water and salts lost in the diarrhea.\n\nBy inserting separate, successive sections of \"V. cholerae\" DNA into the DNA of other bacteria, such as \"E. coli\" that would not naturally produce the protein toxins, researchers have investigated the mechanisms by which \"V. cholerae\" responds to the changing chemical environments of the stomach, mucous layers, and intestinal wall. Researchers have discovered a complex cascade of regulatory proteins controls expression of \"V. cholerae\" virulence determinants. In responding to the chemical environment at the intestinal wall, the \"V. cholerae\" bacteria produce the TcpP/TcpH proteins, which, together with the ToxR/ToxS proteins, activate the expression of the ToxT regulatory protein. ToxT then directly activates expression of virulence genes that produce the toxins, causing diarrhea in the infected person and allowing the bacteria to colonize the intestine. Current research aims at discovering \"the signal that makes the cholera bacteria stop swimming and start to colonize (that is, adhere to the cells of) the small intestine.\"\n\nAmplified fragment length polymorphism fingerprinting of the pandemic isolates of \"V. cholerae\" has revealed variation in the genetic structure. Two clusters have been identified: Cluster I and Cluster II. For the most part, Cluster I consists of strains from the 1960s and 1970s, while Cluster II largely contains strains from the 1980s and 1990s, based on the change in the clone structure. This grouping of strains is best seen in the strains from the African continent.\n\nIn many areas of the world, antibiotic resistance is increasing. In Bangladesh, for example, most cases are resistant to tetracycline, trimethoprim-sulfamethoxazole, and erythromycin. Rapid diagnostic assay methods are available for the identification of multi-drug resistant cases. New generation antimicrobials have been discovered which are effective against cholera bacteria in \"in vitro\" studies.\n\nA rapid dipstick test is available to determine the presence of \"V. cholerae\". In those samples that test positive, further testing should be done to determine antibiotic resistance. In epidemic situations, a clinical diagnosis may be made by taking a patient history and doing a brief examination. Treatment is usually started without or before confirmation by laboratory analysis.\n\nStool and swab samples collected in the acute stage of the disease, before antibiotics have been administered, are the most useful specimens for laboratory diagnosis. If an epidemic of cholera is suspected, the most common causative agent is \"V. cholerae\" O1. If \"V. cholerae\" serogroup O1 is not isolated, the laboratory should test for \"V. cholerae\" O139. However, if neither of these organisms is isolated, it is necessary to send stool specimens to a reference laboratory.\n\nInfection with \"V. cholerae\" O139 should be reported and handled in the same manner as that caused by \"V. cholerae\" O1. The associated diarrheal illness should be referred to as cholera and must be reported in the United States.\n\nThe World Health Organization (WHO) recommends focusing on prevention, preparedness, and response to combat the spread of cholera. They also stress the importance of an effective surveillance system. Governments can play a role in all of these areas, and in preventing cholera or indirectly facilitating its spread.\n\nAlthough cholera may be life-threatening, prevention of the disease is normally straightforward if proper sanitation practices are followed. In developed countries, due to nearly universal advanced water treatment and sanitation practices, cholera is no longer a major health threat. The last major outbreak of cholera in the United States occurred in 1910–1911. Effective sanitation practices, if instituted and adhered to in time, are usually sufficient to stop an epidemic. There are several points along the cholera transmission path at which its spread may be halted:\n\nHandwashing with soap and/or ash after visiting toilets and before handling food or eating is also recommended for cholera prevention by WHO Africa.\n\nSurveillance and prompt reporting allow for containing cholera epidemics rapidly. Cholera exists as a seasonal disease in many endemic countries, occurring annually mostly during rainy seasons. Surveillance systems can provide early alerts to outbreaks, therefore leading to coordinated response and assist in preparation of preparedness plans. Efficient surveillance systems can also improve the risk assessment for potential cholera outbreaks. Understanding the seasonality and location of outbreaks provides guidance for improving cholera control activities for the most vulnerable. For prevention to be effective, it is important that cases be reported to national health authorities.\n\nA number of safe and effective oral vaccines for cholera are available. Dukoral, an orally administered, inactivated whole cell vaccine, has an overall efficacy of about 52% during the first year after being given and 62% in the second year, with minimal side effects. It is available in over 60 countries. However, it is not currently recommended by the Centers for Disease Control and Prevention (CDC) for most people traveling from the United States to endemic countries. The vaccine that the FDA recommends is an oral attenuated live vaccine that is effective as a single dose.\n\nOne injectable vaccine was found to be effective for two to three years. The protective efficacy was 28% lower in children less than 5 years old. However, as of 2010, it has limited availability. Work is under way to investigate the role of mass vaccination. The World Health Organization (WHO) recommends immunization of high-risk groups, such as children and people with HIV, in countries where this disease is endemic. If people are immunized broadly, herd immunity results, with a decrease in the amount of contamination in the environment.\n\nAn effective and relatively cheap method to prevent the transmission of cholera is the use of a folded \"sari\" (a long cloth garment) to filter drinking water. In Bangladesh this practice was found to decrease rates of cholera by nearly half. It involves folding a \"sari\" four to eight times. Between uses the cloth should be rinsed in clean water and dried in the sun to kill any bacteria on it. A nylon cloth appears to work as well.\n\nContinued eating speeds the recovery of normal intestinal function. The World Health Organization recommends this generally for cases of diarrhea no matter what the underlying cause. A CDC training manual specifically for cholera states: “Continue to breastfeed your baby if the baby has watery diarrhea, even when traveling to get treatment. Adults and older children should continue to eat frequently.”\n\nThe most common error in caring for patients with cholera is to underestimate the speed\nand volume of fluids required. In most cases, cholera can be successfully treated with oral rehydration therapy, which is highly effective, safe, and simple to administer. Rice-based solutions are preferred to glucose-based ones due to greater efficiency. In severe cases with significant dehydration, intravenous rehydration may be necessary. Ringer's lactate is the preferred solution, often with added potassium. Large volumes and continued replacement until diarrhea has subsided may be needed. Ten percent of a person's body weight in fluid may need to be given in the first two to four hours. This method was first tried on a mass scale during the Bangladesh Liberation War, and was found to have much success. Despite widespread beliefs, fruit juices and commercial fizzy drinks like cola, are not ideal for rehydration of people with serious infections of the intestines, and the too high sugar content may even harm water uptake.\n\nIf commercially produced oral rehydration solutions are too expensive or difficult to obtain, solutions can be made. One such recipe calls for 1 liter of boiled water, 1/2 teaspoon of salt, 6 teaspoons of sugar, and added mashed banana for potassium and to improve taste.\n\nAs there frequently is initially acidosis, the potassium level may be normal, even though large losses have occurred. As the dehydration is corrected, potassium levels may decrease rapidly, and thus need to be replaced. This may be done by eating foods high in potassium like bananas or green coconut water.\n\nAntibiotic treatments for one to three days shorten the course of the disease and reduce the severity of the symptoms. Use of antibiotics also reduces fluid requirements. People will recover without them, however, if sufficient hydration is maintained. The World Health Organization only recommends antibiotics in those with severe dehydration.\n\nDoxycycline is typically used first line, although some strains of \"V. cholerae\" have shown resistance. Testing for resistance during an outbreak can help determine appropriate future choices. Other antibiotics proven to be effective include cotrimoxazole, erythromycin, tetracycline, chloramphenicol, and furazolidone. Fluoroquinolones, such as ciprofloxacin, also may be used, but resistance has been reported.\n\nAntibiotics improve outcomes in those who are both severely and not severely dehydrated. Azithromycin and tetracycline may work better than doxycycline or ciprofloxacin.\n\nIn Bangladesh zinc supplementation reduced the duration and severity of diarrhea in children with cholera when given with antibiotics and rehydration therapy as needed. It reduced the length of disease by eight hours and the amount of diarrhea stool by 10%. Supplementation appears to be also effective in both treating and preventing infectious diarrhea due to other causes among children in the developing world.\n\nIf people with cholera are treated quickly and properly, the mortality rate is less than 1%; however, with untreated cholera, the mortality rate rises to 50–60%. For certain genetic strains of cholera, such as the one present during the 2010 epidemic in Haiti and the 2004 outbreak in India, death can occur within two hours of becoming ill.\n\nCholera affects an estimated 3–5 million people worldwide, and causes 58,000–130,000 deaths a year as of 2010. This occurs mainly in the developing world. In the early 1980s, death rates are believed to have been greater than 3 million a year. It is difficult to calculate exact numbers of cases, as many go unreported due to concerns that an outbreak may have a negative impact on the tourism of a country. Cholera remains both epidemic and endemic in many areas of the world.\n\nAlthough much is known about the mechanisms behind the spread of cholera, this has not led to a full understanding of what makes cholera outbreaks happen in some places and not others. Lack of treatment of human feces and lack of treatment of drinking water greatly facilitate its spread, but bodies of water can serve as a reservoir, and seafood shipped long distances can spread the disease. Cholera was not known in the Americas for most of the 20th century, but it reappeared towards the end of that century.\n\nThe word cholera is from \"kholera\" from χολή \"kholē\" \"bile\". Cholera likely has its origins in the Indian subcontinent as evidenced by its prevalence in the region for centuries. Early outbreaks in the Indian subcontinent are believed to have been the result of poor living conditions as well as the presence of pools of still water, both of which provide ideal conditions for cholera to thrive. The disease first spread by trade routes (land and sea) to Russia in 1817, later to the rest of Europe, and from Europe to North America and the rest of the world. Seven cholera pandemics have occurred in the past 200 years, with the seventh pandemic originating in Indonesia in 1961.\n\nThe first cholera pandemic occurred in the Bengal region of India, near Calcutta starting in 1817 through 1824. The disease dispersed from India to Southeast Asia, the Middle East, Europe, and Eastern Africa through trade routes. The second pandemic lasted from 1827 to 1835 and particularly affected North American and Europe due to the result of advancements in transportation and global trade, and increased human migration, including soldiers. The third pandemic erupted in 1839, persisted until 1856, extended to North Africa, and reached South America, for the first time specifically affecting Brazil. The fourth pandemic lasted from 1863 to 1875 spread from India to Naples and Spain. The fifth pandemic was from 1881-1896 and started in India and spread to Europe, Asia, and South America. The sixth pandemic started 1899–1923. These epidemics were less fatal due to a greater understanding of the cholera bacteria. Egypt, the Arabian peninsula, Persia, India, and the Philippines were hit hardest during these epidemics, while other areas, like Germany in 1892 and Naples from 1910–1911, also experienced severe outbreaks. The final pandemic originated in 1961 in Indonesia and is marked by the emergence of a new strain, nicknamed \"El Tor\", which still persists today in developing countries.\n\nSince it became widespread in the 19th century, cholera has killed tens of millions of people. In Russia alone, between 1847 and 1851, more than one million people perished of the disease. It killed 150,000 Americans during the second pandemic. Between 1900 and 1920, perhaps eight million people died of cholera in India.\nCholera became the first reportable disease in the United States due to the significant effects it had on health. John Snow, in England, was the first to identify the importance of contaminated water as its cause in 1854. Cholera is now no longer considered a pressing health threat in Europe and North America due to filtering and chlorination of water supplies, but still heavily affects populations in developing countries.\n\nIn the past, vessels flew a yellow quarantine flag if any crew members or passengers were suffering from cholera. No one aboard a vessel flying a yellow flag would be allowed ashore for an extended period, typically 30 to 40 days. In modern sets of international maritime signal flags, the quarantine flag is yellow and black.\n\nHistorically many different claimed remedies have existed in folklore. Many of the older remedies were based on the miasma theory. Some believed that abdominal chilling made one more susceptible and flannel and cholera belts were routine in army kits. In the 1854–1855 outbreak in Naples homeopathic Camphor was used according to Hahnemann. T. J. Ritter's \"Mother's Remedies\" book lists tomato syrup as a home remedy from northern America. Elecampane was recommended in the United Kingdom according to William Thomas Fernie\n\nCholera cases are much less frequent in developed countries where governments have helped to establish water sanitation practices and effective medical treatments. The United States, for example, used to have a severe cholera problem similar to those in some developing countries. There were three large cholera outbreaks in the 1800s, which can be attributed to \"Vibrio cholerae\"'s spread through interior waterways like the Erie Canal and routes along the Eastern Seaboard. The island of Manhattan in New York City touched the Atlantic Ocean, where cholera collected just off the coast. At this time, New York City did not have as effective a sanitation system as it does today, so cholera was able to spread.\n\nCholera morbus is a historical term that was used to refer to gastroenteritis rather than specifically cholera.\n\nThe bacterium was isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.\n\nSpanish physician Jaume Ferran i Clua developed a cholera inoculation in 1885, the first to immunize humans against a bacterial disease.\n\nRussian-Jewish bacteriologist Waldemar Haffkine developed the first cholera vaccine in July 1892.\n\nOne of the major contributions to fighting cholera was made by the physician and pioneer medical scientist John Snow (1813–1858), who in 1854 found a link between cholera and contaminated drinking water. Dr. Snow proposed a microbial origin for epidemic cholera in 1849. In his major \"state of the art\" review of 1855, he proposed a substantially complete and correct model for the cause of the disease. In two pioneering epidemiological field studies, he was able to demonstrate human sewage contamination was the most probable disease vector in two major epidemics in London in 1854. His model was not immediately accepted, but it was seen to be the more plausible, as medical microbiology developed over the next 30 years or so.\n\nCities in developed nations made massive investment in clean water supply and well-separated sewage treatment infrastructures between the mid-1850s and the 1900s. This eliminated the threat of cholera epidemics from the major developed cities in the world. In 1883, Robert Koch identified \"V. cholerae\" with a microscope as the bacillus causing the disease.\n\nRobert Allan Phillips, working at the US Naval Medical Research Unit Two in Southeast Asia, evaluated the pathophysiology of the disease using modern laboratory chemistry techniques and developed a protocol for rehydration. His research led the Lasker Foundation to award him its prize in 1967.\n\nMore recently, in 2002, Alam, \"et al.\", studied stool samples from patients at the International Centre for Diarrhoeal Disease in Dhaka, Bangladesh. From the various experiments they conducted, the researchers found a correlation between the passage of \"V. cholerae\" through the human digestive system and an increased infectivity state. Furthermore, the researchers found the bacterium creates a hyperinfected state where genes that control biosynthesis of amino acids, iron uptake systems, and formation of periplasmic nitrate reductase complexes were induced just before defecation. These induced characteristics allow the cholera vibrios to survive in the \"rice water\" stools, an environment of limited oxygen and iron, of patients with a cholera infection.\n\nIn many developing countries, cholera still reaches its victims through contaminated water sources, and countries without proper sanitation techniques have greater incidence of the disease. Governments can play a role in this. In 2008, for example, the Zimbabwean cholera outbreak was due partly to the government's role, according to a report from the James Baker Institute. The Haitian government’s inability to provide safe drinking water after the 2010 earthquake led to an increase in cholera cases as well.\n\nSimilarly, South Africa’s cholera outbreak was exacerbated by the government’s policy of privatizing water programs. The wealthy elite of the country were able to afford safe water while others had to use water from cholera-infected rivers.\n\nAccording to Rita R. Colwell of the James Baker Institute, if cholera does begin to spread, government preparedness is crucial. A government's ability to contain the disease before it extends to other areas can prevent a high death toll and the development of an epidemic or even pandemic. Effective disease surveillance can ensure that cholera outbreaks are recognized as soon as possible and dealt with appropriately. Oftentimes, this will allow public health programs to determine and control the cause of the cases, whether it is unsanitary water or seafood that have accumulated a lot of \"Vibrio cholerae\" specimens. Having an effective surveillance program contributes to a government’s ability to prevent cholera from spreading. In the year 2000 in the state of Kerala in India, the Kottayam district was determined to be \"Cholera-affected\"; this pronouncement led to task forces that concentrated on educating citizens with 13,670 information sessions about human health. These task forces promoted the boiling of water to obtain safe water, and provided chlorine and oral rehydration salts. Ultimately, this helped to control the spread of the disease to other areas and minimize deaths. On the other hand, researchers have shown that most of the citizens infected during the 1991 cholera outbreak in Bangladesh lived in rural areas, and were not recognized by the government's surveillance program. This inhibited physicians' abilities to detect cholera cases early.\n\nAccording to Colwell, the quality and inclusiveness of a country's health care system affects the control of cholera, as it did in the Zimbabwean cholera outbreak. While sanitation practices are important, when governments respond quickly and have readily available vaccines, the country will have a lower cholera death toll. Affordability of vaccines can be a problem; if the governments do not provide vaccinations, only the wealthy may be able to afford them and there will be a greater toll on the country's poor. The speed with which government leaders respond to cholera outbreaks is important.\n\nBesides contributing to an effective or declining public health care system and water sanitation treatments, government can have indirect effects on cholera control and the effectiveness of a response to cholera. A country's government can impact its ability to prevent disease and control its spread. A speedy government response backed by a fully functioning health care system and financial resources can prevent cholera's spread. This limits cholera's ability to cause death, or at the very least a decline in education, as children are kept out of school to minimize the risk of infection.\n\n\n\n", "id": "7591", "title": "Cholera"}
{"url": "https://en.wikipedia.org/wiki?curid=7592", "text": "Caldera\n\nA caldera is a large cauldron-like depression that forms following the evacuation of a magma chamber/reservoir. When large volumes of magma are erupted over a short time period, structural support for the crust above the magma chamber is lost. The ground surface then collapses downward into the partially emptied magma chamber, leaving a massive depression at the surface (from one to dozens of kilometers in diameter). Although sometimes described as a crater, the feature is actually a type of sinkhole, as it is formed through subsidence and collapse rather than an explosion or impact. Only seven known caldera-forming collapses have occurred since the start of the 20th century, most recently at Bárðarbunga volcano in Iceland.\n\nThe word comes from Spanish ', and this from Latin ', meaning \"cooking pot\". In some texts the English term \"cauldron\" is also used. The term \"caldera\" was introduced into the geological vocabulary by the German geologist Leopold von Buch when he published his memoirs of his 1815 visit to the Canary Islands, where he first saw the Las Cañadas caldera on Tenerife, with Montaña Teide dominating the landscape, and then the Caldera de Taburiente on La Palma.\n\nA collapse is triggered by the emptying of the magma chamber beneath the volcano, sometimes as the result of a large explosive volcanic eruption (see Tambora in 1815), but also during effusive eruptions on the flanks of a volcano (see Piton de la Fournaise in 2007) or in a connected fissure system (see Bárðarbunga in 2014-2015). If enough magma is ejected, the emptied chamber is unable to support the weight of the volcanic edifice above it. A roughly circular fracture, the \"ring fault\", develops around the edge of the chamber. Ring fractures serve as feeders for fault intrusions which are also known as ring dykes. Secondary volcanic vents may form above the ring fracture. As the magma chamber empties, the center of the volcano within the ring fracture begins to collapse. The collapse may occur as the result of a single cataclysmic eruption, or it may occur in stages as the result of a series of eruptions. The total area that collapses may be hundreds or thousands of square kilometers.\n\nSome calderas are known to host rich ore deposits. One of the world's best-preserved mineralized calderas is the Sturgeon Lake Caldera in northwestern Ontario, Canada, which formed during the Neoarchean era about 2,700 million years ago.\n\nIf the magma is rich in silica, the caldera is often filled in with ignimbrite, tuff, rhyolite, and other igneous rocks. Silica-rich magma has a high viscosity, and therefore does not flow easily like basalt. As a result, gases tend to become trapped at high pressure within the magma. When the magma approaches the surface of the Earth, the rapid off-loading of overlying material causes the trapped gases to decompress rapidly, thus triggering explosive destruction of the magma and spreading volcanic ash over wide areas. Further lava flows may be erupted.\n\nIf volcanic activity continues, the center of the caldera may be uplifted in the form of a \"resurgent dome\" such as is seen at Cerro Galán, Lake Toba, Yellowstone, etc., by subsequent intrusion of magma. A \"silicic\" or \"rhyolitic caldera\" may erupt hundreds or even thousands of cubic kilometers of material in a single event. Even small caldera-forming eruptions, such as Krakatoa in 1883 or Mount Pinatubo in 1991, may result in significant local destruction and a noticeable drop in temperature around the world. Large calderas may have even greater effects.\n\nWhen Yellowstone Caldera last erupted some 650,000 years ago, it released about 1,000 km of material (as measured in dense rock equivalent (DRE)), covering a substantial part of North America in up to two metres of debris. By comparison, when Mount St. Helens erupted in 1980, it released ~1.2 km (DRE) of ejecta. The ecological effects of the eruption of a large caldera can be seen in the record of the Lake Toba eruption in Indonesia.\n\nAbout 74,000 years ago, this Indonesian volcano released about 2,800 km DRE of ejecta, the largest known eruption within the Quaternary Period (last 1.8 million years) and the largest known explosive eruption within the last 25 million years. In the late 1990s, anthropologist Stanley Ambrose proposed that a volcanic winter induced by this eruption reduced the human population to about 2,000 - 20,000 individuals, resulting in a population bottleneck (\"see\" Toba catastrophe theory). More recently several geneticists, including Lynn Jorde and Henry Harpending have proposed that the human species was reduced to approximately five to ten thousand people. However, there is no direct evidence that the theory is correct. And there is no evidence for any other animal decline or extinction, even in environmentally sensitive species. There is evidence that human habitation continued in India after the eruption. The theory in its strongest form may be incorrect.\n\nEruptions forming even larger calderas are known, especially La Garita Caldera in the San Juan Mountains of Colorado, where the 5,000-km Fish Canyon Tuff was blasted out in a single major eruption about 27.8 million years ago.\n\nAt some points in geological time, rhyolitic calderas have appeared in distinct clusters. The remnants of such clusters may be found in places such as the San Juan Mountains of Colorado (formed during the Oligocene, Miocene, and Pliocene periods) or the Saint Francois Mountain Range of Missouri (erupted during the Proterozoic).\n\nSome volcanoes, such as shield volcanoes Kīlauea and Mauna Loa (respectively the most active and second largest on Earth, are both on the island of Hawaii), form calderas in a different fashion. The magma feeding these volcanoes is basalt which is silica poor. As a result, the magma is much less viscous than the magma of a rhyolitic volcano, and the magma chamber is drained by large lava flows rather than by explosive events. The resulting calderas are also known as subsidence calderas, and can form more gradually than explosive calderas. For instance, the caldera atop Fernandina Island underwent a collapse in 1968, when parts of the caldera floor dropped 350 meters. Kilauea Caldera has an inner crater known as Halema‘uma‘u, which has often been filled by a lava lake.\n\nIn April 2007, during the eruption, the summit floor of the Piton de la Fournaise on the island of Réunion the floor of the main crater suddenly dropped about 300 m. This was attributed to the withdrawal of magma which was being erupted through a vent lower down on the southern flank of the volcano.\n\nAnother process that may allow a caldera to form can occur if molten lava can escape through a breach on the caldera's rim.\n\nSince the early 1960s, it has been known that volcanism has occurred on other planets and moons in the Solar System. Through the use of manned and unmanned spacecraft, volcanism has been discovered on Venus, Mars, the Moon, and Io, a satellite of Jupiter. None of these worlds have plate tectonics, which contributes approximately 60% of the Earth's volcanic activity (the other 40% is attributed to hotspot volcanism). Caldera structure is similar on all of these planetary bodies, though the size varies considerably. The average caldera diameter on Venus is 68 km. The average caldera diameter on Io is close to 40 km, and the mode is 6 km; Tvashtar Paterae is likely the largest caldera with a diameter of 290 km. The average caldera diameter on Mars is 48 km, smaller than Venus. Calderas on Earth are the smallest of all planetary bodies and vary from 1.6 to 80 km as a maximum.\n\nThe Moon has an outer shell of low-density crystalline rock that is a few hundred kilometers thick, which formed due to a rapid creation. The craters of the moon have been well preserved through time and were once thought to have been the result of extreme volcanic activity, but actually were formed by meteorites, nearly all of which took place in the first few hundred million years after the Moon formed. Around 500 million years afterward, the Moon's mantle was able to be extensively melted due to the decay of radioactive elements. Massive basaltic eruptions took place generally at the base of large impact craters. Also, eruptions may have taken place due to a magma reservoir at the base of the crust. This forms a dome, possibly the same morphology of a shield volcano where calderas universally are known to form. Although caldera-like structures are rare on the Moon, they are not completely absent. The Compton-Belkovich Volcanic Complex on the far side of the Moon is thought to be a caldera, possibly an ash-flow caldera.\n\nThe volcanic activity of Mars is concentrated in two major provinces: Tharsis and Elysium. Each province contains a series of giant shield volcanoes that are similar to what we see on Earth and likely are the result of mantle hot spots. The surfaces are dominated by lava flows, and all have one or more collapse calderas. Mars has the largest volcano in the Solar System, Olympus Mons, which is more than three times the height of Mount Everest, with a diameter of 520 km (323 miles). The summit of the mountain has six nested calderas.\n\nBecause there is no plate tectonics on Venus, heat is mainly lost by conduction through the lithosphere. This causes enormous lava flows, accounting for 80% of Venus' surface area. Many of the mountains are large shield volcanoes that range in size from 150–400 km in diameter and 2–4 km high. More than 80 of these large shield volcanoes have summit calderas averaging 60 km across.\n\nIo, unusually, is heated by solid flexing due to the tidal influence of Jupiter and Io's orbital resonance with neighboring large moons Europa and Ganymede, which keeps its orbit slightly eccentric. Unlike any of the planets mentioned, Io is continuously volcanically active. For example, the NASA \"Voyager 1\" and \"Voyager 2\" spacecraft detected nine erupting volcanoes while passing Io in 1979. Io has many calderas with diameters tens of kilometers across.\n\n\n\n\n\n", "id": "7592", "title": "Caldera"}
{"url": "https://en.wikipedia.org/wiki?curid=7593", "text": "Calculator\n\nAn electronic calculator is a small, portable electronic device used to perform calculations, ranging from basic arithmetic to complex mathematics.\n\nThe first solid state electronic calculator was created in the 1960s, building on the extensive history of tools such as the abacus (developed around 2000 BC), and the mechanical calculator (developed in the 17th century AD). It was developed in parallel with the analog computers of the day.\n\nThe pocket sized devices became available in the 1970s, especially after the first microprocessor, the Intel 4004, developed by Intel for the Japanese calculator company Busicom. They later became used commonly within the petroleum industry (oil and gas).\n\nModern electronic calculators vary: from cheap, give-away, credit-card-sized models to sturdy desktop models with built-in printers. They became popular in the mid-1970s (as integrated circuits made their size and cost small). By the end of that decade, calculator prices had reduced to a point where a basic calculator was affordable to most and they became common in schools.\n\nComputer operating systems as far back as early Unix have included interactive calculator programs such as dc and hoc, and calculator functions are included in almost all personal digital assistant (PDA) type devices (save a few dedicated address book and dictionary devices).\n\nIn addition to general purpose calculators, there are those designed for specific markets. For example, there are scientific calculators which include trigonometric and statistical calculations. Some calculators even have the ability to do computer algebra. Graphing calculators can be used to graph functions defined on the real line, or higher-dimensional Euclidean space. , basic calculators cost little, but the scientific and graphing models tend to cost more. \n\nIn 1986, calculators still represented an estimated 41% of the world's general-purpose hardware capacity to compute information. By 2007, this diminished to less than 0.05%.\n\nModern 2016 electronic calculators contain a keyboard with buttons for digits and arithmetical operations; some even contain \"00\" and \"000\" buttons to make larger or smaller numbers easier to enter. Most basic calculators assign only one digit or operation on each button; however, in more specific calculators, a button can perform multi-function working with key combinations.\n\nCalculators usually have liquid-crystal displays (LCD) as output in place of historical light-emitting diode (LED) displays and vacuum fluorescent displays (VFD); details are provided in the section \"Technical improvements\".\n\nLarge-sized figures and comma separators are often used to improve readability. Various symbols for function commands may also be shown on the display. Fractions such as are displayed as decimal approximations, for example rounded to . Also, some fractions (such as , which is ; to 14 significant figures) can be difficult to recognize in decimal form; as a result, many scientific calculators are able to work in vulgar fractions or mixed numbers.\n\nCalculators also have the ability to store numbers into computer memory. Basic types of these store only one number at a time; more specific types are able to store many numbers represented in variables. The variables can also be used for constructing formulas. Some models have the ability to extend memory capacity to store more numbers; the extended memory address is termed an array index.\n\nPower sources of calculators are: batteries, solar cells or mains electricity (for old models), turning on with a switch or button. Some models even have no turn-off button but they provide some way to put off (for example, leaving no operation for a moment, covering solar cell exposure, or closing their lid). Crank-powered calculators were also common in the early computer era.\n\nIn general, a basic electronic calculator consists of the following components:\nClock rate of a processor chip refers to the frequency at which the central processing unit (CPU) is running. It is used as an indicator of the processor's speed, and is measured in \"clock cycles per second\" or the SI unit hertz (Hz). For basic calculators, the speed can vary from a few hundred hertz to the kilohertz range.\nA basic explanation as to how calculations are performed in a simple 4-function calculator:\n\nTo perform the calculation , one presses keys in the following sequence on most calculators:     .\n\nOther functions are usually performed using repeated additions or subtractions. Where calculators have added functions (such as square root, or trigonometric functions), software algorithms are required to produce high precision results. Sometimes significant design effort is needed to fit all the desired functions in the limited memory space available in the calculator chip, with acceptable calculation time.\n\nThe fundamental difference between a calculator and computer is that a computer can be programmed in a way that allows the program to take different branches according to intermediate results, while calculators are pre-designed with specific functions (such as addition, multiplication, and logarithms) built in. The distinction is not clear-cut: some devices classed as programmable calculators have programming functions, sometimes with support for programming languages (such as RPL or TI-BASIC).\n\nTypically, the user buys the least costly model having a specific feature set, but does not care much about speed, since speed is constrained by how fast the user can press the buttons. Thus, designers of calculators work to minimize the number of logic elements on the chip, not the number of clock cycles needed to do a computation.\n\nFor instance, instead of a hardware multiplier, a calculator might implement floating point mathematics with code in read-only memory (ROM), and compute trigonometric functions with the CORDIC algorithm because CORDIC does not require much multiplication. Bit serial logic designs are more common in calculators whereas bit parallel designs dominate general-purpose computers, because a bit serial design minimizes chip complexity, but takes many more clock cycles. This distinction blurs with high-end calculators, which use processor chips associated with computer and embedded systems design, more so the Z80, MC68000, and ARM architectures, and some custom designs specialized for the calculator market.\n\nThe first known tools used to aid arithmetic calculations were: bones (used to tally items), pebbles, and counting boards, and the abacus, known to have been used by Sumerians and Egyptians before 2000 BC. Except for the Antikythera mechanism (an \"out of the time\" astronomical device), development of computing tools arrived near the start of the 17th century: the geometric-military compass (by Galileo), logarithms and Napier bones (by Napier), and the slide rule (by Edmund Gunter).\nIn 1642, the Renaissance saw the invention of the mechanical calculator (by Wilhelm Schickard and several decades later Blaise Pascal), a device that was at times somewhat over-promoted as being able to perform all four arithmetic operations with minimal human intervention. Pascal's Calculator could add and subtract two numbers directly and thus, if the tedium could be borne, multiply and divide by repetition. Schickard's machine, constructed several decades earlier, used a clever set of mechanised multiplication tables to ease the process of multiplication and division with the adding machine as a means of completing this operation. (Because they were different inventions with different aims a debate about whether Pascal or Schickard should be credited as the \"inventor\" of the adding machine (or calculating machine) is probably pointless.) Schickard and Pascal were followed by Gottfried Leibniz who spent forty years designing a four-operation mechanical calculator, inventing in the process his leibniz wheel, but who couldn't design a fully operational machine. There were also five unsuccessful attempts to design a calculating clock in the 17th century.\nThe 18th century saw the arrival of some interesting improvements, first by Poleni with the first fully functional calculating clock and four-operation machine, but these machines were almost always \"one of the kind\". It was not until the 19th century and the Industrial Revolution that real developments began to occur. Although machines capable of performing all four arithmetic functions existed prior to the 19th century, the refinement of manufacturing and fabrication processes during the eve of the industrial revolution made large scale production of more compact and modern units possible. The Arithmometer, invented in 1820 as a four-operation mechanical calculator, was released to production in 1851 as an adding machine and became the first commercially successful unit; forty years later, by 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883) and Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers.\n\nIt wasn't until 1902 that the familiar push-button user interface was developed, with the introduction of the Dalton Adding Machine, developed by James L. Dalton in the United States.\n\nThe Curta calculator was developed in 1948 and, although costly, became popular for its portability. This purely mechanical hand-held device could do addition, subtraction, multiplication and division. By the early 1970s electronic pocket calculators ended manufacture of mechanical calculators, although the Curta remains a popular collectable item.\n\nThe first mainframe computers, using firstly vacuum tubes and later transistors in the logic circuits, appeared in the 1940s and 1950s. This technology was to provide a stepping stone to the development of electronic calculators.\n\nThe Casio Computer Company, in Japan, released the Model \"14-A\" calculator in 1957, which was the world's first all-electric (relatively) compact calculator. It did not use electronic logic but was based on relay technology, and was built into a desk.\nIn October 1961, the world's first \"all-electronic desktop\" calculator, the British Bell Punch/Sumlock Comptometer ANITA (A New Inspiration To Arithmetic/Accounting) was announced. This machine used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode \"Nixie\" tubes for its display. Two models were displayed, the Mk VII for continental Europe and the Mk VIII for Britain and the rest of the world, both for delivery from early 1962. The Mk VII was a slightly earlier design with a more complicated mode of multiplication, and was soon dropped in favour of the simpler Mark VIII. The ANITA had a full keyboard, similar to mechanical comptometers of the time, a feature that was unique to it and the later Sharp CS-10A among electronic calculators. The ANITA weighed roughly due to its large tube system. Bell Punch had been producing key-driven mechanical calculators of the comptometer type under the names \"Plus\" and \"Sumlock\", and had realised in the mid-1950s that the future of calculators lay in electronics. They employed the young graduate Norbert Kitz, who had worked on the early British Pilot ACE computer project, to lead the development. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick.\n\nThe tube technology of the ANITA was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a cathode ray tube (CRT), and introduced Reverse Polish Notation (RPN) to the calculator market for a price of $2200, which was about three times the cost of an electromechanical calculator of the time. Like Bell Punch, Friden was a manufacturer of mechanical calculators that had decided that the future lay in electronics. In 1964 more all-transistor electronic calculators were introduced: Sharp introduced the CS-10A, which weighed and cost 500,000 yen ($), and Industria Macchine Elettroniche of Italy introduced the IME 84, to which several extra keyboard and display units could be connected so that several people could make use of it (but apparently not at the same time).\n\nThere followed a series of electronic calculator models from these and other manufacturers, including Canon, Mathatronics, Olivetti, SCM (Smith-Corona-Marchant), Sony, Toshiba, and Wang. The early calculators used hundreds of germanium transistors, which were cheaper than silicon transistors, on multiple circuit boards. Display types used were CRT, cold-cathode Nixie tubes, and filament lamps. Memory technology was usually based on the delay line memory or the magnetic core memory, though the Toshiba \"Toscal\" BC-1411 appears to have used an early form of dynamic RAM built from discrete components. Already there was a desire for smaller and less power-hungry machines.\n\nThe Olivetti Programma 101 was introduced in late 1965; it was a stored program machine which could read and write magnetic cards and displayed results on its built-in printer. Memory, implemented by an acoustic delay line, could be partitioned between program steps, constants, and data registers. Programming allowed conditional testing and programs could also be overlaid by reading from magnetic cards. It is regarded as the first personal computer produced by a company (that is, a desktop electronic calculating machine programmable by non-specialists for personal use). The Olivetti Programma 101 won many industrial design awards.\nAnother calculator introduced in 1965 was Bulgaria's ELKA 6521, developed by the Central Institute for Calculation Technologies and built at the Elektronika factory in Sofia. The name derives from \"ELektronen KAlkulator\", and it weighed around . It is the first calculator in the world which includes the square root function. Later that same year were released the ELKA 22 (with a luminescent display) and the ELKA 25, with an in-built printer. Several other models were developed until the first pocket model, the ELKA 101, was released in 1974. The writing on it was in Roman script, and it was exported to western countries.\n\nThe \"Monroe Epic\" programmable calculator came on the market in 1967. A large, printing, desk-top unit, with an attached floor-standing logic tower, it could be programmed to perform many computer-like functions. However, the only \"branch\" instruction was an implied unconditional branch (GOTO) at the end of the operation stack, returning the program to its starting instruction. Thus, it was not possible to include any conditional branch (IF-THEN-ELSE) logic. During this era, the absence of the conditional branch was sometimes used to distinguish a programmable calculator from a computer.\n\nThe first handheld calculator was a prototype called \"Cal Tech\", whose development was led by Jack Kilby at Texas Instruments in 1967. It could add, multiply, subtract, and divide, and its output device was a paper tape.\n\nThe electronic calculators of the mid-1960s were large and heavy desktop machines due to their use of hundreds of transistors on several circuit boards with a large power consumption that required an AC power supply. There were great efforts to put the logic required for a calculator into fewer and fewer integrated circuits (chips) and calculator electronics was one of the leading edges of semiconductor development. U.S. semiconductor manufacturers led the world in large scale integration (LSI) semiconductor development, squeezing more and more functions into individual integrated circuits. This led to alliances between Japanese calculator manufacturers and U.S. semiconductor companies: Canon Inc. with Texas Instruments, Hayakawa Electric (later renamed Sharp Corporation) with North-American Rockwell Microelectronics (later renamed Rockwell International), Busicom with Mostek and Intel, and General Instrument with Sanyo.\n\nBy 1970, a calculator could be made using just a few chips of low power consumption, allowing portable models powered from rechargeable batteries. The first portable calculators appeared in Japan in 1970, and were soon marketed around the world. These included the Sanyo ICC-0081 \"Mini Calculator\", the Canon Pocketronic, and the Sharp QT-8B \"micro Compet\". The Canon Pocketronic was a development of the \"Cal-Tech\" project which had been started at Texas Instruments in 1965 as a research project to produce a portable calculator. The Pocketronic has no traditional display; numerical output is on thermal paper tape. As a result of the \"Cal-Tech\" project, Texas Instruments was granted master patents on portable calculators.\n\nSharp put in great efforts in size and power reduction and introduced in January 1971 the Sharp EL-8, also marketed as the Facit 1111, which was close to being a pocket calculator. It weighed 1.59 pounds (721 grams), had a vacuum fluorescent display, rechargeable NiCad batteries, and initially sold for US $395.\n\nHowever, the efforts in integrated circuit development culminated in the introduction in early 1971 of the first \"calculator on a chip\", the MK6010 by Mostek, followed by Texas Instruments later in the year. Although these early hand-held calculators were very costly, these advances in electronics, together with developments in display technology (such as the vacuum fluorescent display, LED, and LCD), led within a few years to the cheap pocket calculator available to all.\n\nIn 1971 Pico Electronics. and General Instrument also introduced their first collaboration in ICs, a full single chip calculator IC for the Monroe Royal Digital III calculator. Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. Pico and GI went on to have significant success in the burgeoning handheld calculator market.\n\nThe first truly pocket-sized electronic calculator was the Busicom LE-120A \"HANDY\", which was marketed early in 1971. Made in Japan, this was also the first calculator to use an LED display, the first hand-held calculator to use a single integrated circuit (then proclaimed as a \"calculator on a chip\"), the Mostek MK6010, and the first electronic calculator to run off replaceable batteries. Using four AA-size cells the LE-120A measures .\n\nThe first European-made pocket-sized calculator, DB 800 is made in May 1971 by Digitron in Buje, Croatia (former Yugoslavia) with four functions and an eight-digit display and special characters for a negative number and a warning that the calculation has too many digits to display.\n\nThe first American-made pocket-sized calculator, the Bowmar 901B (popularly termed \"The Bowmar Brain\"), measuring , came out in the Autumn of 1971, with four functions and an eight-digit red LED display, for $240, while in August 1972 the four-function Sinclair Executive became the first slimline pocket calculator measuring and weighing . It retailed for around £79 ($). By the end of the decade, similar calculators were priced less than £5 ($).\n\nThe first Soviet Union made pocket-sized calculator, the \"Elektronika B3-04\" was developed by the end of 1973 and sold at the start of 1974.\n\nOne of the first low-cost calculators was the Sinclair Cambridge, launched in August 1973. It retailed for £29.95 ($), or £5 ($) less in kit form. The Sinclair calculators were successful because they were far cheaper than the competition; however, their design led to slow and inaccurate computations of transcendental functions.\n\nMeanwhile, Hewlett-Packard (HP) had been developing a pocket calculator. Launched in early 1972, it was unlike the other basic four-function pocket calculators then available in that it was the first pocket calculator with \"scientific\" functions that could replace a slide rule. The $395 HP-35, along with nearly all later HP engineering calculators, used reverse Polish notation (RPN), also called postfix notation. A calculation like \"8 plus 5\" is, using RPN, performed by pressing \"8\", \"Enter↑\", \"5\", and \"+\"; instead of the algebraic infix notation: \"8\", \"+\", \"5\", \"=\". It had 35 buttons and was based on Mostek Mk6020 chip.\n\nThe first Soviet \"scientific\" pocket-sized calculator the \"B3-18\" was completed by the end of 1975.\n\nIn 1973, Texas Instruments (TI) introduced the SR-10, (\"SR\" signifying slide rule) an \"algebraic entry\" pocket calculator using scientific notation for $150. Shortly after the SR-11 featured an added key for entering Pi (π). It was followed the next year by the SR-50 which added log and trig functions to compete with the HP-35, and in 1977 the mass-marketed TI-30 line which is still produced.\n\nIn 1978 a new company, Calculated Industries arose which focused on specialized markets. Their first calculator, the Loan Arranger (1978) was a pocket calculator marketed to the Real Estate industry with preprogrammed functions to simplify the process of calculating payments and future values. In 1985, CI launched a calculator for the construction industry called the Construction Master which came preprogrammed with common construction calculations (such as angles, stairs, roofing math, pitch, rise, run, and feet-inch fraction conversions). This would be the first in a line of construction related calculators.\n\nThe first desktop \"programmable calculators\" were produced in the mid-1960s by Mathatronics and Casio (AL-1000). These machines were very heavy and costly. The first programmable pocket calculator was the HP-65, in 1974; it had a capacity of 100 instructions, and could store and retrieve programs with a built-in magnetic card reader. Two years later the HP-25C introduced \"continuous memory\", i.e., programs and data were retained in CMOS memory during power-off. In 1979, HP released the first \"alphanumeric\", programmable, \"expandable\" calculator, the HP-41C. It could be expanded with random access memory (RAM, for memory) and read-only memory (ROM, for software) modules, and peripherals like bar code readers, microcassette and floppy disk drives, paper-roll thermal printers, and miscellaneous communication interfaces (RS-232, HP-IL, HP-IB).\n\nThe first Soviet programmable desktop calculator ISKRA 123, powered by the power grid, was released at the start of the 1970s. The first Soviet pocket battery-powered programmable calculator, Elektronika \"B3-21\", was developed by the end of 1977 and released at the start of 1978. The successor of B3-21, the Elektronika B3-34 wasn't backward compatible with B3-21, even if it kept the reverse Polish notation (RPN). Thus B3-34 defined a new command set, which later was used in a series of later programmable Soviet calculators. Despite very limited abilities (98 bytes of instruction memory and about 19 stack and addressable registers), people managed to write all kinds of programs for them, including adventure games and libraries of calculus-related functions for engineers. Hundreds, perhaps thousands, of programs were written for these machines, from practical scientific and business software, which were used in real-life offices and labs, to fun games for children. The Elektronika MK-52 calculator (using the extended B3-34 command set, and featuring internal EEPROM memory for storing programs and external interface for EEPROM cards and other periphery) was used in Soviet spacecraft program (for Soyuz TM-7 flight) as a backup of the board computer.\n\nThis series of calculators was also noted for a large number of highly counter-intuitive mysterious undocumented features, somewhat similar to \"synthetic programming\" of the American HP-41, which were exploited by applying normal arithmetic operations to error messages, jumping to nonexistent addresses and other methods. A number of respected monthly publications, including the popular science magazine \"Nauka i Zhizn\" (\"Наука и жизнь\", \"Science and Life\"), featured special columns, dedicated to optimization methods for calculator programmers and updates on undocumented features for hackers, which grew into a whole esoteric science with many branches, named \"yeggogology\" (\"еггогология\"). The error messages on those calculators appear as a Russian word \"YEGGOG\" (\"ЕГГОГ\") which, unsurprisingly, is translated to \"Error\".\n\nA similar hacker culture in the USA revolved around the HP-41, which was also noted for a large number of undocumented features and was much more powerful than B3-34.\n\nThrough the 1970s the hand-held electronic calculator underwent rapid development. The red LED and blue/green vacuum fluorescent displays consumed a lot of power and the calculators either had a short battery life (often measured in hours, so rechargeable nickel-cadmium batteries were common) or were large so that they could take larger, higher capacity batteries. In the early 1970s liquid-crystal displays (LCDs) were in their infancy and there was a great deal of concern that they only had a short operating lifetime. Busicom introduced the Busicom \"LE-120A \"HANDY\"\" calculator, the first pocket-sized calculator and the first with an LED display, and announced the Busicom \"LC\" with LCD. However, there were problems with this display and the calculator never went on sale. The first successful calculators with LCDs were manufactured by Rockwell International and sold from 1972 by other companies under such names as: Dataking \"LC-800\", Harden \"DT/12\", Ibico \"086\", Lloyds \"40\", Lloyds \"100\", Prismatic \"500\" (a.k.a. \"P500\"), Rapid Data \"Rapidman 1208LC\". The LCDs were an early form using the \"Dynamic Scattering Mode DSM\" with the numbers appearing as bright against a dark background. To present a high-contrast display these models illuminated the LCD using a filament lamp and solid plastic light guide, which negated the low power consumption of the display. These models appear to have been sold only for a year or two.\n\nA more successful series of calculators using a reflective DSM-LCD was launched in 1972 by Sharp Inc with the Sharp \"EL-805\", which was a slim pocket calculator. This, and another few similar models, used Sharp's \"Calculator On Substrate\" (COS) technology. An extension of one glass plate needed for the liquid crystal display was used as a substrate to mount the needed chips based on a new hybrid technology. The COS technology may have been too costly since it was only used in a few models before Sharp reverted to conventional circuit boards.\n\nIn the mid-1970s the first calculators appeared with field-effect, \"twisted nematic\" (TN) LCDs with dark numerals against a grey background, though the early ones often had a yellow filter over them to cut out damaging ultraviolet rays. The advantage of LCDs is that they are passive light modulators reflecting light, which require much less power than light-emitting displays such as LEDs or VFDs. This led the way to the first credit-card-sized calculators, such as the Casio \"Mini Card LC-78\" of 1978, which could run for months of normal use on button cells.\n\nThere were also improvements to the electronics inside the calculators. All of the logic functions of a calculator had been squeezed into the first \"calculator on a chip\" integrated circuits (ICs) in 1971, but this was leading edge technology of the time and yields were low and costs were high. Many calculators continued to use two or more ICs, especially the scientific and the programmable ones, into the late 1970s.\n\nThe power consumption of the integrated circuits was also reduced, especially with the introduction of CMOS technology. Appearing in the Sharp \"EL-801\" in 1972, the transistors in the logic cells of CMOS ICs only used any appreciable power when they changed state. The LED and VFD displays often required added driver transistors or ICs, whereas the LCDs were more amenable to being driven directly by the calculator IC itself.\n\nWith this low power consumption came the possibility of using solar cells as the power source, realised around 1978 by calculators such as the Royal \"Solar 1\", Sharp \"EL-8026\", and Teal \"Photon\".\nAt the start of the 1970s, hand-held electronic calculators were very costly, at two or three weeks' wages, and so were a luxury item. The high price was due to their construction requiring many mechanical and electronic components which were costly to produce, and production runs that were too small to exploit economies of scale. Many firms saw that there were good profits to be made in the calculator business with the margin on such high prices. However, the cost of calculators fell as components and their production methods improved, and the effect of economies of scale was felt.\n\nBy 1976, the cost of the cheapest four-function pocket calculator had dropped to a few dollars, about 1/20th of the cost five years before. The results of this were that the pocket calculator was affordable, and that it was now difficult for the manufacturers to make a profit from calculators, leading to many firms dropping out of the business or closing down. The firms that survived making calculators tended to be those with high outputs of higher quality calculators, or producing high-specification scientific and programmable calculators.\n\nThe first calculator capable of symbolic computing was the HP-28C, released in 1987. It could, for example, solve quadratic equations symbolically. The first graphing calculator was the Casio fx-7000G released in 1985.\n\nThe two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.\n\nThe HP 12c financial calculator is still produced. It was introduced in 1981 and is still being made with few changes. The HP 12c featured the reverse Polish notation mode of data entry. In 2003 several new models were released, including an improved version of the HP 12c, the \"HP 12c platinum edition\" which added more memory, more built-in functions, and the addition of the algebraic mode of data entry.\n\nCalculated Industries competed with the HP 12c in the mortgage and real estate markets by differentiating the key labeling; changing the “I”, “PV”, “FV” to easier labeling terms such as \"Int\", \"Term\", \"Pmt\", and not using the reverse Polish notation. However, CI's more successful calculators involved a line of construction calculators, which evolved and expanded in the 1990s to present. According to Mark Bollman, a mathematics and calculator historian and associate professor of mathematics at Albion College, the \"Construction Master is the first in a long and profitable line of CI construction calculators\" which carried them through the 1980s, 1990s, and to the present.\n\nPersonal computers often come with a calculator utility program that emulates the appearance and functions of a calculator, using the graphical user interface to portray a calculator. One such example is Windows Calculator. Most personal data assistants (PDAs) and smartphones also have such a feature.\n\nIn most countries, students use calculators for schoolwork. There was some initial resistance to the idea out of fear that basic or elementary arithmetic skills would suffer. There remains disagreement about the importance of the ability to perform calculations \"in the head\", with some curricula restricting calculator use until a certain level of proficiency has been obtained, while others concentrate more on teaching estimation methods and problem-solving. Research suggests that inadequate guidance in the use of calculating tools can restrict the kind of mathematical thinking that students engage in. Others have argued that calculator use can even cause core mathematical skills to atrophy, or that such use can prevent understanding of advanced algebraic concepts. In December 2011 the UK's Minister of State for Schools, Nick Gibb, voiced concern that children can become \"too dependent\" on the use of calculators. As a result, the use of calculators is to be included as part of a review of the Curriculum. In United States, many math educators and boards of education enthusiastically endorsed the National Council of Teachers of Mathematics (NCTM) standards and actively promoted the use of classroom calculators from kindergarten through high school.\n\nThese are some of the manufacturers which made a notable contribution to calculator development:\n\n\n\n\n\n", "id": "7593", "title": "Calculator"}
{"url": "https://en.wikipedia.org/wiki?curid=7594", "text": "Cash register\n\nA cash register, also referred to as a till in the United Kingdom and other Commonwealth countries, is a mechanical or electronic device for registering and calculating transactions at a point of sale. It is usually attached to a drawer for storing cash and other valuables. The cash register is also usually attached to a printer, that can print out receipts for record keeping purposes.\n\nAn early mechanical cash register was invented by James Ritty and John Birch following the American Civil War. James was the owner of a saloon in Dayton, Ohio, USA, and wanted to stop employees from pilfering his profits. The Ritty Model I was invented in 1879 after seeing a tool that counted the revolutions of the propeller on a steamship. With the help of James' brother John Ritty, they patented it in 1883. It was called \"Ritty's Incorruptible Cashier\" and it was invented for the purpose to stop cashiers of pilfering and eliminating employee theft or embezzlement.\n\nEarly mechanical registers were entirely mechanical, without receipts. The employee was required to ring up every transaction on the register, and when the total key was pushed, the drawer opened and a bell would ring, alerting the manager to a sale taking place. Those original machines were nothing but simple adding machines.\n\nSince the registration is done with the process of returning change, according to Bill Bryson odd pricing came about because by charging odd amounts like 49 and 99 cents (or 45 and 95 cents when nickels are more used than pennies), the cashier very probably had to open the till for the penny change and thus announce the sale.\nShortly after the patent, Ritty became overwhelmed with the responsibilities of running two businesses, so he sold all of his interests in the cash register business to Jacob H. Eckert of Cincinnati, a china and glassware salesman, who formed the National Manufacturing Company. In 1884 Eckert sold the company to John H. Patterson, who renamed the company the National Cash Register Company and improved the cash register by adding a paper roll to record sales transactions, thereby creating the journal for internal bookkeeping purposes, and the receipt for external bookkeeping purposes. The original purpose of the receipt was enhanced fraud protection. The business owner could read the receipts to ensure that cashiers charged customers the correct amount for each transaction and did not embezzle the cash drawer. It also prevents a customer from defrauding the business by falsely claiming receipt of a lesser amount of change or a transaction that never happened in the first place. The first evidence of an actual cash register was used in Coalton, Ohio, at the old mining company.\n\nIn 1906, while working at the National Cash Register company, inventor Charles F. Kettering designed a cash register with an electric motor.\nA leading designer, builder, manufacturer, seller and exporter of cash registers from the 1950s until the 1970s was London-based (and later Brighton-based) Gross Cash Registers Ltd., founded by brothers Sam and Henry Gross. Their cash registers were particularly popular around the time of decimalisation in Britain in early 1971, Henry having designed one of the few known models of cash register which could switch currencies from £sd to £p so that retailers could easily change from one to the other on or after Decimal Day. Sweda also had decimal-ready registers where the retailer used a special key on Decimal Day for the conversion.\n\nIn some jurisdictions the law also requires customers to collect the receipt and keep it at least for a short while after leaving the shop, again to check that the shop records sales, so that it cannot evade sales taxes.\n\nOften cash registers are attached to scales, barcode scanners, checkstands, and debit card or credit card terminals. Increasingly, dedicated cash registers are being replaced with general purpose computers with POS software. Cash registers use bitmap characters for printing.\n\nToday, point of sale systems scan the barcode (usually EAN or UPC) for each item, retrieve the price from a database, calculate deductions for items on sale (or, in British retail terminology, \"special offer\", \"multibuy\" or \"buy one, get one free\"), calculate the sales tax or VAT, calculate differential rates for preferred customers, actualize inventory, time and date stamp the transaction, record the transaction in detail including each item purchased, record the method of payment, keep totals for each product or type of product sold as well as total sales for specified periods, and do other tasks as well. These POS terminals will often also identify the cashier on the receipt, and carry additional information or offers.\n\nCurrently, many cash registers are individual computers. They may be running traditionally in-house software or general purpose software such as DOS. Many of the newer ones have touch screens. They may be connected to computerized point of sale networks using any type of protocol. Such systems may be accessed remotely for the purpose of obtaining records or troubleshooting. Many businesses also use tablet computers as cash registers, utilizing the sale system as downloadable app-software.\n\nCash registers include a key labeled \"No Sale\", abbreviated \"NS\" on many modern electronic cash registers. Its function is to open the drawer, printing a receipt stating \"No Sale\" and recording in the register log that the register was opened. Some cash registers require a numeric password or physical key to be used when attempting to open the till.\n\nA cash register's drawer can only be opened by an instruction from the cash register except when using special keys, generally held by the owner and some employees (e.g. manager). This reduces the amount of contact most employees have with cash and other valuables. It also reduces risks of an employee taking money from the drawer without a record and the owner's consent, such as when a customer does not expressly ask for a receipt but still has to be given change (cash is more easily checked against recorded sales than inventory).\n\nA cash drawer is usually a compartment underneath a cash register in which the cash from transactions is kept. The drawer typically contains a removable till. The till is usually a plastic or wooden tray divided into compartments used to store each denomination of bank notes and coins separately in order to make counting easier. The removable till allows money to be removed from the sales floor to a more secure location for counting and creating bank deposits. Some modern cash drawers are individual units separate from the rest of the cash register.\n\nA cash drawer is usually of strong construction and may be integral with the register or a separate piece that the register sits atop. It slides in and out of its lockable box and is secured by a spring-loaded catch. When a transaction that involves cash is completed, the register sends an electrical impulse to a solenoid to release the catch and open the drawer.\nCash drawers that are integral to a stand-alone register often have a manual release catch underneath to open the drawer in the event of a power failure. More advanced cash drawers have eliminated the manual release in favor of a cylinder lock, requiring a key to manually open the drawer. The cylinder lock usually has several positions: locked, unlocked, online (will open if an impulse is given), and release. The release position is an intermittent position with a spring to push the cylinder back to the unlocked position. In the \"locked\" position, the drawer will remain latched even when an electric impulse is sent to the solenoid.\n\nDue to the increasing amount of notes and varieties of notes, many cash drawers have opted to store notes in a vertical side facing position instead of the traditional horizontal upward facing position. This enables faster access to each note and allows more varieties of notes to be stored. Sometimes the cashier will even divide the notes without any physical divider at all. Some cash drawers are also flip top in design, where they flip open instead of sliding out like an ordinary drawer, resembling a cashbox instead.\n\nRegisters will typically feature a numerical pad, QWERTY or custom keyboard, touch screen interface, or a combination of these input methods for the cashier to enter products and fees by hand and access information necessary to complete the sale. For older registers as well as at restaurants and other establishments that do not sell barcoded items, the manual input may be the only method of interacting with the register. While customization was previously limited to larger chains that could afford to have physical keyboards custom-built for their needs, the customization of register inputs is now more widespread with the use of touch screens that can display a variety of point of sale software.\n\nModern cash registers may be connected to a handheld or stationary barcode reader so that a customer's purchases can be more rapidly scanned than would be possible by keying numbers into the register by hand. The use of scanners should also help prevent errors that result from manually entering the product's barcode or pricing. At grocers, the register's scanner may be combined with a scale for measuring product that is sold by weight.\n\nCashiers are often required to provide a receipt to the customer after a purchase has been made. Registers typically use thermal printers to print receipts, although older dot matrix printers are still in use at some retailers. Alternatively, retailers can forgo issuing paper receipts in some jurisdictions by instead asking the customer for an email to which their receipt can be sent. The receipts of larger retailers tend to include unique barcodes or other information identifying the transaction so that the receipt can be scanned to facilitate returns or other customer services.\n\nIn stores that use electronic article surveillance, a pad or other surface will be attached to the register that deactivates security devices embedded in or attached to the items being purchased. This will prevent a customer's purchase from setting off security alarms at the store's exit.\n\nSome corporations and supermarkets have introduced self-checkout machines, where the customer is trusted to scan the barcodes (or manually identify uncoded items like fruit), and place the items into a bagging area. The bag is weighed, and the machine halts the checkout when the weight of something in the bag does not match the weight in the inventory database. Normally, an employee is watching over several such checkouts to prevent theft or exploitation of the machines' weaknesses (for example, intentional misidentification of expensive produce or dry goods). Payment on these machines is accepted by debit card/credit card, or cash via coin slot and bank note scanner. Store employees are also needed to authorize \"age-restricted\" purchases, such as alcohol, solvents or knives, which can either be done remotely by the employee observing the self-checkout, or by means of a \"store login\" which the operator has to enter.\n\n\n", "id": "7594", "title": "Cash register"}
{"url": "https://en.wikipedia.org/wiki?curid=7595", "text": "Chronometer\n\nChronometer or chronoscope may refer to:\n\n", "id": "7595", "title": "Chronometer"}
{"url": "https://en.wikipedia.org/wiki?curid=7597", "text": "Processor design\n\nProcessor design is the design engineering task of creating a microprocessor, a component of computer hardware. It is a subfield of electronics engineering and computer engineering. The design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture described in e.g. VHDL or Verilog. This description is then manufactured employing some of the various semiconductor device fabrication processes. This results in a die which is bonded onto a chip carrier. This chip carrier is then soldered onto, or inserted into a socket on, a printed circuit board (PCB).\n\nThe mode of operation of any microprocessor is the execution of lists of instructions. Instructions typically include those to compute or manipulate data values using registers, change or retrieve values in read/write memory, perform relational tests between data values and to control program flow.\n\nCPU design focuses on six main areas:\n\nCPUs designed for high-performance markets might require custom designs for each of these items to achieve frequency, power-dissipation, and chip-area goals whereas CPUs designed for lower performance markets might lessen the implementation burden by acquiring some of these items by purchasing them as intellectual property. Control logic implementation techniques (logic synthesis using CAD tools) can be used to implement datapaths, register files, and clocks. Common logic styles used in CPU design include unstructured random logic, finite-state machines, microprogramming (common from 1965 to 1985), and Programmable logic arrays (common in the 1980s, no longer common).\n\nDevice types used to implement the logic include:\n\nA CPU design project generally has these major tasks:\n\nRe-designing a CPU core to a smaller die-area helps to shrink everything (a \"photomask shrink\"), resulting in the same number of transistors on a smaller die. It improves performance (smaller transistors switch faster), reduces power (smaller wires have less parasitic capacitance) and reduces cost (more CPUs fit on the same wafer of silicon). Releasing a CPU on the same size die, but with a smaller CPU core, keeps the cost about the same but allows higher levels of integration within one very-large-scale integration chip (additional cache, multiple CPUs or other components), improving performance and reducing overall system cost.\n\nAs with most complex electronic designs, the logic verification effort (proving that the design does\nnot have bugs) now dominates the project schedule of a CPU.\n\nKey CPU architectural innovations include index register, cache, virtual memory, instruction pipelining, superscalar, CISC, RISC, virtual machine, emulators, microprogram, and stack.\n\nA variety of have been proposed,\nincluding reconfigurable logic, clockless CPUs, computational RAM, and optical computing.\n\nBenchmarking is a way of testing CPU speed. Examples include SPECint and SPECfp, developed by Standard Performance Evaluation Corporation, and ConsumerMark developed by the Embedded Microprocessor Benchmark Consortium EEMBC.\n\nSome of the commonly used metrics include:\n\nThere may be tradeoffs in optimizing some of these metrics. In particular, many design techniques that make a CPU run faster make the \"performance per watt\", \"performance per dollar\", and \"deterministic response\" much worse, and vice versa.\n\nThere are several different markets in which CPUs are used. Since each of these markets differ in their requirements for CPUs, the devices designed for one market are in most cases inappropriate for the other markets.\n\nThe vast majority of revenues generated from CPU sales is for general purpose computing, that is, desktop, laptop, and server computers commonly used in businesses and homes. In this market, the Intel IA-32 and the 64-bit version x86-64 architecture dominate the market, with its rivals PowerPC and SPARC maintaining much smaller customer bases. Yearly, hundreds of millions of IA-32 architecture CPUs are used by this market. A growing percentage of these processors are for mobile implementations such as netbooks and laptops.\n\nSince these devices are used to run countless different types of programs, these CPU designs are not specifically targeted at one type of application or one function. The demands of being able to run a wide range of programs efficiently has made these CPU designs among the more advanced technically, along with some disadvantages of being relatively costly, and having high power consumption.\n\nIn 1984, most high-performance CPUs required four to five years to develop.\n\nScientific computing is a much smaller niche market (in revenue and units shipped). It is used in government research labs and universities. Before 1990, CPU design was often done for this market, but mass market CPUs organized into large clusters have proven to be more affordable. The main remaining area of active hardware design and research for scientific computing is for high-speed data transmission systems to connect mass market CPUs.\n\nAs measured by units shipped, most CPUs are embedded in other machinery, such as telephones, clocks, appliances, vehicles, and infrastructure. Embedded processors sell in the volume of many billions of units per year, however, mostly at much lower price points than that of the general purpose processors.\n\nThese single-function devices differ from the more familiar general-purpose CPUs in several ways:\n\nThe embedded CPU family with the largest number of total units shipped is the 8051, averaging nearly a billion units per year. The 8051 is widely used because it is very inexpensive. The design time is now roughly zero, because it is widely available as commercial intellectual property. It is now often embedded as a small part of a larger system on a chip. The silicon cost of an 8051 is now as low as US$0.001, because some implementations use as few as 2,200 logic gates and take 0.0127 square millimeters of silicon.\n\nAs of 2009, more CPUs are produced using the ARM architecture instruction set than any other 32-bit instruction set.\nThe ARM architecture and the first ARM chip were designed in about one and a half years and 5 human years of work time.\n\nThe 32-bit Parallax Propeller microcontroller architecture and the first chip were designed by two people in about 10 human years of work time.\n\nThe 8-bit AVR architecture and first AVR microcontroller was conceived and designed by two students at the Norwegian Institute of Technology.\n\nThe 8-bit 6502 architecture and the first MOS Technology 6502 chip were designed in 13 months by a group of about 9 people.\n\nThe 32 bit Berkeley RISC I and RISC II architecture and the first chips were mostly designed by a series of students as part of a four quarter sequence of graduate courses.\nThis design became the basis of the commercial SPARC processor design.\n\nFor about a decade, every student taking the 6.004 class at MIT was part of a team—each team had one semester to design and build a simple 8 bit CPU out of 7400 series integrated circuits.\nOne team of 4 students designed and built a simple 32 bit CPU during that semester.\nSome undergraduate courses require a team of 2 to 5 students to design, implement, and test a simple CPU in a FPGA in a single 15 week semester.\nThe MultiTitan CPU was designed with 2.5 man years of effort, which was considered \"relatively little design effort\" at the time.\n24 people contributed to the 3.5 year MultiTitan research project, which included designing and building a prototype CPU.\n\nFor embedded systems, the highest performance levels are often not needed or desired due to the power consumption requirements. This allows for the use of processors which can be totally implemented by logic synthesis techniques. These synthesized processors can be implemented in a much shorter amount of time, giving quicker time-to-market.\n\n\n", "id": "7597", "title": "Processor design"}
{"url": "https://en.wikipedia.org/wiki?curid=7598", "text": "Carinatae\n\nCarinatae is the group of all birds and their extinct relatives to possess a keel, or \"carina\", on the underside of the breastbone used to anchor large flight muscles.\n\nTraditionally, Carinatae were defined as all birds whose sternum (breast bone) has a keel (\"carina\"). The keel is a strong median ridge running down the length of the sternum. This is an important area for the attachment of flight muscles. Thus, all flying birds have a pronounced keel. Ratites, all of whom are flightless, lack a strong keel. Thus, living birds were divided into carinates (keeled) and ratites (from \"ratis\", \"raft\", referring to the flatness of the sternum). The difficulty with this scheme phylogenetically was that some flightless birds, without strong carinae, are descended directly from ordinary flying birds with carinae. Examples include the kakapo, a flightless parrot, and the dodo, a columbiform (the pigeon family). None of these birds are ratites. Thus, this supposedly distinctive feature was easy to use, but had nothing to do with actual phylogenic relationship.\n\nBeginning in the 1980s, Carinatae was given several phylogenetic definitions. The first was as a node-based clade uniting \"Ichthyornis\" with modern birds. However, in many analyses, this definition would be synonymous with the more widely used name Ornithurae. An alternate definition was provided in 2001, naming Carinatae an apomorphy-based clade defined by the presence of a keeled sternum.\n\nThe most primitive known bird relative with a keeled breastbone is \"Confuciusornis\". While some specimens of this stem-bird have flat breastbones, some show a small ridge that may have supported a cartilaginous keel.\n", "id": "7598", "title": "Carinatae"}
{"url": "https://en.wikipedia.org/wiki?curid=7599", "text": "Cocktail\n\nWhen used to refer to any generic alcoholic mixed drink, cocktail may mean any beverage that contains two or more ingredients if at least one of those ingredients contains alcohol.\n\nWhen a mixed drink contains only a distilled spirit and a mixer, such as soda or fruit juice, it is a highball; many of the International Bartenders Association Official Cocktails are highballs. When a cocktail contains only a distilled spirit and a liqueur, it is a duo and when it adds a mixer, it is a trio. Additional ingredients may be sugar, honey, milk, cream, and various herbs.\nThe origin of the word \"cocktail\" is disputed.\n\nThe first recorded use of \"cocktail\" not referring to a horse is found in \"The Morning Post and Gazetteer in London, England\", March 20, 1798:\n\n<poem>\nMr. Pitt,\ntwo petit vers of \"L'huile de Venus\"\nDitto, one of \"perfeit amour\"\nDitto, \"cock-tail\" (vulgarly called ginger)\n</poem>\n\n\"The Oxford English Dictionary\" cites the word as originating in the U.S. The first recorded use of \"cocktail\" as a beverage (possibly non-alcoholic) in the United States appears in \"The Farmer's Cabinet\", April 28, 1803:\n\nThe first definition of \"cocktail\" known to be an alcoholic beverage appeared in \"The Balance and Columbian Repository\" (Hudson, New York) May 13, 1806; editor Harry Croswell answered the question, \"What is a cocktail?\":\n\nAnatoly Liberman endorses as \"highly probable\" the theory advanced by Låftman (1946), which Liberman summarizes as follows:\n\nSeveral authors have theorized that \"cocktail\" may be a corruption of \"cock ale\".\n\nIn his \"Imbibe!\" (2007) David Wondrich also speculates that \"cocktail\" is a reference to a practice for perking up an old horse by means of a ginger suppository so that the animal would \"cock its tail up and be frisky.\"\n\nThere is a lack of clarity on the origins of cocktails. Traditionally cocktails were a mixture of spirits, sugar, water, and bitters. But by the 1860s, a cocktail frequently included a liqueur.\n\nThe first publication of a bartenders' guide which included cocktail recipes was in 1862 — \"How to Mix Drinks; or, The Bon Vivant's Companion\", by \"Professor\" Jerry Thomas. In addition to recipes for punches, sours, slings, cobblers, shrubs, toddies, flips, and a variety of other mixed drinks were 10 recipes for \"cocktails\". A key ingredient differentiating cocktails from other drinks in this compendium was the use of bitters. Mixed drinks popular today that conform to this original meaning of \"cocktail\" include the Old Fashioned whiskey cocktail, the Sazerac cocktail, and the Manhattan cocktail.\n\nThe ingredients listed (spirits, sugar, water, and bitters) match the ingredients of an Old Fashioned, which originated as a term used by late 19th century bar patrons to distinguish cocktails made the \"old-fashioned\" way from newer, more complex cocktails.\n\nThe term highball appears during the 1890s to distinguish a drink composed only of a distilled spirit and a mixer.\n\nThe first \"cocktail party\" ever thrown was allegedly by Mrs. Julius S. Walsh Jr. of St. Louis, Missouri, in May 1917. Mrs. Walsh invited 50 guests to her home at noon on a Sunday. The party lasted an hour, until lunch was served at 1 pm. The site of this first cocktail party still stands. In 1924, the Roman Catholic Archdiocese of St. Louis bought the Walsh mansion at 4510 Lindell Boulevard, and it has served as the local archbishop's residence ever since.\n\nDuring Prohibition in the United States (1919–1933), when alcoholic beverages were illegal, cocktails were still consumed illegally in establishments known as speakeasies. The quality of liquor available during Prohibition was much worse than previously. There was a shift from whiskey to gin, which does not require aging and is therefore easier to produce illicitly. Honey, fruit juices, and other flavorings served to mask the foul taste of the inferior liquors. Sweet cocktails were easier to drink quickly, an important consideration when the establishment might be raided at any moment.\n\nCocktails became less popular in the late 1960s and through the 1970s, until resurging in the 1980s with vodka often substituting the original gin in drinks such as the martini. Traditional cocktails began to make a comeback in the 2000s, and by the mid-2000s there was a renaissance of cocktail culture in a style typically referred to as mixology that draws on traditional cocktails for inspiration but utilizes novel ingredients and often complex flavors.\n\nLists\nDevices for producing and imbibing\nMedia\n\n", "id": "7599", "title": "Cocktail"}
{"url": "https://en.wikipedia.org/wiki?curid=7601", "text": "Coptic Orthodox Church of Alexandria\n\nThe Coptic Orthodox Church of Alexandria is the largest Christian Church in Egypt, Northeast Africa and the Middle East.\n\nAccording to tradition, the Church was established by Saint Mark, an apostle and evangelist, in the middle of the 1st century (approximately AD 42). The head of the Church and the See of Alexandria is the Patriarch of Alexandria on the Holy See of Saint Mark, who also carries the title of Coptic Pope. The See of Alexandria is titular, and today the Coptic Pope presides from Saint Mark's Coptic Orthodox Cathedral in the Abbassia District in Cairo.\n\nThe Coptic Orthodox Church belongs to the Oriental Orthodox family of Churches, which has been a distinct Christian body since the schism following the Council of Chalcedon in AD 451, when it took a different position over Christology from that of the rest of the Christian Church (which would split 600 years later into the Eastern Orthodox Church and the Roman Catholic Church). The precise Christological differences that caused the split with the Coptic Christians are still disputed, highly technical, and mainly concerned with the nature of Christ. The foundational roots of the Coptic Church are based in Egypt, but it has a worldwide following. The Coptic Church follows the Alexandrian Rite for its liturgy, prayer and devotional patrimony.\n\nEgypt is identified in the Bible as the place of refuge that the Holy Family sought in its flight from Judea: When he [Joseph] arose, he took the young Child and His mother by night and departed for Egypt, and was there until the death of Herod the Great, that it might be fulfilled which was spoken by the Lord through the prophet, saying, \"Out of Egypt I called My Son\" (Matthew 2:12–23).\n\nThe Egyptian Church, which is more than 1,900 years old, and most likely the oldest Christian church in the world, traditionally believed to be founded by St Mark at around AD 42, regards itself as the subject of many prophecies in the Old Testament. Isaiah the prophet, in Chapter 19, Verse 19 says \"In that day there will be an altar to the LORD in the midst of the land of Egypt, and a pillar to the LORD at its border.\"\n\nThe first Christians in Egypt were common people who spoke Egyptian Coptic. There were also Alexandrian Jews such as Theophilus, whom Saint Luke the Evangelist addresses in the introductory chapter of his gospel. When the church was founded by Saint Mark during the reign of the Roman emperor Nero, a great multitude of native Egyptians (as opposed to Greeks or Jews) embraced the Christian faith.\n\nChristianity spread throughout Egypt within half a century of Saint Mark's arrival in Alexandria, as is clear from the New Testament writings found in Bahnasa, in Middle Egypt, which date around the year AD 200, and a fragment of the Gospel of John, written in Coptic, which was found in Upper Egypt and can be dated to the first half of the 2nd century. In the 2nd century, Christianity began to spread to the rural areas, and scriptures were translated into the local languages, namely Coptic.\n\nThe Catechetical School of Alexandria is the oldest catechetical school in the world. St. Jerome records that the Christian School of Alexandria was founded by Saint Mark himself. Around AD 190 under the leadership of the scholar Pantanaeus, the school of Alexandria became an important institution of religious learning, where students were taught by scholars such as Athenagoras, Clement, Didymus, and the native Egyptian Origen, who was considered the father of theology and who was also active in the field of commentary and comparative Biblical studies. Origen wrote over 6,000 commentaries of the Bible in addition to his famous Hexapla.\n\nMany scholars such as Jerome visited the school of Alexandria to exchange ideas and to communicate directly with its scholars. The scope of this school was not limited to theological subjects; science, mathematics and humanities were also taught there. The question-and-answer method of commentary began there, and 15 centuries before Braille, wood-carving techniques were in use there by blind scholars to read and write.\n\nThe Theological college of the catechetical school was re-established in 1893. The new school currently has campuses in Ireland, Cairo, New Jersey, and Los Angeles, where Coptic priests-to-be and other qualified men and women are taught among other subjects Christian theology, history, the Coptic language and art – including chanting, music, iconography, and tapestry.\n\nMany Egyptian Christians went to the desert during the 3rd century, and remained there to pray and work and dedicate their lives to seclusion and worship of God. This was the beginning of the monastic movement, which was organized by Anthony the Great, Saint Paul of Thebes, the world's first anchorite, Saint Macarius the Great and Saint Pachomius the Cenobite in the 4th century.\n\nChristian monasticism was born in Egypt and was instrumental in the formation of the Coptic Orthodox Church character of submission, simplicity and humility, thanks to the teachings and writings of the Great Fathers of Egypt's Deserts. By the end of the 5th century, there were hundreds of monasteries, and thousands of cells and caves scattered throughout the Egyptian desert. A great number of these monasteries are still flourishing and have new vocations to this day.\n\nAll Christian monasticism stems, either directly or indirectly, from the Egyptian example: Saint Basil the Great Archbishop of Caesaria of Cappadocia, founder and organizer of the monastic movement in Asia Minor, visited Egypt around AD 357 and his rule is followed by the Eastern Orthodox Churches; Saint Jerome who translated the Bible into Latin, came to Egypt, while en route to Jerusalem, around AD 400 and left details of his experiences in his letters; Benedict founded the Benedictine Order in the 6th century on the model of Saint Pachomius, but in a stricter form. Countless pilgrims have visited the \"Desert Fathers\" to emulate their spiritual, disciplined lives.\n\nIn the 4th century, an Alexandrian presbyter named Arius began a theological dispute about the nature of Christ that spread throughout the Christian world and is now known as Arianism. The Ecumenical Council of Nicea AD 325 was convened by Constantine under the presidency of Saint Hosius of Cordova and Pope Saint Alexander I of Alexandria to resolve the dispute and eventually led to the formulation of the Symbol of Faith, also known as the Nicene Creed. The Creed, which is now recited throughout the Christian world, was based largely on the teaching put forth by a man who eventually would become Pope Saint Athanasius of Alexandria, the chief opponent of Arius.\n\nIn the year AD 381, Pope Timothy I of Alexandria presided over the second ecumenical council known as the Ecumenical Council of Constantinople, to judge Macedonius, who denied the Divinity of the Holy Spirit. This council completed the Nicene Creed with this confirmation of the divinity of the Holy Spirit:\n\nWe believe in the Holy Spirit, the Lord, the Giver of Life, who proceeds from the Father, who with the Father through the Son is worshiped and glorified who spoke by the Prophets and in One, Holy, Catholic, and Apostolic church. We confess one Baptism for the remission of sins and we look for the resurrection of the dead and the life of the coming age, Amen.\n\nAnother theological dispute in the 5th century occurred over the teachings of Nestorius, the Patriarch of Constantinople who taught that God the Word was not hypostatically joined with human nature, but rather dwelt in the man Jesus. As a consequence of this, he denied the title \"Mother of God\" \"(Theotokos)\" to the Virgin Mary, declaring her instead to be \"Mother of Christ\" \"Christotokos\".\n\nWhen reports of this reached the Apostolic Throne of Saint Mark, Pope Saint Cyril I of Alexandria acted quickly to correct this breach with orthodoxy, requesting that Nestorius repent. When he would not, the Synod of Alexandria met in an emergency session and a unanimous agreement was reached. Pope Cyril I of Alexandria, supported by the entire See, sent a letter to Nestorius known as \"The Third Epistle of Saint Cyril to Nestorius.\" This epistle drew heavily on the established Patristic Constitutions and contained the most famous article of Alexandrian Orthodoxy: \"The Twelve Anathemas of Saint Cyril.\" In these anathemas, Cyril excommunicated anyone who followed the teachings of Nestorius. For example, \"Anyone who dares to deny the Holy Virgin the title \"Theotokos\" is Anathema!\" Nestorius however, still would not repent and so this led to the convening of the First Ecumenical Council of Ephesus (431), over which Cyril presided.\n\nThe Council confirmed the teachings of Saint Athanasius and confirmed the title of Mary as \"Mother of God\". It also clearly stated that anyone who separated Christ into two hypostases was anathema, as Cyril had said that there is \"One Nature [and One Hypostasis] for God the Word Incarnate\" (\"Mia Physis tou Theou Logou Sesarkōmenē\"). Also, the introduction to the creed was formulated as follows:\n\nWe magnify you O Mother of the True Light and we glorify you O saint and Mother of God \"(Theotokos)\" for you have borne unto us the Saviour of the world. Glory to you O our Master and King: Christ, the pride of the Apostles, the crown of the martyrs, the rejoicing of the righteous, firmness of the churches and the forgiveness of sins. We proclaim the Holy Trinity in One Godhead: we worship Him, we glorify Him, Lord have mercy, Lord have mercy, Lord bless us, Amen. [not dissimilar to the \"Axion Estin\" Chant still used in Orthodoxy]\n\nWhen in AD 451, Emperor Marcianus attempted to heal divisions in the Church, the response of Pope Dioscorus – the Pope of Alexandria who was later exiled – was that the emperor should not intervene in the affairs of the Church. It was at Chalcedon that the emperor, through the Imperial delegates, enforced harsh disciplinary measures against Pope Dioscorus in response to his boldness. In 449, Pope Dioscorus headed the 2nd Council of Ephesus, called the \"Robber Council\" by Chalcedonian historians. It held to the Miaphysite formula which upheld the Christology of \"One Incarnate Nature of God the Word\" (Greek: μία φύσις Θεοῦ Λόγου σεσαρκωμένη (\"mia physis Theou Logou sesarkōmenē\")), and upheld the heretic Eutyches claiming he was orthodox.\n\nThe Council of Chalcedon summoned Dioscorus three times to appear at the council, after which he was deposed. The Council of Chalcedon further deposed him for his support of Eutyches, but not necessarily for Eutychian Monophysitism. Dioscorus appealed to the conciliar fathers to allow for a more Miaphysite interpretation of Christology at the council, but was denied. Following his being deposed, the Coptic Church and its faithful felt unfairly underrepresented at the council and oppressed politically by the Byzantine Empire. After the Byzantines appointed Proterius of Alexandria as Patriarch to represent the Chalcedonian Church, the Coptic Church appointed their own Patriarch Timothy Aelurus and broke from the Chalcedonian communion.\n\nThe Council of Chalcedon, from the perspective of the Alexandrine Christology, has deviated from the approved Cyrillian terminology and declared that Christ was one hypostasis in two natures. However, in the Nicene-Constantinopolitan Creed, \"Christ was conceived of the Holy Spirit and of the Virgin Mary,\" thus the foundation of the definition according to the Non-Chalcedonian adherents, according to the Christology of Cyril of Alexandria is valid. There is a change in the Non-Chalcedonian definition here, as the Nicene creed clearly uses the terms \"of\", rather than \"in.\"\n\nIn terms of Christology, the Oriental Orthodox (Non-Chalcedonians) understanding is that Christ is \"One Nature—the Logos Incarnate,\" \"of\" the full humanity and full divinity. The Chalcedonians' understanding is that Christ is \" recognized in\" two natures, full humanity and full divinity. Oriental Orthodoxy contends that such a formulation is no different from what the Nestorians teach. This is the doctrinal perception that makes the apparent difference which separated the Oriental Orthodox from the Eastern Orthodox.\n\nThe Council's findings were rejected by many of the Christians on the fringes of the Byzantine Empire, including Egyptians, Syrians, Armenians, and others.\n\nFrom that point onward, Alexandria would have two patriarchs: the non-Chalcedonian native Egyptian one, now known as the Coptic Pope of Alexandria and Patriarch of All Africa on the Holy Apostolic See of St. Mark and the \"Melkite\" or Imperial Patriarch, now known as the Greek Orthodox Patriarch of Alexandria.\n\nAlmost the entire Egyptian population rejected the terms of the Council of Chalcedon and remained faithful to the native Egyptian Church (now known as the Coptic Orthodox Church of Alexandria). Those who supported the Chalcedonian definition remained in communion with the other leading imperial churches of Rome and Constantinople. The non-Chalcedonian party became what is today called the Oriental Orthodox Church.\n\nThe Coptic Orthodox Church of Alexandria regards itself as having been misunderstood at the Council of Chalcedon. There was an opinion in the Church that viewed that perhaps the Council understood the Church of Alexandria correctly, but wanted to curtail the existing power of the Alexandrine Hierarch, especially after the events that happened several years before at Constantinople from Pope Theophilus of Alexandria towards Patriarch John Chrysostom and the unfortunate turnouts of the Second Council of Ephesus in AD 449, where Eutichus misled Pope Dioscorus and the Council in confessing the Orthodox Faith in writing and then renouncing it after the Council, which in turn, had upset Rome, especially that the Tome which was sent was not read during the Council sessions.\n\nTo make things even worse, the Tome of Pope Leo of Rome was, according to the Alexandria School of Theology, particularly in regards to the definition of Christology, considered influenced by Nestorian heretical teachings. So, due to the above-mentioned, especially in the consecutive sequences of events, the Hierarchs of Alexandria were considered holding too much of power from one hand, and on the other hand, due to the conflict of the Schools of Theology, there would be an impasse and a scapegoat, i.e. Pope Dioscorus. The Tome of Leo has been widely criticized (surprisingly by Roman Catholic and Eastern Orthodox scholars) in the past 50 years as a much less than perfect orthodox theological doctrine.\n\nIt is also to be noted that by anathemizing Pope Leo because of the tone and content of his tome, as per Alexandrine Theology perception, Pope Dioscorus was found guilty of doing so without due process; in other words, the Tome of Leo was not a subject of heresy in the first place, but it was a question of questioning the reasons behind not having it either acknowledged or read at the Second Council of Ephesus in AD 449. Pope Dioscorus of Alexandria was never labeled as heretic by the council's canons.\n\nCopts also believe that the Pope of Alexandria was forcibly prevented from attending the third congregation of the council from which he was ousted, apparently the result of a conspiracy tailored by the Roman delegates.\n\nBefore the current positive era of Eastern and Oriental Orthodox dialogues, Chalcedonians sometimes used to call the non-Chalcedonians \"Monophysites\", though the Coptic Orthodox Church in reality regards Monophysitism as a heresy. The Chalcedonian doctrine in turn came to be known as \"Dyophysite\".\n\nA term that comes closer to Coptic Orthodoxy is Miaphysite, which refers to a conjoined nature for Christ, both human and divine, united indivisibly in the Incarnate Logos. The Coptic Orthodox Church of Alexandria believes that Christ is perfect in His divinity, and He is perfect in His humanity, but His divinity and His humanity were united in one nature called \"the nature of the incarnate word\", which was reiterated by Saint Cyril of Alexandria.\n\nCopts, thus, believe in two natures \"human\" and \"divine\" that are united in one hypostasis \"without mingling, without confusion, and without alteration\". These two natures \"did not separate for a moment or the twinkling of an eye\" (Coptic Liturgy of Saint Basil of Caesarea).\n\nPrior to Chalcedon, the Imperial Church's main division stemmed from Nestorianism, eventually leading the Church of the East to declare its independence in 424. After the Council of Chalcedon in 451, the Coptic Church and its hierarchy felt suspicious of what they believed were Nestorian elements within the Chalcedonian Church. As a result, the anti-Chalcedon partisan, Timotheos Aelurus, consigned himself to depose the Chalcedonian Pope of Alexandria, Proterius of Alexandria, and to set himself up as the Pope of Alexandria in opposition to the entire Chalcedonian Church of the Byzantine Empire. Copts suffered under the rule of the Byzantine Eastern Roman Empire. The Melkite Patriarchs, appointed by the emperors as both spiritual leaders and civil governors, massacred those Egyptians they considered heretics. Many were tortured and martyred in attempts to force their acceptance of the Chalcedonian terms, but the Egyptians remained loyal to the Cyrillian Miaphysitism. One of the most renowned Egyptian saints of the period is Saint Samuel the Confessor.\n\nThe Muslim invasion of Egypt took place in AD 639. Despite the political upheaval, the Egyptian population remained mainly Christian. However, gradual conversions to Islam over the centuries had changed Egypt from a Christian to a largely Muslim country by the end of the 12th century. Egypt's Umayyad rulers taxed Christians at a higher rate than Muslims, driving merchants towards Islam and undermining the economic base of the Coptic Church. Although the Coptic Church did not disappear, the Umayyad tax policies made it difficult for the church to retain the Egyptian elites.\n\nThe position of the Copts began to improve early in the 19th century under the stability and tolerance of the Muhammad Ali Dynasty. The Coptic community ceased to be regarded by the state as an administrative unit. In 1855 the jizya tax was abolished. Shortly thereafter, the Copts started to serve in the Egyptian army.\n\nTowards the end of the 19th century, the Coptic Church underwent phases of new development. In 1853, Pope Cyril IV established the first modern Coptic schools, including the first Egyptian school for girls. He also founded a printing press, which was only the second national press in the country. The Pope established very friendly relations with other denominations, to the extent that when the Greek Patriarch in Egypt had to absent himself from the country for a long period of time, he left his Church under the guidance of the Coptic Patriarch.\n\nThe Theological College of the School of Alexandria was reestablished in 1893. It began its new history with five students, one of whom was later to become its dean. Today it has campuses in Alexandria and Cairo, and in various dioceses throughout Egypt, as well as outside Egypt. It has campuses in New Jersey, Los Angeles, Sydney, Melbourne, and London, where potential clergymen and other qualified men and women study many subjects, including theology, church history, missionary studies, and the Coptic language.\n\nOn 4 November 2012, Bishop Tawadros was chosen as the 118th Pope of Alexandria and the Patriarch of All Africa on the Holy See of Saint Mark. His predecessor was Pope Shenouda III, who died on 17 March 2012.\n\nIn 1959, the Ethiopian Orthodox Tewahedo Church was granted its first own Patriarch by Pope Cyril VI. Furthermore, the Eritrean Orthodox Tewahedo Church similarly became independent of the Ethiopian Orthodox Tewahedo Church in 1994, when four bishops were consecrated by Pope Shenouda III of Alexandria to form the basis of a local Holy Synod of the Eritrean Church. In 1998, the Eritrean Orthodox Tewahedo Church gained its autocephelacy from the Coptic Orthodox Church when its first Patriarch was enthroned by Pope Shenouda III of Alexandria.\n\nThese three churches remain in full communion with each other and with the other Oriental Orthodox churches. The Ethiopian Orthodox Tewahedo Church and the Eritrean Orthodox Tewahedo Church do acknowledge the Honorary Supremacy of the Coptic Orthodox Patriarch of Alexandria, since the Church of Alexandria is technically their Mother Church. Upon their selection, both Patriarchs (Ethiopian & Eritrean) must receive the approval and communion from the Holy Synod of the Apostolic See of Alexandria before their enthronement.\n\nSince the 1980s theologians from the Oriental (non-Chalcedonian) Orthodox and Eastern (Chalcedonian) Orthodox churches have been meeting in a bid to resolve theological differences, and have concluded that many of the differences are caused by the two groups using different terminology to describe the same thing (see Agreed Official Statements on Christology with the Eastern Orthodox Churches).\n\nIn the summer of 2001, the Coptic Orthodox and Greek Orthodox Patriarchates of Alexandria agreed to mutually recognize baptisms performed in each other's churches, making re-baptisms unnecessary, and to recognize the sacrament of marriage as celebrated by the other. Previously, if a Coptic Orthodox and Greek Orthodox wanted to get married, the marriage had to be performed twice, once in each church, for it to be recognized by both. Now it can be done in only one church and be recognized by both.\n\nAccording to Christian Tradition and Canon Law, the Coptic Orthodox Church of Alexandria only ordains men to the priesthood and episcopate, and if they wish to be married, they must be married before they are ordained. In this respect they follow the same practices as the Eastern Orthodox Church and Syrian Orthodox Church.\n\nTraditionally, the Coptic language was used in church services, and the scriptures were written in the Coptic alphabet. However, due to the Arabisation of Egypt, service in churches started to witness increased use of Arabic, while preaching is done entirely in Arabic. Native languages are used, in conjunction with Coptic, during services outside of Egypt.\n\nThe liturgical calendar of the Coptic Orthodox Church is the Coptic calendar (also called the Alexandrian Calendar). This calendar is based on the Egyptian calendar of Ancient Egypt. Coptic Orthodox Christians celebrate Christmas on 29 Koiak, which corresponds to 7 January in the Gregorian Calendar and 25 December in the Julian Calendar. Coptic Christmas was adopted as an official national holiday in Egypt in 2002.\n\nThere are about 18 million Coptic Orthodox Christians in the world. Between 10 and 14 million of them are found in Egypt under the jurisdiction of the Coptic Orthodox Church of Alexandria.<ref name=\"The world factbook/Egypt/\"></ref> Estimates of the size of Egypt's Christian population vary from the low government figures of 6 to 7 million to the 12 million reported by some Christian leaders. The actual numbers may be in the 11 to 13 million range, out of an Egyptian population of more than 90 million. However, in 2011, the Pew Research Center announced that Copts in Egypt constitute 4.5% of the population, while the Catholic Holy See puts Copts at 6 to 8%. These lower figures support a downward trend in the percentage of Copts in Egypt, as recorded in consecutive Egyptian censuses, since a 1927 high where Egyptian Christians formed 8.3% of the population. This decline has been explained by Major-General Abu Bakr al-Guindi, head of Egypt's Central Agency for Public Mobilization and Statistics, as being the result of Copts having the highest emigration rate, the lowest birthrate and the highest income level in the country.\n\nThere are also significant numbers in the diaspora outside of Africa in countries such as the United States, Canada, Australia, France, and Germany. The number of Coptic Orthodox Christians in the diaspora is roughly 2 million.\n\nIn addition, there are between 350,000 and 400,000 native African adherents in East, Central and South Africa, most in Sudan, whose population is less than 200,000. Although under the jurisdiction of the Coptic Orthodox Church, these adherents are not considered Copts, since they are not ethnic Egyptians.\n\nSome accounts regard members of the Ethiopian Orthodox Tewahedo Church (roughly 45 million), the Eritrean Orthodox Tewahedo Church (roughly 2.5 million), as members of the Coptic Orthodox Church. This is however a misnomer, since both the Ethiopian and the Eritrean Churches, although daughter churches of the Church of Alexandria, are currently autocephalous churches.\n\nA 2010 New Year's Eve attack by Islamic fundamentalists on the Coptic Orthodox Church in the city of Alexandria left 21 dead and many more injured. One week later, thousands of Muslims stood as human shields outside churches as Coptic Christians attended Christmas Masses on 6 and 7 January 2011.\n\nOn 30 January 2011, just days after the demonstrations to reform the Egyptian government, Muslims in southern Egypt broke into two homes belonging to Coptic Christians. The Muslim assailants murdered 11 people and wounded four others.\n\nIn Tahrir Square, Cairo, on Wednesday 2 February 2011, Coptic Christians joined hands to provide a protective cordon around their Muslim neighbors during salat (prayers) in the midst of the 2011 Egyptian Revolution.\n\nOn 4 October 2011, military and police squads used force late at night to disperse hundreds of angry Coptic demonstrators and their supporters who were attempting to stage a sit-in outside the Maspero TV headquarters in downtown Cairo to protest attacks on a Christian church in Upper Egypt.\n\nOn 17 March 2012, the Coptic Orthodox Pope, Pope Shenouda III died, leaving many Copts mourning and worrying as tensions rose with Muslims. Pope Shenouda III constantly met with Muslim leaders in order to create peace. Many were worried about Muslims controlling Egypt as the Muslim Brotherhood won 70% of the parliamentary elections.\n\nOn 4 November 2012, Bishop Tawadros was chosen as the 118th Pope. In a ritual filled with prayer, chants and incense at Abbasiya cathedral in Cairo, the 60-year-old bishop's name was picked by a blindfolded child from a glass bowl in which the names of two other candidates had also been placed. The enthronement was scheduled on 18 November 2012.\n\nOn 15 February 2015, militants in Libya claiming loyalty to ISIL released a video depicting the beheading of 21 Coptic Christians. Subsequently, the victims were commemorated as martyr saints on the 8th Amshir of the Coptic calendar, which is February 15 of the Gregorian calendar\n\nOn 11 December 2016 a bomb exploded in a Coptic cathedral in Cairo, killing 25 people. Pope Tawadros II came back to Egypt immediately from Greece, where he had inaugurated a new cathedral in Athens.\n\nOn 9 April 2017 (Palm Sunday), two explosions at Coptic churches in Egypt left at least 45 people dead and injured more than 100. One bomb exploded in St. George’s Church in the Nile Delta city of Tanta, 50 miles north of Cairo, during morning Mass. Later that same day a suicide bomber set off an explosion outside the main Coptic church in Alexandria, St. Mark’s Cathedral, killing at least 13 — including three police officers — and injuring 21 others.\n\nBesides Egypt, the Church of Alexandria has jurisdiction over Pentapolis, Libya, Nubia, Sudan, Ethiopia, Eritrea and all Africa.\n\nBoth the Patriarchate of Addis Ababa and all Ethiopia, and the Patriarchate of Asmara and all Eritrea do acknowledge the supremacy of honor and dignity of the Pope and Patriarch of Alexandria on the basis that both Patriarchates were established by the Throne of Alexandria and that they have their roots in the Apostolic Church of Alexandria, and acknowledge that Saint Mark the Apostlic is the founder of their Churches through the heritage and Apostolic evangelization of the Fathers of Alexandria.\n\nIn other words, the Patriarchates of Ethiopia and Eritrea are daughter Churches of the Holy Apostolic Patriarchate of Alexandria.\n\nIn addition to the above, the countries of Uganda, Kenya, Tanzania, Zambia, Zimbabwe, the Congo, Cameroon, Nigeria, Ghana, Botswana, Malawi, Angola, Namibia and South Africa are under the jurisdiction and the evangelization of the Throne of Alexandria. It is still expanding in the continent of Africa.\n\nEthiopia received Christianity next to Jerusalem only a year after Jesus was crucified through its own apostle (Acts 8: 26–39). Christianity became a national religion of Ethiopia, under the dominion of the Church of Alexandria, in the 4th century. The first bishop of Ethiopia, Saint Frumentius, was consecrated as Bishop of Axum by Pope Athanasius of Alexandria in AD 328. From then on, until 1959, the Pope of Alexandria, as Patriarch of All Africa, always named an Egyptian (a Copt) to be the Archbishop of the Ethiopian Church. On 13 July 1948, the Coptic Church of Alexandria and the Ethiopian Orthodox Tewahido Church reached an agreement concerning the relationship between the two churches. In 1950, the Ethiopian Orthodox Tewahido Church was granted autocephaly by Pope Joseph II of Alexandria, head of the Coptic Orthodox Church. Five Ethiopian bishops were immediately consecrated by the Pope of Alexandria and Patriarch of All Africa, and were empowered to elect a new Patriarch for their church. This promotion was completed when Joseph II consecrated the first Ethiopian-born Archbishop, Abuna Basilios, as head of the Ethiopian Church on 14 January 1951. In 1959, Pope Cyril VI of Alexandria crowned Abuna Basilios as the first Patriarch of Ethiopia.\n\nPatriarch Basilios died in 1971, and was succeeded on the same year by Abuna Theophilos. With the fall of Emperor Haile Selassie I of Ethiopia in 1974, the new Marxist government arrested Abuna Theophilos and secretly executed him in 1979. The Ethiopian government then ordered the Ethiopian Church to elect Abuna Takla Haymanot as Patriarch of Ethiopia. The Coptic Orthodox Church refused to recognize the election and enthronement of Abuna Takla Haymanot on the grounds that the Synod of the Ethiopian Church had not removed Abuna Theophilos, and that the Ethiopian government had not publicly acknowledged his death, and he was thus still legitimate Patriarch of Ethiopia. Formal relations between the two churches were halted, although they remained in communion with each other.\n\nAfter the death of Abuna Takla Haymanot in 1988, Abune Merkorios who had close ties to the Derg (Communist) government was elected Patriarch of Ethiopia. Following the fall of the Derg regime in 1991, Abune Merkorios abdicated under public and governmental pressure and went to exile in the United States. The newly elected Patriarch, Abune Paulos was officially recognized by the Coptic Orthodox Church of Alexandria in 1992 as the legitimate Patriarch of Ethiopia. Formal relations between the Coptic Church of Alexandria and the Ethiopian Orthodox Tewahedo Church were resumed on 13 July 2007. Abune Paulos died in August 2012.\n\nFollowing the independence of Eritrea from Ethiopia in 1993, the newly independent Eritrean government appealed to Pope Shenouda III of Alexandria for Eritrean Orthodox autocephaly. In 1994, Pope Shenouda ordained Abune Phillipos as first Archbishop of Eritrea. The Eritrean Orthodox Tewahedo Church obtained autocephaly on 7 May 1998, and Abune Phillipos was subsequently consecrated as first Patriarch of Eritrea. The two churches remain in full communion with each other and with the other Oriental Orthodox Churches, although the Coptic Orthodox Church of Alexandria, along with the Ethiopian Orthodox Tewahedo Church does not recognize the deposition of the third Patriarch of Eritrea, Abune Antonios.\n\nThere are several and institutions outside of Egypt, including churches and institutions in:\n\n\n\n\nThe patriarch of Alexandria was originally known merely as bishop of Alexandria. However, this title continued to evolve as the Church grew under Theophilus and his nephew and successor Cyril ( 376–444), and especially in the 5th century when the Church developed its hierarchy.\n\nThe bishop of Alexandria, being the successor of the first bishop in Roman Egypt consecrated by Saint Mark, was honored by the other bishops as first among equals \"primus inter pares\". Under the sixth canon of the Council of Nicaea, Cyril was raised to prelate or chief bishop at the head of the episcopates of Egypt, Libya, and the Pentapolis without the existence of intermediate archbishops as existed in other ecclesiastic provinces. He had the privilege of choosing and consecrating bishops.\n\nThe title of \"pope\" has been attributed to the Patriarch of Alexandria since the episcopate of Heraclas, the 13th Patriarch of Alexandria. All the clergy of Alexandria and Lower Egypt honored him with the title \"papas\", which means \"father\" as the archbishop and metropolitan having authority over all bishops, within the Egyptian province, who are under his jurisdiction. Alexandria, while the ecclesiastical and provincial capital, also had the distinction as being the place where Saint Mark was martyred.\n\nThe title \"Patriarch\" originally referred to a clan leader or head of a familial lineage. Ecclesiastically it means a bishop of high rank and was originally used as a title for the bishops of Rome, Constantinople, Jerusalem, Antioch, and Alexandria. For the Coptic patriarch, this title was \"Patriarch of Alexandria and all Africa on the Holy Apostolic Throne of Saint Mark the Evangelist,\" that is \"of Egypt\". The title of \"Patriarch\" was first used around the time of the Third Ecumenical Council of Ephesus, convened in AD 431, and ratified at Chalcedon in AD 451.\n\nIt is to be noted that only the Patriarch of Alexandria has the double title of \"Pope\" and \"Patriarch\" among the Eastern Orthodox and Oriental Orthodox ecumenical church heads.\n\nThe Coptic Orthodox patriarchate of Alexandria is governed by its Holy Synod, which is headed by the Patriarch of Alexandria. Under his authority are the metropolitan archbishops, metropolitan bishops, diocesan bishops, patriarchal exarchs, missionary bishops, auxiliary bishops, suffragan bishops, assistant bishops, chorbishops and the patriarchal vicars for the Church of Alexandria. They are organized as follows:\n\n\n\n\n\n", "id": "7601", "title": "Coptic Orthodox Church of Alexandria"}
{"url": "https://en.wikipedia.org/wiki?curid=7602", "text": "Family International\n\nThe Family International (TFI) is a new religious movement that started in 1968 in Huntington Beach, California, USA. It was initially called Teens for Christ and later gained fame as The Children of God (COG). It was later renamed and reorganized as The Family of Love, which eventually was shortened to The Family. It is currently called The Family International.\n\nTFI initially spread a message of salvation, apocalypticism, spiritual \"revolution and happiness\" and distrust of the outside world, which the members called \"the System\". In 1976, it began a method of evangelism called Flirty Fishing, that used sex to \"show God's love and mercy\" and win converts, resulting in controversy. TFI's founder and prophetic leader, David Berg (who was first called \"Moses David\" in the Texas press), gave himself the titles of \"King\", \"The Last Endtime Prophet\", \"Moses\", and \"David\". He communicated with his followers via \"Mo Letters\"—letters of instruction and counsel on myriad spiritual and practical subjects—until his death in late 1994. After his death, his widow Karen Zerby became the leader of TFI, taking the titles of \"Queen\" and \"prophetess\". She married Steve Kelly, an assistant of Berg's whom Berg had handpicked as her \"consort\". Kelly took the title of \"King Peter\" and became the face of TFI, speaking in public more often than either David Berg or Karen Zerby.\n\nMembers of The Children of God (COG) founded communes, first called \"colonies\" (now referred to as \"homes\"), in various cities. They would proselytize in the streets and distribute pamphlets. Leaders within COG were referred to as \"The Chain\".\n\nNew converts memorized Bible verses known as the \"set card\". The \"set card\" contained over 300 Bible verses as well as 10 chapters from the Bible. New members also took Bible classes, and were expected to emulate the lives of early Christians and reject mainstream denominational Christianity. Most incoming members adopted a new Biblical name. \n\nThe founder of the movement, David Brandt Berg (1919–1994), was a former Christian and Missionary Alliance pastor. He was known within the group as Moses David, Mo, Father David, Dad to adult group members, and eventually as Dear Grandpa to the group's youngest members.\n\nBerg communicated with his followers by writing letters. He published nearly 3,000 letters over the space of 24 years, referred to as the \"Mo Letters\". In a letter written in January 1972, Berg stated that he was God's prophet for the contemporary world, attempting to further solidify his spiritual authority within the group. Berg's letters also contained public acknowledgement of his own failings and weaknesses.\n\nBy 1972, COG had 130 communities around the world. COG members had printed and distributed approximately 42 million tracts, the majority of them concerning the nature of God's salvation and America's doom. Street distribution of Berg's Letters (called \"litnessing\") became the COG's predominant method of both outreach and monetary support for the next five years.\n\nThe Children of God was abolished in February 1978. Berg reorganized the movement amid reports of serious misconduct, financial mismanagement, \"The Chain's\" abuse of authority, and disagreements within \"The Chain\" about the continued use of flirty fishing. Berg dismissed more than 300 members of \"The Chain\", and declared the general dissolution of the COG structure. This shift was known as the \"Reorganization Nationalisation Revolution\" (RNR). One-eighth of the total membership left the movement in total. Those who remained became part of an reorganized movement called the \"Family of Love\", and later, \"The Family\". The majority of the group's beliefs remained the same.\n\nThe \"Family of Love\" era was characterized by international expansion. Regular methods of proselytizing included door-to-door distribution of religious tracts and other literature, and organized classes on various aspects of Christian life, with heavy use of group-created music.\n\nIn 1976, before the dissolution of The Children of God, David Berg had introduced a new proselytizing method called Flirty Fishing (or FFing), which encouraged female members to \"show God's love\" through sexual relationships with potential converts. Flirty Fishing was practiced by members of Berg's inner circle starting in 1973, and was introduced to the general membership in 1976 and became common practice within the group. In some areas Flirty Fishers used escort agencies to meet potential converts. According to TFI \"over 100,000 received God's gift of salvation through Jesus, and some chose to live the life of a disciple and missionary\" as a result of Flirty Fishing. Researcher Bill Bainbridge obtained data from TFI suggesting that, from 1974 until 1987, members had sexual contact with 223,989 people while practicing Flirty Fishing.\n\nAt the end of 1983, The Family (TF) reported 10,000 full-time members living in 1,642 TF Homes. By this time TF's \"Music With Meaning\" radio club had grown to almost 20,000 members. According to statistics published by TFI, evangelistic efforts were resulting in an average of 200,000 conversions to Christ and distribution of nearly 30 million pages of literature per month.\n\nIn March 1989 TF issued a statement that, in \"early 1985\", an urgent memorandum had been sent to all members \"reminding them that any such activities [adult-child sexual contact] are \"strictly forbidden\" within our group\" (emphasis in original), and such activities were grounds for immediate excommunication from the group. In January 2005, Claire Borowik, a spokesperson for TFI, stated that \"[d]ue to the fact that our current zero-tolerance policy regarding sexual interaction between adults and underage minors was not in our literature published before 1986, we came to the realization that during a transitional stage of our movement, from 1978 until 1986, there were cases when some minors were subject to sexually inappropriate advances... This was corrected officially in 1986, when any contact between an adult and minor (any person under 21 years of age) was declared an excommunicable offense\".\n\nDuring the 1990s, allegations of child sexual abuse were brought against TF from members all around the world. \n\nIn the early 1990s, TF members took advantage of the newly opened Eastern Europe (following the fall of Communism) and expanded their evangelism campaigns eastward, alongside many other religious groups. The production and dissemination of millions of pieces of literature earned them the colloquial name \"the poster people\".\n\nAfter Berg's death in October 1994, Karen Zerby (known in the group as Mama Maria, Queen Maria, Maria David, or Maria Fontaine), assumed leadership of the group. She later married her longtime coworker, Steven Douglas Kelly, an American known in the group as Peter Amsterdam or King Peter. He legally changed his name to Christopher Smith. He became her traveling representative due to Zerby's reclusive separation from most of her followers.\n\nIn February 1995, the group introduced the \"Love Charter\", which defined the rights and responsibilities of Charter Members and Homes. The Charter also included the \"\"Fundamental Family Rules\"\", a summary of rules and guidelines from past TF publications which were still in effect.\n\nThe Charter specified greater freedoms for members to choose and follow independent pursuits. The rights defined in the Charter were what a member could expect to receive from the group, as well as how members were to be treated by leaders and fellow group members. The responsibilities specified were what members were expected to give to the group if they wished to remain full-time members, including tithing at least 10% of their income to World Services, giving 3% to the \"Family Aid Fund\" set up to support needy field situations, and 1% to regional \"common pots\", used for local projects, activities, and fellowships. The Charter has been amended over the years in accordance with changes within the group's belief structure. As of 2010, TFI's policies state that members must tithe at least 10% of their income or make a monthly contribution in order to retain membership, in accordance with the traditional biblical practice of tithing.\n\nIn the 1994–95 British court case, the Rt. Hon. Lord Justice Alan Ward ruled that the group, including some of its top leaders, had in the past engaged in abusive sexual practices involving minors and had also used severe corporal punishment and sequestration of minors. He found that by 1995 TF had abandoned these practices and concluded that they were a safe environment for children. Nevertheless, he did require that the group cease all corporal punishment of children in the United Kingdom and denounce any of Berg's writings that were \"responsible for children in TF having been subjected to sexually inappropriate behaviour\".\n\nIn 2004, the movement's name was changed to The Family International. However, TFI members were told that they could retain their former names so long as they do not conceal their affiliation with TFI.\n\nIn 2004, there were also major changes in the group. Internal publications spoke of arresting a general trend towards a less dedicated lifestyle, and the need for re-commitment to the group's mission of fervent evangelism. In the second half of 2004, a six-month period was held to help members refocus their priorities (known as \"The Renewal\"). The group was reorganized, with new levels of membership defined into the following categories: Family Disciples (FD), Missionary Members (MM), Fellow Members (FM), Active Members (AM), and General Members (GM).\n\nThe \"Love Charter\" is The Family's set governing document that entails each member's rights, responsibilities and requirements, while the \"Missionary Member Statutes\" and \"Fellow Member Statutes\" were written for the governance of TFI's Missionary member and Fellow Member circles, respectively. FD Homes were reviewed every six months against a published set of criteria. The \"Love Charter\" increased the number of single family homes as well as homes that relied on jobs such as self-employment.\n\nAccording to TFI statistics, at the beginning of 2005 there were 1,238 TFI Homes and 10,202 members worldwide. Of those, 266 Homes and 4,884 members were FD, 255 Homes and 1,769 members were MM, and 717 Homes and 3,549 members were FM. Statistics on AM and GM categories were unavailable.\n\nTFI's recent teachings center around beliefs they term the \"new [spiritual] weapons\". TFI members believe that they are soldiers in the spiritual war of good versus evil for the souls and hearts of men. Although some of the following beliefs are not new to TFI, they have assumed more importance in recent years.\n\nTFI continues to stress the imminent Second Coming of Christ, preceded by the rise of a worldwide government led by the \"Antichrist\". Doctrines of the \"end times\" influence virtually all long-term decision-making. However, documents issued in 2010 have changed this view to reflect a need for long-term plans and projects.\n\nSecond-generation adults (known as \"SGAs\") are adults born or reared in TFI.\n\nAnti-TFI sentiment has been publicly expressed by some who have left the group; examples include sisters Celeste Jones, Kristina Jones, and Juliana Buhring, who wrote a book on their lives in TFI.\n\nTFI members are expected to respect legal and civil authorities where they live. Members have typically cooperated with appointed authorities, even during the police and social-service raids of their communities in the early 1990s.\n\nTFI finances are based on a system of tithing.\n\nThe group has been criticized by the press and the anti-cult movement. In 1971, an organization called FREECOG was founded by concerned parents and others, including deprogrammer Ted Patrick, to \"free\" members of the COG from their involvement in the group. Academics were divided, with some categorizing the TFI as a \"new religious movement\", and others, such as Benjamin Beit-Hallahmi and John Huxley, labeling the group a \"cult.\"\n\nIn 1972, the Children of God reported 130 communes or \"colonies\" in 15 countries. In 1993, 7,000 of TFI's 10,000 members were under 18 years of age. Recent changes have resulted in a small number of members leaving.\n\n\n\n\n\n\"Jesus Freaks: A True Story of Murder and Madness on the Evangelical Edge\"]. HarperOne. ISBN 0-06-111804-4.\n", "id": "7602", "title": "Family International"}
{"url": "https://en.wikipedia.org/wiki?curid=7603", "text": "CIT\n\nCIT or cit may refer to:\n\n\n\n\n", "id": "7603", "title": "CIT"}
{"url": "https://en.wikipedia.org/wiki?curid=7604", "text": "Code of Hammurabi\n\nThe Code of Hammurabi is a well-preserved Babylonian law code of ancient Mesopotamia, dating back to about 1754 BC (Middle Chronology). It is one of the oldest deciphered writings of significant length in the world. The sixth Babylonian king, Hammurabi, enacted the code, and partial copies exist on a seven and a half foot stone stele and various clay tablets. The code consists of 282 laws, with scaled punishments, adjusting \"an eye for an eye, a tooth for a tooth\" (\"lex talionis\") as graded depending on social status, of slave versus free man.\nNearly one-half of the code deals with matters of contract, establishing, for example, the wages to be paid to an ox driver or a surgeon. Other provisions set the terms of a transaction, establishing the liability of a builder for a house that collapses, for example, or property that is damaged while left in the care of another. A third of the code addresses issues concerning household and family relationships such as inheritance, divorce, paternity, and sexual behavior. Only one provision appears to impose obligations on an official; this provision establishes that a judge who reaches an incorrect decision is to be fined and removed from the bench permanently. A few provisions address issues related to military service.\n\nThe code was discovered by modern archaeologists in 1901, and its \"editio princeps\" translation published in 1902 by Jean-Vincent Scheil. This nearly complete example of the code is carved into a basalt stele in the shape of a huge index finger, tall. The code is inscribed in the Akkadian language, using cuneiform script carved into the stele. It is currently on display in the Louvre, with exact replicas in the Oriental Institute at the University of Chicago, the Clendening History of Medicine Library & Museum at the University of Kansas Medical Center, the library of the Theological University of the Reformed Churches (Dutch: Theologische Universiteit Kampen voor de Gereformeerde Kerken) in the Netherlands, the Pergamon Museum of Berlin, and the National Museum of Iran in Tehran.\n\nHammurabi ruled for nearly 42 years, from about 1792 to 1749 BC according to the Middle chronology. In the preface to the law, he states, \"Anu and Bel called by name me, Hammurabi, the exalted prince, who feared God, to bring about the rule of righteousness in the land, to destroy the wicked and the evil-doers; so that the strong should not harm the weak; so that I should rule over the black-headed people like Shamash, and enlighten the land, to further the well-being of mankind.\" On the stone slab are 44 columns and 28 paragraphs that contained 282 laws. Some of these laws follow along the rules of 'an eye for an eye'.\n\nIt had been taken as plunder by the Elamite king Shutruk-Nahhunte in the 12th century BC and was taken to Susa in Elam (located in the present-day Khuzestan Province of Iran) where it was no longer available to the Babylonian people. However, when Cyrus the Great brought both Babylon and Susa under the rule of his Persian Empire, and placed copies of the document in the Library of Sippar, the text became available for all the peoples of the vast Persian Empire to view.\n\nIn 1901, Egyptologist Gustave Jéquier, a member of an expedition headed by Jacques de Morgan, found the stele containing the Code of Hammurabi during archaeological excavations at the ancient site of Susa in Khuzestan.\n\nThe Code of Hammurabi was one of several sets of laws in the ancient Near East and also one of the first forms of law.\nThe code of laws was arranged in orderly groups, so that all who read the laws would know what was required of them.\nEarlier collections of laws include the Code of Ur-Nammu, king of Ur (\"circa\" 2050 BC), the Laws of Eshnunna (\"circa\" 1930 BC) and the codex of Lipit-Ishtar of Isin (\"circa\" 1870 BC), while later ones include the Hittite laws, the Assyrian laws, and Mosaic Law.\nThese codes come from similar cultures in a relatively small geographical area, and they have passages which resemble each other.\nThe Code of Hammurabi is the longest surviving text from the Old Babylonian period.\nThe code has been seen as an early example of a fundamental law, regulating a government — i.e., a primitive constitution. The code is also one of the earliest examples of the idea of presumption of innocence, and it also suggests that both the accused and accuser have the opportunity to provide evidence. The occasional nature of many provisions suggests that the code may be better understood as a codification of Hammurabi's supplementary judicial decisions, and that, by memorializing his wisdom and justice, its purpose may have been the self-glorification of Hammurabi rather than a modern legal code or constitution. However, its copying in subsequent generations indicates that it was used as a model of legal and judicial reasoning.\n\nThe Code issues justice following the three classes of Babylonian society: property owners, freed men, and slaves. For example, if a doctor killed a rich patient, he would have his hands cut off, but if he killed a slave, only financial restitution was required.\n\nVarious copies of portions of the Code of Hammurabi have been found on baked clay tablets, some possibly older than the celebrated basalt stele now in the Louvre. The Prologue of the Code of Hammurabi (the first 305 inscribed squares on the stele) is on such a tablet, also at the Louvre (Inv #AO 10237). Some gaps in the list of benefits bestowed on cities recently annexed by Hammurabi may imply that it is older than the famous stele (it is currently dated to the early 18th century BC). Likewise, the Museum of the Ancient Orient, part of the Istanbul Archaeology Museums, also has a \"Code of Hammurabi\" clay tablet, dated to 1750 BC, in (Room 5, Inv # Ni 2358).\n\nIn July, 2010, archaeologists reported that a fragmentary Akkadian cuneiform tablet was discovered at Tel Hazor, Israel, containing a \"circa\"-1700 BC text that was said to be partly parallel to portions of the Hammurabi code. The Hazor law code fragments are currently being prepared for publication by a team from the Hebrew University of Jerusalem.\n\nThe laws covered such subjects as:\n\nOne of the best known laws from Hammurabi's code was:\n\nHammurabi had many other punishments, as well. If a son strikes his father, his hands shall be hewn off. Translations vary.\n\n\n\nHistory. Exploring the Primary Sources That Shaped the World: 2350 BCE - 1058 CE. Vol. 1. Dallas, TX: Schlager Group, 2010. 23-31.\nAbrams, 2010. Print.\n\n", "id": "7604", "title": "Code of Hammurabi"}
{"url": "https://en.wikipedia.org/wiki?curid=7605", "text": "Cuba Libre\n\nThe Cuba Libre (; , \"Free Cuba\") is a caffeinated alcoholic highball made of cola, lime, and dark or light rum. This cocktail is often referred to as a Rum and Coke in the United States, Canada, the UK, Ireland, India, Singapore, Australia and New Zealand where the lime juice may or may not be included.\n\nAccounts of the invention of the Cuba Libre vary. One account claims that the drink (Spanish for \"Free Cuba\") was invented in Havana, Cuba around 1901/1902. Patriots aiding Cuba during the Spanish–American War—and, later, expatriates avoiding Prohibition—regularly mixed rum and cola as a highball and a toast to this Caribbean island.\n\nAccording to Bacardi:\nThe world's second most popular drink was born in a collision between the United States and Spain. It happened during the Spanish-American War at the turn of the century when Teddy Roosevelt, the Rough Riders, and Americans in large numbers arrived in Cuba. One afternoon, a group of off-duty soldiers from the U.S. Signal Corps were gathered in a bar in Old Havana. Fausto Rodriguez, a young messenger, later recalled that Captain Russell came in and ordered Bacardi (Gold) rum and Coca-Cola on ice with a wedge of lime. The captain drank the concoction with such pleasure that it sparked the interest of the soldiers around him. They had the bartender prepare a round of the captain's drink for them. The Bacardi rum and Coke was an instant hit. As it does to this day, the drink united the crowd in a spirit of fun and good fellowship. When they ordered another round, one soldier suggested that they toast \"¡Por Cuba Libre!\" in celebration of the newly freed Cuba. The captain raised his glass and sang out the battle cry that had inspired Cuba's victorious soldiers in the War of Independence.\n\nThe Rough Riders left Cuba in September 1898 and included no Signal Corps soldiers, so it is clear that the story reflects an incident during the American military occupation of Cuba, and not during the war itself, which ended in 1898. Coca-Cola was not available in Cuba until 1900. According to a 1965 deposition by Fausto Rodriguez, the Cuba Libre was first mixed at a Cuban bar in August 1900 by a member of the U.S. Signal Corps, referred to as \"John Doe\".\n\nAccording to Havana Club:\nAlong with the Mojito and the Daiquiri, the Cuba Libre shares the mystery of its exact origin. The only certainty is that this cocktail was first sipped in Cuba. The year? 1900. 1900 is generally said to be the year that cola first came to Cuba, introduced to the island by American troops. But \"Cuba Libre!\" was the battle cry of the Cuba Liberation Army during the war of independence that ended in 1898.\n\nThis drink was once viewed as exotic, with its dark syrup, made (at that time) from kola nuts and coca.\n\nSoon, as Charles H. Baker, Jr. points out in his \"Gentlemen's Companion\" of 1934, the Cuba Libre \"caught on everywhere throughout the [American] South ... filtered through the North and West,\" aided by the ample supply of its ingredients. In \"The American Language\", 1921, H.L. Mencken writes of an early variation of the drink: \"The troglodytes of western South Carolina coined 'jump stiddy' for a mixture of Coca-Cola and denatured alcohol (usually drawn from automobile radiators); connoisseurs reputedly preferred the taste of what had been aged in Model-T Fords.\"\n\nThe drink gained further popularity in the United States after The Andrews Sisters recorded a song (in 1945) named after the drink's ingredients, \"Rum and Coca-Cola\". Cola and rum were both cheap at the time and this also contributed to the widespread popularity of the concoction.\n\nThe Cuba Pintada (\"stained Cuba\") is one part rum with two parts club soda and just enough cola so that it tints the club soda. The Cuba Campechana (\"half-and-half Cuba\") contains one part rum topped off with equal parts of club soda and cola. They are both popular refreshments, especially among young people.\n\nOther recent variations are the Cuba Light made with rum and Diet Coke, and the Captain Pepper made with Captain Morgan and Dr. Pepper.\n\nAnother variation of the Cuba Libre is the Cuban Missile Crisis. Compared to a normal Cuba Libre, it uses a higher proof rum, such as Bacardi 151 (75.5%).\n\nA variation of the Cuba Libre popular in the West Indies is a \"Hot\" Cuba Libre which includes a splash of Caribbean hot sauce (for example, Capt'n Sleepy's Quintessential Habanero, or Matouk's).\n\nSome people substitute cream soda and spiced rum to create a bright gold drink, often referred to as a Midas.\n\nAnother common variation is the use of \"golden\" or \"dark\" rum as opposed to white rum. This variation is the most commonly used in Venezuela.\n\n\nNotes\n", "id": "7605", "title": "Cuba Libre"}
{"url": "https://en.wikipedia.org/wiki?curid=7607", "text": "Collagen helix\n\nIn collagen, the collagen helix, or type-2 helix, is a major shape in secondary structure. It consists of a triple helix made of the repetitious amino acid sequence glycine - X - Y, where X and Y are frequently proline or hydroxyproline.A collagen triple helix has 3.3 residues per turn.\nEach of the three chains is stabilized by the steric repulsion due to the pyrrolidine rings of proline and hydroxyproline residues. The pyrrolidine rings keep out of each other’s way when the polypeptide chain assumes this extended helical form, which is much more open than the tightly coiled form of the alpha helix.\nThe three chains are hydrogen bonded to each other. The hydrogen bond donors are the peptide NH groups of glycine residues. The hydrogen bond acceptors are the CO groups of residues on the other chains. The OH group of hydroxyproline also participates in hydrogen bonding. The rise of the collagen helix (superhelix) is 2.9 Å (0.29 nm) per residue.\n", "id": "7607", "title": "Collagen helix"}
{"url": "https://en.wikipedia.org/wiki?curid=7609", "text": "Cosmic censorship hypothesis\n\nThe weak and the strong cosmic censorship hypotheses are two mathematical conjectures about the structure of singularities arising in general relativity.\n\nSingularities that arise in the solutions of Einstein's equations are typically hidden within event horizons, and therefore cannot be seen from the rest of spacetime. Singularities that are not so hidden are called \"naked\". The weak cosmic censorship hypothesis was conceived by Roger Penrose in 1969 and posits that no naked singularities, other than the Big Bang singularity, exist in the universe.\n\nSince the physical behavior of singularities is unknown, if singularities can be observed from the rest of spacetime, causality may break down, and physics may lose its predictive power. The issue cannot be avoided, since according to the Penrose-Hawking singularity theorems, singularities are inevitable in physically reasonable situations. Still, in the absence of naked singularities, the universe, as described by the general theory of relativity, is deterministic —it is possible to predict the entire evolution of the universe (possibly excluding some finite regions of space hidden inside event horizons of singularities), knowing only its condition at a certain moment of time (more precisely, everywhere on a spacelike three-dimensional hypersurface, called the Cauchy surface). Failure of the cosmic censorship hypothesis leads to the failure of determinism, because it is yet impossible to predict the behavior of spacetime in the causal future of a singularity. Cosmic censorship is not merely a problem of formal interest; some form of it is assumed whenever black hole event horizons are mentioned.\n\nThe hypothesis was first formulated by Roger Penrose in 1969, and it is not stated in a completely formal way. In a sense it is more of a research program proposal: part of the research is to find a proper formal statement that is physically reasonable and that can be proved to be true or false (and that is sufficiently general to be interesting). Because the statement is not a strictly formal one, there is sufficient latitude for (at least) two independent formulations, a weak form, and a strong form.\n\nThe weak and the strong cosmic censorship hypothesis are two conjectures concerned with the global geometry of spacetimes.\n\nThe weak cosmic censorship hypothesis asserts there can be no singularity visible from future null infinity. In other words, singularities need to be hidden from an observer at infinity by the event horizon of a black hole. Mathematically, the conjecture states that, for generic initial data, the maximal Cauchy development possesses a complete future null infinity.\n\nThe strong cosmic censorship hypothesis asserts that, generically, general relativity is a deterministic theory, in the same sense that classical mechanics is a deterministic theory. In other words, the classical fate of all observers should be predictable from the initial data. Mathematically, the conjecture states that the maximal Cauchy development of generic compact or asymptotically flat initial data is locally inextendible as a regular Lorentzian manifold.\n\nThe two conjectures are mathematically independent, as there exist spacetimes for which the weak cosmic censorship is valid but the strong cosmic censorship is violated and, conversely, there exist spacetimes for which the weak cosmic censorship is violated but the strong cosmic censorship is valid.\n\nThe Kerr metric, corresponding to a black hole of mass formula_1 and angular momentum formula_2, can be used to derive the effective potential for particle orbits restricted to the equator (as defined by rotation). This potential looks like:\nwhere formula_4 is the coordinate radius, formula_5 and formula_6 are the test-particle's conserved energy and angular momentum respectively (constructed from the Killing vectors).\n\nTo preserve \"cosmic censorship\", the black hole is restricted to the case of formula_7. For there to exist an event horizon around the singularity, the requirement formula_7 must be satisfied. This amounts to the angular momentum of the black hole being constrained to below a critical value, outside of which the horizon would disappear. \n\nThe following thought experiment is reproduced from Hartle's \"Gravity\":\n\nImagine specifically trying to violate the censorship conjecture. This could be done by somehow imparting an angular momentum upon the black hole, making it exceed the critical value (assume it starts infinitesimally below it). This could be done by sending a particle of angular momentum formula_9. Because this particle has angular momentum, it can only be captured by the black hole if the maximum potential of the black hole is less than formula_10.\n\nSolving the above effective potential equation for the maximum under the given conditions results in a maximum potential of exactly formula_10! Testing other values shows that no particle with enough angular momentum to violate the censorship conjecture would be able to enter the black hole, because they have too much angular momentum to fall in.\n\nThere are a number of difficulties in formalizing the hypothesis:\n\n\nIn 1991, John Preskill and Kip Thorne bet against Stephen Hawking that the hypothesis was false. Hawking conceded the bet in 1997, due to the discovery of the special situations just mentioned, which he characterized as \"technicalities\". Hawking later reformulated the bet to exclude those technicalities. The revised bet is still open, the prize being \"clothing to cover the winner's nakedness\".\n\nAn exact solution to the scalar-Einstein equations formula_14 which forms a counterexample to many formulations of the \ncosmic censorship hypothesis was found by Mark D. Roberts in 1985:\nwhere formula_16 is a constant.\n\n\n\n", "id": "7609", "title": "Cosmic censorship hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=7610", "text": "Catholic (term)\n\nThe word catholic (with lowercase \"c\"; derived via Late Latin \"catholicus\", from the Greek adjective (\"katholikos\"), meaning \"universal\") comes from the Greek phrase (\"katholou\"), meaning \"on the whole\", \"according to the whole\" or \"in general\", and is a combination of the Greek words meaning \"about\" and meaning \"whole\". The term Catholic (usually written with uppercase \"C\" in English) was first used to describe the Christian Church in the early 2nd century to emphasize its universal scope. In the context of Christian ecclesiology, it has a rich history and several usages. \n\nThe word in English can mean either \"of the Roman Catholic faith\" or \"relating to the historic doctrine and practice of the Western Church\". Many Christians use it to refer more broadly to the whole Christian Church or to all believers in Jesus Christ regardless of denominational affiliation; it can also more narrowly refer to Catholicism, which encompasses several historic churches sharing major beliefs. \"Catholicos\", the title used for the head of some churches in Eastern Christian traditions, is derived from the same linguistic origin.\n\nIn non-ecclesiastical use, it derives its English meaning directly from its root, and is currently used to mean the following:\n\nThe term has been incorporated into the name of the largest Christian communion, the Roman Catholic Church (also called the Catholic Church). All of the three main branches of Christianity in the East (Eastern Orthodox Church, Oriental Orthodox Church and Church of the East) had always identified themselves as \"Catholic\" in accordance with Apostolic traditions and the Nicene Creed. Anglicans, Lutherans, and some Methodists also believe that their churches are \"Catholic\" in the sense that they too are in continuity with the original universal church founded by the Apostles. However, each church defines the scope of the \"Catholic Church\" differently. For instance, the Roman Catholic, Eastern Orthodox, Oriental Orthodox churches, and Church of the East, each maintain that their own denomination is identical with the original universal church, from which all other denominations broke away.\n\nDistinguishing beliefs of Catholicism, the beliefs of most Christians who call themselves \"Catholic\", include the episcopal polity, that bishops are considered the highest order of ministers within the Christian religion, as well as the Nicene Creed of AD 381. In particular, along with unity, sanctity, and apostolicity, catholicity is considered one of Four Marks of the Church, found the line of the Nicene Creed: \"I believe in one holy catholic and apostolic Church.\"\n\nDuring the medieval and modern times, additional distinctions arose regarding the use of the terms \"Western Catholic\" and \"Eastern Catholic\". Before the East–West Schism, those terms had just the basic geographical meanings, since only one undivided Catholicity existed, uniting the Latin speaking Christians of West and the Greek speaking Christians of the East. After the split of 1054 terminology became much more complicated, resulting in the creation of parallel and confronting terminological systems.\n\nThe earliest recorded evidence of the use of the term \"Catholic Church\" is the \"Letter to the Smyrnaeans\" that Ignatius of Antioch wrote in about 107 to Christians in Smyrna. Exhorting Christians to remain closely united with their bishop, he wrote: \"Wherever the bishop shall appear, there let the multitude [of the people] also be; even as, wherever Jesus Christ is, there is the Catholic Church.\"\n\nOf the meaning for Ignatius of this phrase J.H. Srawley wrote:\nThis is the earliest occurrence in Christian literature of the phrase 'the Catholic Church' (ἡ καθολικὴ ἐκκλησία). The original sense of the word is 'universal'. Thus Justin Martyr (\"Dial\". 82) speaks of the 'universal or general resurrection', using the words ἡ καθολικὴ ἀνάστασις. Similarly here the Church universal is contrasted with the particular Church of Smyrna. Ignatius means by the Catholic Church 'the aggregate of all the Christian congregations' (Swete, \"Apostles Creed\", p. 76). So too the letter of the Church of Smyrna is addressed to all the congregations of the Holy Catholic Church in every place. And this primitive sense of 'universal' the word has never lost, although in the latter part of the second century it began to receive the secondary sense of 'orthodox' as opposed to 'heretical'. Thus it is used in an early Canon of Scripture, the Muratorian fragment (\"circa\" 170 A.D.), which refers to certain heretical writings as 'not received in the Catholic Church'. So too Cyril of Jerusalem, in the fourth century, says that the Church is called Catholic not only 'because it is spread throughout the world', but also 'because it teaches completely and without defect all the doctrines which ought to come to the knowledge of men'. This secondary sense arose out of the original meaning because Catholics claimed to teach the whole truth, and to represent the whole Church, while heresy arose out of the exaggeration of some one truth and was essentially partial and local.\n\nBy \"Catholic Church\" Ignatius designated the universal church. Ignatius considered that certain heretics of his time, who disavowed that Jesus was a material being who actually suffered and died, saying instead that \"he only seemed to suffer\" (Smyrnaeans, 2), were not really Christians.\n\nThe term is also used in the \"Martyrdom of Polycarp\" (155) and in the Muratorian fragment (about 177).\n\nAs mentioned in the above quotation from J.H. Srawley, Cyril of Jerusalem (c. 315–386), who is venerated as a saint by the Roman Catholic Church, the Eastern Orthodox Church, and the Anglican Communion, distinguished what he called the \"Catholic Church\" from other groups who could also refer to themselves as an ἐκκλησία (assembly or church):\nSince the word Ecclesia is applied to different things (as also it is written of the multitude in the theatre of the Ephesians, \"And when he had thus spoken, he dismissed the Assembly\" (Acts 19:14), and since one might properly and truly say that there is a \"Church of evil doers\", I mean the meetings of the heretics, the Marcionists and Manichees, and the rest, for this cause the Faith has securely delivered to you now the Article, \"And in one Holy Catholic Church\"; that you may avoid their wretched meetings, and ever abide with the Holy Church Catholic in which you were regenerated. And if ever you are sojourning in cities, inquire not simply where the Lord's House is (for the other sects of the profane also attempt to call their own dens houses of the Lord), nor merely where the Church is, but where is the Catholic Church. For this is the peculiar name of this Holy Church, the mother of us all, which is the spouse of our Lord Jesus Christ, the Only-begotten Son of God(Catechetical Lectures, XVIII, 26).\n\nTheodosius I, Emperor from 379 to 395, declared \"Catholic\" Christianity the official religion of the Roman Empire, declaring in the Edict of Thessalonica of 27 February 380:\n\nIt is our desire that all the various nations which are subject to our clemency and moderation, should continue the profession of that religion which was delivered to the Romans by the divine Apostle Peter, as it has been preserved by faithful tradition and which is now professed by the Pontiff Damasus and by Peter, Bishop of Alexandria, a man of apostolic holiness. According to the apostolic teaching and the doctrine of the Gospel, let us believe in the one Deity of the Father, Son and Holy Spirit, in equal majesty and in a holy Trinity. We authorize the followers of this law to assume the title \"Catholic\" Christians; but as for the others, since in our judgment they are foolish madmen, we decree that they shall be branded with the ignominious name of heretics, and shall not presume to give their conventicles the name of churches. They will suffer in the first place the chastisement of the divine condemnation, and in the second the punishment which our authority, in accordance with the will of heaven, will decide to inflict. Theodosian Code XVI.i.2\n\nOnly slightly later, Saint Augustine of Hippo (354–430) also used the term \"Catholic\" to distinguish the \"true\" church from heretical groups:\nIn the Catholic Church, there are many other things which most justly keep me in her bosom. The consent of peoples and nations keeps me in the Church; so does her authority, inaugurated by miracles, nourished by hope, enlarged by love, established by age. The succession of priests keeps me, beginning from the very seat of the Apostle Peter, to whom the Lord, after His resurrection, gave it in charge to feed His sheep (Jn 21:15–19), down to the present episcopate.\nAnd so, lastly, does the very name of Catholic, which, not without reason, amid so many heresies, the Church has thus retained; so that, though all heretics wish to be called Catholics, yet when a stranger asks where the Catholic Church meets, no heretic will venture to point to his own chapel or house.\nSuch then in number and importance are the precious ties belonging to the Christian name which keep a believer in the Catholic Church, as it is right they should ... With you, where there is none of these things to attract or keep me... No one shall move me from the faith which binds my mind with ties so many and so strong to the Christian religion... For my part, I should not believe the gospel except as moved by the authority of the Catholic Church. —St. Augustine (354–430): \"Against the Epistle of Manichaeus called Fundamental\", chapter 4: Proofs of the Catholic Faith.\n\nA contemporary of Augustine, St. Vincent of Lerins, wrote in 434 (under the pseudonym Peregrinus) a work known as the \"Commonitoria\" (\"Memoranda\"). While insisting that, like the human body, church doctrine develops while truly keeping its identity (sections 54-59, chapter XXIII), he stated:\n\nDuring early centuries of Christian history, majority of Christians who followed doctrines represented in Nicene Creed were bound by one common and undivided Catholicity that was uniting the Latin speaking Christians of West and the Greek speaking Christians of the East. In those days, terms \"eastern Catholic\" and \"western Catholic\" had their basic geographical meanings, generally corresponding to existing linguistic distinctions between Greek East and Latin West. In spite of various and quite frequent theological and ecclesiastical disagreements between major Christian sees, common Catholicity was preserved until the great disputes that arose between 9th and 11th century. After the East–West Schism, the notion of common Catholicity was broken and each side started to develop its own terminological practice.\n\nAll major theological and ecclesiastical disputes in the Christian East or West have been commonly accompanied by attempts of arguing sides to deny each other the right to use the word \"Catholic\" as term of self-designation. After the acceptance of Filioque clause into the Nicene Creed by the Rome, Orthodox Christians in the East started to refer to adherents of Filioquism in the West just as \"Latins\" considering them no longer to be \"Catholics\".\n\nThe dominant view in the Eastern Orthodox Church, that all Western Christians who accepted Filioque interpolation and unorthodox Pneumatology ceased to be Catholics, was held and promoted by famous Eastern Orthodox canonist Theodore Balsamon who was patriarch of Antioch. He wrote in 1190:\n\nOn the other side of the widening rift, Eastern Orthodox were considered by western theologians to be \"Schismatics\". Relations between East and West were further estranged by the tragic events of the Massacre of the Latins in 1182 and Sack of Constantinople in 1204. Those bloody events were followed by several failed attempts to reach reconciliation (see: Second Council of Lyon, Council of Florence, Union of Brest, Union of Uzhhorod). During the late medieval and early modern period, terminology became much more complicated, resulting in the creation of parallel and confronting terminological systems that exist today in all of their complexity.\n\nDuring the Early Modern period, a special term \"Acatholic\" was widely used in the West to mark all those who were considered to hold heretical theological views and irregular ecclesiastical practices. In the time of Counter-Reformation the term \"Acatholic\" was used by zealous members of the Catholic Church to designate Protestants as well as Eastern Orthodox Christians. The term was considered to be so insulting that the Council of the Serbian Orthodox Church, held in Temeswar in 1790, decided to send an official plea to emperor Leopold II, begging him to ban the use of the term \"Acatholic\".\n\nThe Augsburg Confession found within the Book of Concord, a compendium of belief of the Lutheran Churches, teaches that \"the faith as confessed by Luther and his followers is nothing new, but the true catholic faith, and that their churches represent the true catholic or universal church\". When the Lutherans presented the Augsburg Confession to Charles V, Holy Roman Emperor in 1530, they believe to have \"showed that each article of faith and practice was true first of all to Holy Scripture, and then also to the teaching of the church fathers and the councils\".\n\nThe term \"Catholic\" is commonly associated with the whole of the church led by the Roman Pontiff, the Catholic Church. Other Christian churches that use the description \"Catholic\" include the Eastern Orthodox Church and other churches that believe in the historic episcopate (bishops), such as the Anglican Communion. Many of those who apply the term \"Catholic Church\" to all Christians object to the use of the term to designate what they view as only one church within what they understand as the \"whole\" Catholic Church. In the English language, the first known use of the term is in Andrew of Wyntoun's \"Orygynale Cronykil of Scotland\", \"He was a constant Catholic/All Lollard he hated and heretic.\"\n\nThe Catholic Church, led by the Pope in Rome, usually distinguishes itself from other churches by calling itself the \"Catholic\", however has also used the description \"Roman Catholic\". Even apart from documents drawn up jointly with other churches, it has sometimes, in view of the central position it attributes to the See of Rome, adopted the adjective \"Roman\" for the whole church, Eastern as well as Western, as in the papal encyclicals \"Divini illius Magistri\" and \"Humani generis\". Another example is its self-description as \"the holy Catholic Apostolic Roman Church\" (or, by separating each adjective, as the \"Holy, Catholic, Apostolic and Roman Church\") in the 24 April 1870 Dogmatic Constitution on the Catholic Faith of the First Vatican Council. In all of these documents it also refers to itself both simply as the Catholic Church and by other names. The Eastern Catholic Churches, while united with Rome in the faith, have their own traditions and laws, differing from those of the Latin Rite and those of other Eastern Catholic Churches.\n\nThe contemporary Catholic Church has always considered itself to be the historic Catholic Church, and consider all others as \"non-Catholics\". This practice is an application of the belief that not all who claim to be Christians are part of the Catholic Church, as Ignatius of Antioch, the earliest known writer to use the term \"Catholic Church\", considered that certain heretics who called themselves Christians only seemed to be such.\nRegarding the relations with Eastern Christians, Pope Benedict XVI stated his wish to restore full unity with the Orthodox. The Roman Catholic Church considers that almost all of the ancient theological differences have been satisfactorily addressed (the Filioque clause, the nature of purgatory, etc.), and has declared that differences in traditional customs, observances and discipline are no obstacle to unity.\n\nRecent historic ecumenical efforts on the part of the Catholic Church have focused on healing the rupture between the Western (\"Catholic\") and the Eastern (\"Orthodox\") churches. Pope John Paul II often spoke of his great desire that the Catholic Church \"once again breathe with both lungs\", thus emphasizing that the Roman Catholic Church seeks to restore full communion with the separated Eastern churches.\n\nAll of the three main branches of Christianity in the East (Eastern Orthodox Church, Oriental Orthodox Church and Church of the East) are continuing to identify themselves as \"Catholic\" in accordance with Apostolic traditions and the Nicene Creed. The Eastern Orthodox Church firmly upholds the ancient doctrines of Eastern Orthodox Catholicity and commonly uses the term \"Catholic\", as in the title of \"The Longer Catechism of the Orthodox, Catholic, Eastern Church\". So does the Coptic Orthodox Church that belongs to Oriental Orthodoxy and considers its communion to be \"the True Church of the Lord Jesus Christ\". Non of the Eastern Churches, Orthodox or Oriental, have any intention to abandon ancient traditions of their own Catholicity.\n\nMost Reformation and post-Reformation churches use the term \"Catholic\" (often with a lower-case \"c\") to refer to the belief that all Christians are part of one Church regardless of denominational divisions; e.g., Chapter XXV of the Westminster Confession of Faith refers to the \"catholic or universal Church\". It is in line with this interpretation, which applies the word \"catholic\" (universal) to no one denomination, that they understand the phrase \"one holy catholic and apostolic Church\" in the Nicene Creed, the phrase \"the Catholic faith\" in the Athanasian Creed and the phrase \"holy catholic church\" in the Apostles' Creed.\n\nThe term is used also to mean those Christian churches that maintain that their episcopate can be traced unbrokenly back to the apostles and consider themselves part of a \"catholic\" (universal) body of believers. Among those who regard themselves as \"Catholic\" but not \"Roman Catholic\" are Anglicans and Lutherans, who stress that they are both Reformed and Catholic. The Old Catholic Church and the various groups classified as Independent Catholic Churches also lay claim to the description \"Catholic\". Traditionalist Catholics, even if they may not be in communion with Rome, consider themselves to be not only Catholics but the \"true\" Roman Catholics. \n\nSome use the term \"Catholic\" to distinguish their own position from a Calvinist or Puritan form of Reformed-Protestantism. These include a faction of Anglicans often also called Anglo-Catholics, 19th century Neo-Lutherans, 20th century High Church Lutherans or evangelical-Catholics and others.\n\nMethodists and Presbyterians believe their denominations owe their origins to the Apostles and the early church, but do not claim descent from ancient church structures such as the episcopate. However, both of these churches hold that they are a part of the catholic (universal) church. According to \"Harper's New Monthly Magazine\": As such, according to one viewpoint, for those who \"belong to the Church,\" the term Methodist Catholic, or Presbyterian Catholic, or Baptist Catholic, is as proper as the term Roman Catholic. It simply means that body of Christian believers over the world who agree in their religious views, and accept the same ecclesiastical forms.\n\nSome Independent Catholics accept that, among bishops, that of Rome is \"primus inter pares\", and hold that conciliarism is a necessary check against ultramontanism. They are however, by definition, not recognised by the Catholic Church.\n\nSome Protestant churches avoid using the term completely, to the extent among many Lutherans of reciting the Creed with the word \"Christian\" in place of \"catholic\". The Orthodox churches share some of the concerns about Roman Catholic papal claims, but disagree with some Protestants about the nature of the church as one body.\n", "id": "7610", "title": "Catholic (term)"}
{"url": "https://en.wikipedia.org/wiki?curid=7611", "text": "Crystal Eastman\n\nCrystal Catherine Eastman (June 25, 1881 – July 8, 1928) was an American lawyer, antimilitarist, feminist, socialist, and journalist. She is best remembered as a leader in the fight for women's suffrage, as a co-founder and co-editor with her brother Max Eastman of the radical arts and politics magazine \"The Liberator,\" co-founder of the Women's International League for Peace and Freedom, and co-founder in 1920 of the American Civil Liberties Union. In 2000 she was inducted into the National Women's Hall of Fame in Seneca Falls, New York.\n\nCrystal Eastman was born in Marlborough, Massachusetts, on June 25, 1881, the third of four children. In 1883 their parents, Samuel Elijah Eastman and Annis Bertha Ford, moved the family to Canandaigua, New York, where her brother Max was born. A third brother, Anstice Ford Eastman, who became a general surgeon, was born in 1989 and died in 1937. The following year their older brother died at age seven. In 1889, their mother became one of the first women ordained as a Protestant minister in America when she became a minister of the Congregational Church. Her father was also a Congregational minister, and the two served as pastors at the church of Thomas K. Beecher near Elmira. This part of New York was in the so-called \"Burnt Over District.\" During the Second Great Awakening earlier in the 19th century, its frontier had been a center of evangelizing and much religious excitement, which resulted in the founding of the Shakers and Mormonism. During the antebellum period, some were inspired by religious ideals to support such progressive social causes as abolitionism and the Underground Railroad.\n\nCrystal and her brother Max Eastman were influenced by this progressive tradition. Their parents were friendly with the writer Mark Twain. From this association young Crystal also became acquainted with him.\nShe was the sister of the socialist activist Max Eastman, with whom she was quite close throughout her life. The two lived together for several years on 11th Street in Greenwich Village among other radical activists. The group, including Ida Rauh, Inez Milholland, Floyd Dell, and Doris Stevens, also spent summers and weekends in Croton-on-Hudson.\n\nEastman graduated from Vassar College in 1903 and received an M.A. in sociology (a relatively new field) from Columbia University in 1904. Gaining her law degree from New York University Law School, she graduated second in the class of 1907.\n\nSocial work pioneer and journal editor Paul Kellogg offered Eastman her first job, investigating labor conditions for The Pittsburgh Survey sponsored by the Russell Sage Foundation. Her report, \"Work Accidents and the Law\" (1910), became a classic and resulted in the first workers' compensation law, which she drafted while serving on a New York state commission.\n\nShe continued to campaign for occupational safety and health while working as an investigating attorney for the U.S. Commission on Industrial Relations during Woodrow Wilson's presidency. She was at one time called the \"most dangerous woman in America,\" due to her free-love idealism and outspoken nature.\n\nDuring a brief marriage to Wallace J. Benedict which ended in divorce, Eastman moved to Milwaukee and managed the unsuccessful 1912 Wisconsin suffrage campaign.\n\nWhen she returned east in 1913, she joined Alice Paul, Lucy Burns, and others in founding the militant Congressional Union, which became the National Woman's Party. After the passage of the 19th Amendment gave women the vote in 1920, Eastman, Paul, and two others wrote the Equal Rights Amendment, first introduced in 1923. One of the few socialists to endorse the ERA, she warned that protective legislation for women would mean only discrimination against women. Eastman claimed that one could assess the importance of the ERA by the intensity of the opposition to it, but she felt that it was still a struggle worth fighting. She also delivered the speech, \"Now We Can Begin\" following the ratification of the Nineteenth Amendment, outlining the work that needed to be done in the political and economic spheres to achieve gender equality.\n\nDuring World War I, Eastman was one of the founders of the Woman's Peace Party, soon joined by Jane Addams, Lillian D. Wald, and others. She served as president of the New York branch. Renamed the Women's International League for Peace and Freedom in 1921, it remains the oldest extant women's peace organization. Eastman also became executive director of the American Union Against Militarism, which lobbied against America's entrance into the European war and more successfully against war with Mexico in 1916, sought to remove profiteering from arms manufacturing, and campaigned against conscription and imperial adventures.\n\nWhen the United States entered World War I, Eastman organized with Roger Baldwin and Norman Thomas the National Civil Liberties Bureau to protect conscientious objectors, or in her words: \"To maintain something over here that will be worth coming back to when the weary war is over.\" The NCLB grew into the American Civil Liberties Union, with Baldwin at the head and Eastman functioning as attorney-in-charge. Eastman is credited as a founding member of the ACLU, but her role as founder of the NCLB may have been largely ignored by posterity due to her personal differences with Baldwin.\n\nIn 1916 Eastman married the British editor and antiwar activist Walter Fuller, who had come to the United States to direct his sisters’ singing of folksongs. They had two children, Jeffrey and Annis. They worked together as activists until the end of the war; then he worked as the managing editor of \"The Freeman\" until 1922, when he returned to England. He died in 1927, nine months before Crystal, ending his career editing \"Radio Times\" for the BBC.\n\nAfter Max Eastman's periodical \"The Masses\" was forced to close by government censorship in 1917, he and Crystal co-founded a radical journal of politics, art, and literature, \"The Liberator\" early in 1918. She and Max co-edited it until they put it in the hands of faithful friends in 1922.\n\nAfter the war, Eastman organized the First Feminist Congress in 1919.\n\nShe traveled by ship to London to be with her husband at times. In New York, her activities led to her being blacklisted during the Red Scare of 1919-1920. She struggled to find paying work.\n\nDuring the 1920s her only paid work was as a columnist for feminist journals, notably \"Equal Rights\" and \"Time and Tide\". Eastman claimed that \"life was a big battle for the complete feminist,\" but she was convinced that the complete feminist would someday achieve total victory.\n\nCrystal Eastman died on July 8, 1928, of nephritis. Her friends were entrusted with her two children, then orphans, to rear them until adulthood.\n\nEastman has been called one of the United States' most neglected leaders, because, although she wrote pioneering legislation and created long-lasting political organizations, she disappeared from history for fifty years. Freda Kirchwey, then editor of \"The Nation\", wrote at the time of her death: \"When she spoke to people—whether it was to a small committee or a swarming crowd—hearts beat faster. She was for thousands a symbol of what the free woman might be.\"\n\nHer speech \"Now We Can Begin\", given in 1920, is listed as #83 in American Rhetoric's Top 100 Speeches of the 20th Century (listed by rank).\n\nIn 2000 Eastman was inducted into the (American) National Women's Hall of Fame in Seneca Falls, New York.\n\nEastman's papers are housed at Harvard University.\n\nThe Library of Congress has the following publications by Eastman in its collection, much of it published posthumously:\n\n\n", "id": "7611", "title": "Crystal Eastman"}
{"url": "https://en.wikipedia.org/wiki?curid=7612", "text": "Christopher Alexander\n\nChristopher Wolfgang Alexander (born 4 October 1936 in Vienna, Austria) is a widely influential architect and design theorist, and currently emeritus professor at the University of California, Berkeley. His theories about the nature of human-centered design have affected fields beyond architecture, including urban design, software, sociology and others. Alexander has designed and personally built over 100 buildings, both as an architect and a general contractor. \n\nIn software, Alexander is regarded as the father of the pattern language movement. The first wiki—the technology behind Wikipedia—led directly from Alexander's work, according to its creator, Ward Cunningham. Alexander's work has also influenced the development of agile software development and Scrum.\n\nIn architecture, Alexander's work is used by a number of different contemporary architectural communities of practice, including the New Urbanist movement, to help people to reclaim control over their own built environment. However, Alexander is controversial among some mainstream architects and critics, in part because his work is often harshly critical of much of contemporary architectural theory and practice.\n\nAlexander is known for many books on the design and building process, including \"Notes on the Synthesis of Form, A City is Not a Tree\" (first published as a paper and recently re-published in book form), \"The Timeless Way of Building, A New Theory of Urban Design,\" and \"The Oregon Experiment.\" More recently he published the four-volume \"The Nature of Order: An Essay on the Art of Building and the Nature of the Universe,\" about his newer theories of \"morphogenetic\" processes, and \"The Battle for the Life and Beauty of the Earth\", about the implementation of his theories in a large building project in Japan.\n\nAlexander is perhaps best known for his 1977 book \"A Pattern Language,\" a perennial seller some four decades after publication. Reasoning that users are more sensitive to their needs than any architect could be, he produced and validated (in collaboration with his students Sara Ishikawa, Murray Silverstein, Max Jacobson, Ingrid King, and Shlomo Angel) a \"pattern language\" to empower anyone to design and build at any scale.\n\nAs a young child Alexander emigrated in fall 1938 with his parents from Austria to England, when his parents were forced to flee the Nazi regime. He spent much of his childhood in Chichester and Oxford, England, where he began his education in the sciences. He moved from England to the United States in 1958 to study at Harvard University and Massachusetts Institute of Technology. He moved to Berkeley, California in 1963 to accept an appointment as Professor of Architecture, a position he would hold for almost 40 years. In 2002, after his retirement, Alexander moved to Arundel, England, where he continued to write, teach and build. Alexander is married to Margaret Moore Alexander, and he has two daughters, Sophie and Lily, by his former wife Pamela.\n\nAlexander attended Oundle School, England. In 1954, he was awarded the top open scholarship to Trinity College, Cambridge University in chemistry and physics, and went on to read mathematics. He earned a Bachelor's degree in Architecture and a Master's degree in Mathematics. He took his doctorate at Harvard (the first Ph.D. in Architecture ever awarded at Harvard University), and was elected fellow at Harvard. During the same period he worked at MIT in transportation theory and computer science, and worked at Harvard in cognition and cognitive studies.\n\nAlexander was elected to the Society of Fellows, Harvard University 1961-64; awarded the First Medal for Research by the American Institute of Architects, 1972; elected member of the Swedish Royal Academy, 1980; winner of the Best Building in Japan award, 1985; winner of the ACSA (Association of Collegiate Schools of Architecture) Distinguished Professor Award, 1986 and 1987; invited to present the Louis Kahn Memorial Lecture, 1992; awarded the Seaside Prize, 1994; elected a Fellow of the American Academy of Arts and Sciences, 1996; one of the two inaugural recipients of the Athena Award, given by the Congress for the New Urbanism (CNU), 2006;. awarded (\"in absentia\") the Vincent Scully Prize by the National Building Museum, 2009; awarded the lifetime achievement award by the Urban Design Group, 2011; winner of the Global Award for Sustainable Architecture, 2014.\n\n\"The Timeless Way of Building\" (1979) described the perfection of use to which buildings could aspire:\n\n\"A Pattern Language: Towns, Buildings, Construction\" (1977) described a practical architectural system in a form that a theoretical mathematician or computer scientist might call a generative grammar.\n\nThe work originated from an observation that many medieval cities are attractive and harmonious. The authors said that this occurs because they were built to local regulations that required specific features, but freed the architect to adapt them to particular situations.\n\nThe book provides rules and pictures, and leaves decisions to be taken from the precise environment of the project. It describes exact methods for constructing practical, safe and attractive designs at every scale, from entire regions, through cities, neighborhoods, gardens, buildings, rooms, built-in furniture, and fixtures down to the level of doorknobs.\n\nA notable value is that the architectural system consists only of classic patterns tested in the real world and reviewed by multiple architects for beauty and practicality.\n\nThe book includes all needed surveying and structural calculations, and a novel simplified building system that copes with regional shortages of wood and steel, uses easily stored inexpensive materials, and produces long-lasting classic buildings with small amounts of materials, design and labor. It first has users prototype a structure on-site in temporary materials. Once accepted, these are finished by filling them with very-low-density concrete. It uses vaulted construction to build as high as three stories, permitting very high densities.\n\nThis book's method was adopted by the University of Oregon, as described in \"The Oregon Experiment\" (1975), and remains the official planning instrument. It has also been adopted in part by some cities as a building code.\n\nThe idea of a pattern language appears to apply to any complex engineering task, and has been applied to some of them. It has been especially influential in software engineering where patterns have been used to document collective knowledge in the field.\n\n\"A New Theory of Urban Design\" (1987) coincided with a renewal of interest in urbanism among architects, but stood apart from most other expressions of this by assuming a distinctly anti-masterplanning stance. An account of a design studio conducted with Berkeley students, it shows how convincing urban networks can be generated by requiring individual actors to respect only \"local\" rules, in relation to neighbours. A vastly undervalued part of the Alexander canon, \"A New Theory\" is important in understanding the generative processes which give rise to the shanty towns latterly championed by Stewart Brand, Robert Neuwirth, and the Prince of Wales.\n\n\"The Nature of Order: An Essay on the Art of Building and the Nature of the Universe\" (2003–04), which includes The \"Phenomenon of Life\", \"The Process of Creating Life\", \"A Vision of a Living World\" and \"The Luminous Ground\", is Alexander's most comprehensive and elaborate work. In it, he puts forth a new theory about the nature of space and describes how this theory influences thinking about architecture, building, planning, and the way in which we view the world in general. The mostly static patterns from \"A Pattern Language\" have been amended by more dynamic sequences, which describe how to work towards patterns (which can roughly be seen as the end result of sequences). Sequences, like patterns, promise to be tools of wider scope than building (just as his theory of space goes beyond architecture).\n\nThe online publication \"Katarxis 3\" (September 2004) includes several essays by Christopher Alexander, as well as the legendary debate between Alexander and Peter Eisenman from 1982.\n\nAlexander's latest book, \"The Battle for the Life and Beauty of the Earth: A Struggle Between Two World-Systems\" (2012), is the story of the largest project he and his colleagues had ever tackled, the construction of a new High School/College campus in Japan. He also uses the project to connect with themes in his four-volume series. He contrasts his approach, (System A) with the construction processes endemic in the US and Japanese economies (System B). As Alexander describes it, System A is focused on enhancing the life/spirit of spaces within given constraints (land, budget, client needs, etc.) (drawings are sketches - decisions on placing buildings, materials used, finish and such are made in the field as construction proceeds, with adjustments as needed to meet overall budget); System B ignores, and tends to diminish or destroy that quality (architect responsible for drawings which the builder uses to build structures at the lowest possible cost). In the last few chapters he describes \"centers\" as a way of thinking about the connections among spaces, and about what brings more wholeness and life to a space.\n\nAmong Alexander's most notable built works are the Eishin Campus near Tokyo (the building process of which is outlined in his 2012 book \"The Battle for the Life and Beauty of the Earth\"); the West Dean Visitors Centre in West Sussex, England; the Julian Street Inn (a homeless shelter) in San Jose, California (both described in \"Nature of Order\"); the Sala House and the Martinez House (experimental houses in Albany and Martinez, California made of lightweight concrete); the low-cost housing in Mexicali, Mexico (described in \"The Production of Houses\"); and several private houses (described and illustrated in \"The Nature of Order\"). Alexander's built work is characterized by a special quality (which he used to call \"the quality without a name\", but named \"wholeness\" in \"Nature of Order\") that relates to human beings and induces feelings of belonging to the place and structure. This quality is found in the most loved traditional and historic buildings and urban spaces, and is precisely what Alexander has tried to capture with his sophisticated mathematical design theories. Paradoxically, achieving this connective human quality has also moved his buildings away from the abstract imageability valued in contemporary architecture, and this is one reason why his buildings are under-appreciated at present.\n\nHis former student and colleague Michael Mehaffy wrote an introductory essay on Alexander's built work in the online publication \"Katarxis 3\", which includes a gallery of Alexander's major built projects through September 2004.\n\nIn addition to his lengthy teaching career at Berkeley (during which a number of international students began to appreciate and apply his methods), Alexander was a key faculty member at both The Prince of Wales's Summer Schools in Civil Architecture (1990–1994) and The Prince's Foundation for the Built Environment.\n\nAlexander's work has widely influenced architects; among those who acknowledge his influence are Sarah Susanka, Andres Duany, and Witold Rybczynski. Robert Campbell, the Pulitzer Prize-winning architecture critic for the \"Boston Globe\", stated that Alexander \"has had an enormous critical influence on my life and work, and I think that's true of a whole generation of people.\"\n\nArchitecture critic Peter Buchanan, in an essay for \"The Architectural Review\"s 2012 campaign \"The Big Rethink\", argues that Alexander's work as reflected in \"A Pattern Language\" is \"thoroughly subversive and forward looking rather than regressive, as so many misunderstand it to be.\" He continues:\nMany urban development projects continue to incorporate Alexander's ideas. For example, in the UK the developers Living Villages have been highly influenced by Alexander's work and used \"A Pattern Language\" as the basis for the design of The Wintles in Bishops Castle, Shropshire. Sarah Susanka's \"Not So Big House\" movement adapts and popularizes Alexander's patterns and outlook.\n\nAlexander's \"Notes on the Synthesis of Form\" was said to be required reading for researchers in computer science throughout the 1960s. It had an influence in the 1960s and 1970s on programming language design, modular programming, object-oriented programming, software engineering and other design methodologies. Alexander's mathematical concepts and orientation were similar to Edsger Dijkstra's influential \"A Discipline of Programming\".\n\nThe greatest influence of \"A Pattern Language\" in computer science is the design patterns movement. Alexander's philosophy of incremental, organic, coherent design also influenced the extreme programming movement. The Wiki was invented to allow the Hillside Group to work on programming design patterns. More recently the \"deep geometrical structures\" as discussed in \"The Nature of Order\" have been cited as having importance for object-oriented programming, particularly in C++.\n\nWill Wright wrote that Alexander's work was influential in the origin of the \"SimCity\" computer games, and in his later game \"Spore\".\n\nAlexander has often led his own software research, such as the 1996 Gatemaker project with Greg Bryant.\n\nAlexander discovered and conceived a recursive structure, so called wholeness, which is defined mathematically, exists in space and matter physically, and reflects in our minds and cognition psychologically. He had his idea of wholeness back to early 1980s when he finished his very first version of \"The Nature of Order\". In fact, his idea of wholeness or degree of wholeness relying a recursive structure of centers resemble in spirit to Google's PageRank.\n\nThe fourth volume of \"The Nature of Order\" approaches religious questions from a scientific and philosophical rather than mystical direction. In it, Alexander describes deep ties between the nature of matter, human perception of the universe, and the geometries people construct in buildings, cities, and artifacts. He suggests a crucial link between traditional practices and beliefs, and recent scientific advances. Despite his leanings toward Deism, and his naturalistic and anthropological approach to religion, Alexander maintains that he is a practicing member of the Catholic Church, believing it to embody a great deal of accumulated human truth within its rituals.\n\nThe life's work of Alexander is dedicated to turn design from unselfconscious behavior to selfconscious behavior, so called design science. In his very first book \"Notes on the Synthesis of Forms\", he has set what he wanted to do. He was inspired by traditional buildings, and tried to derive some 253 patterns for architectural design. Later on, we further distill 15 geometric properties to characterize living structure in \"The Nature of Order\". The design principles are differentiation and adaptation.\n\nAlexander's published works include:\n\n\nUnpublished:\n\n\n\n", "id": "7612", "title": "Christopher Alexander"}
{"url": "https://en.wikipedia.org/wiki?curid=7614", "text": "Clabbers\n\nClabbers is a game played by tournament Scrabble players for fun, or occasionally at Scrabble variant tournaments. The name derives from the fact that the words CLABBERS and SCRABBLE form an anagram pair.\n\nThe rules are identical to those of Scrabble, except that valid plays are only required to form anagrams of acceptable words; in other words, the letters in a word do not need to be placed in the correct order. If a word is challenged, the player who played the word must then name an acceptable word that anagrams to the tiles played.\n\nBecause the number of \"words\" that can be formed is vastly larger than in standard English, the board usually ends up tightly packed in places, and necessarily quite empty in others. Game scores will often be much higher than in standard Scrabble, due to the relative ease of making high-scoring overlap plays and easier access to premium squares.\n\nThe Internet Scrabble Club offers the ability to play Clabbers online.\n\nHorizontal words from top to bottom (# denotes words that exist in the Collins English Dictionary but not the TWL). Some of the words below have multiple anagrams:\n\n\nVertical words from left to right\n\n\n", "id": "7614", "title": "Clabbers"}
{"url": "https://en.wikipedia.org/wiki?curid=7616", "text": "Canopus (disambiguation)\n\nCanopus may refer to:\n\n", "id": "7616", "title": "Canopus (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=7617", "text": "Corum Jhaelen Irsei\n\nCorum Jhaelen Irsei (\"the Prince in the Scarlet Robe\") is the name of a fictional fantasy hero in a series of two trilogies written by author Michael Moorcock.\n\nCorum is the last survivor of the Vadhagh race and an incarnation aspect of the Eternal Champion, a being that exists in all worlds to ensure there is \"Cosmic Balance\".\n\nThis trilogy consists of \"The Knight of the Swords\" (1971), \"The Queen of the Swords\" (1971), and \"The King of the Swords\" (1971). In the United Kingdom it has been collected as an omnibus edition titled \"Corum\", \"Swords of Corum\" and most recently \"Corum: The Prince in the Scarlet Robe\" (vol. 30 of Orion's Fantasy Masterworks series). In the United States the first trilogy has been published as \"Corum: The Coming of Chaos\". \n\nCorum is a Vadhagh, one of a race of long-lived beings with limited magical abilities dedicated to peaceful pursuits such as art and poetry. A group of \"Mabden\" (men) led by the savage Earl Glandyth-a-Krae raid the family castle and slaughter everyone with the exception of Corum, who escapes. Arming himself, Corum attacks and kills several of the Mabden before being captured and tortured. After having his left hand cut off and right eye put out, Corum escapes by moving into another plane of existence, becoming invisible to the Mabden. They depart and Corum is found by The Brown Man, a dweller of the forest of Laar able to see Corum while out of phase. The Brown Man takes Corum to a being called Arkyn, who treats his wounds and explains he has a higher purpose. \n\nTravelling to Moidel's Castle, Corum encounters his future lover, the Margravine Rhalina. Rhalina, a mabden woman of the civilized land of Lwym an Esh, uses sorcery (a ship summoned from the depths of the ocean and manned by her drowned dead husband and crew) to ward off an attack by Glandyth-a-Krae. Determined to restore himself, Corum and Rhalina travel to the island of Shool, a near immortal and mad sorcerer. During the journey Corum observes a mysterious giant who trawls the ocean with a net. On arrival at the island Shool takes Rhalina hostage, and then provides Corum with two artifacts to replace his lost hand and eye: the Hand of Kwll and the Eye of Rhynn. The Eye of Rhynn allows Corum to see into an undead netherworld where the last beings killed by Corum exist until summoned by the Hand of Kwll. \n\nShool then explains that Corum's ill fortune has been caused by the Chaos God Arioch, the Knight of the Swords. When Arioch and his fellow Chaos Lords conquered the Fifteen Planes, the balance between the forces of Law and Chaos tipped in favor of Chaos, and their minions - such as Glandyth-a-Krae - embarked on a bloody rampage. Shool sends Corum to Arioch's fortress to steal the Heart of Arioch, which the sorcerer intends to use to attain greater power. Corum confronts Arioch, and learns Shool is nothing more than a pawn of the Chaos God. Arioch then ignores Corum, who discovers the location of the Heart. Corum is then attacked by Arioch, but the Hand of Kwll crushes the Heart and banishes the Chaos God forever. Before fading from existence, Arioch warns Corum that he has now earned the enmity of the Sword Rulers. Corum returns to the island to rescue Rhalina, and observes Shool has become a powerless moron, and is devoured by his own creations soon afterwards. Corum learns Arkyn is in fact a Lord of Law, and that this is the first step towards Law regaining control of the Fifteen Planes.\n\nOn another five planes, the forces of Chaos - led by Xiombarg, Queen of the Swords - reign supreme and are on the verge on eradicating the last resistance from the forces of Law. The avatars of the Bear and Dog gods plot with Earl Glandyth-a-Krae to murder Corum and return Arioch to the Fifteen Planes. Guided by Arkyn, Corum, Rhalina and companion Jhary-a-Conel cross the planes and encounter the King Without A Country, the last of his people who in turn is seeking the City in the Pyramid. The group locate the City, which is in fact a floating arsenal powered by advanced technology and inhabited by a people originally from Corum's world and his distant kin. \n\nBesieged by the forces of Chaos, the City requires certain rare minerals to continue to power their weapons. Corum and Jhary attempt to locate the minerals and also encounter Xiombarg, who learns of Corum's identity. Corum slows Xiombarg's forces by defeating their leader, Prince Gaynor the Damned. Xiombarg is goaded into attacking the City directly in revenge for Arioch's banishment. Arkyn provides the minerals and confronts Xiombarg, who has manifested in a vulnerable state. As Arkyn banishes Xiombarg, Corum and his allies devastate the forces of Chaos. Glandyth-a-Krae, however, escapes, and seeks revenge.\n\nA spell - determined to have been cast by the forces of Chaos - forces the inhabitants of Corum's plane to war with each other (including the City in the Pyramid). Desperate to stop the slaughter, Corum, Rhalina and Jhary-a-Conel travel to the last five planes, ruled by Mabelode, the King of the Swords. Rhalina is taken hostage by the forces of Chaos and Corum has several encounters with the forces of Chaos, including Earl Glandyth-a-Krae. \n\nCorum also meets two other aspects of the Eternal Champion: Elric and Erekosë, with all three seeking the mystical city of Tanelorn for their own purposes. After a brief adventure in the \"Vanishing Tower\", the other heroes depart and Corum and Jhary arrive at their version of Tanelorn. Corum discovers one of the \"Lost Gods\", the being Kwll, who is imprisoned and cannot be freed until whole. Corum offers Kwll his hand, on the condition that he aid them against Mabelode. Kwll accepts the terms, but reneges on the bargain until persuaded to assist. Corum is also stripped of his artificial eye, which belongs to Rhynn - actually the mysterious giant Corum had previously encountered. Kwll transports Corum and Jhary to the court of Mabelode, with the pair fleeing with Rhalina when Kwll directly challenges the Chaos God.\n\nHaving found out Corum's location by torturing and killing the Brown Man of Laar, Glandyth-a-Krae marshalled his allies to Moidel's Castle. Glandyth had kept Corum's former hand and eye as souvenirs, and showed them to Corum to provoke a reaction.\n\nIn a final battle Corum avenged his family by killing Glandyth-a-Krae and decimating the last of Chaos' mortal forces. Kwll later located Corum and revealed that all the gods - of both Chaos and Law - have been slain in order to free humanity and allow it to shape its own destiny.\n\nThis trilogy consists of \"The Bull and the Spear\" (1973), \"The Oak and the Ram\" (1973), and \"The Sword and the Stallion\" (1974). It was titled \"The Prince with the Silver Hand\" in the United Kingdom and \"The Chronicles of Corum\" in the United States respectively. The previous trilogy hinted at a Celtic or proto-Celtic setting for the stories - the terms \"mabden\" (human beings) and \"shefanhow\" (demons) occuring in these books are both Cornish language words. The Silver Hand trilogy is more explicit in its Celtic connections, with overt borrowings from Celtic mythology.\n\nSet eighty years after the defeat of the Sword Rulers, Corum has become despondent and alone since the death of his Mabden bride Rhalina. Plagued by voices at night, Corum believes he has gone insane until old friend Jhary-a-Conel advises Corum it is in fact a summons from another world. Listening to the voices allows Corum to pass to the other world, which is in fact the distant future. The descendants of Rhalina's folk, the Tuha-na-Cremm Croich (see: Crom Cruach), who call Corum \"Corum Llew Ereint\" (see: Lludd Llaw Eraint), face extinction by the Fhoi Myore (Fomorians). The Fhoi Myore, seven powerful but diseased and barely sentient giants, with the aid of their allies have conquered the land and plunged it into eternal winter. Allying himself with King Mannach, ruler of the Tuha-na-Cremm Croich, Corum falls in love with his daughter Medhbh (see: Medb).\n\nCorum also hears the prophecy of a seeress, who claims Corum should fear a brother (who will apparently slay him), a harp and above all, beauty. Corum seeks the lost artifacts of the Tuha-na-Cremm Croich - a sacred Bull, a spear, an oak, a ram, a sword and a stallion - which will restore the land. Corum gains new allies, Goffanon (a blacksmith and diminutive giant, a member of the Sidhe race) and Goffanon's cousin and true giant Illbrec. They battle the Fhoi Myore, who themselves have allies: a returned Prince Gaynor, the wizard Calatin and his clone of Corum, the Brothers of the Pine, the undead Ghoolegh and a host of giant demonic dogs. After being instrumental in the death of two of the Fhoi Myore and restoring to his senses the enscorceled Amergin, the High King and Chief Druid of the Tuha-na-Cremm Croich, Corum and his allies fight a final battle in which all their foes are destroyed.\n\nCorum decides not to return his own world, and is attacked by his clone, whom he defeats with the aid of a spell placed on his silver hand by Medhbh. Medhbh, however, attacks and wounds Corum, having been told by the being the Dagdah that their world must be free of all gods and demi-gods if they are to flourish as a people. Corum is then killed with his own sword by his animated silver hand, thereby fulfilling the prophecy.\n\n\"The Swords\" trilogy:\n\n\"The Silver Hand\" trilogy:\n\nAdditional appearances:\n\nThe August Derleth Award won by:\n\n\nFirst Comics published \"The Chronicles of Corum\", a twelve issue limited series (Jan. 1986 - Dec. 1988) that adapted the \"Swords Trilogy\", and was followed by the four issue limited series \"Corum: The Bull and the Spear\" (Jan. - July (bi-monthly) 1989), which adapted the first book in the second trilogy. \n\nDarcsyde Productions produced a supplement for use with Chaosium's \"Stormbringer\" (2001) role-playing game adapting the characters and settings from the \"Corum\" series for role-playing.\n\nGollancz have announced plans to release the entire Corum stories in both print and ebook form, commencing in 2013. The ebooks will be available via Gollancz's SF Gateway site.\n\n", "id": "7617", "title": "Corum Jhaelen Irsei"}
{"url": "https://en.wikipedia.org/wiki?curid=7618", "text": "Cumberland (disambiguation)\n\nCumberland is one of the historic counties of England.\n\nCumberland may also refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "7618", "title": "Cumberland (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=7619", "text": "Capella (disambiguation)\n\nCapella, meaning \"small she goat\" in classical Latin, and chapel in medieval Latin, may refer to:\n\n\n\n\n\n\n", "id": "7619", "title": "Capella (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=7622", "text": "Complex instruction set computing\n\nComplex instruction set computing (CISC ) is a processor design, where single instructions can execute several low-level operations (such as a load from memory, an arithmetic operation, and a memory store) or are capable of multi-step operations or addressing modes within single instructions. The term was retroactively coined in contrast to reduced instruction set computer (RISC) and has therefore become something of an umbrella term for everything that is not RISC, from large and complex mainframe computers to simplistic microcontrollers where memory load and store operations are not separated from arithmetic instructions. \n\nA modern RISC processor can therefore be much more complex than, say, a modern microcontroller using a CISC-labeled instruction set, especially in terms of electronic circuit complexity, but also in terms of the number of instructions or the complexity of their encoding patterns. The only typical differentiating characteristic is that most RISC designs use uniform instruction length for almost all instructions, and employ strictly separate load/store-instructions.\n\nExamples of instruction set architectures that have been retroactively labeled CISC are System/360 through z/Architecture, the PDP-11 and VAX architectures, Data General Nova and many others. Well known microprocessors and microcontrollers that have also been labeled CISC in many academic publications include the Motorola 6800, 6809 and 68000-families; the Intel 8080, iAPX432 and x86-family; the Zilog Z80, Z8 and Z8000-families; the National Semiconductor 32016 and NS320xx-line; the MOS Technology 6502-family; the Intel 8051-family; and others.\n\nSome designs have been regarded as borderline cases by some writers. For instance, the Microchip Technology PIC has been labeled RISC in some circles and CISC in others. The 6502 and 6809 have both been described as \"RISC-like\", although they have complex addressing modes as well as arithmetic instructions that operate on memory, contrary to the RISC-principles.\n\nBefore the RISC philosophy became prominent, many computer architects tried to bridge the so-called semantic gap, i.e. to design instruction sets that directly supported high-level programming constructs such as procedure calls, loop control, and complex addressing modes, allowing data structure and array accesses to be combined into single instructions. Instructions are also typically highly encoded in order to further enhance the code density. The compact nature of such instruction sets results in smaller program sizes and fewer (slow) main memory accesses, which at the time (early 1960s and onwards) resulted in a tremendous savings on the cost of computer memory and disc storage, as well as faster execution. It also meant good programming productivity even in assembly language, as high level languages such as Fortran or Algol were not always available or appropriate (microprocessors in this category are sometimes still programmed in assembly language for certain types of critical applications).\n\nIn the 1970s, analysis of high level languages indicated some complex machine language implementations and it was determined that new instructions could improve performance. Some instructions were added that were never intended to be used in assembly language but fit well with compiled high-level languages. Compilers were updated to take advantage of these instructions. The benefits of semantically rich instructions with compact encodings can be seen in modern processors as well, particularly in the high-performance segment where caches are a central component (as opposed to most embedded systems). This is because these fast, but complex and expensive, memories are inherently limited in size, making compact code beneficial. Of course, the fundamental reason they are needed is that main memories (i.e. dynamic RAM today) remain slow compared to a (high performance) CPU core.\n\nWhile many designs achieved the aim of higher throughput at lower cost and also allowed high-level language constructs to be expressed by fewer instructions, it was observed that this was not \"always\" the case. For instance, low-end versions of complex architectures (i.e. using less hardware) could lead to situations where it was possible to improve performance by \"not\" using a complex instruction (such as a procedure call or enter instruction), but instead using a sequence of simpler instructions.\n\nOne reason for this was that architects (microcode writers) sometimes \"over-designed\" assembly language instructions, i.e. including features which were not possible to implement efficiently on the basic hardware available. This could, for instance, be \"side effects\" (above conventional flags), such as the setting of a register or memory location that was perhaps seldom used; if this was done via ordinary (non duplicated) internal buses, or even the \"external\" bus, it would demand extra cycles every time, and thus be quite inefficient.\n\nEven in balanced high-performance designs, highly encoded and (relatively) high-level instructions could be complicated to decode and execute efficiently within a limited transistor budget. Such architectures therefore required a great deal of work on the part of the processor designer in cases where a simpler, but (typically) slower, solution based on decode tables and/or microcode sequencing is not appropriate. At a time when transistors and other components were a limited resource, this also left fewer components and less opportunity for other types of performance optimizations.\n\nThe circuitry that performs the actions defined by the microcode in many (but not all) CISC processors is, in itself, a processor which in many ways is reminiscent in structure to very early CPU designs. In the early 1970s, this gave rise to ideas to return to simpler processor designs in order to make it more feasible to cope without (\"then\" relatively large and expensive) ROM tables and/or PLA structures for sequencing and/or decoding. The first (retroactively) RISC-\"labeled\" processor (IBM 801 IBM's Watson Research Center, mid-1970s) was a tightly pipelined simple machine originally intended to be used as an internal microcode kernel, or engine, in CISC designs, but also became the processor that introduced the RISC idea to a somewhat larger public. Simplicity and regularity also in the visible instruction set would make it easier to implement overlapping processor stages (pipelining) at the machine code level (i.e. the level seen by compilers). However, pipelining at that level was already used in some high performance CISC \"supercomputers\" in order to reduce the instruction cycle time (despite the complications of implementing within the limited component count and wiring complexity feasible at the time). Internal microcode execution in CISC processors, on the other hand, could be more or less pipelined depending on the particular design, and therefore more or less akin to the basic structure of RISC processors.\n\nIn a more modern context, the complex variable-length encoding used by some of the typical CISC architectures makes it complicated, but still feasible, to build a superscalar implementation of a CISC programming model \"directly\"; the in-order superscalar original Pentium and the out-of-order superscalar Cyrix 6x86 are well known examples of this. The frequent memory accesses for operands of a typical CISC machine may limit the instruction level parallelism that can be extracted from the code, although this is strongly mediated by the fast cache structures used in modern designs, as well as by other measures. Due to inherently compact and semantically rich instructions, the average amount of work performed per machine code unit (i.e. per byte or bit) is higher for a CISC than a RISC processor, which may give it a significant advantage in a modern cache based implementation.\n\nTransistors for logic, PLAs, and microcode are no longer scarce resources; only large high-speed cache memories are limited by the maximum number of transistors today. Although complex, the transistor count of CISC decoders do not grow exponentially like the total number of transistors per processor (the majority typically used for caches). Together with better tools and enhanced technologies, this has led to new implementations of highly encoded and variable length designs without load-store limitations (i.e. non-RISC). This governs re-implementations of older architectures such as the ubiquitous x86 (see below) as well as new designs for microcontrollers for embedded systems, and similar uses. The superscalar complexity in the case of modern x86 was solved by converting instructions into one or more micro-operations and dynamically issuing those micro-operations, i.e. indirect and dynamic superscalar execution; the Pentium Pro and AMD K5 are early examples of this. It allows a fairly simple superscalar design to be located after the (fairly complex) decoders (and buffers), giving, so to speak, the best of both worlds in many respects.\n\nThe terms CISC and RISC have become less meaningful with the continued evolution of both CISC and RISC designs and implementations. The first highly (or tightly) pipelined x86 implementations, the 486 designs from Intel, AMD, Cyrix, and IBM, supported every instruction that their predecessors did, but achieved \"maximum efficiency\" only on a fairly simple x86 subset that was only a little more than a typical RISC instruction set (i.e. without typical RISC \"load-store\" limitations). The Intel P5 Pentium generation was a superscalar version of these principles. However, modern x86 processors also (typically) decode and split instructions into dynamic sequences of internally buffered micro-operations, which not only helps execute a larger subset of instructions in a pipelined (overlapping) fashion, but also facilitates more advanced extraction of parallelism out of the code stream, for even higher performance.\n\nContrary to popular simplifications (present also in some academic texts), not all CISCs are microcoded or have \"complex\" instructions. As CISC became a catch-all term meaning anything that's not a load-store (RISC) architecture, it's not the number of instructions, nor the complexity of the implementation or of the instructions themselves, that define CISC, but the fact that arithmetic instructions also perform memory accesses. Compared to a small 8-bit CISC processor, a RISC floating-point instruction is complex. CISC does not even need to have complex addressing modes; 32 or 64-bit RISC processors may well have more complex addressing modes than small 8-bit CISC processors.\n\nA PDP-10, a PDP-8, an Intel 386, an Intel 4004, a Motorola 68000, a System z mainframe, a Burroughs B5000, a VAX, a Zilog Z80000, and a MOS Technology 6502 all vary wildly in the number, sizes, and formats of instructions, the number, types, and sizes of registers, and the available data types. Some have hardware support for operations like scanning for a substring, arbitrary-precision BCD arithmetic, or transcendental functions, while others have only 8-bit addition and subtraction. But they are all in the CISC category because they have \"load-operate\" instructions that load and/or store memory contents within the same instructions that perform the actual calculations. For instance, the PDP-8, having only 8 fixed-length instructions and no microcode at all, is a CISC because of \"how\" the instructions work, PowerPC, which has over 230 instructions (more than some VAXes) and complex internals like register renaming and a reorder buffer is a RISC, while Minimal CISC has 8 instructions, but is clearly a CISC because it combines memory access and computation in the same instructions.\n\nSome of the problems and contradictions in this terminology will perhaps disappear as more systematic terms, such as (non) load/store, become more popular and eventually replace the imprecise and slightly counter-intuitive RISC/CISC terms.\n\n\n\n", "id": "7622", "title": "Complex instruction set computing"}
{"url": "https://en.wikipedia.org/wiki?curid=7624", "text": "CISC\n\nCISC may refer to:\n\n", "id": "7624", "title": "CISC"}
{"url": "https://en.wikipedia.org/wiki?curid=7626", "text": "Cetacea\n\nCetacea () are a widely distributed and diverse clade of aquatic mammals that today consists of the whales, dolphins, and porpoises. Cetaceans are carnivorous and finned. Most species live in the sea, some in rivers. The name is derived from the Latin \"cetus\" \"whale\" and Greek \"ketos\" \"huge fish\".\n\nThere are around 89 extant species, which are divided into two groups or parvorders, the Odontoceti or toothed whales, a group of more than 70 species that includes the dolphins and porpoises, and the Mysticeti or baleen whales, of which there are now 15 species. The extinct ancestors of modern whales are the Archaeoceti.\n\nWhile cetaceans were historically thought to have descended from mesonychids, molecular evidence supports them as a relative of Artiodactyls (even-toed ungulates). Cetaceans belong to the order Cetartiodactyla (formed by combining Cetacea + Artiodactyla) and their closest living relatives are hippopotamuses and other hoofed mammals (camels, pigs, and ruminants), having diverged about 50 million years ago.\n\nCetaceans range in size from the and Maui's dolphin to the and blue whale, which is also the largest animal ever known to have existed. Several species exhibit sexual dimorphism. They have streamlined bodies and two (external) limbs that are modified into flippers. Though not as flexible or agile as seals, cetaceans can swim very quickly, with the killer whale able to travel at in short bursts and the fin whale able to cruise at . Dolphins are able to make very tight turns while swimming at high speeds. The hindlimbs of cetaceans are internal, and are thought to be vestigial. Baleen whales have short hairs on their mouth, unlike the toothed whales. Cetaceans have well-developed senses—their eyesight and hearing are adapted for both air and water, and baleen whales have a tactile system in their vibrissae. They have a layer of fat, or blubber, under the skin to maintain body heat in cold water. Some species are well adapted for diving to great depths.\n\nAlthough cetaceans are widespread, most species prefer the colder waters of the Northern and Southern Hemispheres. They spend their lives in the water, having to mate, give birth, molt or escape from predators, like killer whales, underwater. This has drastically affected their anatomy to be able to do so. They feed largely on fish and marine invertebrates; but a few, like the killer whale, feed on large mammals and birds, such as penguins and seals. Some baleen whales (mainly gray whales and right whales) are specialised for feeding on benthic creatures. Male cetaceans typically mate with more than one female (polygyny), although the degree of polygyny varies with the species. Cetaceans are not shown to have pair bonds. Male cetacean strategies for reproductive success vary between herding females, defending potential mates from other males, or whale song which attracts mates. Calves are typically born in the fall and winter months, and females bear almost all the responsibility for raising them. Mothers of some species fast and nurse their young for a relatively short period of time, which is more typical of baleen whales as their main food source (invertebrates) aren't found in their breeding and calving grounds (tropics). Cetaceans produce a number of vocalizations, notably the clicks and whistles of dolphins, the moaning songs of the humpback whale.\n\nThe meat, blubber and oil of cetaceans have traditionally been used by indigenous peoples of the Arctic. Cetaceans have been depicted in various cultures worldwide. Dolphins are commonly kept in captivity and are even sometimes trained to perform tricks and tasks, other cetaceans aren't as often kept in captivity (with usually unsuccessful attempts). Cetaceans have been relentlessly hunted by commercial industries for their products, although this is now forbidden by international law. The baiji (Chinese river dolphin) has become \"Possibly Extinct\" in the past century, while the vaquita and Yangtze finless porpoise are ranked Critically Endangered by the International Union for Conservation of Nature. Besides hunting, cetaceans also face threats from accidental trapping, marine pollution, and ongoing climate change.\nThe two parvorders, baleen whales (Mysticeti) and toothed whales (Odontoceti), are thought to have diverged around thirty-four million years ago.\n\nBaleen whales have bristles made of keratin instead of teeth. The bristles filter krill and other small invertebrates from seawater. Grey whales feed on bottom-dwelling mollusks. Rorqual family (balaenopterids) use throat pleats to expand their mouths to take in food and sieve out the water. Balaenids (right whales and bowhead whales) have massive heads that can make up 40% of their body mass. Most mysticetes prefer the food-rich colder waters of the Northern and Southern Hemispheres, migrating to the Equator to give birth. During this process, they are capable of fasting for several months, relying on their fat reserves.\n\nThe parvorder of Odontocetes – the toothed whales – include sperm whales, beaked whales, killer whales, dolphins and porpoises. They have conical teeth designed for catching fish or squid. A few, such as the killer whale, feed on mammals, such as pinnipeds and other whales. They have well-developed senses – their eyesight and hearing are adapted for both air and water, and they have advanced sonar capabilities using their melon. Their hearing is so well-adapted for both air and water that some blind specimens can survive. Some species, such as sperm whales, are well adapted for diving to great depths. Several species of odontocetes show sexual dimorphism, in which the males differ from the females, usually for purposes of sexual display or aggression. Toothed whales feed largely on fish and marine invertebrates.\n\nCetacean bodies are generally similar to that of fish, which can be attributed to their lifestyle and the habitat conditions. Their body is well-adapted to their habitat, although they share essential characteristics with other higher mammals (Eutheria).\n\nThey have a streamlined shape, and their forelimbs are flippers. Almost all have a dorsal fin on their backs that can take on many forms depending on the species. A few species, such as the beluga whale, lack them. Both the flipper and the fin are for stabilization and steering in the water.\n\nThe male genitals and mammary glands of females are sunken into the body.\n\nThe body is wrapped in a thick layer of fat, known as blubber, used for thermal insulation and gives cetaceans their smooth, streamlined body shape. In larger species, it can reach a thickness up to half a meter (1.6 ft).\n\nSexual dimorphism evolved in many toothed whales. Sperm whales, narwhals, many members of the beaked whale family, several species of the porpoise family, killer whales, pilot whales, eastern spinner dolphins and northern right whale dolphins show this characteristic. Males in these species developed external features absent in females that are advantageous in combat or display. For example, male sperm whales are up to 63% percent larger than females, and many beaked whales possess tusks used in competition among males.\n\nThey have a cartilaginous fluke at the end of their tails that is used for propulsion. The fluke is set horizontally on the body, unlike fish, which have vertical tails.\n\nHind legs are not present in cetaceans, nor are any other external body attachments such as a pinna and hair.\n\nWhales have an elongated head, especially baleen whales, due to the wide overhanging jaw. Bowhead whale plates can be long. Their nostril(s) make up the blowhole, with one in toothed whales and two in baleen whales.\n\nThe nostrils are located on top of the head above the eyes so that the rest of the body can remain submerged while surfacing for air. The back of the skull is significantly shortened and deformed. By shifting the nostrils to the top of the head, the nasal passages extend perpendicularly through the skull. The teeth or baleen in the upper jaw sit exclusively on the maxilla. The braincase is concentrated through the nasal passage to the front and is correspondingly higher, with individual cranial bones that overlap. The bony otic capsule, the petrosal, is connected to the skull with cartilage, so that it can swing independently.\n\nIn toothed whales, connective tissue exists in the melon as a head buckle. This is filled with air sacs and fat that aid in buoyancy and biosonar. The sperm whale has a particularly pronounced melon; this is called the spermaceti organ and contains the eponymous spermaceti, hence the name \"sperm whale\". Even the long tusk of the narwhal is a vice-formed tooth. In many toothed whales, the depression in their skull is due to the formation of a large melon and multiple, asymmetric air bags.\n\nRiver dolphins, unlike most other cetaceans, can turn their head 90°. Other cetaceans have fused neck vertebrae and are unable to turn their head at all.\n\nThe baleen of baleen whales consists of long, fibrous strands of keratin. Located in place of the teeth, it has the appearance of a huge fringe and is used to sieve the water for plankton and krill.\n\nThe neocortex of many cetaceans is home to elongated spindle neurons that, prior to 2007, were known only in hominids. In humans, these cells are involved in social conduct, emotions, judgment and theory of mind. Cetacean spindle neurons are found in areas of the brain homologous to where they are found in humans, suggesting they perform a similar function.\n\nBrain size was previously considered a major indicator of intelligence. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately two-thirds or three-quarter exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such an analysis provides an encephalization quotient that can be used as an indication of animal intelligence. Sperm whales have the largest brain mass of any animal on earth, averaging and in mature males. The brain to body mass ratio in some odontocetes, such as belugas and narwhals, is second only to humans. In some whales, however, it is less than half that of humans: 0.9% versus 2.1%. The sperm whale (\"Physeter macrocephalus\") is the largest of all toothed predatory animals and possesses the largest brain.\n\nThe cetacean skeleton is largely made up of cortical bone, which stabilizes the animal in the water. For this reason, the usual terrestrial compact bones, which are finely woven cancellous bone, are replaced with lighter and more elastic material. In many places, bone elements are replaced by cartilage and even fat, thereby improving their hydrostatic qualities. The ear and the muzzle contain a bone shape that is exclusive to cetaceans with a high density, resembling porcelain. This conducts sound better than other bones, thus aiding biosonar.\n\nThe number of vertebrae that make up the spine varies by species, ranging from forty to ninety-three. The cervical spine, found in all mammals, consists of seven vertebrae which, however, are reduced or fused. This gives stability during swimming at the expense of mobility. The fins are carried by the thoracic vertebrae, ranging from nine to seventeen individual vertebrae. The sternum is cartilaginous. The last two to three pairs of ribs are not connected and hang freely in the body wall. The stable lumbar and tail include the other vertebrae. Below the caudal vertebrae is the chevron bone; the vortex developed provides additional attachment points for the tail musculature.\n\nThe front limbs are paddle-shaped with shortened arms and elongated finger bones, to support movement. They are connected by cartilage. The second and third fingers display a proliferation of the finger members, a so-called hyperphalangy. The shoulder joint is the only functional joint in all cetaceans except for the Amazon river dolphin. The collarbone is completely absent. The rear limbs are vestigial, without connections to the spine.\n\nCetaceans have powerful hearts. Blood oxygen is distributed effectively throughout the body. They are warm-blooded, i.e., they hold a nearly constant body temperature.\n\nCetaceans have lungs, meaning they breathe air. An individual can last without a breath from a few minutes to over two hours depending on the species. Cetacea are deliberate breathers who must be awake to inhale and exhale. When stale air, warmed from the lungs, is exhaled, it condenses as it meets colder external air. As with a terrestrial mammal breathing out on a cold day, a small cloud of 'steam' appears. This is called the 'spout' and varies across species in shape, angle and height. Species can be identified at a distance using this characteristic.\n\nThe structure of the respiratory and circulatory systems is of particular importance for the life of marine mammals. The oxygen balance is effective. Each breath can replace up to 90% of the total lung volume. For land mammals, in comparison, this value is usually about 15%. During inhalation, about twice as much oxygen is absorbed by the lung tissue as in a land mammal. As with all mammals, the oxygen is stored in the blood and the lungs, but in cetaceans, it is also stored in various tissues, mainly in the muscles. The muscle pigment, myoglobin, provides an effective bond. This additional oxygen storage is vital for deep diving, since beyond a depth around , the lung tissue is almost completely compressed by the water pressure.\n\nThe stomach consists of three chambers. The first region is formed by a loose gland and a muscular forestomach (missing in beaked whales), which is then followed by the main stomach and the pylorus. Both are equipped with glands to help digestion. A bowel adjoins the stomachs, whose individual sections can only be distinguished histologically. The liver is large and separate from the gall bladder.\n\nThe kidneys are long and flattened. The salt concentration in cetacean blood is lower than that in seawater, requiring kidneys to excrete salt. This allows the animals to drink seawater.\n\nCetacean eyes are set on the sides rather than the front of the head. This means only species with pointed 'beaks' (such as dolphins) have good binocular vision forward and downward. Tear glands secrete greasy tears, which protect the eyes from the salt in the water. The lens is almost spherical, which is most efficient at focusing the minimal light that reaches deep water. Cetaceans make up for their generally poor vision (except dolphins) with excellent hearing.\n\nAt least one species, the tucuxi or Guiana dolphin, is able to use electroreception to sense prey.\n\nWhile the teeth are divided into incisors, canines and molars among terrestrial archaeocetes, the teeth of modern cetaceans are brought into line with each other, which can be seen among the fish-eating odontocetes (transition from heterodont to homodont).\n\nThe external ear has lost the pinna (visible ear), but still retains a narrow external auditory meatus. To register sounds, instead, the posterior part of the mandible has a thin lateral wall (the pan bone) fronting a concavity that houses a fat pad. The pad passes anteriorly into the greatly enlarged mandibular foramen to reach in under the teeth and posteriorly to reach the thin lateral wall of the ectotympanic. The ectotympanic offers a reduced attachment area for the tympanic membrane. The connection between this auditory complex and the rest of the skull is reduced—to a single, small cartilage in oceanic dolphins.\n\nIn odontocetes, the complex is surrounded by spongy tissue filled with air spaces, while in mysticetes, it is integrated into the skull as with land mammals. In odontocetes, the tympanic membrane (or ligament) has the shape of a folded-in umbrella that stretches from the ectotympanic ring and narrows off to the malleus (quite unlike the flat, circular membrane found in land mammals.) In mysticetes, it also forms a large protrusion (known as the \"glove finger\"), which stretches into the external meatus and the stapes are larger than in odontocetes. In some small sperm whales, the malleus is fused with the ectotympanic.\n\nThe ear ossicles are pachyosteosclerotic (dense and compact) and differently shaped from land mammals (other aquatic mammals, such as sirenians and earless seals, have also lost their pinnae). T semicircular canals are much smaller relative to body size than in other mammals.\n\nThe auditory bulla is separated from the skull and composed of two compact and dense bones (the periotic and tympanic) referred to as the tympanoperiotic complex. This complex is located in a cavity in the middle ear, which, in the Mysticeti, is divided by a bony projection and compressed between the exoccipital and squamosal, but in the odontoceti, is large and completely surrounds the bulla (hence called \"peribullar\"), which is, therefore, not connected to the skull except in physeterids. In the Odontoceti, the cavity is filled with a dense foam in which the bulla hangs suspended in five or more sets of ligaments. The pterygoid and peribullar sinuses that form the cavity tend to be more developed in shallow water and riverine species than in pelagic Mysticeti. In Odontoceti, the composite auditory structure is thought to serve as an acoustic isolator, analogous to the lamellar construction found in the temporal bone in bats.\n\nCetaceans use sound to communicate, using groans, moans, whistles, clicks or the 'singing' of the humpback whale.\n\nOdontoceti are generally capable of echolocation. They can discern the size, shape, surface characteristics, distance and movement of an object. They can search for, chase and catch fast-swimming prey in total darkness. Most Odontoceti can distinguish between prey and nonprey (such as humans or boats); captive Odontoceti can be trained to distinguish between, for example, balls of different sizes or shapes.\n\nMysticeti have exceptionally thin, wide basilar membranes in their cochleae without stiffening agents, making their ears adapted for processing low to infrasonic frequencies. Echolocation clicks also contain characteristic details unique to each animal, which may suggest that toothed whales can discern between their own click and that of others.\n\nThe initial karyotype includes a set of chromosomes from 2n = 44. They have four pairs of telocentric chromosomes (whose centromeres sit at one of the telomeres), two to four pairs of subtelocentric and one or two large pairs of submetacentric chromosomes. The remaining chromosomes are metacentric—the centromere is approximately in the middle—and are rather small. Sperm whales, beaked whales and right whales converge to a reduction in the number of chromosomes to 2n = 42.\n\nCetaceans are found in all oceans. River dolphin species live exclusively in fresh water. While many marine species, such as the blue whale, the humpback whale and the killer whale, have a distribution area that includes nearly the entire ocean, some species occur only locally or in broken populations. These include the vacquita, which inhabits a small part of the Gulf of California and Hector's dolphin, which lives in some coastal waters in New Zealand. Both species prefer deeper marine areas and species that live frequently or exclusively in coastal and shallow water areas.\n\nMany species inhabit specific latitudes, often in tropical or subtropical waters, such as Bryde's whale or Risso's dolphin. Others are found only in a specific body of water. The southern right whale dolphin and the hourglass dolphin live only in the Southern Ocean. The narwhal and the beluga live only in the Arctic Ocean. Sowerby's beaked whale and the Clymene dolphin exist only in the Atlantic and the Pacific white-sided dolphin and the northern straight dolphin live only in the North Pacific.\n\nCosmopolitan species may be found in the Pacific, Atlantic and Indian Oceans. However, northern and southern populations become genetically separated over time. In some species, this separation leads eventually to a divergence of the species, such as produced the southern right whale, North Pacific right whale and North Atlantic right whale. Migratory species' reproductive sites often lie in the tropics and their feeding grounds in polar regions.\n\nThirty-two species are found in European waters, including twenty-five toothed and seven baleen species.\n\nConscious breathing cetaceans sleep but cannot afford to be unconscious for long, because they may drown. While knowledge of sleep in wild cetaceans is limited, toothed cetaceans in captivity have been recorded to exhibit unihemispheric slow-wave sleep (USWS), which means they sleep with one side of their brain at a time, so that they may swim, breathe consciously and avoid both predators and social contact during their period of rest.\n\nA 2008 study found that sperm whales sleep in vertical postures just under the surface in passive shallow 'drift-dives', generally during the day, during which whales do not respond to passing vessels unless they are in contact, leading to the suggestion that whales possibly sleep during such dives.\n\nWhile diving, the animals reduce their oxygen consumption by lowering the heart activity and blood circulation; individual organs receive no oxygen during this time. Some rorquals can dive for up to 40 minutes, sperm whales between 60 and 90 minutes and bottlenose whales for two hours. Diving depths average about . Species such as sperm whales can dive to , although more commonly .\n\nMost whales are social animals, although a few species live in pairs or are solitary. A group, known as a pod, usually consists of ten to fifty animals, but on occasion, such as mass availability of food or during mating season, groups may encompass more than one thousand individuals. Inter-species socialization can occur.\n\nPods have a fixed hierarchy, with the priority positions determined by biting, pushing or ramming. The behavior in the group is aggressive only in situations of stress such as lack of food, but usually it is peaceful. Contact swimming, mutual fondling and nudging are common. The playful behavior of the animals, which is manifested in air jumps, somersaults, surfing, or fin hitting, occurs more often than not in smaller cetaceans, such as dolphins and porpoises.\n\nMales in some baleen species communicate via whale song, sequences of high pitched sounds. These \"songs\" can be heard for hundreds of kilometers. Each population generally shares a distinct song, which evolves over time. Sometimes, an individual can be identified by its distinctive vocals, such as the 52-hertz whale that sings at a higher frequency than other whales. Some individuals are capable of generating over 600 distinct sounds. In baleen species such as humpbacks, blues and fins, male-specific song is believed to be used to attract and display fitness to females.\n\nPod groups also hunt, often with other species. Many species of dolphins hunt accompany large tunas on hunting expeditions, following large schools of fish. The killer whale hunts in pods and targets belugas and even larger whales. Humpback whales, among others, form in collaboration bubble carpets to herd krill or plankton into bait balls before lunging at them.\n\nCetacea are known to teach, learn, cooperate, scheme and grieve.\n\nSmaller cetaceans, such as dolphins and porpoises, engage in complex play behavior, including such things as producing stable underwater toroidal air-core vortex rings or \"bubble rings\". The two main methods of bubble ring production are rapid puffing of air into the water and allowing it to rise to the surface, forming a ring, or swimming repeatedly in a circle and then stopping to inject air into the helical vortex currents thus formed. They also appear to enjoy biting the vortex rings, so that they burst into many separate bubbles and then rise quickly to the surface. Whales produce bubble nets to aid in herding prey.\n\nLarger whales are also thought to engage in play. The southern right whale elevates its tail fluke above the water, remaining in the same position for a considerable time. This is known as \"sailing\". It appears to be a form of play and is most commonly seen off the coast of Argentina and South Africa. Humpback whales also display this behaviour.\n\nSelf-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning (thinking about thinking) that humans exploit. Cetaceans appear to possess self-awareness. The most widely used test for self-awareness in animals is the mirror test, in which a temporary dye is placed on an animal's body and the animal is then presented with a mirror. Researchers then explore whether the animal shows signs of self-recognition.\n\nCritics claim that the results of these tests are susceptible to the Clever Hans effect. This test is much less definitive than when used for primates. Primates can touch the mark or the mirror, while cetaceans cannot, making their alleged self-recognition behavior less certain. Skeptics argue that behaviors said to identify self-awareness resemble existing social behaviors, so researchers could be misinterpreting self-awareness for social responses. Advocates counter that the behaviors are different from normal responses to another individual. Cetaceans show less definitive behavior of self-awareness, because they have no pointing ability.\n\nIn 1995, Marten and Psarakos used video to test dolphin self-awareness. They showed dolphins real-time footage of themselves, recorded footage and another dolphin. They concluded that their evidence suggested self-awareness rather than social behavior. While this particular study has not been replicated, dolphins later \"passed\" the mirror test.\n\nMost cetaceans sexually mature at seven to 10 years. An exception to this is the La Plata dolphin, which is sexually mature at two years, but lives only to about 20. The sperm whale reaches sexual maturity within about 20 years and a lifespan between 50 and 100 years.\n\nFor most species, reproduction is seasonal. Ovulation coincides with male fertility. This cycle is usually coupled with seasonal movements that can be observed in many species. Most toothed whales have no fixed bonds. In many species, females choose several partners during a season. Baleen whales are largely monogamous within each reproductive period.\n\nGestation ranges from 9 to 16 months. Duration is not necessarily a function of size. Porpoises and blue whales gestate for about 11 months. During gestation, the embryo is fed by a special nutritive tissue, the placenta.\n\nCetaceans usually bear one calf. In the case of twins, one usually dies, because the mother cannot produce sufficient milk for both. The fetus is positioned for a tail-first delivery, so that the risk of drowning during delivery is minimal. After birth, the mother carries the infant to the surface for its first breath. At birth they are about one-third of their adult length and tend to be independently active, comparable to terrestrial mammals.\n\nLike other placental mammals, cetaceans give birth to well-developed calves and nurse them with milk from their mammary glands. When suckling, the mother actively splashes milk into the mouth of the calf, using the muscles of her mammary glands, as the calf has no lips. This milk usually has a high fat content, ranging from 16 to 46%, causing the calf to increase rapidly in size and weight.\n\nIn many small cetaceans, suckling lasts for about four months. In large species, it lasts for over a year and involves a strong bond between mother and offspring.\n\nThe mother is solely responsible for brooding. In some species, so-called \"aunts\" occasionally suckle the young.\n\nThis reproductive strategy provides a few offspring that have a high survival rate.\n\nAmong cetaceans, whales are distinguished by an unusual longevity compared to other higher mammals. Some species, such as the bowhead whale (\"Balaena mysticetus\"), can reach over 200 years. Based on the annual rings of the bony otic capsule, the age of the oldest known specimen is a male determined to be 211 years at the time of death.\n\nUpon death, whale carcasses fall to the deep ocean and provide a substantial habitat for marine life. Evidence of whale falls in present-day and fossil records shows that deep-sea whale falls support a rich assemblage of creatures, with a global diversity of 407 species, comparable to other neritic biodiversity hotspots, such as cold seeps and hydrothermal vents.\n\nDeterioration of whale carcasses happens through three stages. Initially, organisms such as sharks and hagfish scavenge the soft tissues at a rapid rate over a period of months and as long as two years. This is followed by the colonization of bones and surrounding sediments (which contain organic matter) by enrichment opportunists, such as crustaceans and polychaetes, throughout a period of years. Finally, sulfophilic bacteria reduce the bones releasing hydrogen sulfide enabling the growth of chemoautotrophic organisms, which in turn, support organisms such as mussels, clams, limpets and sea snails. This stage may last for decades and supports a rich assemblage of species, averaging 185 per site.\n\nBrucellosis affects almost all mammals. It is distributed worldwide, while fishing and pollution have caused porpoise population density pockets, which risks further infection and disease spreading. \"Brucella ceti\", most prevalent in dolphins, has been shown to cause chronic disease, increasing the chance of failed birth and miscarriages, male infertility, neurobrucellosis, cardiopathies, bone and skin lesions, strandings and death. Until 2008, no case had ever been reported in porpoises, but isolated populations have an increased risk and consequentially a high mortality rate.\n\nMolecular biology and immunology show that cetaceans are phylogenetically closely related with the even-toed ungulates (Artiodactyla). Whales direct lineage began in the early Eocene, more than 50 million years ago, with early artiodactyls. Fossil discoveries at the beginning of the 21st century confirmed this.\n\nMost molecular biological evidence suggests that hippos are the closest living relatives. Common anatomical features include similarities in the morphology of the posterior molars, and the bony ring on the temporal bone (bulla) and the involucre, a skull feature that was previously associated only with cetaceans. The fossil record, however, does not support this relationship, because the hippo lineage dates back only about 15 million years. The most striking common feature is the talus, a bone in the upper ankle. Early cetaceans, archaeocetes, show double castors, which only occur in even-toed ungulates. Corresponding findings are from Tethys Sea deposits in northern India and Pakistan. The Tethys Sea was a shallow sea between the Asian continent and northward-bound Indian plate.\n\nMysticetes evolved baleen around 25 million years ago and lost their teeth.\n\nThe direct ancestors of today's cetaceans are probably found within the Dorudontidae whose most famous member, \"Dorudon atrox\", lived at the same time as \"Basilosaurus\". Both groups had already developed the typical anatomical features of today's whales, such as hearing. Life in the water for a formerly terrestrial creature required significant adjustments such as the fixed bulla, which replaces the mammalian eardrum, as well as sound-conducting elements for submerged directional hearing. Their wrists were stiffened and probably contributed to the typical build of flippers. The hind legs existed, however, but were significantly reduced in size and with a vestigial pelvis connection.\n\nThe fossil record traces the gradual transition from terrestrial to aquatic life. The regression of the hind limbs allowed greater flexibility of the spine. This made it possible for whales to move around with the vertical tail hitting the water. The front legs transformed into flippers, costing them their mobility on land.\n\nOne of the oldest members of ancient cetaceans (Archaeoceti) is \"Pakicetus\" from the Middle Eocene. This is an animal the size of a wolf, whose skeleton is known only partially. It had functioning legs and lived near the shore. This suggests the animal could still move on land. The long snout had carnivorous dentition.\n\nThe transition from land to sea dates to about 49 million years ago, with the \"Ambulocetus\" (\"running whale\"), discovered in Pakistan. It was up to long. The limbs of this archaeocete were adapted to swimming, but terrestrial locomotion was still possible. It probably crawled like a seal or crocodile. The snout was elongated with overhead nostrils and eyes. The tail was strong and supported movement through water. \"Ambulocetus\" probably lived in mangroves in brackish water and fed in the riparian zone as a predator of fish and other vertebrates.\n\nDating from about 45 million years ago are species such as \"Indocetus\", \"Kutchicetus\", \"Rodhocetus\" and \"Andrewsiphius\", all of which were adapted to life in water. The hind limbs of these species were regressed and their body shapes resemble modern whales. Protocetidae family member \"Rodhocetus\" is considered the first to be fully aquatic. The body was streamlined and delicate with extended hand and foot bones. The merged pelvic lumbar spine was present, making it possible to support the floating movement of the tail. It was likely a good swimmer, but could probably move only clumsily on land, much like a modern seal.\n\nSince the late Eocene, about 40 million years ago, cetaceans populated the subtropical oceans and no longer emerged on land. An example is the 18-m-long \"Basilosaurus\", sometimes referred to as \"Zeuglodon\". The transition from land to water was completed in about 10 million years. The Wadi Al-Hitan (\"Whale Valley\") in Egypt contains numerous skeletons of \"Basilosaurus\", as well as other marine vertebrates.\n\nBaleen whales (Mysticeti) owe their name to their baleen. Toothed whales (Odontoceti), which include the dolphins and porpoises, have conical teeth or spade-shaped teeth and can perceive their environment through biosonar.\n\nThe infraorder comprises the families Balaenidae (right and bowhead whales), Balaenoptera (rorquals), Eschrichtiidae (the gray whale), Delphinidae (oceanic dolphins), Monodontidae (Arctic whales), Phocoenidae (porpoises), Physeteridae (sperm whales), Kogiidae (lesser sperm whales), Platanistidae (Old World river dolphins), Iniidae (New World river dolphins), Pontoporiidae (the La plata dolphin) and Ziphidae (beaked whales).\n†Recently extinct\n\nThe primary threats to cetaceans come from people, both directly from whaling or drive hunting and indirect threats from fishing and pollution.\n\nWhaling is the practice of hunting whales, mainly baleen and sperm whales. This activity has gone on since the Stone Age.\n\nIn the Middle Ages, reasons for whaling included their meat, oil usable as fuel and the jawbone, which was used in house construction. At the end of the Middle Ages, early whaling fleets aimed at baleen whales, such as bowheads. In the 16th and 17th centuries, the Dutch fleet had about 300 whaling ships with 18,000 crewmen.\n\nIn the 18th and 19th centuries, baleen whales especially were hunted for their baleen, which was used as a replacement for wood, or in products requiring strength and flexibility such as corsets and crinoline skirts. In addition, the spermaceti found in the sperm whale was used as a machine lubricant and the ambergris as a material for pharmaceutical and perfume industries. In the second half of the 19th century, the explosive harpoon was invented, leading to a massive increase in the catch size.\n\nLarge ships were used as \"mother\" ships for the whale handlers. In the first half of the 20th century, whales were of great importance as a supplier of raw materials. Whales were intensively hunted during this time; in the 1930s, 30,000 whales were killed. This increased to over 40,000 animals per year up to the 1960s, when stocks of large baleen whales collapsed.\n\nMost hunted whales are now threatened, with some great whale populations exploited to the brink of extinction. Atlantic and Korean gray whale populations were completely eradicated and the North Atlantic right whale population fell to some 300-600. The blue whale population is estimated to be around 14,000.\n\nThe first efforts to protect whales came in 1931. Some particularly endangered species, such as the humpback whale (which then numbered about 100 animals), were placed under international protection and the first protected areas were established. In 1946, the International Whaling Commission (IWC) was established, to monitor and secure whale stocks. Whaling for commercial purposes was prohibited worldwide by this organization from 1985 to 2005.\n\nThe stocks of species such as humpback and blue whales have recovered, though they are still threatened. The United States Congress passed the Marine Mammal Protection Act of 1972 sustain the marine mammal population. It prohibits the taking of marine mammals. Japanese whaling ships are allowed to hunt whales of different species for ostensibly scientific purposes. Aboriginal whaling is still permitted, but under limited circumstances as defined by IWC. Iceland and Norway do not recognize the ban and operate commercial whaling. Norway and Japan are committed to ending the ban.\n\nDolphins and other smaller cetaceans are hunted in an activity known as dolphin drive hunting. This is accomplished by driving a pod together with boats, usually into a bay or onto a beach. Their escape is prevented by closing off the route to the ocean with other boats or nets. Dolphins are hunted this way in several places around the world, including the Solomon Islands, the Faroe Islands, Peru and Japan (the most well-known practitioner). Dolphins are mostly hunted for their meat, though some end up in dolphinaria. Despite the controversy thousands of dolphins are caught in drive hunts each year.\n\nDolphin pods often reside near large tuna shoals. This is known to fishermen, who look for dolphins to catch tuna. Dolphins are much easier to spot from a distance than tuna, since they regularly breathe. The fishermen pull their nets hundreds of meters wide in a circle around the dolphin groups, in the expectation that they will net a tuna shoal. When the nets are pulled together, the dolphins become entangled under water and drown. Line fisheries in larger rivers are threats to river dolphins.\n\nA greater threat than by-catch for small cetaceans is targeted hunting. In Southeast Asia, they are sold as fish-replacement to locals, since the region's edible fish promise higher revenues from exports. In the Mediterranean, small cetaceans are targeted to ease pressure on edible fish.\n\nA stranding is when a cetacean leaves the water to lie on a beach. In some cases, groups of whales strand together. The best known are mass strandings of pilot whales and sperm whales. Stranded cetaceans usually die, because their as much as body weight compresses their lungs or breaks their ribs. Smaller whales can die of heatstroke because of their thermal insulation.\n\nThe causes are not clear. Possible reasons for mass beachings are:\n\nSince 2000, whale strandings frequently occurred following military sonar testing. In December 2001, the US Navy admitted partial responsibility for the beaching and the deaths of several marine mammals in March 2000. The coauthor of the interim report stated that animals killed by active sonar of some Navy ships were injured. Generally, underwater noise, which is still on the increase, is increasingly tied to strandings; because it impairs communication and sense of direction.\n\nClimate change influences the major wind systems and ocean currents, which also lead to cetacean strandings. Researchers studying strandings on the Tasmanian coast from 1920-2002 found that greater strandings occurred at certain time intervals. Years with increased strandings were associated with severe storms, which initiated cold water flows close to the coast. In nutrient-rich, cold water, cetaceans expect large prey animals, so they follow the cold water currents into shallower waters, where the risk is higher for strandings. Whales and dolphins who live in pods may accompany sick or debilitated pod members into shallow water, stranding them at low tide. Once stranded, large whales are crushed by their own body weight, if they cannot quickly return to the water. In addition, body temperature regulation is compromised.\n\nHeavy metals, residues of many plant and insect venoms and plastic waste flotsam are not biodegradable. Sometimes, cetaceans consume these hazardous materials, mistaking them for food items. As a result, the animals are more susceptible to disease and have fewer offspring.\n\nDamage to the ozone layer reduces plankton reproduction because of its resulting radiation. This shrinks the food supply for many marine animals, but the filter-feeding baleen whales are most impacted. Even the Nekton is, in addition to intensive exploitation, damaged by the radiation.\n\nFood supplies are also reduced long-term by ocean acidification due to increased absorption of increased atmospheric carbon dioxide. The CO reacts with water to form carbonic acid, which reduces the construction of the calcium carbonate skeletons of food supplies for zooplankton that baleen whales depend on.\n\nThe military and resource extraction industries operate strong sonar and blasting operations. Marine seismic surveys use loud, low-frequency sound that show what is lying underneath the Earth's surface. Vessel traffic also increases noise in the oceans. Such noise can disrupt cetacean behavior such as their use of biosonar for orientation and communication. Severe instances can panic them, driving them to the surface. This leads to bubbles in blood gases and can cause decompression sickness. Naval exercises with sonar regularly results in fallen cetaceans that wash up with fatal decompression. Sounds can be disruptive at distances of more than . Damage varies across frequency and species.\n\nIn Aristotle's time, the 4th century BCE, whales were regarded as fish due to their superficial similarity. Aristotle, however, observed many physiological and anatomical similarities with the terrestrial vertebrates, such as blood (circulation), lungs, uterus and fin anatomy. His detailed descriptions were assimilated by the Romans, but mixed with a more accurate knowledge of the dolphins, as mentioned by Pliny the Elder in his \"Natural history\". In the art of this and subsequent periods, dolphins are portrayed with a high-arched head (typical of porpoises) and a long snout. The harbour porpoise was one of the most accessible species for early cetologists; because it could be seen close to land, inhabiting shallow coastal areas of Europe. Much of the findings that apply to all cetaceans were first discovered in porpoises. One of the first anatomical descriptions of the airways of a harbor porpoise dates from 1671 by John Ray. It nevertheless referred to the porpoise as a fish.\n\nIn the 10th edition of Systema Naturae (1758), Swedish biologist and taxonomist Carl Linnaeus asserted that cetaceans were mammals and not fish. His groundbreaking binomial system formed the basis of modern whale classification.\n\nCetaceans play a role in human culture.\n\nStone Age petroglyphs , such as those in Roddoy and Reppa (Norway), depict them. Whale bones were used for many purposes. In the Neolithic settlement of Skara Brae on Orkney sauce pans were made from whale vertebrae.\n\nThe whale was first mentioned in ancient Greece by Homer. There, it is called Ketos, a term that initially included all large marine animals. From this was derived the Roman word for whale, Cetus. Other names were phálaina (Aristotle, Latin form of ballaena) for the female and, with an ironic characteristic style, musculus (Mouse) for the male. North Sea whales were called Physeter, which was meant for the sperm whale \"Physter macrocephalus\". Whales are described in particular by Aristotle, Pliny and Ambrose. All mention both live birth and suckling. Pliny describes the problems associated with the lungs with spray tubes and Ambrose claimed that large whales would take their young into their mouth to protect them.\n\nIn the Bible especially, the leviathan plays a role as a sea monster. The essence, which features a giant crocodile or a dragon and a whale, was created according to the Bible by God () and should again be destroyed by him ( and ). In the Book of Job, the leviathan is described in more detail ( to ).\n\nIn - is a more recognizable description of a whale alongside the prophet Jonah, who, on his flight from the city of Nineveh is swallowed by a whale.\n\nDolphins are mentioned far more often than whales. Aristotle discusses the sacred animals of the Greeks in his \"Historia Animalium\" and gives details of their role as aquatic animals. The Greeks admired the dolphin as a \"king of the aquatic animals\" and referred to them erroneously as fish. Its intelligence was apparent both in its ability to escape from fishnets and in its collaboration with fishermen.\n\nRiver dolphins are known from the Ganges and - erroneously - the Nile. In the latter case it was equated with sharks and catfish. Supposedly they attacked even crocodiles.\n\nDolphins appear in Greek mythology. Because of their intelligence, they rescued multiple people from drowning. They were said to love music - probably not least because of their own song - they saved, in the legends, famous musicians such as Arion of Lesbos from Methymna or Kairanos from Miletus. Because of their mental faculties, dolphins were considered for the god Dionysus.\n\nDolphins belong to the domain of Poseidon and led him to his wife Amphitrite. Dolphins are associated with other gods, such as Apollo, Dionysus and Aphrodite. The Greeks paid tribute to both whales and dolphins with their own constellation. The constellation of the Whale (Ketos, lat. Cetus) is located south of the Dolphin (Delphi, lat. Delphinus) north of the zodiac.\n\nAncient art often included dolphin representations, including the Cretan Minoans. Later they appeared on reliefs, gems, lamps, coins, mosaics and gravestones. A particularly popular representation is that of Arion or the Taras (mythology) riding on a dolphin. In early Christian art, the dolphin is a popular motif, at times used as a symbol of Christ.\n\nSt. Brendan described in his travel story \"Navigatio Sancti Brendani\" an encounter with a whale, between the years 565–573. He described how he and his companions entered a treeless island, which turned out to be a giant whale, which he called Jasconicus. He met this whale seven years later and rested on his back.\n\nMost descriptions of large whales from this time until the whaling era, beginning in the 17th century, were of beached whales, which resembled no other animal. This was particularly true for the sperm whale, the most frequently stranded in larger groups. Raymond Gilmore documented seventeen sperm whales in the estuary of the Elbe from 1723 to 1959 and thirty-one animals on the coast of Great Britain in 1784. In 1827, a blue whale beached itself off the coast of Ostend. Whales were used as attractions in museums and traveling exhibitions.\nWhalers in the 17–19th centuries depicted whales in drawings and recounted tales of their occupation. Although they knew that whales were harmless giants, they described battles with harpooned animals. These included descriptions of sea monsters, including huge whales, sharks, sea snakes, giant squid and octopuses.\n\nAmong the first whalers who described their experiences on whaling trips was Captain William Scoresby from Great Britain, who published the book \"Northern Whale Fishery\", describing the hunt for northern baleen whales. This was followed by Thomas Beale, a British surgeon, in his book \"Some observations on the natural history of the sperm whale\" in 1835; and Frederick Debell Bennett's \"The tale of a whale hunt\" in 1840. Whales were described in narrative literature and paintings, most famously in the novels \"Moby Dick\" by Herman Melville and \"20,000 Leagues Under the Sea\" by Jules Verne. In the 1882 children's book \"Adventures of Pinocchio\" by Carlo Collodi, the wooden figures Pinocchio and Geppettos' creators were swallowed by a whale.\n\nBaleen was used to make vessel components such as the bottom of a bucket in the Scottish National Museum. The Norse crafted ornamented plates from baleen, sometimes interpreted as ironing boards.\n\nIn the Canadian Arctic (east coast) in Punuk and Thule culture (1000-1600 C.E.), I baleen was used to construct houses in place of wood as roof support for winter houses, with half of the building buried under the ground. The actual roof was probably made of animal skins that were covered with soil and moss.\n\nIn the 20th century perceptions of cetaceans changed. They transformed from monsters into objects of wonder. As science revealed them to be intelligent and peaceful animals. Hunting was replaced by whale and dolphin tourism. This change is reflected in films and novels. For example, the protagonist of the series Flipper was a bottle-nose dolphin. The TV series SeaQuest DSV (1993-1996), the movies Free Willy, and the book series The Hitchhiker's Guide to the Galaxy by Douglas Adams are examples.\n\nThe study of whale song also produced a popular Judy Collins album, Songs of the Humpback Whale.\n\nWhales and dolphins have been kept in captivity for use in education, research and entertainment since the 19th century.\n\nBeluga whales were the first whales to be kept in captivity. Other species were too rare, too shy or too big. The first was shown at Barnum's Museum in New York City in 1861. For most of the 20th century, Canada was the predominant source. They were taken from the St. Lawrence River estuary until the late 1960s, after which they were predominantly taken from the Churchill River estuary until capture was banned in 1992. Russia then became the largest provider. Belugas are caught in the Amur Darya delta and their eastern coast and are transported domestically to aquaria or dolphinaria in Moscow, St. Petersburg and Sochi, or exported to countries such as Canada. They have not been domesticated.\n\nAs of 2006, 30 belugas lived in Canada and 28 in the United States. 42 deaths in captivity had been reported. A single specimen can reportedly fetch up to US$100,000 (UK£64,160). The beluga's popularity is due to its unique color and its facial expressions. The latter is possible because while most cetacean \"smiles\" are fixed, the extra movement afforded by the beluga's unfused cervical vertebrae allows a greater range of apparent expression.\n\nThe killer whale's intelligence, trainability, striking appearance, playfulness in captivity and sheer size have made it a popular exhibit at aquaria and aquatic theme parks. From 1976 to 1997, fifty-five whales were taken from the wild in Iceland, nineteen from Japan and three from Argentina. These figures exclude animals that died during capture. Live captures fell dramatically in the 1990s and by 1999, about 40% of the forty-eight animals on display in the world were captive-born.\n\nOrganizations such as World Animal Protection and the Whale and Dolphin Conservation Society campaign against the practice of keeping them in captivity.\n\nIn captivity, they often develop pathologies, such as the dorsal fin collapse seen in 60–90% of captive males. Captives have reduced life expectancy, on average only living into their 20s, although some live longer, including several over 30 years old and two, Corky II and Lolita, in their mid-40s. In the wild, females who survive infancy live 46 years on average and up to 70–80 years. Wild males who survive infancy live 31 years on average and can reach 50–60 years.\n\nCaptivity usually bears little resemblance to wild habitat and captive whales' social groups are foreign to those found in the wild. Critics claim captive life is stressful due to these factors and the requirement to perform circus tricks that are not part of wild killer whale behavior. Wild killer whales may travel up to in a day and critics say the animals are too big and intelligent to be suitable for captivity. Captives occasionally act aggressively towards themselves, their tankmates, or humans, which critics say is a result of stress. Killer whales are well known for their performances in shows, but the number of orcas kept in captivity is small, especially when compared to the number of bottlenose dolphins, with only forty-four captive orcas being held in aquaria as of 2012.\n\nEach country has its own tank requirements; in the US, the minimum enclosure size is set by the Code of Federal Regulations, 9 CFR E § 3.104, under the \"Specifications for the Humane Handling, Care, Treatment and Transportation of Marine Mammals\".\nAggression among captive killer whales is common. They attack each other and their trainers as well. In 2013, SeaWorld's treatment of killer whales in captivity was the basis of the movie \"Blackfish\", which documents the history of Tilikum, a killer whale at SeaWorld Orlando, who had been involved in the deaths of three people. The film was a sensation, leading the company to announce in 2016 that it would phase out its killer whale program after various unsuccessful attempts to restore its reputation and stock price.\n\nDolphins and porpoises are kept in captivity. Bottlenose dolphins are the most common, as they are relatively easy to train, have a long lifespan in captivity and have a friendly appearance. Bottlenose dolphins live in captivity across the world, though exact numbers are hard to determine. Other species kept in captivity are spotted dolphins, false killer whales and common dolphins, Commerson's dolphins, as well as rough-toothed dolphins, but all in much lower numbers. There are also fewer than ten pilot whales, Amazon river dolphins, Risso's dolphins, spinner dolphins, or tucuxi in captivity. Two unusual and rare hybrid dolphins, known as wolphins, are kept at Sea Life Park in Hawaii, which is a cross between a bottlenose dolphin and a false killer whale. Also, two common/bottlenose hybrids reside in captivity at Discovery Cove and SeaWorld San Diego.\n\nIn repeated attempts in the 1960s and 1970s, narwhals kept in captivity died within months. A breeding pair of pygmy right whales were retained in a netted area. They were eventually released in South Africa. In 1971, SeaWorld captured a California gray whale calf in Mexico at Scammon's Lagoon. The calf, later named Gigi, was separated from her mother using a form of lasso attached to her flukes. Gigi was displayed at SeaWorld San Diego for a year. She was then released with a radio beacon affixed to her back; however, contact was lost after three weeks. Gigi was the first captive baleen whale. JJ, another gray whale calf, was kept at SeaWorld San Diego. JJ was an orphaned calf that beached itself in April 1997 and was transported two miles to SeaWorld. The calf was a popular attraction and behaved normally, despite separation from his mother. A year later, the then whale though smaller than average, was too big to keep in captivity, and was released on April 1, 1998. A captive Amazon river dolphin housed at Acuario de Valencia is the only trained river dolphin in captivity.\n\n", "id": "7626", "title": "Cetacea"}
{"url": "https://en.wikipedia.org/wiki?curid=7627", "text": "The Canterbury Tales\n\nThe Canterbury Tales (Middle English: \"Tales of Caunterbury\") is a collection of 24 stories that runs to over 17,000 lines written in Middle English by Geoffrey Chaucer between 1387–1400. In 1386, Chaucer became Controller of Customs and Justice of Peace and, three years later, Clerk of the King's work in 1389. It was during these years that Chaucer began working on his most famous text, \"The Canterbury Tales\". The tales (mostly written in verse, although some are in prose) are presented as part of a story-telling contest by a group of pilgrims as they travel together on a journey from London to Canterbury to visit the shrine of Saint Thomas Becket at Canterbury Cathedral. The prize for this contest is a free meal at the Tabard Inn at Southwark on their return.\n\nAfter a long list of works written earlier in his career, including \"Troilus and Criseyde\", \"House of Fame\", and \"Parliament of Fowls\", \"The Canterbury Tales\" is near-unanimously seen as Chaucer's magnum opus. He uses the tales and descriptions of its characters to paint an ironic and critical portrait of English society at the time, and particularly of the Church. Chaucer's use of such a wide range of classes and types of people was without precedent in English. Although the characters are fictional, they still offer a variety of insights into customs and practices of the time. Often, such insight leads to a variety of discussions and disagreements among people in the 14th century. For example, although various social classes are represented in these stories and all of the pilgrims are on a spiritual quest, it is apparent that they are more concerned with worldly things than spiritual. Structurally, the collection resembles \"The Decameron\", which Chaucer may have read during his first diplomatic mission to Italy in 1372.\n\nIt has been suggested that the greatest contribution of \"The Canterbury Tales\" to English literature was the popularization of the English vernacular in mainstream literature, as opposed to French, Italian or Latin. English had, however, been used as a literary language centuries before Chaucer's time, and several of Chaucer's contemporaries—John Gower, William Langland, the Pearl Poet, and Julian of Norwich—also wrote major literary works in English. It is unclear to what extent Chaucer was seminal in this evolution of literary preference.\n\nWhile Chaucer clearly states the addressees of many of his poems, the intended audience of \"The Canterbury Tales\" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for nobility.\n\n\"The Canterbury Tales\" is generally thought to have been incomplete at the end of Chaucer's life. In the General Prologue, some thirty pilgrims are introduced. According to the Prologue, Chaucer's intention was to write two stories from the perspective of each pilgrim on the way to and from their ultimate destination, St. Thomas Becket's shrine (making for a total of four stories per pilgrim). Although perhaps incomplete, \"The Canterbury Tales\" is revered as one of the most important works in English literature. Not only do readers find it entertaining, but it is also open to a wide range of interpretations.\n\nThe question of whether \"The Canterbury Tales\" is finished has not yet been answered. There are 83 known manuscripts of the work from the late medieval and early Renaissance periods, more than any other vernacular literary text with the exception of \"The Prick of Conscience\". This is taken as evidence of the tales' popularity during the century after Chaucer's death. Fifty-five of these manuscripts are thought to have been complete at one time, while 28 are so fragmentary that it is difficult to ascertain whether they were copied individually or as part of a set. The \"Tales\" vary in both minor and major ways from manuscript to manuscript; many of the minor variations are due to copyists' errors, while others suggest that Chaucer added to and revised his work as it was being copied and (possibly) distributed.\n\nEven the earliest surviving manuscripts are not Chaucer's originals, the oldest being MS Peniarth 392 D (called \"Hengwrt\"), compiled by a scribe shortly after Chaucer's death. The most beautiful of the manuscripts of the tales is the Ellesmere Manuscript, and many editors have followed the order of the Ellesmere over the centuries, even down to the present day. The first version of \"The Canterbury Tales\" to be published in print was William Caxton's 1478 edition. Only 10 copies of this edition are known to exist, including one held by the British Library and one held by the Folger Shakespeare Library. Since this print edition was created from a now-lost manuscript, it is counted as among the 83 manuscripts. In 2004, Professor Linne Mooney claimed that she was able to identify the scrivener who worked for Chaucer as an Adam Pinkhurst. Mooney, then a professor at the University of Maine and a visiting fellow at Corpus Christi College, Cambridge, said she could match Pinkhurst's signature, on an oath he signed, to his lettering on a copy of \"The Canterbury Tales\" that was transcribed from Chaucer's working copy.\n\nNo authorial, arguably complete version of the \"Tales\" exists and no consensus has been reached regarding the order in which Chaucer intended the stories to be placed.\n\nTextual and manuscript clues have been adduced to support the two most popular modern methods of ordering the tales. Some scholarly editions divide the \"Tales\" into ten \"Fragments\". The tales that make up a Fragment are closely related and contain internal indications of their order of presentation, usually with one character speaking to and then stepping aside for another character. However, between Fragments, the connection is less obvious. Consequently, there are several possible orders; the one most frequently seen in modern editions follows the numbering of the Fragments (ultimately based on the Ellesmere order). Victorians frequently used the nine \"Groups\", which was the order used by Walter William Skeat whose edition \"Chaucer: Complete Works\" was used by Oxford University Press for most of the twentieth century, but this order is now seldom followed.\n\nAn alternative ordering (seen in an early manuscript containing \"The Canterbury Tales\", the early-fifteenth century Harley MS. 7334) places Fragment VIII before VI. Fragments I and II almost always follow each other, just as VI and VII, IX and X do in the oldest manuscripts. Fragments IV and V, by contrast, vary in location from manuscript to manuscript.\n\nChaucer wrote in late Middle English, which has clear differences from Modern English. From philological research, we know certain facts about the pronunciation of English during the time of Chaucer. Chaucer pronounced \"-e\" at the end of words, so that \"care\" was , not as in Modern English. Other silent letters were also pronounced, so that the word \"knight\" was , with both the \"k\" and the \"gh\" pronounced, not . In some cases, vowel letters in Middle English were pronounced very differently from Modern English, because the Great Vowel Shift had not yet happened. For instance, the long \"e\" in \"wepyng\" \"weeping\" was pronounced as , as in modern German or Italian, not as . Below is an IPA transcription of the opening lines of \"The Merchant's Prologue\":\n\nAlthough no manuscript exists in Chaucer's own hand, two were copied around the time of his death by Adam Pinkhurst, a scribe with whom he seems to have worked closely before, giving a high degree of confidence that Chaucer himself wrote the \"Tales\". Because the final \"-e\" sound was lost soon after Chaucer's time, scribes did not accurately copy it, and this gave scholars the impression that Chaucer himself was inconsistent in using it. It has now been established, however, that \"-e\" was an important part of Chaucer's grammar, and helped to distinguish singular adjectives from plural and subjunctive verbs from indicative.\n\nNo other work prior to Chaucer's is known to have set a collection of tales within the framework of pilgrims on a pilgrimage. It is obvious, however, that Chaucer borrowed portions, sometimes very large portions, of his stories from earlier stories, and that his work was influenced by the general state of the literary world in which he lived. Storytelling was the main entertainment in England at the time, and storytelling contests had been around for hundreds of years. In 14th-century England the English Pui was a group with an appointed leader who would judge the songs of the group. The winner received a crown and, as with the winner of \"The Canterbury Tales\", a free dinner. It was common for pilgrims on a pilgrimage to have a chosen \"master of ceremonies\" to guide them and organise the journey. Harold Bloom suggests that the structure is mostly original, but inspired by the \"pilgrim\" figures of Dante and Virgil in \"The Divine Comedy\".\n\n\"The Decameron\" by Giovanni Boccaccio contains more parallels to \"The Canterbury Tales\" than any other work. Like the \"Tales\", it features a number of narrators who tell stories along a journey they have undertaken (to flee from the Black Death). It ends with an apology by Boccaccio, much like Chaucer's Retraction to the \"Tales\". A quarter of the tales in \"The Canterbury Tales\" parallel a tale in the \"Decameron\", although most of them have closer parallels in other stories. Some scholars thus find it unlikely that Chaucer had a copy of the work on hand, surmising instead that he must have merely read the \"Decameron\" at some point. Each of the tales has its own set of sources that have been suggested by scholars, but a few sources are used frequently over several tales. They include poetry by Ovid, the Bible in one of the many vulgate versions in which it was available at the time (the exact one is difficult to determine), and the works of Petrarch and Dante. Chaucer was the first author to utilise the work of these last two, both Italians. Boethius' \"Consolation of Philosophy\" appears in several tales, as the works of John Gower do. Gower was a known friend to Chaucer. A full list is impossible to outline in little space, but Chaucer also, lastly, seems to have borrowed from numerous religious encyclopaedias and liturgical writings, such as John Bromyard's \"Summa praedicantium\", a preacher's handbook, and Jerome's \"Adversus Jovinianum\". Many scholars say there is a good possibility Chaucer met Petrarch or Boccaccio.\n\n\"The Canterbury Tales\" is a collection of stories built around a frame narrative or frame tale, a common and already long established genre of its period. Chaucer's \"Tales\" differs from most other story \"collections\" in this genre chiefly in its intense variation. Most story collections focused on a theme, usually a religious one. Even in the \"Decameron\", storytellers are encouraged to stick to the theme decided on for the day. The idea of a pilgrimage to get such a diverse collection of people together for literary purposes was also unprecedented, though \"the association of pilgrims and storytelling was a familiar one\". Introducing a competition among the tales encourages the reader to compare the tales in all their variety, and allows Chaucer to showcase the breadth of his skill in different genres and literary forms.\n\nWhile the structure of the \"Tales\" is largely linear, with one story following another, it is also much more than that. In the \"General Prologue\", Chaucer describes not the tales to be told, but the people who will tell them, making it clear that structure will depend on the characters rather than a general theme or moral. This idea is reinforced when the Miller interrupts to tell his tale after the Knight has finished his. Having the Knight go first gives one the idea that all will tell their stories by class, with the Monk following the Knight. However, the Miller's interruption makes it clear that this structure will be abandoned in favour of a free and open exchange of stories among all classes present. General themes and points of view arise as the characters tell their tales, which are responded to by other characters in their own tales, sometimes after a long lapse in which the theme has not been addressed.\n\nLastly, Chaucer does not pay much attention to the progress of the trip, to the time passing as the pilgrims travel, or to specific locations along the way to Canterbury. His writing of the story seems focused primarily on the stories being told, and not on the pilgrimage itself.\n\nThe variety of Chaucer's tales shows the breadth of his skill and his familiarity with many literary forms, linguistic styles, and rhetorical devices. Medieval schools of rhetoric at the time encouraged such diversity, dividing literature (as Virgil suggests) into high, middle, and low styles as measured by the density of rhetorical forms and vocabulary. Another popular method of division came from St. Augustine, who focused more on audience response and less on subject matter (a Virgilian concern). Augustine divided literature into \"majestic persuades\", \"temperate pleases\", and \"subdued teaches\". Writers were encouraged to write in a way that kept in mind the speaker, subject, audience, purpose, manner, and occasion. Chaucer moves freely between all of these styles, showing favouritism to none. He not only considers the readers of his work as an audience, but the other pilgrims within the story as well, creating a multi-layered rhetorical puzzle of ambiguities. Thus Chaucer's work far surpasses the ability of any single medieval theory to uncover.\n\nWith this, Chaucer avoids targeting any specific audience or social class of readers, focusing instead on the characters of the story and writing their tales with a skill proportional to their social status and learning. However, even the lowest characters, such as the Miller, show surprising rhetorical ability, although their subject matter is more lowbrow. Vocabulary also plays an important part, as those of the higher classes refer to a woman as a \"lady\", while the lower classes use the word \"wenche\", with no exceptions. At times the same word will mean entirely different things between classes. The word \"pitee\", for example, is a noble concept to the upper classes, while in the \"Merchant's Tale\" it refers to sexual intercourse. Again, however, tales such as the \"Nun's Priest's Tale\" show surprising skill with words among the lower classes of the group, while the \"Knight's Tale\" is at times extremely simple.\n\nChaucer uses the same meter throughout almost all of his tales, with the exception of \"Sir Thopas\" and his prose tales. It is a decasyllable line, probably borrowed from French and Italian forms, with riding rhyme and, occasionally, a caesura in the middle of a line. His meter would later develop into the heroic meter of the 15th and 16th centuries and is an ancestor of iambic pentameter. He avoids allowing couplets to become too prominent in the poem, and four of the tales (the Man of Law's, Clerk's, Prioress', and Second Nun's) use rhyme royal.\n\n\"The Canterbury Tales\" was written during a turbulent time in English history. The Catholic Church was in the midst of the Western Schism and, though it was still the only Christian authority in Europe, was the subject of heavy controversy. Lollardy, an early English religious movement led by John Wycliffe, is mentioned in the \"Tales\", which also mention a specific incident involving pardoners (sellers of indulgences, which were believed to relieve the temporal punishment due for sins that were already forgiven in the Sacrament of Confession) who nefariously claimed to be collecting for St. Mary Rouncesval hospital in England. \"The Canterbury Tales\" is among the first English literary works to mention paper, a relatively new invention that allowed dissemination of the written word never before seen in England. Political clashes, such as the 1381 Peasants' Revolt and clashes ending in the deposing of King Richard II, further reveal the complex turmoil surrounding Chaucer in the time of the \"Tales\"' writing. Many of his close friends were executed and he himself moved to Kent to get away from events in London.\n\nWhile some readers look to interpret the characters of \"The Canterbury Tales\" as historical figures, other readers choose to interpret its significance in less literal terms. After analysis of Chaucer's diction and historical context, his work appears to develop a critique of society during his lifetime. Within a number of his descriptions, his comments can appear complimentary in nature, but through clever language, the statements are ultimately critical of the pilgrim's actions. It is unclear whether Chaucer would intend for the reader to link his characters with actual persons. Instead, it appears that Chaucer creates fictional characters to be general representations of people in such fields of work. With an understanding of medieval society, one can detect subtle satire at work.\n\nThe \"Tales\" reflect diverse views of the Church in Chaucer's England. After the Black Death, many Europeans began to question the authority of the established Church. Some turned to lollardy, while others chose less extreme paths, starting new monastic orders or smaller movements exposing church corruption in the behaviour of the clergy, false church relics or abuse of indulgences. Several characters in the \"Tales\" are religious figures, and the very setting of the pilgrimage to Canterbury is religious (although the prologue comments ironically on its merely seasonal attractions), making religion a significant theme of the work.\n\nTwo characters, the Pardoner and the Summoner, whose roles apply the Church's secular power, are both portrayed as deeply corrupt, greedy, and abusive. Pardoners in Chaucer's day were those people from whom one bought Church \"indulgences\" for forgiveness of sins, who were guilty of abusing their office for their own gain. Chaucer's Pardoner openly admits the corruption of his practice while hawking his wares. Summoners were Church officers who brought sinners to the Church court for possible excommunication and other penalties. Corrupt summoners would write false citations and frighten people into bribing them to protect their interests. Chaucer's Summoner is portrayed as guilty of the very kinds of sins for which he is threatening to bring others to court, and is hinted as having a corrupt relationship with the Pardoner. In The Friar's Tale, one of the characters is a summoner who is shown to be working on the side of the devil, not God.\n\nChurchmen of various kinds are represented by the Monk, the Prioress, the Nun's Priest, and the Second Nun. Monastic orders, which originated from a desire to follow an ascetic lifestyle separated from the world, had by Chaucer's time become increasingly entangled in worldly matters. Monasteries frequently controlled huge tracts of land on which they made significant sums of money, while peasants worked in their employ. The Second Nun is an example of what a Nun was expected to be: her tale is about a woman whose chaste example brings people into the church. The Monk and the Prioress, on the other hand, while not as corrupt as the Summoner or Pardoner, fall far short of the ideal for their orders. Both are expensively dressed, show signs of lives of luxury and flirtatiousness and show a lack of spiritual depth. The Prioress's Tale is an account of Jews murdering a deeply pious and innocent Christian boy, a blood libel against Jews that became a part of English literary tradition. The story did not originate in the works of Chaucer and was well known in the 14th century.\n\nPilgrimage was a very prominent feature of medieval society. The ultimate pilgrimage destination was Jerusalem, but within England Canterbury was a popular destination. Pilgrims would journey to cathedrals that preserved relics of saints, believing that such relics held miraculous powers. Saint Thomas Becket, Archbishop of Canterbury, had been murdered in Canterbury Cathedral by knights of Henry II during a disagreement between Church and Crown. Miracle stories connected to his remains sprang up soon after his death, and the cathedral became a popular pilgrimage destination. The pilgrimage in the work ties all of the stories together and may be considered a representation of Christians' striving for heaven, despite weaknesses, disagreement, and diversity of opinion.\n\nThe upper class or nobility, represented chiefly by the Knight and his Squire, was in Chaucer's time steeped in a culture of chivalry and courtliness. Nobles were expected to be powerful warriors who could be ruthless on the battlefield yet mannerly in the King's Court and Christian in their actions. Knights were expected to form a strong social bond with the men who fought alongside them, but an even stronger bond with a woman whom they idealised to strengthen their fighting ability. Though the aim of chivalry was to noble action, its conflicting values often degenerated into violence. Church leaders frequently tried to place restrictions on jousts and tournaments, which at times ended in the death of the loser. The Knight's Tale shows how the brotherly love of two fellow knights turns into a deadly feud at the sight of a woman whom both idealise. To win her, both are willing to fight to the death. Chivalry was in Chaucer's day on the decline, and it is possible that The Knight's Tale was intended to show its flaws, although this is disputed. Chaucer himself had fought in the Hundred Years' War under Edward III, who heavily emphasised chivalry during his reign. Two tales, \"Sir Topas\" and \"The Tale of Melibee\" are told by Chaucer himself, who is travelling with the pilgrims in his own story. Both tales seem to focus on the ill-effects of chivalry—the first making fun of chivalric rules and the second warning against violence.\n\nThe \"Tales\" constantly reflect the conflict between classes. For example, the division of the three estates: the characters are all divided into three distinct classes, the classes being \"those who pray\" (the clergy), \"those who fight\" (the nobility), and \"those who work\" (the commoners and peasantry). Most of the tales are interlinked by common themes, and some \"quit\" (reply to or retaliate against) other tales. Convention is followed when the Knight begins the game with a tale, as he represents the highest social class in the group. But when he is followed by the Miller, who represents a lower class, it sets the stage for the \"Tales\" to reflect both a respect for and a disregard for upper class rules. Helen Cooper, as well as Mikhail Bakhtin and Derek Brewer, call this opposition \"the ordered and the grotesque, Lent and Carnival, officially approved culture and its riotous, and high-spirited underside.\" Several works of the time contained the same opposition.\n\nChaucer's characters each express different—sometimes vastly different—views of reality, creating an atmosphere of testing, empathy, and relativism. As Helen Cooper says, \"Different genres give different readings of the world: the fabliau scarcely notices the operations of God, the saint's life focuses on those at the expense of physical reality, tracts and sermons insist on prudential or orthodox morality, romances privilege human emotion.\" The sheer number of varying persons and stories renders the \"Tales\" as a set unable to arrive at any definite truth or reality.\n\nThe concept of liminality figures prominently within \"The Canterbury Tales\". A liminal space, which can be both geographical as well as metaphorical or spiritual, is the transitional or transformational space between a “real” (secure, known, limited) world and an unknown or imaginary space of both risk and possibility. The notion of a pilgrimage is itself a liminal experience, because it centers on travel between destinations and because pilgrims undertake it hoping to become more holy in the process. Thus, the structure of \"The Canterbury Tales\" itself is liminal; it not only covers the distance between London and Canterbury, but the majority of the tales refer to places entirely outside the geography of the pilgrimage. Jean Jost summarises the function of liminality in \"The Canterbury Tales\",\n\n\"Both appropriately and ironically in this raucous and subversive liminal space, a ragtag assembly gather together and tell their equally unconventional tales. In this unruly place, the rules of tale telling are established, themselves to be both disordered and broken; here the tales of game and earnest, solas and sentence, will be set and interrupted. Here the sacred and profane adventure begins, but does not end. Here, the condition of peril is as prominent as that of protection. The act of pilgrimaging itself consists of moving from one urban space, through liminal rural space, to the next urban space with an ever fluctuating series of events and narratives punctuating those spaces. The goal of pilgrimage may well be a religious or spiritual space at its conclusion, and reflect a psychological progression of the spirit, in yet another kind of emotional space.\"\n\nLiminality is also evident in the individual tales. An obvious instance of this is the Friar’s Tale in which the yeoman devil is a liminal figure because of his transitory nature and function; it is his purpose to issue souls from their current existence to hell, an entirely different one. The Franklin’s Tale is a Breton Lai tale, which takes the tale into a liminal space by invoking not only the interaction of the supernatural and the mortal, but also the relation between the present and the imagined past.\n\nIt is sometimes argued that the greatest contribution that this work made to English literature was in popularising the literary use of the vernacular English, rather than French or Latin. English had, however, been used as a literary language for centuries before Chaucer's life, and several of Chaucer's contemporaries—John Gower, William Langland, and the Pearl Poet—also wrote major literary works in English. It is unclear to what extent Chaucer was responsible for starting a trend rather than simply being part of it. It is interesting to note that, although Chaucer had a powerful influence in poetic and artistic terms, which can be seen in the great number of forgeries and mistaken attributions (such as \"The Floure and the Leafe\", which was translated by John Dryden), modern English spelling and orthography owe much more to the innovations made by the Court of Chancery in the decades during and after his lifetime.\n\nWhile Chaucer clearly states the addressees of many of his poems (the \"Book of the Duchess\" is believed to have been written for John of Gaunt on the occasion of his wife's death in 1368), the intended audience of \"The Canterbury Tales\" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for the nobility. He is referred to as a noble translator and poet by Eustache Deschamps and by his contemporary John Gower. It has been suggested that the poem was intended to be read aloud, which is probable as this was a common activity at the time. However, it also seems to have been intended for private reading as well, since Chaucer frequently refers to himself as the writer, rather than the speaker, of the work. Determining the intended audience directly from the text is even more difficult, since the audience is part of the story. This makes it difficult to tell when Chaucer is writing to the fictional pilgrim audience or the actual reader.\n\nChaucer's works may have been distributed in some form during his lifetime in part or in whole. Scholars speculate that manuscripts were circulated among his friends, but likely remained unknown to most people until after his death. However, the speed with which copyists strove to write complete versions of his tale in manuscript form shows that Chaucer was a famous and respected poet in his own day. The Hengwrt and Ellesmere manuscripts are examples of the care taken to distribute the work. More manuscript copies of the poem exist than for any other poem of its day except \"The Prick of Conscience\", causing some scholars to give it the medieval equivalent of bestseller status. Even the most elegant of the illustrated manuscripts, however, is not nearly as highly decorated as the work of authors of more respectable works such as John Lydgate's religious and historical literature.\n\nJohn Lydgate and Thomas Occleve were among the first critics of Chaucer's \"Tales\", praising the poet as the greatest English poet of all time and the first to show what the language was truly capable of poetically. This sentiment was universally agreed upon by later critics into the mid-15th century. Glosses included in \"The Canterbury Tales\" manuscripts of the time praised him highly for his skill with \"sentence\" and rhetoric, the two pillars by which medieval critics judged poetry. The most respected of the tales was at this time the Knight's, as it was full of both.\n\nThe incompleteness of the \"Tales\" led several medieval authors to write additions and supplements to the tales to make them more complete. Some of the oldest existing manuscripts of the tales include new or modified tales, showing that even early on, such additions were being created. These emendations included various expansions of the \"Cook's Tale\", which Chaucer never finished, \"The Plowman's Tale\", \"The Tale of Gamelyn\", the \"Siege of Thebes\", and the \"Tale of Beryn\".\n\nThe \"Tale of Beryn\", written by an anonymous author in the 15th century, is preceded by a lengthy prologue in which the pilgrims arrive at Canterbury and their activities there are described. While the rest of the pilgrims disperse throughout the town, the Pardoner seeks the affections of Kate the barmaid, but faces problems dealing with the man in her life and the innkeeper Harry Bailey. As the pilgrims turn back home, the Merchant restarts the storytelling with \"Tale of Beryn\". In this tale, a young man named Beryn travels from Rome to Egypt to seek his fortune only to be cheated by other businessmen there. He is then aided by a local man in getting his revenge. The tale comes from the French tale \"Bérinus\" and exists in a single early manuscript of the tales, although it was printed along with the tales in a 1721 edition by John Urry.\n\nJohn Lydgate wrote \"The Siege of Thebes\" in about 1420. Like the \"Tale of Beryn\", it is preceded by a prologue in which the pilgrims arrive in Canterbury. Lydgate places himself among the pilgrims as one of them and describes how he was a part of Chaucer's trip and heard the stories. He characterises himself as a monk and tells a long story about the history of Thebes before the events of the \"Knight's Tale\". John Lydgate's tale was popular early on and exists in old manuscripts both on its own and as part of the \"Tales\". It was first printed as early as 1561 by John Stow, and several editions for centuries after followed suit.\n\nThere are actually two versions of \"The Plowman's Tale\", both of which are influenced by the story \"Piers Plowman\", a work written during Chaucer's lifetime. Chaucer describes a Plowman in the \"General Prologue\" of his tales, but never gives him his own tale. One tale, written by Thomas Occleve, describes the miracle of the Virgin and the Sleeveless Garment. Another tale features a pelican and a griffin debating church corruption, with the pelican taking a position of protest akin to John Wycliffe's ideas.\n\n\"The Tale of Gamelyn\" was included in an early manuscript version of the tales, Harley 7334, which is notorious for being one of the lower-quality early manuscripts in terms of editor error and alteration. It is now widely rejected by scholars as an authentic Chaucerian tale, although some scholars think he may have intended to rewrite the story as a tale for the Yeoman. Dates for its authorship vary from 1340 to 1370.\n\nMany literary works (both fiction and non-fiction alike) have used a similar frame narrative to \"The Canterbury Tales\" as an homage. Science-fiction writer Dan Simmons wrote his Hugo Award winning novel \"Hyperion\" based on an extra-planetary group of pilgrims. Evolutionary biologist Richard Dawkins used \"The Canterbury Tales\" as a structure for his 2004 non-fiction book about evolution titled \"\". His animal pilgrims are on their way to find the common ancestor, each telling a tale about evolution.\n\nHenry Dudeney's book \"The Canterbury Puzzles\" contains a part reputedly lost from what modern readers know as Chaucer's tales.\n\nHistorical-mystery novelist P.C. Doherty wrote a series of novels based on \"The Canterbury Tales\", making use of both the story frame and Chaucer's characters.\n\nCanadian author Angie Abdou translates \"The Canterbury Tales\" to a cross section of people, all snow-sports enthusiasts but from different social backgrounds, converging on a remote back-country ski cabin in British Columbia in the 2011 novel \"The Canterbury Trail\".\n\n\"The Two Noble Kinsmen\", by William Shakespeare and John Fletcher, a retelling of \"The Knight's Tale\", was first performed in 1613 or 1614 and published in 1634. In 1961, Erik Chisholm completed his opera, \"The Canterbury Tales\". The opera is in three acts: The Wyf of Bath’s Tale, The Pardoner’s Tale and The Nun’s Priest’s Tale. Nevill Coghill's modern English version formed the basis of a musical version that was first staged in 1964.\n\n\"A Canterbury Tale\", a 1944 film jointly written and directed by Michael Powell and Emeric Pressburger, is loosely based on the narrative frame of Chaucer's tales. The movie opens with a group of medieval pilgrims journeying through the Kentish countryside as a narrator speaks the opening lines of the \"General Prologue\". The scene then makes a now-famous transition to the time of World War II. From that point on, the film follows a group of strangers, each with his or her own story and in need of some kind of redemption, who are making their way to Canterbury together. The film's main story takes place in an imaginary town in Kent and ends with the main characters arriving at Canterbury Cathedral, bells pealing and Chaucer's words again resounding. \"A Canterbury Tale\" is recognised as one of the Powell-Pressburger team's most poetic and artful films. It was produced as wartime propaganda, using Chaucer's poetry, referring to the famous pilgrimage, and offering photography of Kent to remind the public of what made Britain worth fighting for. In one scene a local historian lectures an audience of British soldiers about the pilgrims of Chaucer's time and the vibrant history of England.\n\nPier Paolo Pasolini's 1972 film \"The Canterbury Tales\" features several of the tales, some of which keep close to the original tale and some of which are embellished. The \"Cook's Tale\", for instance, which is incomplete in the original version, is expanded into a full story, and the \"Friar's Tale\" extends the scene in which the Summoner is dragged down to hell. The film includes these two tales as well as the \"Miller's Tale\", the \"Summoner's Tale\", the \"Wife of Bath's Tale\", and the \"Merchant's Tale\".\n\nSeveral more recent films, while they are not based on the tales, do have references to them. For example, in the 1995 film \"Se7en\", the \"Parson's Tale\" is an important clue to the methods of a serial killer who chooses his victims based on the seven deadly sins. The 2001 film \"A Knight's Tale\" took its name from \"The Knight's Tale\". Although it bears little resemblance to the tale, it does feature what Martha Driver and Sid Ray call an \"MTV-generation\" Chaucer who is a gambling addict with a way with words. Scattered references to the \"Tales\" include Chaucer's declaration that he will use his verse to vilify a summoner and a pardoner who have cheated him.\n\nOn April 26, 1986, American radio personality Garrison Keillor opened \"The News from Lake Wobegon\" portion of the first live TV broadcast of his \"A Prairie Home Companion\" radio show with a reading of the original Middle English text of the General Prologue. He commented, \"Although those words were written more than 600 years ago, they still describe spring.\"\n\nTelevision adaptations include Alan Plater's 1975 re-telling of the stories in a series of plays for BBC2: \"Trinity Tales\". In 2003, BBC again featured modern re-tellings of selected tales.\n\nThe 2014 young adult short novel \"Anaheim Tales\" by M.L. Millard was inspired by and references \"The Canterbury Tales\".\n\nIn rock music, the English singer Sting paid tribute to Chaucer and the book with his 1993 concept album \"Ten Summoner's Tales\", which he described as ten songs (plus an epilogue number) with no theme or subject tying them together. Sting's real name is Gordon Sumner, hence the reference to the \"Summoner\" character in the record's title. In essence, the collection of songs was composed as \"a musical \"Canterbury Tales\"\". \"The Canterbury Tales\" is also mentioned in the lyrics of the 1984 song \"Cinderella Search\" by rock band Marillion.\n\n\n\n\n\n", "id": "7627", "title": "The Canterbury Tales"}
{"url": "https://en.wikipedia.org/wiki?curid=7628", "text": "Christine de Pizan\n\nChristine de Pizan (also seen as de Pisan ; ; 1364 – c. 1430) was an Italian French late medieval author. She served as a court writer for several dukes (Louis of Orleans, Philip the Bold of Burgundy, and John the Fearless of Burgundy) and the French royal court during the reign of Charles VI. She wrote both poetry and prose works such as biographies and books containing practical advice for women. She completed forty-one works during her 30-year career from 1399 to 1429. She married in 1380 at the age of 15, and was widowed 10 years later. Much of the impetus for her writing came from her need to earn a living to support her mother, a niece and her two surviving children. She spent most of her childhood and all of her adult life in Paris and then the abbey at Poissy, and wrote entirely in her adopted language, Middle French.\n\nHer early courtly poetry is marked by her knowledge of aristocratic custom and fashion of the day, particularly involving women and the practice of chivalry. Her early and later allegorical and didactic treatises reflect both autobiographical information about her life and views and also her own individualized and humanist approach to the scholastic learned tradition of mythology, legend, and history she inherited from clerical scholars and to the genres and courtly or scholastic subjects of contemporary French and Italian poets she admired. Supported and encouraged by important royal French and English patrons, she influenced 15th-century English poetry. Her success stems from a wide range of innovative writing and rhetorical techniques that critically challenged renowned writers such as Jean de Meun, author of the \"Romance of the Rose\", which she criticized as immoral.\n\nIn recent decades, Christine de Pizan's work has been returned to prominence by the efforts of scholars such as Charity Cannon Willard, Earl Jeffrey Richards and Simone de Beauvoir. Certain scholars have argued that she should be seen as an early feminist who efficiently used language to convey that women could play an important role within society. This characterization has been challenged by other critics, who say that it is either an anachronistic use of the word or a misinterpretation of her writing and intentions.\n\nChristine de Pizan was born in 1364 in Venice, Italy. She was the daughter of Tommaso di Benvenuto da Pizzano (Thomas de Pizan, named for the family's origins in the town of Pizzano, south east of Bologna), a physician, court astrologer, and Councillor of the Republic of Venice. Following her birth, Thomas de Pizan accepted an appointment to the court of Charles V of France, as the king's astrologer, alchemist, and physician. In this atmosphere, Christine was able to pursue her intellectual interests. She successfully educated herself by immersing herself in languages, in the rediscovered classics and humanism of the early Renaissance, and in Charles V's royal archive that housed a vast number of manuscripts. But she did not assert her intellectual abilities, or establish her authority as a writer until she was widowed at the age of 25.\n\nShe married Etienne du Castel, a royal secretary to the court, at the age of 15. She had three children, a daughter (who became a nun at the Dominican Abbey in Poissy in 1397 as a companion to the king's daughter, Marie), a son Jean, and another child who died in childhood. Christine's family life was threatened in 1387 when her husband, while in Beauvais on a mission with the king, suddenly died in an epidemic. Following Castel's death, she was left to support her mother, a niece, and her two children. When she tried to collect money from her husband's estate, she faced complicated lawsuits regarding the recovery of salary due her husband. On 4 June 1389, in a judgment concerning a lawsuit filed against her by the archbishop of Sens and François Chanteprime, councillors of the king, Christine was styled \"damoiselle\" and widow of \"Estienne du Castel.\" Note that in letters he signed as secretary of the king in 1381 and 1382 the signature of Etienne was \"Ste de Castel.\" The abbreviation of his first name could be read both as a phonetic abbreviation of Estienne and as the first letters of his name in Latin: Stephanus.\n\nIn order to support herself and her family, Christine turned to writing. By 1393, she was writing love ballads, which caught the attention of wealthy patrons within the court. These patrons were intrigued by the novelty of a female writer and had her compose texts about their romantic exploits. Her output during this period was prolific. Between 1393 and 1412, she composed over 300 ballads, and many more shorter poems.\n\nChristine's participation in a literary debate, in 1401–1402, allowed her to move beyond the courtly circles, and ultimately to establish her status as a writer concerned with the position of women in society. During these years, she involved herself in a renowned literary controversy, the \"Querelle du Roman de la Rose\". She helped to instigate this debate by beginning to question the literary merits of Jean de Meun's the \"Romance of the Rose\". Written in the 13th century, the \"Romance of the Rose\" satirizes the conventions of courtly love while critically depicting women as nothing more than seducers. Christine specifically objected to the use of vulgar terms in Jean de Meun's allegorical poem. She argued that these terms denigrated the proper and natural function of sexuality, and that such language was inappropriate for female characters such as Madame Raison. According to her, noble women did not use such language. Her critique primarily stems from her belief that Jean de Meun was purposely slandering women through the debated text.\n\nThe debate itself was extensive and at its end, the principal issue was no longer Jean de Meun’s literary capabilities. The principal issue had shifted to the unjust slander of women within literary texts. This dispute helped to establish Christine's reputation as a female intellectual who could assert herself effectively and defend her claims in the male-dominated literary realm. She continued to counter abusive literary treatments of women.\n\nChristine produced a large number of vernacular works, in both prose and verse. Her works include political treatises, mirrors for princes, epistles, and poetry.\n\nBy 1405, Christine had completed her most famous literary works, \"The Book of the City of Ladies\" and \"The Treasure of the City of Ladies\". The first of these shows the importance of women's past contributions to society, and the second strives to teach women of all estates how to cultivate useful qualities. For example, one section of the book tells wives: \"If she wants to act prudently and have the praise of both the world and her husband, she will be cheerful to him all the time\"\n\nIn \"The Book of the City of Ladies\" Christine created a symbolic city in which women are appreciated and defended. She constructed three allegorical figures – Reason, Justice, and Rectitude – in the common pattern of literature in that era, when many books and poetry utilized stock allegorical figures to express ideas or emotions. She enters into a dialogue, a movement between question and answer, with these allegorical figures that is from a completely female perspective. Together, they create a forum to speak on issues of consequence to all women. Only female voices, examples and opinions provide evidence within this text. Christine, through Lady Reason in particular, argues that stereotypes of women can be sustained only if women are prevented from entering into the conversation. Overall, she hoped to establish truths about women that contradicted the negative stereotypes that she had identified in previous literature.\n\nIn \"The Treasure of the City of Ladies\", she highlights the persuasive effect of women’s speech and actions in everyday life. In this particular text, Christine argues that women must recognize and promote their ability to make peace between people. This ability will allow women to mediate between husband and subjects. She also argues that slanderous speech erodes one’s honor and threatens the sisterly bond among women. Christine then argues that \"skill in discourse should be a part of every woman’s moral repertoire\". She believed that a woman’s influence is realized when her speech accords value to chastity, virtue, and restraint. She argued that rhetoric is a powerful tool that women could employ to settle differences and to assert themselves. The \"Treasure of the City of Ladies\" provides glimpses into women's lives in 1400, from the great lady in the castle down to the merchant's wife, the servant, and the peasant. She offers advice to governesses, widows, and even prostitutes.\n\nDe Pizan was greatly interested in history, ranging from the Matter of Troy to the \"founding of the royal house of France\" (for her the latter was a consequence of the former). She obtained her knowledge of Troy from the \"Histoire ancienne jusqu'à César\", and chose an anti-Trojan position. Hector especially served as a model and a measure of masculinity for her.\n\nIn the \"Querelle du Roman de la Rose,\" she responded to Jean de Montreuil, who had sent her a treatise defending the sentiments expressed in the \"Romance of the Rose\". She begins by styling her opponent as an \"expert in rhetoric\" in contrast to herself, \"a woman ignorant of subtle understanding and agile sentiment.\" In this particular apologetic response, de Pizan belittles her own style. She is employing a rhetorical strategy by writing against the grain of her meaning, also known as antiphrasis. Her ability to employ rhetorical strategies continued when Christine began to compose literary texts following the \"Querelle du Roman de la Rose.\" \n\nHer final work was a poem eulogizing Joan of Arc, the peasant girl who said God had commanded her to secure the French throne for Charles VII. Written in 1429, \"The Poem of Joan of Arc\" (\"Ditie de Jehanne dArc\") celebrates the appearance of a woman whom Christine describes in the poem as \"a simple shepherdess\" while commenting: \"It is a fact well worth remembering That God should now have wished (and this is the truth!) to bestow such great blessings on France, through a young virgin\", adding \"For there will be a King of France called Charles [VII], son of Charles [VI], who will be supreme ruler over all Kings.\" After completing this particular poem, it seems that Christine de Pizan, at the age of 65, decided to end her literary career.\n\nChristine specifically sought out other women to collaborate in the creation of her work. She makes special mention of a manuscript illustrator we know only as Anastasia, whom she described as the most talented of her day.\n\nIn her own day, Christine de Pizan was primarily a court writer who wrote commissioned works for aristocratic families, as well as addressing literary debates of the era. In modern times, she has been labeled a poetic mediator who engaged with historical texts to interpolate her royal readers and encourage ethical and judicious conduct. Some rhetorical scholars have concluded, from studying her persuasive strategies, that she forged a rhetorical identity for herself and encouraged women to embrace this identity. Some have argued that Christine de Pizan \"began her literary career by singing, alone in her room, and she finished by shouting in the public square.\" She left an influential footprint in the field of rhetorical discourse in an otherwise male-dominated literary field. She left forty-one surviving poetic works and a number of prose books. Simone de Beauvoir wrote in 1949 that \"Épître au Dieu d'Amour\" was \"the first time we see a woman take up her pen in defence of her sex\".\n\n\n\nThe artwork \"The Dinner Party\" features a place setting for Christine de Pizan.\n\n\n\n\n", "id": "7628", "title": "Christine de Pizan"}
{"url": "https://en.wikipedia.org/wiki?curid=7630", "text": "Catharism\n\nCatharism (; from the Greek: , \"katharoi\", \"the pure [ones]\") was a Christian dualist or Gnostic revival movement that thrived in some areas of Southern Europe, particularly northern Italy and southern France, between the 12th and 14th centuries. The followers were known as Cathars and are now mainly remembered for a prolonged period of persecution by the Catholic church which did not recognise their belief as truly Christian. It appeared in Europe in the Languedoc region of France in the 11th century and this is when the name first appears. The beliefs are believed to have been brought from Persia or the Byzantine Empire.\n\nCathar beliefs varied between communities, because Catharism was initially taught by ascetic priests who had set few guidelines. The Catholic Church denounced its practices including the \"Consolamentum\" ritual, by which Cathar individuals were baptized and raised to the status of 'perfect'.\n\nCatharism had its roots in the Paulician movement in Armenia and eastern Byzantine Anatolia and the Bogomils of the First Bulgarian Empire, who were influenced by the Paulicians resettled in Thrace (Philipopolis) by the Byzantines. Though the term \"Cathar\" () has been used for centuries to identify the movement, whether the movement identified itself with this name is debatable. In Cathar texts, the terms \"Good Men\" (\"Bons Hommes\") or \"Good Christians\" are the common terms of self-identification.\n\nThe idea of two Gods or principles, one being good and the other evil, was central to Cathar beliefs. The good God was the God of the New Testament and the creator of the spiritual realm, contrasted with the evil Old Testament God—the creator of the physical world whom many Cathars, and particularly their persecutors, identified as Satan. All visible matter, including the human body, was created by this evil god; matter was therefore tainted with sin. This was the antithesis to the monotheistic Catholic Church, whose fundamental principle was that there was only one God, who created all things visible and invisible. Cathars thought human spirits were the genderless spirits of angels trapped within the physical creation of the evil god, cursed to be reincarnated until the Cathar faithful achieved salvation through a ritual called the consolamentum.\n\nFrom the beginning of his reign, Pope Innocent III attempted to end Catharism by sending missionaries and by persuading the local authorities to act against them. In 1208 Innocent's papal legate Pierre de Castelnau was murdered while returning to Rome after excommunicating Count Raymond VI of Toulouse, who, in his view, was too lenient with the Cathars. Pope Innocent III then abandoned the option of sending Catholic missionaries and jurists, declared Pierre de Castelnau a martyr and launched the Albigensian Crusade which all but ended Catharism.\n\nThe origins of the Cathars' beliefs are unclear, but most theories agree they came from the Byzantine Empire, mostly by the trade routes and spread from the First Bulgarian Empire to the Netherlands. The name of Bulgarians (\"Bougres\") was also applied to the Albigensians, and they maintained an association with the similar Christian movement of the Bogomils (\"Friends of God\") of Thrace. \"That there was a substantial transmission of ritual and ideas from Bogomilism to Catharism is beyond reasonable doubt.\" Their doctrines have numerous resemblances to those of the Bogomils and the Paulicians, who influenced them, as well as the earlier Marcionites, who were found in the same areas as the Paulicians, the Manicheans and the Christian Gnostics of the first few centuries AD, although, as many scholars, most notably Mark Pegg, have pointed out, it would be erroneous to extrapolate direct, historical connections based on theoretical similarities perceived by modern scholars.\n\nSt John Damascene, writing in the 8th century AD, also notes of an earlier sect called the \"Cathari\", in his book \"On Heresies\", taken from the epitome provided by Epiphanius of Salamis in his \"Panarion\". He says of them: \"They absolutely reject those who marry a second time, and reject the possibility of penance [that is, forgiveness of sins after baptism]\". These are probably the same Cathari who are mentioned in Canon 8 of the First Ecumenical Council of Nicaea in the year 325, which states \"...[I]f those called Cathari come over [to the faith], let them first make profession that they are willing to communicate [share full communion] with the twice-married, and grant pardon to those who have lapsed...\"\n\nIt is likely that we have only a partial view of their beliefs, because the writings of the Cathars were mostly destroyed because of the doctrinal threat perceived by the Papacy; much of our existing knowledge of the Cathars is derived from their opponents. Conclusions about Cathar ideology continue to be fiercely debated with commentators regularly accusing their opponents of speculation, distortion and bias. There are a few texts from the Cathars themselves which were preserved by their opponents (the \"Rituel Cathare de Lyon\") which give a glimpse of the inner workings of their faith, but these still leave many questions unanswered. One large text which has survived, \"The Book of Two Principles\" (\"Liber de duobus principiis\"), elaborates the principles of dualistic theology from the point of view of some of the Albanenses Cathars.\n\nIt is now generally agreed by most scholars that identifiable historical Catharism did not emerge until at least 1143, when the first confirmed report of a group espousing similar beliefs is reported being active at Cologne by the cleric Eberwin of Steinfeld. A landmark in the \"institutional history\" of the Cathars was the Council, held in 1167 at Saint-Félix-Lauragais, attended by many local figures and also by the Bogomil \"papa\" Nicetas, the Cathar bishop of (northern) France and a leader of the Cathars of Lombardy.\n\nThe Cathars were largely a homegrown, Western European/Latin Christian phenomenon, springing up in the Rhineland cities (particularly Cologne) in the mid-12th century, northern France around the same time, and particularly southern France—the Languedoc—and the northern Italian cities in the mid-late 12th century. In the Languedoc and northern Italy, the Cathars attained their greatest popularity, surviving in the Languedoc, in much reduced form, up to around 1325 and in the Italian cities until the Inquisitions of the 1260s–1300s finally rooted them out.\n\nCathars, in general, formed an anti-sacerdotal party in opposition to the Catholic Church, protesting against what they perceived to be the moral, spiritual and political corruption of the Church.\n\nG. K. Chesterton, the English Roman Catholic author, claimed: \"... the medieval system began to be broken to pieces intellectually, long before it showed the slightest hint of falling to pieces morally. The huge early heresies, like the Albigenses, had not the faintest excuse in moral superiority.\"\n\nContemporary reports suggest otherwise, however. St Bernard of Clairvaux, for instance, although opposed to the Cathars, said of them in Sermon 65 on the \"Song of Songs\":\n\nWhen Bishop Fulk of Toulouse, a key leader of the anti-Cathar persecutions, excoriated the Languedoc Knights for not pursuing the heretics more diligently, he received the reply:\nIn contrast to the Catholic Church, the Cathars had but one sacrament, the Consolamentum, or Consolation. This involved a brief spiritual ceremony to remove all sin from the believer and to induct him or her into the next higher level as a perfect. Unlike the Roman Catholic sacrament of Penance, the Consolamentum could be taken only once.\n\nThus it has been alleged that many believers would eventually receive the Consolamentum as death drew near, performing the ritual of liberation at a moment when the heavy obligations of purity required of Perfecti would be temporally short. Some of those who received the sacrament of the consolamentum upon their death-beds may thereafter have shunned further food or drink in order to speed death. This has been termed the \"endura\". It was claimed by some of the Catholic writers that when a Cathar, after receiving the Consolamentum, began to show signs of recovery he or she would be smothered in order to ensure his or her entry into paradise. Other than at such moments of \"extremis\", little evidence exists to suggest this was a common Cathar practice.\n\nThe Cathars also refused the Catholic Sacrament of the eucharist saying that it could not possibly be the body of Christ. They also refused to partake in the practice of Baptism by water. The following two quotes are taken from the Catholic Inquisitor Bernard Gui's experiences with the Cathar practices and beliefs:\nSome believe that the Catharist conception of Jesus resembled nontrinitarian modalistic monarchianism (Sabellianism) in the West and adoptionism in the East.\n\nBernard of Clairvaux's biographer and other sources accuse some Cathars of Arianism, and some scholars see Cathar Christology as having traces of earlier Arian roots. According to some of their contemporary enemies Cathars did not accept the Trinitarian understanding of Jesus, but considered him the human form of an angel similar to Docetic Christology. Zoé Oldenbourg (2000) compared the Cathars to \"Western Buddhists\" because she considered that their view of the doctrine of \"resurrection\" taught by Jesus was, in fact, similar to the Buddhist doctrine of reincarnation. The Cathars taught that to regain angelic status one had to renounce the material self completely. Until one was prepared to do so, he/she would be stuck in a cycle of reincarnation, condemned to live on the corrupt Earth.\n\nThe alleged sacred texts of the Cathars besides the \"New Testament\", include \"The Gospel of the Secret Supper, or John's Interrogation\" and \"The Book of the Two Principles\".\n\nKilling was abhorrent to the Cathars. Consequently, abstention from all animal food (sometimes exempting fish) was enjoined of the Perfecti. The Perfecti avoided eating anything considered to be a by-product of sexual reproduction. War and capital punishment were also condemned—an abnormality in Medieval Europe. In a world where few could read, their rejection of oath-taking marked them as social outcasts.\n\nCathars also rejected marriage. Their theology was based principally on the belief that the physical world, including the flesh, was irredeemably evil—as it stemmed from the evil principle or \"demiurge\". Therefore, reproduction was viewed by them as a moral evil to be avoided—as it continued the chain of reincarnation and suffering in the material world. It was claimed by their opponents that, given this loathing for procreation, they generally resorted to sodomy. Such was the situation that a charge of heresy leveled against a suspected Cathar was usually dismissed if the accused could show he was legally married.\n\nIt has been alleged that the Cathar Church of the Languedoc had a relatively flat structure, distinguishing between \"perfecti\" (a term they did not use, instead \"bonhommes\") and \"credentes\". By about 1140, liturgy and a system of doctrine had been established. It created a number of bishoprics, first at Albi around 1165 and after the 1167 Council at Saint-Félix-Lauragais sites at Toulouse, Carcassonne, and Agen, so that four bishoprics were in existence by 1200.\n\nIn about 1225, during a lull in the Albigensian Crusade, the bishopric of Razès was added. Bishops were supported by their two assistants: a \"filius maior\" (typically the successor) and a \"filius minor\", who were further assisted by deacons. The \"perfecti\" were the spiritual elite, highly respected by many of the local people, leading a life of austerity and charity. In the apostolic fashion they ministered to the people and travelled in pairs.\n\nCatharism has been seen as giving women the greatest opportunities for independent action since women were found as being believers as well as Perfecti, who were able to administer the sacrament of the \"consolamentum\".\n\nThe Cathars believed that one would be repeatedly reincarnated until one commits to the self-denial of the material world, which meant that a man could be reincarnated as a woman and vice versa, thereby rendering gender completely meaningless. The spirit was of utmost importance to the Cathars and was described as being immaterial and sexless. Because of this belief, the Cathars saw women equally capable of being spiritual leaders, which undermined the very concept of gender as held by the Catholic Church and did not go unnoticed.\n\nThe women that were accused of being heretics in early medieval Christianity included those labeled Gnostics, Cathars, and Beguines, as well as several other groups that were sometimes \"tortured and executed\". The Cathars, like the Gnostics who preceded them, assigned more importance to the role of Mary Magdalene in the spread of early Christianity than the Church previously did. Her vital role as a teacher contributed to the Cathar belief that women could serve as spiritual leaders. Women were found to be included in the Perfecti in significant numbers, with numerous receiving the \"consolamentum\" after being widowed. Having reverence for the Gospel of John, the Cathars saw Mary Magdalene as perhaps even more important than Saint Peter, the founder of the Church.\n\nThe Cathar movement proved to be extremely successful in gaining female followers because of its proto-feminist teachings along with the general feeling of exclusion from the Catholic church. Catharism attracted numerous women with the promise of a sacerdotal role that the Catholic Church did not allow. Catharism let women become a perfect of the faith, a position of far more prestige than anything the Church offered. These female perfects were required to adhere to a strict and ascetic lifestyle, but were still able to have their own houses. Although many women found something attractive in Catharism, not all found its teachings convincing. A notable example is Hildegard of Bingen, who in 1163 gave a widely renowned sermon against the Cathars in Cologne. During this speech, Hildegard announced a state of eternal punishment and damnation to all those who accepted Cathar beliefs.\n\nWhile women perfects rarely traveled to preach the faith, they still played a vital role in the spreading of the Catharism by establishing group homes for women. Though it was extremely uncommon, there were isolated cases of female Cathars departing from their homes to spread the faith. In the Cathar group homes, women were educated in the faith and these women would go on to bear children who would then also become believers. Through this pattern the faith grew exponentially through the efforts of women as each generation passed. Among some groups of Cathars there were even more women than there were men.\n\nDespite women having an instrumental role in the growing of the faith, misogyny was not completely absent from the Cathar movement. Some seemingly misogynistic Cathar beliefs include that one's last incarnation had to be experienced as a man to break the cycle. This belief was inspired by later French Cathars, which taught that women must be reborn as men in order to achieve salvation. Another one is that the sexual allure of women impedes man's ability to reject the material world. Toward the end of the Cathar movement, French Catharism became more misogynistic and started the practice of excluding women perfects. However, the influence of these type of misogynistic beliefs and practices remained rather limited on the whole of Catharism as later Italian perfects still included women.\n\nIn 1147, Pope Eugene III sent a legate to the Cathar district in order to arrest the progress of the Cathars. The few isolated successes of Bernard of Clairvaux could not obscure the poor results of this mission, which clearly showed the power of the sect in the Languedoc at that period. The missions of Cardinal Peter of St. Chrysogonus to Toulouse and the Toulousain in 1178, and of Henry of Marcy, cardinal-bishop of Albano, in 1180–81, obtained merely momentary successes. Henry's armed expedition, which took the stronghold at Lavaur, did not extinguish the movement.\n\nDecisions of Catholic Church councils—in particular, those of the Council of Tours (1163) and of the Third Council of the Lateran (1179)—had scarcely more effect upon the Cathars. When Pope Innocent III came to power in 1198, he was resolved to deal with them.\n\nAt first Innocent tried peaceful conversion, and sent a number of legates into the Cathar regions. They had to contend not only with the Cathars, the nobles who protected them, and the people who respected them, but also with many of the bishops of the region, who resented the considerable authority the Pope had conferred upon his legates. In 1204, Innocent III suspended a number of bishops in Occitania; in 1205 he appointed a new and vigorous bishop of Toulouse, the former troubadour Foulques. In 1206 Diego of Osma and his canon, the future Saint Dominic, began a programme of conversion in Languedoc; as part of this, Catholic-Cathar public debates were held at Verfeil, Servian, Pamiers, Montréal and elsewhere.\n\nSaint Dominic met and debated with the Cathars in 1203 during his mission to the Languedoc. He concluded that only preachers who displayed real sanctity, humility and asceticism could win over convinced Cathar believers. The institutional Church as a general rule did not possess these spiritual warrants. His conviction led eventually to the establishment of the Dominican Order in 1216. The order was to live up to the terms of his famous rebuke, \"Zeal must be met by zeal, humility by humility, false sanctity by real sanctity, preaching falsehood by preaching truth.\" However, even St. Dominic managed only a few converts among the Cathari.\n\nIn January 1208 the papal legate, Pierre de Castelnau—a Cistercian monk, theologian and canon lawyer—was sent to meet the ruler of the area, Raymond VI, Count of Toulouse. Known for excommunicating noblemen who protected the Cathars, Castelnau excommunicated Raymond for abetting heresy following an allegedly fierce argument during which Raymond supposedly threatened Castelnau with violence. Shortly thereafter, Castelnau was murdered as he returned to Rome, allegedly by a knight in the service of Count Raymond. His body was returned and laid to rest in the Abbey at Saint Gilles.\n\nAs soon as he heard of the murder, the Pope ordered the legates to preach a crusade against the Cathars and wrote a letter to Philip Augustus, King of France, appealing for his intervention—or an intervention led by his son, Louis. This was not the first appeal but some see the murder of the legate as a turning point in papal policy. Others claim it as a fortuitous event in allowing the Pope to excite popular opinion and to renew his pleas for intervention in the south. The chronicler of the crusade which followed, Peter of Vaux de Cernay, portrays the sequence of events in such a way that, having failed in his effort to peaceably demonstrate the errors of Catharism, the Pope then called a formal crusade, appointing a series of leaders to head the assault.\n\nThe French King refused to lead the crusade himself, and could not spare his son to do so either—despite his victory against John, King of England, there were still pressing issues with Flanders and the empire and the threat of an Angevin revival. Philip did however sanction the participation of some of his more bellicose and ambitious—some might say dangerous—barons, notably Simon de Montfort and Bouchard de Marly. There followed twenty years of war against the Cathars and their allies in the Languedoc: the Albigensian Crusade.\n\nThis war pitted the nobles of the north of France against those of the south. The widespread northern enthusiasm for the Crusade was partially inspired by a papal decree permitting the confiscation of lands owned by Cathars and their supporters. This not only angered the lords of the south but also the French King, who was at least nominally the suzerain of the lords whose lands were now open to despoliation and seizure. Philip Augustus wrote to Pope Innocent in strong terms to point this out—but the Pope did not change his policy. As the Languedoc was supposedly teeming with Cathars and Cathar sympathisers, this made the region a target for northern French noblemen looking to acquire new fiefs. The barons of the north headed south to do battle.\n\nTheir first target was the lands of the Trencavel, powerful lords of Albi, Carcassonne and the Razes—but a family with few allies in the Midi. Little was thus done to form a regional coalition and the crusading army was able to take Carcassonne, the Trencavel capital, incarcerating Raymond Roger Trencavel in his own citadel where he died, allegedly of natural causes; champions of the Occitan cause from that day to this believe he was murdered. Simon de Montfort was granted the Trencavel lands by the Pope and did homage for them to the King of France, thus incurring the enmity of Peter II of Aragon who had held aloof from the conflict, even acting as a mediator at the time of the siege of Carcassonne. The remainder of the first of the two Cathar wars now essentially focused on Simon's attempt to hold on to his fabulous gains through winters where he was faced, with only a small force of confederates operating from the main winter camp at Fanjeaux, with the desertion of local lords who had sworn fealty to him out of necessity—and attempts to enlarge his newfound domains in the summer when his forces were greatly augmented by reinforcements from northern France, Germany and elsewhere.\n\nSummer campaigns saw him not only retake, sometimes with brutal reprisals, what he had lost in the 'close' season, but also seek to widen his sphere of operation—and we see him in action in the Aveyron at St. Antonin and on the banks of the Rhone at Beaucaire. Simon's greatest triumph was the victory against superior numbers at the Battle of Muret—a battle which saw not only the defeat of Raymond of Toulouse and his Occitan allies—but also the death of Peter of Aragon—and the effective end of the ambitions of the house of Aragon/Barcelona in the Languedoc. This was in the medium and longer term of much greater significance to the royal house of France than it was to de Montfort—and with the battle of Bouvines was to secure the position of Philip Augustus vis a vis England and the Empire. The Battle of Muret was a massive step in the creation of the unified French kingdom and the country we know today—although Edward III, the Black Prince and Henry V would threaten later to shake these foundations.\n\nThe crusader army came under the command, both spiritually and militarily, of the papal legate Arnaud-Amaury, Abbot of Cîteaux. In the first significant engagement of the war, the town of Béziers was besieged on 22 July 1209. The Catholic inhabitants of the city were granted the freedom to leave unharmed, but many refused and opted to stay and fight alongside the Cathars.\n\nThe Cathars spent much of 1209 fending off the crusaders. The Béziers army attempted a sortie but was quickly defeated, then pursued by the crusaders back through the gates and into the city. Arnaud-Amaury, the Cistercian abbot-commander, is supposed to have been asked how to tell Cathars from Catholics. His reply, recalled by Caesarius of Heisterbach, a fellow Cistercian, thirty years later was \"\"Caedite eos. Novit enim Dominus qui sunt eius\"\"—\"Kill them all, the Lord will recognise His own\". The doors of the church of St Mary Magdalene were broken down and the refugees dragged out and slaughtered. Reportedly at least 7,000 innocent men, women and children were killed there by Christian forces.\n\nElsewhere in the town, many more thousands were mutilated and killed. Prisoners were blinded, dragged behind horses, and used for target practice. What remained of the city was razed by fire. Arnaud-Amaury wrote to Pope Innocent III, \"Today your Holiness, twenty thousand heretics were put to the sword, regardless of rank, age, or sex.\" \"The permanent population of Béziers at that time was then probably no more than 5,000, but local refugees seeking shelter within the city walls could conceivably have increased the number to 20,000.\"\n\nAfter the success of his siege of Carcassonne, which followed the Massacre at Béziers in 1209, Simon de Montfort was designated as leader of the Crusader army. Prominent opponents of the Crusaders were Raymond Roger Trencavel, viscount of Carcassonne, and his feudal overlord Peter II, the king of Aragon, who held fiefdoms and had a number of vassals in the region. Peter died fighting against the crusade on 12 September 1213 at the Battle of Muret. Simon de Montfort was killed on 25 June 1218 after maintaining a siege of Toulouse for nine months.\n\nThe official war ended in the Treaty of Paris (1229), by which the king of France dispossessed the house of Toulouse of the greater part of its fiefs, and that of the Trencavels (Viscounts of Béziers and Carcassonne) of the whole of their fiefs. The independence of the princes of the Languedoc was at an end. But in spite of the wholesale massacre of Cathars during the war, Catharism was not yet extinguished and Catholic forces would continue to pursue Cathars.\n\nIn 1215, the bishops of the Catholic Church met at the Fourth Council of the Lateran under Pope Innocent III; part of the agenda was combating the Cathar heresy.\n\nThe Inquisition was established in 1234 to uproot the remaining Cathars. Operating in the south at Toulouse, Albi, Carcassonne and other towns during the whole of the 13th century, and a great part of the 14th, it succeeded in crushing Catharism as a popular movement and driving its remaining adherents underground. Cathars who refused to recant were hanged, or burnt at the stake.\n\nFrom May 1243 to March 1244, the Cathar fortress of Montségur was besieged by the troops of the seneschal of Carcassonne and the archbishop of Narbonne. On 16 March 1244, a large and symbolically important massacre took place, where over 200 Cathar Perfects were burnt in an enormous pyre at the \"prat dels cremats\" (\"field of the burned\") near the foot of the castle. Moreover, the Church decreed lesser chastisements against laymen suspected of sympathy with Cathars, at the 1235 Council of Narbonne.\nA popular though as yet unsubstantiated theory holds that a small party of Cathar Perfects escaped from the fortress before the massacre at \"prat dels cremats\". It is widely held in the Cathar region to this day that the escapees took with them \"le trésor cathar\". What this treasure consisted of has been a matter of considerable speculation: claims range from sacred Gnostic texts to the Cathars' accumulated wealth, which might have included the Holy Grail (see the Section on Historical Scholarship, below).\n\nHunted by the Inquisition and deserted by the nobles of their districts, the Cathars became more and more scattered fugitives: meeting surreptitiously in forests and mountain wilds. Later insurrections broke out under the leadership of Roger-Bernard II, Count of Foix, Aimery III of Narbonne and Bernard Délicieux, a Franciscan friar later prosecuted for his adherence to another heretical movement, that of the Spiritual Franciscans at the beginning of the 14th century. But by this time the Inquisition had grown very powerful. Consequently, many presumed to be Cathars were summoned to appear before it. Precise indications of this are found in the registers of the Inquisitors, Bernard of Caux, Jean de St Pierre, Geoffroy d'Ablis, and others. The parfaits it was said only rarely recanted, and hundreds were burnt. Repentant lay believers were punished, but their lives were spared as long as they did not relapse. Having recanted, they were obliged to sew yellow crosses onto their outdoor clothing and to live apart from other Catholics, at least for a while.\n\nAfter several decades of harassment and re-proselytising, and perhaps even more importantly, the systematic destruction of their religious texts, the sect was exhausted and could find no more adepts. The leader of a Cathar revival in the Pyrenean foothills, Peire Autier was captured and executed in April 1310 in Toulouse. After 1330, the records of the Inquisition contain very few proceedings against Cathars. The last known Cathar perfectus in the Languedoc, Guillaume Bélibaste, was executed in the autumn of 1321.\n\nFrom the mid-12th century onwards, Italian Catharism came under increasing pressure from the Pope and the Inquisition, \"spelling the beginning of the end\". Other movements, such as the Waldensians and the pantheistic Brethren of the Free Spirit, which suffered persecution in the same area, survived in remote areas and in small numbers into the 14th and 15th centuries. Some Waldensian ideas were absorbed into early Protestant sects, such as the Hussites, Lollards, and the Moravian Church (Herrnhuters of Germany).\n\nAfter the suppression of Catharism, the descendants of Cathars were at times required to live outside towns and their defences. They thus retained a certain Cathar identity, despite having returned to the Catholic religion.\n\nAny use of the term \"Cathar\" to refer to people after the suppression of Catharism in the 14th century is a cultural or ancestral reference, and has no religious implication. Nevertheless, interest in the Cathars, their history, legacy and beliefs continues.\n\nThe term \"Pays Cathare\", French meaning \"Cathar Country\" is used to highlight the Cathar heritage and history of the region where Catharism was traditionally strongest. This area is centred around fortresses such as Montségur and Carcassonne; also the French département of the Aude uses the title \"Pays Cathare\" in tourist brochures. These areas have ruins from the wars against the Cathars which are still visible today.\n\nSome criticise the promotion of the identity of \"Pays Cathare\" as an exaggeration for tourist purposes. Actually, most of the promoted Cathar castles were not built by Cathars but by local lords and later many of them were rebuilt and extended for strategic purposes. Good examples of these are the magnificent castles of Queribus and Peyrepertuse which are both perched on the side of precipitous drops on the last folds of the Corbieres mountains. They were for several hundred years frontier fortresses belonging to the French crown and most of what is still there dates from a post-Cathar era. The Cathars sought refuge at these sites. Many consider the County of Foix to be the actual historical centre of Catharism.\n\nIn an effort to find the few remaining heretics in and around the village of Montaillou, Jacques Fournier, Bishop of Pamiers, future Pope Benedict XII, had those suspected of heresy interrogated in the presence of scribes who recorded their conversations. The late 13th- to early 14th-century document, discovered in the Vatican archives in the 1960s, and edited by Jean Duvernoy is the oldest known account of the daily lives of ordinary people told in their own words. It was the basis for Emmanuel Le Roy Ladurie's work \"Montaillou: The Promised Land of Error\".\n\nThe publication of the early scholarly book \"Crusade against the Grail\" by the young German Otto Rahn in the 1930s rekindled interest in the connection between the Cathars and the Holy Grail, especially in Germany. Rahn was convinced that the 13th-century work \"Parzival\" by Wolfram von Eschenbach was a veiled account of the Cathars. The philosopher and Nazi government official Alfred Rosenberg speaks favourably of the Cathars in \"The Myth of the Twentieth Century\".\n\nAcademic books in English first appeared at the beginning of the millennium: for example, Malcolm Lambert's \"The Cathars\" and Malcolm Barber's \"The Cathars\".\n\nStarting in the 1990s and continuing to the present day, historians like R.I Moore have radically challenged the extent to which Catharism, as an institutionalized religion, actually existed. Building off the work of French historians such as Monique Zerner and Uwe Brunn, Moore's \"The War on Heresy\" argues that Catharism was \"contrived from the resources of [the] well-stocked imaginations’ of churchmen, with occasional reinforcement from miscellaneous and independent manifestations of local anticlericalism or apostolic enthusiasm.\" In short, Moore claims that the men and women persecuted as Cathars were not the followers of a secret religion imported from the East, instead they were part of a broader spiritual revival taking place in the later twelfth and early thirteenth century. Moore's work is indicative of a larger historiographical trend towards examination of how heresy was constructed by the Church.\n\nThe principal legacy of the Cathar movement is in the poems and songs of the Cathar troubadors, though this artistic legacy is only a smaller part of the wider Occitan linguistic and artistic heritage. Recent artistic projects concentrating on the Cathar element in Provençal and troubador art include commercial recording projects by Thomas Binkley, electric hurdy-gurdy artist Valentin Clastrier and his CD Heresie dedicated to the church at Cathars, La Nef, and Jordi Savall.\n\nThe Cathars have been depicted or re-interpreted in popular books, video games, and films such as \"The Holy Blood and the Holy Grail\", \"The Bone Clocks\", \"Labyrinth\", \"The Winter Ghosts\", \"The Apocalypse Fire\", \"\", Paulo Coelho's Brida, Bernard Cornwell's \"The Grail Quest\" series and Theodore Roszak's \"Flicker\". A number of semi-fictional conspiracy theories have been published that integrate the Cathars into their ideas, especially in France and Germany.\n\nCatharism, along with other Christian movements including Fraticelli, Waldensianism, and Lollardy, is featured in the grand strategy game \"Crusader Kings II\", which is notable as being the only Catholic heresy in-game that allows female priests; it also grants the option of absolute cognatic succession laws (such as absolute primogeniture) and the appointment of female generals and councilors.\n\nIn Whit Stillman's film Damsels in Distress, one of the characters, Xavier, is a follower of Catharism and refers to a revival of the movement among some of his contemporaries.\n\n\nNotes\nBibliography\n\n", "id": "7630", "title": "Catharism"}
{"url": "https://en.wikipedia.org/wiki?curid=7632", "text": "Cerebrospinal fluid\n\nCerebrospinal fluid (CSF) is a clear, colorless body fluid found in the brain and spinal cord. It is produced in the choroid plexuses of the ventricles of the brain. It acts as a cushion or buffer for the brain, providing basic mechanical and immunological protection to the brain inside the skull. The CSF also serves a vital function in cerebral autoregulation of cerebral blood flow.\n\nThe CSF occupies the subarachnoid space (between the arachnoid mater and the pia mater) and the ventricular system around and inside the brain and spinal cord. It constitutes the content of the ventricles, cisterns, and sulci of the brain, as well as the central canal of the spinal cord.\n\nThere is also a connection from the subarachnoid space to the bony labyrinth of the inner ear via the perilymphatic duct where the perilymph is continuous with the cerebrospinal fluid.\n\nThere is about 125-150mL of CSF at any one time. This CSF circulates within the ventricular system of the brain. The ventricles are a series of cavities filled with CSF. The majority of CSF is produced from within the two lateral ventricles. From here, the CSF passes through the interventricular foramina to the third ventricle, then the cerebral aqueduct to the fourth ventricle. From the fourth ventricle, the fluid passes into the subarachnoid space through four openings – the central canal of the spinal cord, the median aperture, and the two lateral apertures. CSF is present within the subarachnoid space, which covers the brain, spinal cord, and stretches below the end of the spinal cord to the sacrum. There is connection from the subarachnoid space to the bony labyrinth of the inner ear making the cerebrospinal fluid continuous with the perilymph.\n\nA new hypothesis (2014) by Klarica and Oreskovic poses that there is no unidirectional CSF circulation, but cardiac cycle-dependent bi-directional systolic-diastolic to-and-fro cranio-spinal CSF movements.\n\nThe CSF is derived from blood plasma and is largely similar to it, except that CSF is nearly protein-free compared with plasma and has some different electrolyte levels. Owing to the way it is produced, CSF has a higher chloride level than plasma, and an equivalent sodium level.\n\nCSF contains approximately 0.3% plasma proteins, or approximately 15 to 40 mg/dL, depending on sampling site. In general, globular proteins and albumin are in lower concentration in ventricular CSF compared to lumbar or cisternal fluid. This continuous flow into the venous system dilutes the concentration of larger, lipid-insoluble molecules penetrating the brain and CSF. CSF is normally free of red blood cells, and at most contains only a few white blood cells. Any white blood cell count higher than this constitutes pleocytosis.\n\nAround the third week of development, the embryo is a three-layered disc, covered on the dorsal surface by a layer of ectoderm (so called presumptive epidermis). In the middle of this surface is a linear structure called the notochord, a tube-like formation derived from the dorsal mesoderm. The notochord releases extracellular molecules that affect the transformation of the overlying ectoderm into nervous tissue.\n\nAs the brain develops, by the fourth week of embryological development three swellings have formed within the embryo around the canal, near where the head will develop. These swellings represent different components of the central nervous system: the prosencephalon, mesencephalon and rhombencephalon.\n\nThe developing forebrain surrounds the neural cord. As the forebrain develops, the neural cord within it becomes a ventricle, ultimately forming the lateral ventricles. Along the inner surface of both ventricles, the ventricular wall remains thin, and a choroid plexus develops, producing and releasing CSF. The CSF quickly fills the neural canal.\n\nCSF serves several purposes:\n\n\nThe brain produces roughly 500 mL of cerebrospinal fluid per day, at a rate of about 25mL an hour. This transcellular fluid is constantly reabsorbed, so that only 125-150 mL is present at any one time.\n\nMost (about two-thirds to 80%) of CSF is produced by the choroid plexus. The choroid plexus is a network of blood vessels present within sections of the four ventricles of the brain. It is present throughout the ventricular system except for the cerebral aqueduct, frontal horn of the lateral ventricle, and occipital horn of the lateral ventricle. CSF is also produced by the single layer of column-shaped ependymal cells which line the ventricles; by the lining surrounding the subarachnoid space; and a small amount directly from the tiny spaces surrounding blood vessels around the brain.\n\nCSF is produced by the choroid plexus in two steps. A filtered form of plasma leaks from the fenestrated capillaries in the choroid plexus. The lining of the choroid plexus are similar to ependymal cells around them, but possess tight junctions between them, which act to prevent most substances flowing freely into the CSF. To create the fluid, the lining cells of the choroid plexus actively secrete sodium into the ventricles. This creates osmotic pressure and draws water into the CSF. Chloride, with a negative charge, moves with the positively charged sodium and an electroneutral charge is maintained. Potassium, glucose and bicarbonate are all also transported out of the cell. As a result, CSF contains a higher concentration of sodium and chloride than blood plasma, but less potassium, calcium and glucose and protein. At a molecular level, an Na/K ATPase transporter found on the surface of the choroid lining cells facing the CSF, appears to play a critical role. Other molecules likely to play a role include Aquaporins, channels involved in the transport of water,the Na-K-Cl cotransporter, and the intracellular carbonic anhydrase enzyme, found within choroid plexus cells.\n\nOrešković and Klarica hypothesise that CSF is not primarily produced by the choroid plexus, but is being permanently produced inside the entire CSF system, as a consequence of water filtration through the capillary walls into the interstitial fluid of the surrounding brain tissue, regulated by AQP-4.\n\nCSF returns to the vascular system by entering the dural venous sinuses via arachnoid granulations. These are outpouchings of the arachnoid mater into the venous sinuses around the brain, with valves to ensure one-way drainage. CSF has also been seen to drain into lymphatic vessels, particularly those surrounding the nose; however the pathway and extent are currently not known, but may involve CSF flow along some cranial nerves and be more prominant in the neonate. CSF turns over at a rate of three to four times a day.\n\nWhen CSF pressure is elevated, cerebral blood flow may be constricted. When disorders of CSF flow occur, they may therefore affect not only CSF movement but also craniospinal compliance and the intracranial blood flow, with subsequent neuronal and glial vulnerabilities. The venous system is also important in this equation. Infants and patients shunted as small children may have particularly unexpected relationships between pressure and ventricular size, possibly due in part to venous pressure dynamics. This may have significant treatment implications, but the underlying pathophysiology needs to be further explored.\n\nCSF can leak from the dura as a result of different causes such as physical trauma or a lumbar puncture, or from no known cause when it is termed spontaneous cerebrospinal fluid leak. The leakage can cause a lack of CSF pressure and volume which can allow the brain to descend through the foramen magnum in the occipital bone where the lower portion of the brain may impact on cranial nerve complexes causing a variety of sensory symptoms.\n\nThe \"IgG index\" of cerebrospinal fluid is a measure of the immunoglobulin G content, and is elevated in multiple sclerosis. It is defined as\n\"IgG index = (IgG / IgG ) / (albumin / albumin\"). A cutoff value has been suggested to be 0.73, with a higher value indicating presence of multiple sclerosis.\n\nHydrocephalus is an abnormal accumulation of cerebrospinal fluid (CSF) in the ventricles of the brain and can be caused by an impaired flow of cerebrospinal fluid, reabsorption, or excessive production of CSF. Hydrocephalus may cause increased intracranial pressure inside the skull. It may lead to enlargement of the skull and head if hydrocephalus occurs during fetal development. It is usually accompanied by mental disability, sometimes by convulsive episodes and also tunnel vision. Hydrocephalus may become fatal if it is not corrected quickly. It is more common in infants, and in older adults.\n\nCSF pressure, as measured by lumbar puncture, is 10–18 cmHO (8–15 mmHg or 1.1–2 kPa) with the patient lying on the side and 20–30 cmHO (16–24 mmHg or 2.1–3.2 kPa) with the patient sitting up. In newborns, CSF pressure ranges from 8 to 10 cmHO (4.4–7.3 mmHg or 0.78–0.98 kPa). Most variations are due to coughing or internal compression of jugular veins in the neck. When lying down, the CSF pressure as estimated by lumbar puncture is similar to the intracranial pressure.\n\nCSF can be tested for the diagnosis of a variety of neurological diseases, usually obtained by a procedure called lumbar puncture.\n\nLumbar puncture is carried out under sterile conditions by inserting a needle into the subarachnoid space, usually between the third and fourth lumbar vertebrae. CSF is extracted through the needle, and tested. Cells in the fluid are counted, as are the levels of protein and glucose. These parameters alone may be extremely beneficial in the diagnosis of subarachnoid hemorrhage and central nervous system infections (such as meningitis). Moreover, a CSF culture examination may yield the microorganism that has caused the infection. By using more sophisticated methods, such as the detection of the oligoclonal bands, an ongoing inflammatory condition (for example, multiple sclerosis) can be recognized. A beta-2 transferrin assay is highly specific and sensitive for the detection of CSF leakage.\n\nLumbar puncture can also be performed to measure the intracranial pressure, which might be increased in certain types of hydrocephalus. However, a lumbar puncture should never be performed if increased intracranial pressure is suspected due to certain situations such as a tumour, because it can lead to brain herniation and ultimately death.\n\nAbout one third of people experience a headache after lumbar puncture.\n\nThis fluid has an importance in anesthesia. Baricity refers to the density of a substance compared to the density of human cerebrospinal fluid. Baricity is used in general anesthesia to determine the manner in which a particular drug will spread in the intrathecal space.\n\nA 2010 study showed analysis of CSF for three protein biomarkers that can indicate the presence of Alzheimer's disease. The three biomarkers are CSF amyloid beta 1–42, total CSF tau protein and P-Tau. In the study, the biomarker test showed good sensitivity, identifying 90% of persons with Alzheimer's disease, but poor specificity, as 36% of control subjects were positive for the biomarkers. The researchers suggested the low specificity may be explained by developing but not yet symptomatic disease in controls.\n\nVarious comments by ancient physicians have been read as referring to CSF. Hippocrates discussed \"water\" surrounding the brain when describing congenital hydrocephalus, and Galen referred to \"excremental liquid\" in the ventricles of the brain, which he believed was purged into the nose. But for some 16 intervening centuries of ongoing anatomical study, CSF remains unmentioned in the literature. This is perhaps because of the prevailing autopsy technique, which involved cutting off the head, thereby removing evidence of the CSF before the brain was examined. The modern rediscovery of CSF is now credited to Emanuel Swedenborg. In a manuscript written between 1741 and 1744, unpublished in his lifetime, Swedenborg referred to CSF as \"spirituous lymph\" secreted from the roof of the fourth ventricle down to the medulla oblongata and spinal cord. This manuscript was eventually published in translation in 1887.\n\nAlbrecht von Haller, a Swiss physician and physiologist, made note in his 1747 book on physiology that the \"water\" in the brain was secreted into the ventricles and absorbed in the veins, and when secreted in excess, could lead to hydrocephalus.\n\nFrancois Magendie studied the properties of CSF by vivisection. He discovered the foramen Magendie, the opening in the roof of the fourth ventricle, but mistakenly believed that CSF was secreted by the pia mater.\n\nThomas Willis (noted as the discoverer of the circle of Willis) made note of the fact that the consistency of the CSF is altered in meningitis. In 1869 Gustav Schwalbe proposed that CSF drainage could occur via lymphatic vessels. \n\nIn 1891, W. Essex Wynter began treating tubercular meningitis by tapping the subarachnoid space, and Heinrich Quincke began to popularize lumbar puncture, which he advocated for both diagnostic and therapeutic purposes. In 19th and early 20th century literature, particularly German medical literature, \"liquor cerebrospinalis\" was a term used to refer to CSF.\n\nIn 1912, a neurologist William Mestrezat gave the first accurate description of the chemical composition of the CSF. In 1914, Harvey W. Cushing published conclusive evidence that the CSF is secreted by the choroid plexus.\n\n\n", "id": "7632", "title": "Cerebrospinal fluid"}
{"url": "https://en.wikipedia.org/wiki?curid=7633", "text": "Cordial\n\nCordial may refer to:\n\nFood and drink:\n\nOther uses:\n", "id": "7633", "title": "Cordial"}
{"url": "https://en.wikipedia.org/wiki?curid=7635", "text": "Charles F. Hockett\n\nCharles Francis Hockett (January 17, 1916 – November 3, 2000) was an American linguist who developed many influential ideas in American structuralist linguistics. He represents the post-Bloomfieldian phase of structuralism often referred to as \"distributionalism\" or \"taxonomic structuralism\". His academic career spanned over half a century at Cornell and Rice universities.\n\nAt the age of 16, Hockett enrolled at Ohio State University in Columbus, Ohio where he received a Bachelor of Arts and Master of Arts in ancient history. While enrolled at Ohio State, Hockett became interested in the work of Leonard Bloomfield, a leading figure in the field of structural linguistics. Hockett continued his education at Yale University where he studied anthropology and linguistics and received his PhD in anthropology in 1939. While studying at Yale, Hockett studied with several other influential linguists such as Edward Sapir, George P. Murdock, and Benjamin Whorf. Hockett's dissertation was based on his fieldwork in Potawatomi; his paper on Potawatomi syntax was published in \"Language\" in 1939. In 1948 his dissertation was published as a series in the International Journal of American Linguistics. Following fieldwork in Kickapoo and Michoacán, Mexico, Hockett did two years of postdoctoral study with Leonard Bloomfield in Chicago and Michigan.\n\nHockett began his teaching career in 1946 as an assistant professor of linguistics in the Division of Modern Languages at Cornell University where he was responsible for directing the Chinese language program. In 1957, Hockett became a member of Cornell's anthropology department and continued to teach anthropology and linguistics until he retired to emeritus status in 1982. In 1986, he took up an adjunct post at Rice University in Houston, Texas, where he remained active until his death in 2000.\n\nCharles Hockett held membership among many academic institutions such as the National Academy of Sciences the American Academy of Arts and Sciences, and the Society of Fellows at Harvard University. He served as president of both the Linguistic Society of America and the Linguistic Association of Canada and the United States.\n\nIn addition to making many contributions to the field of structural linguistics, Hockett also considered such things as Whorfian Theory, jokes, the nature of writing systems, slips of the tongue, and animal communication and their relativeness to speech.\n\nOutside the realm of linguistics and anthropology, Hockett practiced musical performance and composition. Hockett composed a full-length opera called \"The Love of Doña Rosita\" which was based on a play by Federico García Lorca and premiered at Ithaca College by the Ithaca Opera.\n\nHockett and his wife Shirley were vital leaders in the development of the Cayuga Chamber Orchestra in Ithaca, New York. In appreciation of the Hocketts' hard work and dedication to the Ithaca community, Ithaca College established the Charles F. Hockett Music Scholarship, the Shirley and Chas Hockett Chamber Music Concert Series, and the Hockett Family Recital Hall.\n\nIn his \"Note on Structure\" he argues that linguistics can be seen as a game and as a science. A linguist as player has a freedom for experimentation on all the utterances of a language, but no criterion to compare his analysis with other linguists. Late in his career, he was known for his stinging criticism of Chomskyan linguistics.\n\nAfter carefully examining the generative school's proposed innovations in Linguistics, Hockett decided that this approach was of little value. His book \"The State of the Art\" outlined his criticisms of the generative approach. In his paraphrase a key principle of the Chomskyean paradgim is that there are an infinite number of grammatical sentences in any particular language. \nThe grammar of a language is a finite system that characterizes an infinite set of (well-formed) sentences. More specifically, the grammar of a language is a \"well-defined system\"by definition not more powerful than a universal Turing machine (and, in fact, surely a great deal weaker.\n\nThe crux of Hockett's rebuttal is that the set of grammatical sentences in a language is not infinite, but rather ill-defined. Hockett proposes that \"no physical system is well-defined\".\n\nLater in \"Where the tongue slips, there slip I\" he writes as follows. \n\nIt is currently fashionable to assume that, underlying the actual more or less bumbling speech behavior of any human being, there is a subtle and complicated but determinate linguistic \"competence\": a sentence-generating device whose design can only be roughly guessed at by any techniques so far available to us. This point of view makes linguistics very hard and very erudite, so that anyone who actually does discover facts about underlying \"competence\" is entitled to considerable kudos.\n\nWithin this popular frame of reference, a theory of \"performance\" -- of the \"generation of speech\" -- must take more or less the following form. If a sentence is to be uttered aloud, or even thought silently to oneself, it must first be built by the internal \"competence\" of the speaker, the functioning of which is by definition such that the sentence will be legal (\"grammatical\") in every respect. But that is not enough; the sentence as thus constructed must then be \"performed\", either overtly so that others may hear it, or covertly so that it is perceived only bh the speaker himself. It is in this second step that blunders may appear. That which is generated by the speaker's internal \"competence\"is what the speaker \"intends to say,\" and is the only real concern of linguistics: blunders in actually performed speech are instructions from elsewhere. Just if there are no such intrusions is what is performed an instance of \"smooth speech\".\n\nI believe this view is unmitigated nonsense, unsupported by any empirical evidence of any sort. In its place, I propose the following.\n\n\"All\" speech, smooth as well as blunderful, can be and must be accounted for essentially in terms of the three mechanisms we have listed: analogy, blending, and editing. An individual's language, at a given moment, is a set of habits--that is, of analogies, where different analogies are in conflict, one may appear as a constraint on the working of another. Speech actualizes habits--and changes the habits as it does so. Speech reflects awareness of norms; but norms are themselves entirely a matter of analogy (that is, of habit), not some different kind of thing. \n\nDespite his criticisms, Hockett always expressed gratitude to the generative school for seeing real problems in the preexisting approaches. \n\nThere are many situations in which bracketing does not serve to disambiguate. As already noted, words that belong together cannot always be spoken togheter, and when they are not, bracketing is difficult or impossible. In the 1950s this drove some grammarians to drink and other to transformations, but both are only anodynes, not answers\n\nOne of Hockett’s most important contributions was his development of the design-feature approach to comparative linguistics. He attempted to distinguish the similarities and differences among animal communication systems and human language.\n\nHockett initially developed seven features, which were published in the 1959 paper “Animal ‘Languages’ and Human Language.” However, after many revisions, he settled on 13 design-features in the \"Scientific American \" \"The Origin of Speech.\"\n\nHockett argued that while every communication system has some of the 13 design features, only human, spoken language has all 13 features. In turn, that differentiates human spoken language from animal communication and other human communication systems such as written language.\n\n\nWhile Hockett believed that all communication systems, animal and human alike, share many of these features, only human language contains all 13 design features. Additionally, traditional transmission, and duality of patterning are key to human language.\n\n\n\nForaging honey bees communicate with other members of their hive when they have discovered a relevant source of pollen, nectar, or water. In an effort to convey information about the location and the distance of such resources, honeybees participate in a particular figure-eight dance known as the waggle dance.\n\nIn Hockett's \"The Origin of Speech\", he determined that the honeybee communication system of the waggle dance holds the following design features:\n\n\nGibbons are small apes in the family Hylobatidae. While they share the same kingdom, phylum, class, and order of humans and are relatively close to man, Hockett distinguishes between the gibbon communication system and human language by noting that gibbons are devoid of the last four design features.\n\nGibbons possess the first nine design features, but do not possess the last four (displacement, productivity, traditional transmission, and duality of patterning).\n\n\nIn a report published in 1968 with anthropologist and scientist Stuart A. Altmann, Hockett derived three more Design Features, bringing the total to 16. These are the additional three:\n\n\nCognitive scientist and linguist at the University of Sussex Larry Trask offered an alternative term and definition for number 14, Prevarication:\n\nThere has since been one more Feature added to the list, by Dr. William Taft Stuart, a director of the Undergraduate Studies program at the University of Maryland: College Park’s Anthropology school, part of the College of Behavioral and Social Sciences. His “extra” Feature is:\n\nThis follows the definition of Grammar and Syntax, as given by Merriam-Webster’s Dictionary:\n\nAdditionally, Dr. Stuart defends his postulation with references to famous linguist Noam Chomsky and University of New York psychologist Gary Marcus. Chomsky theorized that humans are unique in the animal world because of their ability to utilize Design Feature 5: Total Feedback, or recursive grammar. This includes being able to correct oneself and insert explanatory or even non sequitur statements into a sentence, without breaking stride, and keeping proper grammar throughout.\n\nWhile there have been studies attempting to disprove Chomsky, Marcus states that, \"An intriguing possibility is that the capacity to recognize recursion might be found only in species that can acquire new patterns of vocalization, for example, songbirds, humans and perhaps some cetaceans.\" This is in response to a study performed by psychologist Timothy Gentner of the University of California at San Diego. Gentner’s study found that starling songbirds use recursive grammar to identify “odd” statements within a given “song.” However, the study does not necessarily debunk Chomsky’s observation because it has not yet been proven that songbirds have the semantic ability to generalize from patterns.\n\nThere is also thought that symbolic thought is necessary for grammar-based speech, and thus Homo Erectus and all preceding “humans” would have been unable to comprehend modern speech. Rather, their utterances would have been halting and even quite confusing to us, \ntoday.\n\nThe University of Oxford: Phonetics Laboratory Faculty of Linguistics, Philology and Phonetics published the following chart, detailing how Hockett's (and Altmann's) Design Features fit into other forms of communication, in animals:\n\n\n\n", "id": "7635", "title": "Charles F. Hockett"}
{"url": "https://en.wikipedia.org/wiki?curid=7638", "text": "Consilience\n\nIn science and history, consilience (also convergence of evidence or concordance of evidence) refers to the principle that evidence from independent, unrelated sources can \"converge\" to strong conclusions. That is, when multiple sources of evidence are in agreement, the conclusion can be very strong even when none of the individual sources of evidence is significantly so on its own. Most established scientific knowledge is supported by a convergence of evidence: if not, the evidence is comparatively weak, and there will not likely be a strong scientific consensus.\n\nThe principle is based on the unity of knowledge; measuring the same result by several different methods should lead to the same answer. For example, it should not matter whether one measures the distance between the Great Pyramids of Giza by laser rangefinding, by satellite imaging, or with a meter stick - in all three cases, the answer should be approximately the same. For the same reason, different dating methods in geochronology should concur, a result in chemistry should not contradict a result in geology, etc.\n\nThe word \"consilience\" was originally coined as the phrase \"consilience of inductions\" by William Whewell (\"consilience\" refers to a \"jumping together\" of knowledge). The word comes from Latin \"com-\" \"together\" and \"-siliens\" \"jumping\" (as in resilience).\n\nConsilience requires the use of independent methods of measurement, meaning that the methods have few shared characteristics. That is, the mechanism by which the measurement is made is different; each method is dependent on an unrelated natural phenomenon. For example, the accuracy of laser rangefinding measurements is based on the scientific understanding of lasers, while satellite pictures and meter sticks rely on different phenomena. Because the methods are independent, when one of several methods is in error, it is very unlikely to be in error in the \"same way\" as any of the other methods, and a difference between the measurements will be observed. If the scientific understanding of the properties of lasers were inaccurate, then the laser measurement would be inaccurate but the others would not.\n\nAs a result, when several different methods agree, this is strong evidence that \"none\" of the methods are in error and the conclusion is correct. This is because of a greatly reduced likelihood of errors: for a consensus estimate from multiple measurements to be wrong, the errors would have to be similar for all samples and all methods of measurement, which is extremely unlikely. Random errors will tend to cancel out as more measurements are made, due to regression to the mean; systematic errors will be detected by differences between the measurements (and will also tend to cancel out since the direction of the error will still be random). This is how scientific theories reach high confidence – over time, they build up a large degree of evidence which converges on the same conclusion.\n\nWhen results from different strong methods do appear to conflict, this is treated as a serious problem to be reconciled. For example, in the 19th century, the Sun appeared to be no more than 20 million years old, but the Earth appeared to be no less than 300 million years (resolved by the discovery of nuclear fusion and radioactivity, and the theory of quantum mechanics); or current attempts to resolve theoretical differences between quantum mechanics and general relativity.\n\nBecause of consilience, the strength of evidence for any particular conclusion is related to how many independent methods are supporting the conclusion, as well as how different these methods are. Those techniques with the fewest (or no) shared characteristics provide the strongest consilience and result in the strongest conclusions. This also means that confidence is usually strongest when considering evidence from different fields, because the techniques are usually very different.\n\nFor example, the theory of evolution is supported by a convergence of evidence from genetics, molecular biology, paleontology, geology, biogeography, comparative anatomy, comparative physiology, and many other fields. In fact, the evidence within each of these fields is itself a convergence providing evidence for the theory. (As a result, to disprove evolution, most or all of these independent lines of evidence would have to be found to be in error.) The strength of the evidence, considered together as a whole, results in the strong scientific consensus that the theory is correct. In a similar way, evidence about the history of the universe is drawn from astronomy, astrophysics, planetary geology, and physics.\n\nFinding similar conclusions from multiple independent methods is also evidence for the reliability of the methods themselves, because consilience eliminates the possibility of all potential errors that do not affect all the methods equally. This is also used for the validation of new techniques through comparison with the consilient ones. If only partial consilience is observed, this allows for the detection of errors in methodology; any weaknesses in one technique can be compensated for by the strengths of the others. Alternatively, if using more than one or two techniques for every experiment is infeasible, some of the benefits of consilience may still be obtained if it is well-established that these techniques usually give the same result.\n\nConsilience is important across all of science, including the social sciences, and is often used as an argument for scientific realism by philosophers of science. Each branch of science studies a subset of reality that depends on factors studied in other branches. Atomic physics underlies the workings of chemistry, which studies emergent properties that in turn are the basis of biology. Psychology is not separate from the study of properties emergent from the interaction of neurons and synapses. Sociology, economics, and anthropology are each, in turn, studies of properties emergent from the interaction of countless individual humans. The concept that all the different areas of research are studying one real, existing universe is an apparent explanation of why scientific knowledge determined in one field of inquiry has often helped in understanding other fields.\n\nConsilience does not forbid deviations: in fact, since not all experiments are perfect, some deviations from established knowledge are expected. However, when the convergence is strong enough, then new evidence inconsistent with the previous conclusion is not usually enough to outweigh that convergence. Without an equally strong convergence on the new result, the weight of evidence will still favor the established result. This means that the new evidence is most likely to be wrong.\n\nScience denialism (for example, AIDS denialism) is often based on a misunderstanding of this property of consilience. A denier may promote small gaps not yet accounted for by the consilient evidence, or small amounts of evidence contradicting a conclusion without accounting for the pre-existing strength resulting from consilience. More generally, to insist that all evidence converge precisely with no deviations would be naïve falsificationism, equivalent to considering a single contrary result to falsify a theory when another explanation, such as equipment malfunction or misinterpretation of results, is much more likely.\n\nHistorical evidence also converges in an analogous way. For example: if five ancient historians, none of whom knew each other, all claim that Julius Caesar seized power in Rome in 49 BCE, this is strong evidence in favor of that event occurring even if each individual historian is only partially reliable. By contrast, if the same historian had made the same claim five times in five different places (and no other types of evidence were available), the claim is much weaker because it originates from a single source. The evidence from the ancient historians could also converge with evidence from other fields, such as archaeology: for example, evidence that many senators fled Rome at the time, that the battles of Caesar’s civil war occurred, and so forth.\n\nConsilience has also been discussed in reference to Holocaust denial. \nThat is, individually the evidence may underdetermine the conclusion, but together they overdetermine it. A similar way to state this is that to ask for one \"particular\" piece of evidence in favor of a conclusion is a flawed question.\n\nIn addition to the sciences, consilience can be important to the arts, ethics and religion. Both artists and scientists have identified the importance of biology in the process of artistic innovation.\n\nConsilience has its roots in the ancient Greek concept of an intrinsic orderliness that governs our cosmos, inherently comprehensible by logical process, a vision at odds with mystical views in many cultures that surrounded the Hellenes. The rational view was recovered during the high Middle Ages, separated from theology during the Renaissance and found its apogee in the Age of Enlightenment.\n\nWhewell’s definition was that:\nMore recent descriptions include:\n\nAlthough the concept of consilience in Whewell's sense was widely discussed by philosophers of science, the term was unfamiliar to the broader public until the end of the 20th century, when it was revived in \"Consilience: The Unity of Knowledge,\" a 1998 book by the humanist biologist Edward Osborne Wilson, as an attempt to bridge the culture gap between the sciences and the humanities that was the subject of C. P. Snow's \"The Two Cultures and the Scientific Revolution\" (1959).\n\nWilson held that with the rise of the modern sciences, the sense of unity gradually was lost in the increasing fragmentation and specialization of knowledge in the last two centuries. He asserted that the sciences, humanities, and arts have a common goal: to give a purpose to understanding the details, to lend to all inquirers \"a conviction, far deeper than a mere working proposition, that the world is orderly and can be explained by a small number of natural laws.\" Wilson's concept is a much broader notion of consilience than that of Whewell, who was merely pointing out that generalizations invented to account for one set of phenomena often account for others as well.\n\nA parallel view lies in the term universology, which literally means \"the science of the universe.\" Universology was first promoted for the study of the interconnecting principles and truths of all domains of knowledge by Stephen Pearl Andrews, a 19th-century utopian futurist and anarchist.\n\n\n", "id": "7638", "title": "Consilience"}
{"url": "https://en.wikipedia.org/wiki?curid=7642", "text": "Clarence Brown\n\nClarence Leon Brown (May 10, 1890 – August 17, 1987) was an American film director.\n\nBorn in Clinton, Massachusetts, the son of Larkin Harry Brown (1866-1942) a cotton manufacturer and Katherine Ann Brown (nee Gaw) (1865-1954), Brown moved to Tennessee when he was 11 years old. He attended Knoxville High School and the University of Tennessee, both in Knoxville, Tennessee, graduating from the university at the age of 19 with two degrees in engineering. An early fascination in automobiles led Brown to a job with the Stevens-Duryea Company, then to his own Brown Motor Car Company in Alabama. He later abandoned the car dealership after developing an interest in motion pictures around 1913. He was hired by the Peerless Studio at Fort Lee, New Jersey, and became an assistant to the French-born director Maurice Tourneur.\n\nAfter serving in World War I, Brown was given his first co-directing credit (with Tourneur) for \"The Great Redeemer\" (1920). Later that year, he directed a major portion of \"The Last of the Mohicans\" after Tourneur was injured in a fall.\n\nBrown moved to Universal in 1924, and then to MGM, where he stayed until the mid-1950s. At MGM he was one of the main directors of their female stars; he directed Joan Crawford six times and Greta Garbo seven.\n\nHe was nominated five times (see below) for the Academy Award as a director and once as a producer, but he never received an Oscar. However, he won Best Foreign Film for \"Anna Karenina\", starring Garbo at the 1935 Venice International Film Festival.\n\nBrown's films gained a total of 38 Academy Award nominations and earned nine Oscars. Brown himself received six Academy Award nominations and in 1949, he won the British Academy Award for the film version of William Faulkner's \"Intruder in the Dust.\n\nIn 1957, Brown was awarded The George Eastman Award, given by George Eastman House for distinguished contribution to the art of film. Brown retired a wealthy man due to his real estate investments, but refused to watch new movies, as he feared they might cause him to restart his career.\nThe Clarence Brown Theater, on the campus of the University of Tennessee, is named in his honor. He is tied with Robert Altman and Alfred Hitchcock for the most Academy Award nominations for best director without a single win.\n\nBrown died from kidney failure on August 17, 1987, at the age of 97. He is interred at Forest Lawn Memorial Park in Glendale, California. \n\nOn February 8, 1960, Brown received a star on the Hollywood Walk of Fame at 1752 Vine Street, for his contributions to the motion pictures industry\n\n\nNOTE: In 1929/1930, Brown received one Academy Award nomination for two films. According to the Academy of Motion Picture Arts and Sciences, \"As allowed by the award rules for this year, a single nomination could honor work in one or more films.\"\n\n\n", "id": "7642", "title": "Clarence Brown"}
{"url": "https://en.wikipedia.org/wiki?curid=7643", "text": "Conciliation\n\nConciliation is an alternative dispute resolution (ADR) process whereby the parties to a dispute use a conciliator, who meets with the parties both separately and together in an attempt to resolve their differences. They do this by lowering tensions, improving communications, interpreting issues, encouraging parties to explore potential solutions and assisting parties in finding a mutually acceptable outcome.\n\nConciliation differs from arbitration in that the conciliation process, in and of itself, has no legal standing, and the conciliator usually has no authority to seek evidence or call witnesses, usually writes no decision, and makes no award.\n\nConciliation differs from mediation in that in conciliation, often the parties are in need of restoring or repairing a relationship, either personal or business.\n\nA conciliator assists each of the parties to independently develop a list of all of their objectives (the outcomes which they desire to obtain from the conciliation). The conciliator then has each of the parties separately prioritize their own list from most to least important. He/She then goes back and forth between the parties and encourages them to \"give\" on the objectives one at a time, starting with the least important and working toward the most important for each party in turn. The parties rarely place the same priorities on all objectives, and usually have some objectives that are not listed by the other party. Thus the conciliator can quickly build a string of successes and help the parties create an atmosphere of trust which the conciliator can continue to develop.\n\nMost successful conciliators are highly skilled negotiators. Some conciliators operate under the auspices of any one of several non-governmental entities, and for governmental agencies such as the Federal Mediation and Conciliation Service in the United States.\n\nHistorical conciliation is an applied conflict resolution approach that utilizes historical narratives to positively transform relations between societies in conflicts. Historical conciliation can utilize many different methodologies, including mediation, sustained dialogue, apologies, acknowledgement, support of public commemoration activities, and public diplomacy.\n\nHistorical conciliation is not an excavation of objective facts. The point of facilitating historical questions is not to discover all the facts in regard to who was right or wrong. Rather, the objective is to discover the complexity, ambiguity, and emotions surrounding both dominant and non-dominant cultural and individual narratives of history. It is also not a rewriting of history. The goal is not to create a combined narrative that everyone agrees upon. Instead, the aim is to create room for critical thinking and more inclusive understanding of the past and conceptions of “the other.”\n\nConflicts that are addressed through historical conciliation have their roots in conflicting identities of the people involved. Whether the identity at stake is their ethnicity, religion or culture, it requires a comprehensive approach that takes people’s needs, hopes, fears, and concerns into account.\n\nJapanese law makes extensive use of in civil disputes. The most common forms are civil conciliation and domestic conciliation, both of which are managed under the auspice of the court system by one judge and two non-judge \"conciliators.\"\n\nCivil conciliation is a form of dispute resolution for small lawsuits, and provides a simpler and cheaper alternative to litigation. Depending on the nature of the case, non-judge experts (doctors, appraisers, actuaries, and so on) may be called by the court as conciliators to help decide the case.\n\nDomestic conciliation is most commonly used to handle contentious divorces, but may apply to other domestic disputes such as the annulment of a marriage or acknowledgment of paternity. Parties in such cases are required to undergo conciliation proceedings and may only bring their case to court once conciliation has failed.\n", "id": "7643", "title": "Conciliation"}
{"url": "https://en.wikipedia.org/wiki?curid=7645", "text": "Cyclone (programming language)\n\nThe Cyclone programming language is intended to be a safe dialect of the C language. Cyclone is designed to avoid buffer overflows and other vulnerabilities that are endemic in C programs, without losing the power and convenience of C as a tool for system programming.\n\nCyclone development was started as a joint project of AT&T Labs Research and Greg Morrisett's group at Cornell in 2001. Version 1.0 was released on May 8, 2006.\n\nCyclone attempts to avoid some of the common pitfalls of C, while still maintaining its look and performance. To this end, Cyclone places the following limits on programs:\n\nTo maintain the tool set that C programmers are used to, Cyclone provides the following extensions:\n\nFor a better high-level introduction to Cyclone, the reasoning behind Cyclone and the source of these lists, see this paper.\n\nCyclone looks, in general, much like C, but it should be viewed as a C-like language.\n\nCyclone implements three kinds of pointer:\nThe purpose of introducing these new pointer types is to avoid common problems when using pointers. Take for instance a function, called codice_17 that takes a pointer to an int:\n\nAlthough the person who wrote the function codice_17 could have inserted codice_1 checks, let us assume that for performance reasons they did not. Calling codice_20 will result in undefined behavior (typically, although not necessarily, a SIGSEGV signal being sent to the application). To avoid such problems, Cyclone introduces the codice_14 pointer type, which can never be codice_1. Thus, the \"safe\" version of codice_17 would be:\n\nThis tells the Cyclone compiler that the argument to codice_17 should never be codice_1, avoiding the aforementioned undefined behavior. The simple change of codice_13 to codice_14 saves the programmer from having to write codice_1 checks and the operating system from having to trap codice_1 pointer dereferences. This extra limit, however, can be a rather large stumbling block for most C programmers, who are used to being able to manipulate their pointers directly with arithmetic. Although this is desirable, it can lead to buffer overflows and other \"off-by-one\"-style mistakes. To avoid this, the codice_16 pointer type is delimited by a known bound, the size of the array. Although this adds overhead due to the extra information stored about the pointer, it improves safety and security. Take for instance a simple (and naïve) codice_31 function, written in C:\n\nThis function assumes that the string being passed in is terminated by NULL (codice_32). However, what would happen if codice_33 were passed to this string? This is perfectly legal in C, yet would cause codice_31 to iterate through memory not necessarily associated with the string codice_35. There are functions, such as codice_36 which can be used to avoid such problems, but these functions are not standard with every implementation of ANSI C. The Cyclone version of codice_31 is not so different from the C version:\n\nHere, codice_31 bounds itself by the length of the array passed to it, thus not going over the actual length. Each of the kinds of pointer type can be safely cast to each of the others, and arrays and strings are automatically cast to codice_16 by the compiler. (Casting from codice_16 to codice_13 invokes a bounds check, and casting from codice_16 to codice_14 invokes both a codice_1 check and a bounds check. Casting from codice_13 or codice_16 results in no checks whatsoever; the resulting codice_16 pointer has a size of 1.)\n\nConsider the following code, in C:\n\nThis returns an object that is allocated on the stack of the function codice_48, which is not available after the function returns. While gcc and other compilers will warn about such code, the following will typically compile without warnings:\n\nCyclone does regional analysis of each segment of code, preventing dangling pointers, such as the one returned from this version of codice_48. All of the local variables in a given scope are considered to be part of the same region, separate from the heap or any other local region. Thus, when analyzing codice_48, the compiler would see that codice_51 is a pointer into the local stack, and would report an error.\n\n\n\n\nPresentations:\n", "id": "7645", "title": "Cyclone (programming language)"}
{"url": "https://en.wikipedia.org/wiki?curid=7646", "text": "Cognitivism\n\nCognitivism may refer to:\n\n", "id": "7646", "title": "Cognitivism"}
{"url": "https://en.wikipedia.org/wiki?curid=7647", "text": "Counter (digital)\n\nA counter circuit is usually constructed of a number of flip-flops connected in cascade. Counters are a very widely used component in digital circuits, and are manufactured as separate integrated circuits and also incorporated as parts of larger integrated circuits.\n\nIn electronics, counters can be implemented quite easily using register-type circuits such as the flip-flop, and a wide variety of classifications exist:\n\nEach is useful for different applications. Usually, counter circuits are digital in nature, and count in natural binary. Many types of counter circuits are available as digital building blocks, for example a number of chips in the 4000 series implement different counters.\n\nOccasionally there are advantages to using a counting sequence other than the natural binary sequence—such as the binary coded decimal counter, a linear feedback shift register counter, or a Gray-code counter.\n\nCounters are useful for digital clocks and timers, and in oven timers, VCR clocks, etc.\n\nAn asynchronous (ripple) counter is a single d-type flip-flop, with its J (data) input fed from its own inverted output. This circuit can store one bit, and hence can count from zero to one before it overflows (starts over from 0). This counter will increment once for every clock cycle and takes two clock cycles to overflow, so every cycle it will alternate between a transition from 0 to 1 and a transition from 1 to 0. Notice that this creates a new clock with a 50% duty cycle at exactly half the frequency of the input clock. If this output is then used as the clock signal for a similarly arranged D flip-flop (remembering to invert the output to the input), one will get another 1 bit counter that counts half as fast. Putting them together yields a two-bit counter:\nYou can continue to add additional flip-flops, always inverting the output to its own input, and using the output from the previous flip-flop as the clock signal. The result is called a ripple counter, which can count to where \"n\" is the number of bits (flip-flop stages) in the counter. Ripple counters suffer from unstable outputs as the overflows \"ripple\" from stage to stage, but they do find frequent application as dividers for clock signals, where the instantaneous count is unimportant, but the division ratio overall is (to clarify this, a 1-bit counter is exactly equivalent to a divide by two circuit; the output frequency is exactly half that of the input when fed with a regular train of clock pulses).\n\nThe use of flip-flop outputs as clocks leads to timing skew between the count data bits, making this ripple technique incompatible with normal synchronous circuit design styles.\n\nIn synchronous counters, the clock inputs of all the flip-flops are connected together and are triggered by the input pulses. Thus, all the flip-flops change state simultaneously (in parallel). The circuit below is a 4-bit synchronous counter. The J and K inputs of FF0 are connected to HIGH. FF1 has its J and K inputs connected to the output of FF0, and the J and K inputs of FF2 are connected to the output of an AND gate that is fed by the outputs of FF0 and FF1.\nA simple way of implementing the logic for each bit of an ascending counter (which is what is depicted in the adjacent image) is for each bit to toggle when all of the less significant bits are at a logic high state. For example, bit 1 toggles when bit 0 is logic high; bit 2 toggles when both bit 1 and bit 0 are logic high; bit 3 toggles when bit 2, bit 1 and bit 0 are all high; and so on.\n\nSynchronous counters can also be implemented with hardware finite-state machines, which are more complex but allow for smoother, more stable transitions.\n\nA decade counter is one that counts in decimal digits, rather than binary. A decade counter may have each (that is, it may count in binary-coded decimal, as the 7490 integrated circuit did) or other binary encodings. \"A decade counter is a binary counter that is designed to count to 1010b (decimal 10). An ordinary four-stage counter can be easily modified to a decade counter by adding a NAND gate as in the schematic to the right. Notice that FF2 and FF4 provide the inputs to the NAND gate. The NAND gate outputs are connected to the CLR input of each of the FFs.\" \nA decade counter is one that counts in decimal digits, rather than binary. It counts from 0 to 9 and then resets to zero. The counter output can be set to zero by pulsing the reset line low. The count then increments on each clock pulse until it reaches 1001 (decimal 9). When it increments to 1010 (decimal 10) both inputs of the NAND gate go high. The result is that the NAND output goes low, and resets the counter to zero. D going low can be a CARRY OUT signal, indicating that there has been a count of ten.\n\nA ring counter is a circular shift register which is initiated such that only one of its flip-flops is the state one while others are in their zero states.\n\nA ring counter is a shift register (a cascade connection of flip-flops) with the output of the last one connected to the input of the first, that is, in a ring. Typically, a pattern consisting of a single bit is circulated so the state repeats every n clock cycles if n flip-flops are used.\n\nA Johnson counter (or switch-tail ring counter, twisted ring counter, walking ring counter, or Möbius counter) is a modified ring counter, where the output from the last stage is inverted and fed back as input to the first stage. The register cycles through a sequence of bit-patterns, whose length is equal to twice the length of the shift register, continuing indefinitely. These counters find specialist applications, including those similar to the decade counter, digital-to-analog conversion, etc. They can be implemented easily using D- or JK-type flip-flops.\n\nIn computability theory, a counter is considered a type of memory. A counter stores a single natural number (initially zero) and can be arbitrarily long. A counter is usually considered in conjunction with a finite-state machine (FSM), which can perform the following operations on the counter:\n\n\nThe following machines are listed in order of power, with each one being strictly more powerful than the one below it:\n\nFor the first and last, it doesn't matter whether the FSM is a deterministic finite automaton or a nondeterministic finite automaton. They have the same power. The first two and the last one are levels of the Chomsky hierarchy.\n\nThe first machine, an FSM plus two counters, is equivalent in power to a Turing machine. See the article on counter machines for a proof.\n\nA web counter or hit counter is a computer software program that indicates the number of visitors, or hits, a particular webpage has received. Once set up, these counters will be incremented by one every time the web page is accessed in a web browser.\n\nThe number is usually displayed as an inline digital image or in plain text or on a physical counter such as a mechanical counter. Images may be presented in a variety of fonts, or styles; the classic example is the wheels of an odometer.\n\n\"Web counter\" was popular in the 1980s and 1990s, later replaced by more detailed and complete web traffic measures.\n\nMany automation systems use PC and laptops to monitor different parameters of machines and production data. Counters may count parameters such as the number of pieces produced, the production batch number, and measurements of the amounts of material used.\n\nLong before electronics became common, mechanical devices were used to count events. These are known as tally counters. They typically consist of a series of disks mounted on an axle, with the digits 0 through 9 marked on their edge. The right most disk moves one increment with each event. Each disk except the left-most has a protrusion that, after the completion of one revolution, moves the next disk to the left one increment. Such counters were used as odometers for bicycles and cars and in tape recorders, fuel dispensers, in production machinery as well as in other machinery. One of the largest manufacturers was the Veeder-Root company, and their name was often used for this type of counter.\n\nHand held tally counters are used mainly for stocktaking and for counting people attending events.\n\nElectromechanical counters were used to accumulate totals in tabulating machines that pioneered the data processing industry.\n\n", "id": "7647", "title": "Counter (digital)"}
{"url": "https://en.wikipedia.org/wiki?curid=7649", "text": "Cervical mucus method\n\nCervical mucus method may refer to a specific method of fertility awareness or natural family planning:\n\n", "id": "7649", "title": "Cervical mucus method"}
{"url": "https://en.wikipedia.org/wiki?curid=7651", "text": "Coleridge\n\nColeridge may refer to:\n\n\n\n", "id": "7651", "title": "Coleridge"}
{"url": "https://en.wikipedia.org/wiki?curid=7655", "text": "Clay Mathematics Institute\n\nThe Clay Mathematics Institute (CMI) is a private, non-profit foundation, based in Peterborough, New Hampshire, United States. CMI's scientific activities are managed from the President's office in Oxford, United Kingdom. The institute is \"dedicated to increasing and disseminating mathematical knowledge.\" It gives out various awards and sponsorships to promising mathematicians. The institute was founded in 1998 through the sponsorship of Boston businessman Landon T. Clay. Harvard mathematician Arthur Jaffe was the first president of CMI. \n\nWhile the institute is best known for its Millennium Prize Problems, it carries out a wide range of activities, including a postdoctoral program (ten Clay Research Fellows are supported currently), conferences, workshops, and summer schools.\n\nThe institute is run according to a standard structure comprising a scientific advisory committee that decides on grant-awarding and research proposals, and a board of directors that oversees and approves the committee's decisions. , the board is made up of members of the Clay family, whereas the advisory committee is composed of leading authorities in mathematics, namely Sir Andrew Wiles, Michael Hopkins, Carlos Kenig, Andrei Okounkov, and Simon Donaldson. Nicholas Woodhouse is the current president of CMI.\n\nThe institute is best known for establishing the Millennium Prize Problems on May 24, 2000. These seven problems are considered by CMI to be \"important classic questions that have resisted solution over the years.\" For each problem, the first person to solve it will be awarded $1,000,000 by the CMI. In announcing the prize, CMI drew a parallel to Hilbert's problems, which were proposed in 1900, and had a substantial impact on 20th century mathematics. Of the initial 23 Hilbert problems, most of which have been solved, only the Riemann hypothesis (formulated in 1859) is included in the seven Millennium Prize Problems.\n\nFor each problem, the Institute had a professional mathematician write up an official statement of the problem, which will be the main standard by which a given solution will be measured against. The seven problems are:\n\n\nSome of the mathematicians who were involved in the selection and presentation of the seven problems were Atiyah, Bombieri, Connes, Deligne, Fefferman, Milnor, Mumford, Wiles, and Witten.\n\nIn recognition of major breakthroughs in mathematical research, the institute has an annual prize - the Clay Research Award. Its recipients to date are Ian Agol, Manindra Agrawal, Yves Benoist, Manjul Bhargava, Danny Calegari, Alain Connes, Nils Dencker, Alex Eskin, David Gabai, Ben Green, Larry Guth, Christopher Hacon, Richard S. Hamilton, Michael Harris, Jeremy Kahn, Nets Katz, Laurent Lafforgue, Gérard Laumon, Vladimir Markovic, James McKernan, Maryam Mirzakhani, Ngô Bảo Châu, Rahul Pandharipande, Jonathan Pila, Jean-François Quint, Peter Scholze, Oded Schramm, Stanislav Smirnov, Terence Tao, Clifford Taubes, Richard Taylor, Claire Voisin, Jean-Loup Waldspurger, Andrew Wiles, and Edward Witten.\n\nBesides the Millennium Prize Problems, the Clay Mathematics Institute supports mathematics via the awarding of research fellowships (which range from two to five years, and are aimed at younger mathematicians), as well as shorter-term scholarships for programs, individual research, and book writing. The institute also has a yearly Clay Research Award, recognizing major breakthroughs in mathematical research. Finally, the institute organizes a number of summer schools, conferences, workshops, public lectures, and outreach activities aimed primarily at junior mathematicians (from the high school to postdoctoral level). CMI publications are available in PDF form at most six months after they appear in print.\n\n\n", "id": "7655", "title": "Clay Mathematics Institute"}
{"url": "https://en.wikipedia.org/wiki?curid=7659", "text": "Cerebral arteriovenous malformation\n\nA cerebral arteriovenous malformation (cerebral AVM, CAVM, cAVM) is an abnormal connection between the arteries and veins in the brain—specifically, an arteriovenous malformation in the cerebrum.\n\nThe most frequently observed problems, related to an AVM, are headaches and seizures, backaches, neckaches and eventual nausea, as the coagulated blood makes its way down to be dissolved in the individual's spinal fluid. It is supposed that 15% of the population, at detection, have no symptoms at all. Other common symptoms are a pulsing noise in the head, progressive weakness and numbness and vision changes as well as debilitating, excruciating pain.\n\nIn serious cases, the blood vessels rupture and there is bleeding within the brain (intracranial hemorrhage). Nevertheless, in more than half of patients with AVM, hemorrhage is the first symptom. Symptoms due to bleeding include loss of consciousness, sudden and severe headache, nausea, vomiting, incontinence, and blurred vision, amongst others. Impairments caused by local brain tissue damage on the bleed site are also possible, including seizure, one-sided weakness (hemiparesis), a loss of touch sensation on one side of the body and deficits in language processing (aphasia). Ruptured AVMs are responsible for considerable mortality and morbidity.\n\nAVMs in certain critical locations may stop the circulation of the cerebrospinal fluid, causing accumulation of the fluid within the skull and giving rise to a clinical condition called hydrocephalus. A stiff neck can occur as the result of increased pressure within the skull and irritation of the meninges.\n\nAn AVM diagnosis is established by neuroimaging studies after a complete neurological and physical examination. Three main techniques are used to visualize the brain and search for AVM: computed tomography (CT), magnetic resonance imaging (MRI), and cerebral angiography. A CT scan of the head is usually performed first when the subject is symptomatic. It can suggest the approximate site of the bleed. MRI is more sensitive than CT in the diagnosis of AVMs and provides better information about the exact location of the malformation. More detailed pictures of the tangle of blood vessels that compose an AVM can be obtained by using radioactive agents injected into the blood stream. If a CT is used in conjunctiangiogram, this is called a computerized tomography angiogram; while, if MRI is used it is called magnetic resonance angiogram. The best images of an AVM are obtained through cerebral angiography. This procedure involves using a catheter, threaded through an artery up to the head, to deliver a contrast agent into the AVM. As the contrast agent flows through the AVM structure, a sequence of X-ray images are obtained.\n\nA common method of grading cerebral AVMs is the Spetzler-Martin (SM) grade. This system was designed to assess the patient's risk of neurological deficit after open surgical resection (surgical morbidity), based on characteristics of the AVM itself. Based on this system, AVMs may be classified as grades 1 - 5. This system was not intended to characterize risk of hemorrhage. \n\"Eloquent cortex\" is a name used by neurologists for areas of cortex that, if removed will result in loss of sensory processing or linguistic ability, minor paralysis, or paralysis.\n\nThe risk of post-surgical neurological deficit (difficulty with language, motor weakness, vision loss) increases with increasing Spetzler-Martin grade.\n\nA limitation of the Spetzler-Martin Grading system is that it does not include the following factors: Patient age, hemorrhage, diffuseness of nidus, and arterial supply. In 2010 a new supplemented Spetzler-Martin system (SM-supp, Lawton-Young) was devised adding these variables to the SM system. Under this new system AVMs are classified from grades 1 - 10. It has since been determined to have greater greater predictive accuracy that Spetzler-Martin grades alone. \nAVMs are an abnormal connection between the arteries and veins in the human brain. Arteriovenous malformations are most commonly of prenatal origin. The cause of AVMs remains unknown. In a normal brain oxygen enriched blood from the heart travels in sequence through smaller blood vessels going from arteries, to arterioles and then capillaries. Oxygen is removed in the latter vessel to be used by the brain. After the oxygen is removed blood reaches venules and later veins which will take it back to the heart and lungs. On the other hand, when there is an AVM blood goes directly from arteries to veins through the abnormal vessels disrupting the normal circulation of blood.\n\nThe main risk is intracranial hemorrhage. This risk is difficult to quantify since many patients with asymptomatic AVMs will never come to medical attention. Small AVMs tend to bleed more often than do larger ones, the opposite of cerebral aneurysms. If a rupture or bleeding incident occurs, the blood may penetrate either into the brain tissue (cerebral hemorrhage) or into the subarachnoid space, which is located between the sheaths (meninges) surrounding the brain (subarachnoid hemorrhage). Bleeding may also extend into the ventricular system (intraventricular hemorrhage). Cerebral hemorrhage appears to be most common.\n\nOne long-term study (mean follow up greater than 20 years) of over 150 symptomatic AVMs (either presenting with bleeding or seizures) found the risk of cerebral hemorrhage to be approximately 4% per year, slightly higher than the 2-3% seen in other studies. A simple, rough approximation of a patient's lifetime bleeding risk is 105 - (patient age in years), assuming a 3% bleed risk annually. For example, a healthy 30-year-old patient would have approximately a 75% lifetime risk of at least one bleeding event. Ruptured AVMs are a significant source or morbidity and mortality; post rupture, as many as 29% of patients will die, and only 55% will be able to live independently.\n\nTreatment depends on the location and size of the AVM and whether there is bleeding or not.\n\nThe treatment in the case of sudden bleeding is focused on restoration of vital function. Anticonvulsant medications such as phenytoin are often used to control seizure; medications or procedures may be employed to relieve intracranial pressure. Eventually, curative treatment may be required to prevent recurrent hemorrhage. However, any type of intervention may also carry a risk of creating a neurological deficit.\n\nPreventive treatment of as yet unruptured brain AVMs has been controversial, as several studies suggested favorable long-term outcome for unruptured AVM patients not undergoing intervention. The NIH-funded longitudinal ARUBA study (\"A Randomized trial of Unruptured Brain AVMs) compares the risk of stroke and death in patients with preventive AVM eradication versus those followed without intervention. Interim results suggest that fewer strokes occur as long as patients with unruptured AVM do not undergo intervention. Because of the higher than expected event rate in the interventional arm of the ARUBA study, NIH/NINDS stopped patient enrollment in April 2013, while continuing to follow all participants to determine whether the difference in stroke and death in the two arms changes over time.\n\nSurgical elimination of the blood vessels involved is the preferred curative treatment for many types of AVM. Surgery is performed by a neurosurgeon who temporarily removes part of the skull (craniotomy), separates the AVM from surrounding brain tissue, and resects the abnormal vessels. While surgery can result in an immediate, complete removal of the AVM, risks exist depending on the size and the location of the malformation. The AVM must be resected en bloc, for partial resection will likely cause severe hemorrhage. The preferred treatment of Spetzler-Martin grade 1 and 2 AVMs in young, healthy patients is surgical resection due to the relatively small risk of neurological damage compared to the high lifetime risk of hemorrhage. Grade 3 AVMs may or may not be amenable to surgery. Grade 4 and 5 AVMs are not usually surgically treated.\n\nRadiosurgery has been widely used on small AVMs with considerable success. The Gamma Knife is an apparatus used to precisely apply a controlled radiation dosage to the volume of the brain occupied by the AVM. While this treatment does not require an incision and craniotomy (with their own inherent risks), three or more years may pass before the complete effects are known, during which time patients are at risk of bleeding. Complete obliteration of the AVM may or may not occur after several years, and repeat treatment may be needed. Radiosurgery is itself not without risk. In one large study, nine percent of patients had transient neurological symptoms, including headache, after radiosurgery for AVM. However, most symptoms resolved, and the long-term rate of neurological symptoms was 3.8%.\n\nEmbolization is performed by interventional neuroradiologists and the occlusion of blood vessels most commonly is obtained with Ethylene-vinyl alcohol copolymer (Onyx) or N-butyl cyanoacrylate (NBCA). These substances are introduced by a radiographically guided catheter, and block vessels responsible for blood flow into the AVM. Embolization is frequently used as an adjunct to either surgery or radiation treatment. Embolization reduces the size of the AVM and during surgery it reduces the risk of bleeding. However, embolization alone may completely obliterate some AVMs. In high flow intranidal fistulas balloons can also be used to reduce the flow so that embolization can be done safely.\n\nThe annual new detection rate incidence of AVMs is approximately 1 per 100,000 a year. The point prevalence in adults is approximately 18 per 100,000. AVMs are more common in males than females, although in females pregnancy may start or worsen symptoms due the increase in blood flow and volume it usually brings. There is a significant preponderance (15-20%) of AVM in patients with hereditary hemorrhagic telangiectasia (Osler-Weber-Rendu syndrome).\n\nNo randomized, controlled clinical trial has established a survival benefit for treating patients (either with open surgery or radiosurgery) with AVMs that have not yet bled.\n\n", "id": "7659", "title": "Cerebral arteriovenous malformation"}
