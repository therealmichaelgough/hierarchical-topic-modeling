{"url": "https://en.wikipedia.org/wiki?curid=5644", "text": "Comedy film\n\nComedy is a genre of film in which the main emphasis is on humor. These films are designed to make the audience laugh through amusement and most often work by exaggerating characteristics for humorous effect. Films in this style traditionally have a happy ending (black comedy being an exception). One of the oldest genres in film, some of the very first silent movies were comedies, as slapstick comedy often relies on visual depictions, without requiring sound. When sound films became more prevalent during the 1920s, comedy films took another swing, as laughter could result from burlesque situations but also dialogue.\n\nComedy, compared with other film genres, puts much more focus on individual stars, with many former stand-up comics transitioning to the film industry due to their popularity. While many comic films are lighthearted stories with no intent other than to amuse, others contain political or social commentary (such as \"Wag the Dog\" and \"Man of the Year\"). \n\nA comedy of manners satirizes the manners and affectations of a social class, often represented by stock characters. Also, satirical comedy-drama & the plot is often concerned with an illicit love affair or some other scandal. However, the plot is generally less important for its comedic effect than its witty dialogue. This form of comedy has a long ancestry, dating back at least as far as \"Much Ado about Nothing\" created by William Shakespeare.\n\nSlapstick films involve exaggerated, boisterous action to create impossible and humorous situations. Because it relies predominately on visual depictions of events, it does not require sound. Accordingly, the subgenre was ideal for silent movies and was prevalent during that era. Popular silent stars of the slapstick genre include Buster Keaton, Charlie Chaplin, Roscoe Arbuckle, and Harold Lloyd. Some of these stars, as well as acts such as Laurel and Hardy and The Three Stooges, also found success incorporating slapstick comedy into sound films.\n\nIn a fish out of water comedy, the main character or character finds himself in an unusual environment, which drives most of the humour. Situations can be neo noir crime comedy, satirical comedy-drama & black comedy as sometimes as fantasy comedy behinds swapping gender roles, as in \"Tootsie\" (1982); an age changing role, as in \"Big\" (1988); a freedom-loving individual fitting into a structured environment, as in \"Police Academy\" (1984); a rural backwoodsman in the big city, as in \"\"Crocodile\" Dundee\", and so forth. The Coen Brothers are known for using this technique in all of their films, though not always to comic effect. Some films including people fitting the \"fish-out-of-water\" bill include \"The Big Lebowski\" (1998) and \"A Serious Man\" (2009).\n\nA parody or spoof film is a comedy that satirizes other film genres or classic films. Such films mockumentary, employ sarcasm, stereotyping, mockery of scenes from other films, and the obviousness of meaning in a character's actions. Examples of this form include \"Mud and Sand\" (1922), \"Blazing Saddles\" (1974), \"Airplane!\" (1980), \"Young Frankenstein\" (1974), and \"Scary Movie (2000).\n\nThe anarchic comedy film, as its name suggests, is a random or stream-of-consciousness type of humour which often lampoons a form of authority. The genre dates from the silent era, and the most famous examples of this type of film would be those produced by Monty Python. Others include \"Duck Soup\" (1933) and \"National Lampoon's Animal House\" (1978).\n\nThe black comedy film deals with normally taboo subjects, including death, murder, crime, suicide and war, in a satirical manner. Examples include \"Arsenic and Old Lace\" (1944), \"Monsieur Verdoux\" (1947), \"Kind Hearts and Coronets\" (1949), \"The Ladykillers\" (1955), \"\" (1964), \"The Loved One\" (1965), \"MASH\" (1970), \"Monty Python's The Meaning of Life\" (1983), \"Brazil\" (1985), \"The War of the Roses\" (1989), \"Heathers\" (1989), \"Your Friends & Neighbors\" (1998), \"Keeping Mum\" (2005), and \"Burn After Reading\" (2008).\n\nGross out films are a relatively recent development and rely heavily on vulgar, sexual or \"toilet\" humour. Examples include \"Porky's\" (1982), \"Dumb and Dumber\" (1994), \"There's Something About Mary\" (1998), and \"American Pie\" (1999).\n\nIt was not uncommon for the early romantic comedy film to also be a screwball comedy film. This form of comedy film was particularly popular during the 1930s and 1940s. There is no consensus definition of this film style, and it is often loosely applied to slapstick or romantic comedy films. Typically it can include a romantic element, an interplay between people of different economic strata, quick and witty repartee, some form of role reversal, and a happy ending. Some examples of the screwball comedy are: \"It Happened One Night\" (1934), \"Bringing Up Baby\" (1938), \"Philadelphia Story\" (1940), \"His Girl Friday\" (1940), and more recently \"What's Up, Doc?\" (1972).\n\nFilms in this subgenre blend comic antics and action where the film stars combine wit and one-liners with a thrilling plot and daring stunts. The genre became a specific draw in North America in the eighties when comedians such as Eddie Murphy started taking more action oriented roles such as in \"48 Hrs.\" and \"Beverly Hills Cop\". These type of films are often buddy films, with mismatched partners such as in \"Midnight Run\", \"Rush Hour\", \"21 Jump Street\", \"Bad Boys\", \"Starsky and Hutch\", and \"Hot Fuzz\". Slapstick martial arts films became a mainstay of Hong Kong action cinema through the work of Jackie Chan among others. It may also focus on superheroes such as \"The Incredibles\", \"Hancock\", \"Kick-Ass\", and \"Mystery Men.\n\nComedy horror is a type of film in which the usual dark themes and \"scare tactics\" attributed to horror films are treated with a humorous approach. These films either use goofy horror clichés such as in \"Scream\", \"Young Frankenstein\", \"Little Shop of Horrors\", \"Haunted Mansion\", and \"Scary Movie\" where campy styles are favoured. Some are much more subtle and don't parody horror, such as \"An American Werewolf In London\". Another style of comedy horror can also rely on over the top violence and gore such as in \"Dead Alive\" (1992), \"The Evil Dead\" (1981), \"The Toxic Avenger\" (1984), and \"Club Dread\" - such films are sometimes known as \"splatstick\", a portmanteau of the words \"splatter\" and \"slapstick\". It would be reasonable to put \"Ghostbusters\" in this category.\n\nA genre that combines elements of comedy and thrillers, a combination of humor and suspense or action. Films such as \"Silver Streak\", \"Charade\", \"Kiss Kiss Bang Bang\", \"In Bruges\", \"Mr. and Mrs. Smith\", \"Grosse Point Blank\", \"The Thin Man\", \"The Big Fix\", and \"The Lady Vanishes\".\n\nFantasy comedy films are types of films that uses magic, supernatural and or mythological figures for comic purposes. Most fantasy comedy includes an element of parody, or satire, turning many of the fantasy conventions on their head such as the hero becoming a cowardly fool, the princess being a klutz. Examples of these films include \"Being John Malkovich\", \"Ernest Saves Christmas\", \"Ernest Scared Stupid\", \"Night at the Museum\", \"Groundhog Day\", \"Click\", and \"Shrek\".\n\nSci-fi comedy films, like most hybrid genre of comedy, use the elements of science fiction films to over the top extremes and exaggerated science fiction stereotypical characters. Examples of these types of films include \"Back to the Future\", \"Spaceballs\", \"Ghostbusters\", \"Evolution\", \"Innerspace\", \"Galaxy Quest\", \"Mars Attacks!\", \"Men in Black\", and \"The World's End\".\n\nMilitary comedy films involve comic situations in a military setting. When a film is primarily about the experience of civilians called into military service and still feeling out of place, it may be referred to as a \"service comedy\". Because war is such a grim subject, many military comedies are set in peacetime or during wartime but away from battle zones. Military and service comedies include:\n\nThe romantic comedy film subgenre typically involves the development of a relationship between a man and a woman. The stereotyped plot line follows the \"boy-gets-girl\", \"boy-loses-girl\", \"boy gets girl back again\" sequence. Naturally, there are innumerable variants to this plot, and much of the generally light-hearted comedy lies in the social interactions and sexual tensions between the pair. Examples of this style of film include \"It\" (1927), \"City Lights\" (1931), \"It's a Wonderful World\" (1939), \"The Shop Around the Corner\" (1940), \"Sabrina\" (1954), \"Annie Hall\" (1977), \"When Harry Met Sally...\" (1989), \"Pretty Woman\" (1990), \"Four Weddings and a Funeral\" (1994), and \"There's Something About Mary\" (1998).\n\n\n\n", "id": "5644", "title": "Comedy film"}
{"url": "https://en.wikipedia.org/wiki?curid=5645", "text": "Cult film\n\nA cult film, also commonly referred to as a cult classic, is a film that has acquired a cult following. Cult films are known for their dedicated, passionate fanbase, an elaborate subculture that engage in repeated viewings, quoting dialogue, and audience participation. Inclusive definitions allow for major studio productions, especially box office bombs, while exclusive definitions focus more on obscure, transgressive films shunned by the mainstream. The difficulty in defining the term and subjectivity of what qualifies as a cult film mirror classificatory disputes about art. The term \"cult film\" itself was first used in the 1970s to describe the culture that surrounded underground films and midnight movies, though \"cult\" was in common use in film analysis for decades prior to that.\n\nCult films trace their origin back to controversial and suppressed films kept alive by dedicated fans. In some cases, reclaimed or rediscovered films have acquired cult followings decades after their original release, occasionally for their camp value. Other cult films have since become well-respected or reassessed as classics; there is debate as to whether these popular and accepted films are still cult films. After failing in the cinema, some cult films have become regular fixtures on cable television or profitable sellers on home video. Others have inspired their own film festivals. Cult films can both appeal to specific subcultures and form their own subcultures. Other media that reference cult films can easily identify which demographics they desire to attract and offer savvy fans an opportunity to demonstrate their knowledge.\n\nCult films frequently break cultural taboos, and many feature excessive displays of violence, gore, sexuality, profanity, or combinations thereof. This can lead to controversy, censorship, and outright bans; less transgressive films may attract similar amounts of controversy when critics call them frivolous or incompetent. Films that fail to attract requisite amounts of controversy may face resistance when labeled as cult films. Mainstream films and big budget blockbusters have attracted cult followings similar to more underground and lesser known films; fans of these films often emphasize the films' niche appeal and reject the more popular aspects. Fans who like the films for the wrong reasons, such as perceived elements that represent mainstream appeal and marketing, will often be ostracized or ridiculed. Likewise, fans who stray from accepted subcultural scripts may experience similar rejection.\n\nSince the late 1970s, cult films have become increasingly popular. Films that once would have been limited to obscure cult followings are now capable of breaking into the mainstream, and showings of cult films have proved to be a profitable business venture. Overbroad usage of the term has resulted in controversy, as purists state it has become a meaningless descriptor applied to any film that is the slightest bit weird or unconventional; others accuse Hollywood studios of trying to artificially create cult films or use the term as a marketing tactic. Films are frequently stated to be an \"instant cult classic\" now, occasionally before they are released. Fickle fans on the Internet have latched on to unreleased films only to abandon them later on release. At the same time, other films have acquired massive, quick cult followings, thanks to spreading virally through social media. Easy access to cult films via video on demand and peer-to-peer file sharing has led some critics to pronounce the death of cult films.\n\nA cult film is any film that has a cult following, although the term is not easily defined and can be applied to a wide variety of films. The definition is occasionally expanded to exclude films that have been released by major studios or have big budgets, try specifically to become cult films, or become accepted by mainstream audiences and critics. Cult films are defined by audience reaction as much as they are content. This may take the form of elaborate and ritualized audience participation, film festivals, or cosplay. Over time, the definition has become more vague and inclusive as it drifts away from earlier, stricter views. Increasing use of the term by mainstream publications has resulted in controversy, as cinephiles argue that the term has become meaningless or \"elastic, a catchall for anything slightly maverick or strange\". Academic Mark Shiel has criticized the term itself as being a weak concept, reliant on subjectivity; different groups can interpret films in their own terms. According to feminist scholar Joanne Hollows, this subjectivity causes films with strong female cult followings to be perceived as too mainstream and not transgressive enough to qualify as a cult film. Academic Mike Chopra‑Gant says that cult films become decontextualized when studied as a group, and Shiel criticizes this recontextualization as cultural commodification.\n\nIn 2008, \"Cineaste\" asked a range of academics for their definition of a cult film. Several people defined cult films primarily in terms of their opposition to mainstream films and conformism, explicitly requiring a transgressive element, though others disputed the transgressive potential, given the demographic appeal to white males and mainstreaming of cult films. Jeffrey Andrew Weinstock instead called them mainstream films with transgressive elements. Most definitions also required a strong community aspect, such as obsessed fans or ritualistic behavior. Citing misuse of the term, Mikel J. Koven took a self-described hard-line stance that rejected definitions that use any other criteria. Matt Hills instead stressed the need for an open-ended definition rooted in structuration, where the film and the audience reaction are interrelated and neither is prioritized. Ernest Mathijs focused on the accidental nature of cult followings, arguing that cult film fans consider themselves too savvy to be marketed to, while Jonathan Rosenbaum rejected the continued existence of cult films and called the term a marketing buzzword. Mathijs suggests that cult films help to understand ambiguity and incompleteness in life given the difficulty in even defining the term. That cult films can have opposing qualities – such as good and bad, failure and success, innovative and retro – helps to illustrate that art is subjective and never self-evident. This ambiguity leads critics of postmodernism to accuse cult films of being beyond criticism, as the emphasis is now on personal interpretation rather than critical analysis or metanarratives. These inherent dichotomies can lead audiences to be split between ironic and earnest fans.\n\nWriting in \"Defining Cult Movies\", Jancovich et al. quote academic Jeffrey Sconce, who defines cult films in terms of paracinema, marginal films that exist outside critical and cultural acceptance: everything from exploitation to beach party musicals to softcore pornography. However, they reject cult films as having a single unifying feature; instead, they state that cult films are united in their \"subcultural ideology\" and opposition to mainstream tastes, itself a vague and undefinable term. Cult followings themselves can range from adoration to contempt, and they have little in common except for their celebration of nonconformity – even the bad films ridiculed by fans are artistically nonconformist, albeit unintentionally. At the same time, they state that bourgeois, masculine tastes are frequently reinforced, which makes cult films more of an internal conflict within the bourgeoisie, rather than a rebellion against it. This results in an anti-academic bias despite the use of formal methodologies, such as defamiliarization. This contradiction exists in many subcultures, especially those dependent on defining themselves in terms of opposition to the mainstream. This nonconformity is eventually co-opted by the dominant forces, such as Hollywood, and marketed to the mainstream. Academic Xavier Mendik also defines cult films as opposing the mainstream and further proposes that films can become cult by virtue of their genre or content, especially if it is transgressive. Due to their rejection of mainstream appeal, Mendik says cult films can be more creative and political; times of relative political instability produce more interesting films.\n\nCult films have existed since the early days of cinema. Film critic Harry Allan Potamkin traces them back to 1910s France and the reception of Pearl White, William S. Hart, and Charlie Chaplin, which he described as \"a dissent from the popular ritual\". \"Nosferatu\" (1922) was an unauthorized adaptation of Bram Stoker's \"Dracula\". Stoker's widow sued the production company and drove it to bankruptcy. All known copies of the film were destroyed, and \"Nosferatu\" become an early cult film, kept alive by a cult following that circulated illegal bootlegs. Academic Chuck Kleinhans identifies the Marx Brothers as making other early cult films. On their original release, some highly regarded classics from the Golden Age of Hollywood were panned by critics and audiences, relegated to cult status. \"The Night of the Hunter\" (1955) was a cult film for years, quoted often and championed by fans, before it was reassessed as an important and influential classic. During this time, American exploitation films and imported European art films were marketed similarly. Although critics Pauline Kael and Arthur Knight argued against arbitrary divisions into high and low culture, American films settled into rigid genres; European art films continued to push the boundaries of simple definitions, and these exploitative art films and artistic exploitation films would go on to influence American cult films. Much like later cult films, these early exploitation films encouraged audience participation, influenced by live theater and vaudeville.\n\nModern cult films grew from 1960s counterculture and underground films, popular among those who rejected mainstream Hollywood films. These underground film festivals led to the creation of midnight movies, which attracted cult followings. The term \"cult film\" itself was an outgrowth of this movement and was first used in the 1970s, though \"cult\" had been in use for decades in film analysis with both positive and negative connotations. These films were more concerned with cultural significance than the social justice sought by earlier avant-garde films. Midnight movies became more popular and mainstream, peaking with the release of \"The Rocky Horror Picture Show\" (1975), which finally found its audience several years after its release. Eventually, the rise of home video would marginalize midnight movies once again, after which many directors joined the burgeoning independent film scene or went back underground. Home video would give a second life to box office flops, as positive word-of-mouth or excessive replay on cable television led these films to develop an appreciative audience, as well as obsessive replay and study. For example, \"The Beastmaster\" (1982), despite its failure at the box office, became one of the most played movies on American cable television and developed into a cult film. Home video and television broadcasts of cult films were initially greeted with hostility. Joanne Hollows states that they were seen as turning cult films mainstream – in effect, feminizing them by opening them to distracted, passive audiences.\nReleases from major studios – such as \"The Big Lebowski\" (1998), which was distributed by Universal Studios – can become cult films when they fail at the box office and develop a cult following through reissues, such as midnight movies, festivals, and home video. Hollywood films, due to their nature, are more likely to attract this kind of attention, which leads to a mainstreaming effect of cult culture. With major studios behind them, even financially unsuccessful films can be re-released multiple times, which plays into a trend to capture audiences through repetitious reissues. The constant use of profanity and drugs in otherwise mainstream, Hollywood films, such as \"The Big Lebowski\", can alienate critics and audiences yet lead to a large cult following among more open-minded demographics not often associated with cult films, such as Wall Street bankers and professional soldiers. Thus, even comparatively mainstream films can satisfy the traditional demands of a cult film, perceived by fans as transgressive, niche, and uncommercial. Discussing his reputation for making cult films, Bollywood director Anurag Kashyap said, \"I didn't set out to make cult films. I wanted to make box-office hits.\" Writing in \"Cult Cinema\", academics Ernest Mathijs and Jamie Sexton state that this acceptance of mainstream culture and commercialism is not out of character, as cult audiences have a more complex relationship to these concepts: they are more opposed to mainstream values and excessive commercialism than they are anything else.\n\nIn a global context, popularity can vary widely by territory, especially with regard to limited releases. \"Mad Max\" (1979) was an international hit – except in America where it became an obscure cult favorite, ignored by critics and available for years only in a dubbed version though it earned over $100M internationally. Foreign cinema can put a different spin on popular genres, such as Japanese horror, which was initially a cult favorite in America. Asian imports to the West are often marketed as exotic cult films and of interchangeable national identity, which academic Chi-Yun Shin criticizes as reductive. Foreign influence can affect fan response, especially on genres tied to a national identity; when they become more global in scope, questions of authenticity may arise. Filmmakers and films ignored in their own country can become the objects of cult adoration in another, producing perplexed reactions in their native country. Cult films can also establish an early viability for more mainstream films both for filmmakers and national cinema. The early cult horror films of Peter Jackson were so strongly associated with his homeland that they affected the international reputation of New Zealand and its cinema. As more artistic films emerged, New Zealand was perceived as a legitimate competitor to Hollywood, which mirrored Jackson's career trajectory. \"Heavenly Creatures\" (1994) acquired its own cult following, became a part of New Zealand's national identity, and paved the way for big-budget, Hollywood-style epics, such as Jackson's \"Lord of the Rings\" trilogy.\n\nMathijs states that cult films and fandom frequently involve nontraditional elements of time and time management. Fans will often watch films obsessively, an activity that is viewed by the mainstream as wasting time yet can be seen as resisting the commodification of leisure time. They may also watch films idiosyncratically: sped up, slowed down, frequently paused, or at odd hours. Cult films themselves subvert traditional views of time – time travel, non-linear narratives, and ambiguous establishments of time are all popular. Mathijs also identifies specific cult film viewing habits, such as viewing horror films on Halloween, sentimental melodrama on Christmas, and romantic films on Valentine's Day. These films are often viewed as marathons where fans can gorge themselves on their favorites. Mathijs states that cult films broadcast on Christmas have a nostalgic factor. These films, ritually watched every season, give a sense of community and shared nostalgia to viewers. New films often have trouble making inroads against the institutions of \"It's A Wonderful Life\" (1946) and \"Miracle on 34th Street\" (1947). These films provide mild criticism of consumerism while encouraging family values. Halloween, on the other hand, allows flaunting society's taboos and testing one's fears. Horror films have appropriated the holiday, and many horror films debut on Halloween. Mathijs criticizes the over-cultified, commercialized nature of Halloween and horror films, which feed into each other so much that Halloween has turned into an image or product with no real community. Mathijs states that Halloween horror conventions can provide the missing community aspect.\n\nDespite their oppositional nature, cult films can produce celebrities. Like cult films themselves, authenticity is an important aspect of their popularity. Actors can become typecast as they become strongly associated with such iconic roles. Tim Curry, despite his acknowledged range as an actor, found casting difficult after he achieved fame in \"The Rocky Horror Picture Show\". Even when discussing unrelated projects, interviewers frequently bring up the role, which causes him to tire of discussing it. Mary Woronov, known for her transgressive roles in cult films, eventually transitioned to mainstream films. She was expected to recreate the transgressive elements of her cult films within the confines of mainstream cinema. Instead of the complex gender deconstructions of her Andy Warhol films, she became typecast as a lesbian or domineering woman. Sylvia Kristel, after starring in \"Emmanuelle\" (1974), found herself highly associated with the film and the sexual liberation of the 1970s. Caught between the transgressive elements of her cult film and the mainstream appeal of soft-core pornography, she was unable to work in anything but exploitation films and \"Emmanuelle\" sequels. Despite her immense popularity and cult following, she would rate only a footnote in most histories of European cinema if she was even mentioned. Similarly, Chloë Sevigny has struggled with her reputation as a cult independent film star famous for her daring roles in transgressive films. Cult films can also trap directors. Leonard Kastle, who directed \"The Honeymoon Killers\" (1969), never directed another film again. Despite his cult following, which included François Truffaut, he was unable to find financing for any of his other screenplays. Qualities that bring cult films to prominence – such as an uncompromising, unorthodox vision – caused Alejandro Jodorowsky to languish in obscurity for years.\n\nTransgressive films as a distinct artistic movement began in the 1970s. Unconcerned with genre distinctions, they drew inspiration equally from the nonconformity of European art cinema and experimental film, the gritty subject matter of Italian neorealism, and the shocking images of 1960s exploitation. Some used hardcore pornography and horror, occasionally at the same time. In the 1980s, filmmaker Nick Zedd identified this movement as the Cinema of Transgression and later wrote a manifesto. Popular in midnight showings, they were mainly limited to large urban areas, which led academic Joan Hawkins to label them as \"downtown culture\". These films acquired a legendary reputation as they were discussed and debated in alternative weeklies, such as \"The Village Voice\". Home video would finally allow general audiences to see them, which gave many people their first taste of underground film. Ernest Mathijs says that cult films often disrupt viewer expectations, such as giving characters transgressive motivations or focusing attention on elements outside the film. Cult films can also transgress national stereotypes and genre conventions, such as \"Battle Royale\" (2000), which broke many rules of teenage slasher films. The reverse – when films based on cult properties lose their transgressive edge – can result in derision and rejection by fans. Audience participation itself can be transgressive, such as breaking long-standing taboos against talking during films and throwing things at the screen.\n\nAccording to Mathijs, critical reception is important to a film's perception as cult, through topicality and controversy. Topicality, which can be regional (such as objection to government funding of the film) or critical (such as philosophical objections to the themes), enables attention and a contextual response. Cultural topics make the film relevant and can lead to controversy, such as a moral panic, which provides opposition. Cultural values transgressed in the film, such as sexual promiscuity, can be attacked by proxy, through attacks on the film. These concerns can vary from culture to culture, and they need not be at all similar. However, Mathijs says the film must invoke metacommentary for it to be more than simply culturally important. While referencing previous arguments, critics may attack its choice of genre or its very right to exist. Taking stances on these varied issues, critics assure their own relevance while helping to elevate the film to cult status. Perceived racist and reductive remarks by critics can rally fans and raise the profile of cult films, an example of which would be Rex Reed's comments about Korean culture in his review of \"Oldboy\" (2003). Critics can also polarize audiences and lead debates, such as how Joe Bob Briggs and Roger Ebert dueled over \"I Spit On Your Grave\" (1978). Briggs would later contribute a commentary track to the DVD release in which he describes it as a feminist film. Films which do not attract enough controversy may be ridiculed and rejected when suggested as cult films.\n\nAcademic Peter Hutchings, noting the many definitions of a cult film that require transgressive elements, states that cult films are known in part for their excesses. Both subject matter and its depiction are portrayed in extreme ways that break taboos of good taste and aesthetic norms. Violence, gore, sexual perversity, and even the music can be pushed to stylistic excess far beyond that allowed by mainstream cinema. Film censorship can make these films obscure and difficult to find, common criteria used to define cult films. Despite this, these films remain well-known and prized among collectors. Fans will occasionally express frustration with dismissive critics and conventional analysis, which they believe marginalizes and misinterprets paracinema. In marketing these films, young men are predominantly targeted. Horror films in particular can draw fans who seek the most extreme films. Audiences can also ironically latch on to offensive themes, such as misogyny, using these films as catharsis for the things that they hate most in life. Exploitative, transgressive elements can be pushed to excessive extremes for both humor and satire. Frank Henenlotter faced censorship and ridicule, but he found acceptance among audiences receptive to themes that Hollywood was reluctant to touch, such as violence, drug addiction, and misogyny. Lloyd Kaufman sees his films' political statements as more populist and authentic than the hypocrisy of mainstream films and celebrities. Despite featuring an abundance of fake blood, vomit, and diarrhea, Kaufman's films have attracted positive attention from critics and academics. Excess can also exist as camp, such as films that highlight the excesses of 1980s fashion and commercialism.\n\nFilms that are influenced by unpopular styles or genres can become cult films. Director Jean Rollin worked within \"cinéma fantastique\", an unpopular genre in modern France. Influenced by American films and early French fantasists, he drifted between art, exploitation, and pornography. His films were reviled by critics, but he retained a cult following drawn by the nudity and eroticism. Similarly, Jess Franco chafed under fascist censorship in Spain but became influential in Spain's horror boom of the 1960s. These transgressive films that straddle the line between art and horror may have overlapping cult followings, each with their own interpretation and reasons for appreciating it. The films that followed Jess Franco were unique in their rejection of mainstream art. Popular among fans of European horror for their subversiveness and obscurity, these later Spanish films allowed political dissidents to criticize the fascist regime within the cloak of exploitation and horror. Unlike most exploitation directors, they were not trying to establish a reputation. They were already established in the art-house world and intentionally chose to work within paracinema as a reaction against the New Spanish Cinema, an artistic revival supported by the fascists. As late as the 1980s, critics still cited Pedro Almodóvar's anti-macho iconoclasm as a rebellion against fascist mores, as he grew from countercultural rebel to mainstream respectability. Transgressive elements that limit a director's appeal in one country can be celebrated or highlighted in another. Takashi Miike has been marketed in the West as a shocking and avant-garde filmmaker despite his many family-friendly comedies, which have not been imported.\n\nThe transgressive nature of cult films can lead to their censorship. During the 1970s and early 1980s, a wave of explicit, graphic exploitation films caused controversy. Called \"video nasties\" within the UK, they ignited calls for censorship and stricter laws on home video releases, which were largely unregulated. Consequently, the British Board of Film Classification banned many popular cult films due to issues of sex, violence, and incitement to crime. Released during the cannibal boom, \"Cannibal Holocaust\" (1980) was banned in dozens of countries and caused the director to be briefly jailed over fears that it was a real snuff film. Although opposed to censorship, director Ruggero Deodato would later agree with cuts made by the BBFC which removed unsimulated animal killings, which limited the film's distribution. Frequently banned films may introduce questions of authenticity as fans question whether they have seen a truly uncensored cut. Cult films have been falsely claimed to have been banned to increase their transgressive reputation and explain their lack of mainstream penetration. Marketing campaigns have also used such claims to raise interest among curious audiences. Home video has allowed cult film fans to import rare or banned films, finally giving them a chance to complete their collection with imports and bootlegs. Cult films previously banned are sometimes released with much fanfare and the fans assumed to be already familiar with the controversy. Personal responsibility is often highlighted, and a strong anti-censorship message may be present. Previously lost scenes cut by studios can be re-added and restore a director's original vision, which draws similar fanfare and acclaim from fans. Imports are sometimes censored to remove elements that would be controversial, such as references to Islamic spirituality in Indonesian cult films.\n\nAcademics have written of how transgressive themes in cult films can be regressive. David Church and Chuck Kleinhans describe an uncritical celebration of transgressive themes in cult films, including misogyny and racism. Church has also criticized gendered descriptions of transgressive content that celebrate masculinity. Joanne Hollows further identifies a gendered component to the celebration of transgressive themes in cult films, where male terms are used to describe films outside the mainstream while female terms are used to describe mainstream, conformist cinema. Jacinda Read's expansion states that cult films, despite their potential for empowerment of the marginalized, are more often used by politically incorrect males. Knowledgeable about feminism and multiculturalism, they seek a refuge from the academic acceptance of these progressive ideals. Their playful and ironic acceptance of regressive lad culture invites, and even dares, condemnation from academics and the uncool. Thus, cult films become a tool to reinforce mainstream values through transgressive content; Rebecca Feasy states that cultural hierarchies can also be reaffirmed through mockery of films perceived to be lacking masculinity. However, the sexploitation films of Doris Wishman took a feminist approach which avoids and subverts the male gaze and traditional goal-oriented methods. Wishman's subject matter, though exploitative and transgressive, was always framed in terms of female empowerment and the feminine spectator. Her use of common cult film motifs – female nudity and ambiguous gender – were repurposed to comment on feminist topics. Similarly, the films of Russ Meyer were a complicated combination of transgressive, mainstream, progressive, and regressive elements. They attracted both acclaim and denouncement from critics and progressives. Transgressive films imported from cultures that are recognizably different yet still relatable can be used to progressively examine issues in another culture.\n\nCult films can be used to help define or create groups as a form of subcultural capital; knowledge of cult films proves that one is \"authentic\" or \"non-mainstream\". They can be used to provoke an outraged response from the mainstream, which further defines the subculture, as only members could possibly tolerate such deviant entertainment. More accessible films have less subcultural capital; among extremists, banned films will have the most. By referencing cult films, media can identify desired demographics, strengthen bonds with specific subcultures, and stand out among those who understand the intertextuality. Popular films from previous eras may be reclaimed by genre fans long after they have been forgotten by the original audiences. This can be done for authenticity, such as horror fans who seek out now-obscure titles from the 1950s instead of the modern, well-known remakes. Authenticity may also drive fans to deny genre categorization to films perceived as too mainstream or accessible. Authenticity in performance and expertise can drive fan acclaim. Authenticity can also drive fans to decry the mainstream in the form of hostile critics and censors. Especially when promoted by enthusiastic and knowledgeable programmers, choice of venue can be an important part of expressing individuality. Besides creating new communities, cult films can link formerly disparate groups, such as fans and critics. As these groups intermix, they can influence each other, though this may be resisted by older fans, unfamiliar with these new references. In extreme cases, cult films can lead to the creation of religions, such as Dudeism. For their avoidance of mainstream culture and audiences, enjoyment of irony, and celebration of obscure subcultures, academic Martin Roberts compares cult film fans to hipsters.\nA film can become the object of a cult following within a particular region or culture if it has unusual significance. For example, Norman Wisdom's films, friendly to Marxist interpretation, amassed a cult following in Albania, as they were among the few Western films allowed by the country's Communist rulers. \"The Wizard of Oz\" (1939) and its star, Judy Garland, hold special significance to American and British gay culture, although it is a widely viewed and historically important film in greater American culture. Similarly, James Dean and his brief film career have become icons of alienated youth. Cult films can have such niche appeal that they are only popular within certain subcultures, such as \"Reefer Madness\" (1936) and \"Hemp for Victory\" (1942) among stoner subculture. Beach party musicals, popular among American surfers, failed to find an equivalent audience when imported to the United Kingdom. When films target subcultures like this, they may seem unintelligible without the proper cultural capital. Films which appeal to teenagers may offer subcultural identities that are easily recognized and differentiate various subcultural groups. Films which appeal to stereotypical male activities, such as sports, can easily gain strong male cult followings. Sports metaphors are often used in the marketing of cult films to males, such as emphasizing the \"extreme\" nature of the film, which increases the appeal to youth subcultures fond of extreme sports.\nMatt Hills' concept of the \"cult blockbuster\" involves cult followings inside larger, mainstream films. Although these are big budget, mainstream films, they still attract cult followings. The cult fans differentiate themselves from ordinary fans in several ways: longstanding devotion to the film, distinctive interpretations, and fan works. Hills identifies three different cult followings for \"The Lord of the Rings\", each with their own fandom separate from the mainstream. Academic Emma Pett identifies \"Back to the Future\" (1985) as another example of a cult blockbuster. Although the film topped the charts when it was released, it has developed a nostalgic cult following over the years. The hammy acting by Christopher Lloyd and quotable dialogue draw a cult following, as they mimic traditional cult films. Blockbuster science fiction films that include philosophical subtexts, such as \"The Matrix\", allow cult film fans to enjoy them on a higher level than the mainstream. \"Star Wars\", with its large cult following in geek subculture, has been cited as both a cult blockbuster and a cult film. Although a mainstream epic, \"Star Wars\" has provided its fans with a spirituality and culture outside of the mainstream. Fans, in response to the popularity of these blockbusters, will claim elements for themselves while rejecting others. For example, in the \"Star Wars\" film series, mainstream criticism of Jar Jar Binks focused on racial stereotyping; although cult film fans will use that to bolster their arguments, he is rejected because he represents mainstream appeal and marketing. Also, instead of valuing textual rarity, fans of cult blockbusters will value repeat viewings. They may also engage in behaviors more traditional for fans of cult television and other serial media, as cult blockbusters are often franchised, preconceived as a film series, or both. To reduce mainstream accessibility, a film series can be self-reflexive and full of in-jokes that only longtime fans can understand.\n\nCult films can create their own subculture. \"Rocky Horror\", originally made to exploit the popularity of glam subculture, became what academic Gina Marchetti called a \"sub-subculture\", a variant that outlived its parent subculture. Although often described as primarily composed of obsessed fans, cult film fandom can include many newer, less experienced members. Familiar with the film's reputation and having watched clips on YouTube, these fans may take the next step and enter the film's fandom. If they are the majority, they may alter or ignore long-standing traditions, such as audience participation rituals; rituals which lack perceived authenticity may be criticized, but accepted rituals bring subcultural capital to veteran fans who introduce them to the newer members. Fans who flaunt their knowledge receive negative reactions. Newer fans may cite the film itself as their reason for attending a showing, but longtime fans often cite the community. Organized fandoms may spread and become popular as a way of introducing new people to the film, as well as theatrical screenings being privileged by the media and fandom itself. Fandom can also be used as a process of legitimation. Fans of cult films, as in media fandom, are frequently producers instead of mere consumers. Unconcerned with traditional views on intellectual property, these fan works are often unsanctioned, transformative, and ignore fictional canon.\n\nLike cult films themselves, magazines and websites dedicated to cult films revel in their self-conscious offensiveness. They maintain a sense of exclusivity by offending mainstream audiences with misogyny, gore, and racism. Obsessive trivia can be used to bore mainstream audiences while building up subcultural capital. Specialist stores on the fringes of society (or websites which prominently partner with hardcore pornographic sites) can be used to reinforce the outsider nature of cult film fandom, especially when they use erotic or gory imagery. By assuming a preexisting knowledge of trivia, non-fans can be excluded. Previous articles and controversies can also be alluded to without explanation. Casual readers and non-fans will thus be left out of discussions and debates, as they lack enough information to meaningfully contribute. When fans like a cult film for the wrong reasons, such as casting or characters aimed at mainstream appeal, they may be ridiculed. Thus, fandom can keep the mainstream at bay while defining themselves in terms of the \"Other\", a philosophical construct divergent from social norms. Commercial aspects of fandom (such as magazines or books) can also be defined in terms of \"otherness\" and thus valid to consume: consumers purchasing independent or niche publications are discerning consumers, but the mainstream is denigrated. Irony or self-deprecating humor can also be used. In online communities, different subcultures attracted to transgressive films can clash over values and criteria for subcultural capital. Even within subcultures, fans who break subcultural scripts, such as denying the affectivity of a disturbing film, will be ridiculed for their lack of authenticity.\n\nThe critic Michael Medved characterized examples of the \"so bad it's good\" class of low-budget cult film through books such as \"The Golden Turkey Awards\". These films include financially fruitless and critically scorned films that have become inadvertent comedies to film buffs, such as \"Plan 9 from Outer Space\" (1959) and \"The Room\" (2003). Similarly, Paul Verhoeven's \"Showgirls\" (1995) bombed in theaters but developed a cult following on video. Catching on, Metro-Goldwyn-Mayer capitalized on the film's ironic appeal and marketed it as a cult film. Sometimes fans will impose their own interpretation of films which have attracted derision, such as reinterpreting an earnest melodrama as a comedy. Jacob deNobel of the \"Carroll County Times\" states that films can be perceived as nonsensical or inept when audiences misunderstand avant-garde filmmaking or misinterpret parody. Films such as \"Rocky Horror\" can be misinterpreted as \"weird for weirdness sake\" by people unfamiliar with the cult films that it parodies. deNobel ultimately rejects the use of the label \"so bad it's good\" as mean-spirited and often misapplied. Alamo Drafthouse programmer Zack Carlson has further said that any film which succeeds in entertaining an audience is good, regardless of irony. The rise of the Internet and on-demand films has led critics to question whether \"so bad it's good\" films have a future now that people have such diverse options in both availability and catalog, though fans eager to experience the worst films ever made can lead to lucrative showings for local theaters and merchandisers.\n\nChuck Kleinhans states that the difference between a guilty pleasure and a cult film can be as simple as the number of fans; David Church raises the question of how many people it takes to form a cult following, especially now that home video makes fans difficult to count. As these cult films become more popular, they can bring varied responses from fans that depend on different interpretations, such as camp, irony, genuine affection, or combinations thereof. Earnest fans, who recognize and accept the film's faults, can make minor celebrities of the film's cast, though the benefits are not always clear. Cult film stars known for their camp can inject subtle parody or signal when films should not be taken seriously. Campy actors can also provide comic book supervillains for serious, artistic-minded films. This can draw fan acclaim and obsession more readily than subtle, method-inspired acting. Mark Chalon Smith of the \"Los Angeles Times\" says technical faults may be forgiven if a film makes up for them in other areas, such as camp or transgressive content. Smith states that the early films of John Waters are amateurish and less influential than claimed, but Waters' outrageous vision cements his place in cult cinema. Films such as \"Myra Breckinridge\" (1970) and \"Beyond the Valley of the Dolls\" (1970) can experience critical reappraisal later, once their camp excess and avant-garde filmmaking are better accepted, and films that are initially dismissed as frivolous are often reassessed as campy. Films that intentionally try to appeal to fans of camp may end up alienating them, as the films become perceived as trying too hard or not authentic.\n\nAccording to academic Brigid Cherry, nostalgia \"is a strong element of certain kinds of cult appeal.\" When Veoh added many cult films to their site, they cited nostalgia as a factor for their popularity. Academic I. Q. Hunter describes cult films as \"New Hollywood \"in extremis\"\" and a form of nostalgia for that period. Ernest Mathijs instead states that cult films use nostalgia as a form of resistance against progress and capitalistic ideas of a time-based economy. By virtue of the time travel plot, \"Back to the Future\" permits nostalgia for both the 1950s and 1980s. Many members of its nostalgic cult following are too young to have been alive during those periods, which Emma Pett interprets as fondness for retro aesthetics, nostalgia for when they saw the film rather than when it was released, and looking to the past to find a better time period. Similarly, films directed by John Hughes have taken hold in midnight movie venues, trading off of nostalgia for the 1980s and an ironic appreciation for their optimism. Mathijs and Sexton describe \"Grease\" (1978) as a film nostalgic about an imagined past that has acquired a nostalgic cult following. Other cult films, such as \"Streets of Fire\" (1984), create a new fictional world based on nostalgic views of the past. Cult films may also subvert nostalgia, such as \"The Big Lebowski\", which introduces many nostalgic elements and then reveals them as fake and hollow. Author China Miéville praises the use of satire in Donnie Darko for its avoidance of falling into facile and comforting nostalgia, but Nathan Lee of the \"New York Sun\" identifies the retro aesthetic and nostalgic pastiche as factors in its popularity among midnight movie crowds.\n\nAuthor Tomas Crowder-Taraborrelli describes midnight movies as a reaction against the political and cultural conservatism in America, and Joan Hawkins identifies the movement as running the gamut from anarchist to libertarian, united in their anti-establishment attitude and punk aesthetic. These films are resistant to simple categorization and are defined by the fanaticism and ritualistic behaviors of their audiences. Midnight movies require a night life and an audience willing to invest themselves actively. Hawkins states that these films took a rather bleak point of view due to the living conditions of the artists and the economic prospects of the 1970s. Like the surrealists and dadaists, they not only satirically attacked society but also the very structure of film – a counter-cinema that deconstructs narrative and traditional processes. In the late 1980s and 1990s, midnight movies transitioned from underground showings to home video viewings; eventually, a desire for community brought a resurgence, and \"The Big Lebowski\" kick-started a new generation. Demographics shifted, and more hip and mainstream audiences were drawn to them. Although studios expressed skepticism, large audiences were drawn to box office flops, such as \"Donnie Darko\" (2001) and \"Office Space\" (1999). Modern midnight movies retain their popularity and have been strongly diverging from mainstream films shown at midnight. Mainstream cinemas, eager to disassociate themselves from negative associations and increase profits, have begun abandoning midnight screenings. Although classic midnight movies have dropped off in popularity, they still bring reliable crowds.\n\nAlthough seemingly at odds with each other, art and exploitation films are frequently treated as equal and interchangeable in cult fandom, listed alongside each other and described in similar terms: their ability to provoke a response. The most exploitative aspects of art films are thus played up and their academic recognition ignored. This flattening of culture follows the popularity of post-structuralism, which rejects a hierarchy of artistic merit and equates exploitation and art. Mathijs and Sexton state that although cult films are not synonymous with exploitation, as is occasionally assumed, this is a key component; they write that exploitation, which exists on the fringes of the mainstream and deals with taboo subjects, is well-suited for cult followings. Academic David Andrews writes that cult softcore films are \"the most masculinized, youth-oriented, populist, and openly pornographic softcore area.\" The sexploitation films of Russ Meyer were among the first to abandon all hypocritical pretenses of morality and were technically proficient enough to gain a cult following. His persistent vision saw him received as an auteur worthy of academic study; director John Waters attributes this to Meyer's ability to create complicated, sexually charged films without resorting to explicit sex. Myrna Oliver described Doris Wishman's exploitation films as \"crass, coarse, and camp ... perfect fodder for a cult following.\" \"Sick films\", the most disturbing and graphically transgressive films, have their own distinct cult following; these films transcend their roots in exploitation, horror, and art films. In 1960s and 1970s America, exploitation and art films shared audiences and marketing, especially in New York City's grindhouse cinemas.\n\nMathijs and Sexton state that genre is an important part of cult films; cult films will often mix, mock, or exaggerate the tropes associated with traditional genres. Science fiction, fantasy, and horror are known for their large and dedicated cult followings; as science fiction films become more popular, fans emphasize non-mainstream and less commercial aspects of it. B films, which are often conflated with exploitation, are as important to cult films as exploitation. Teodor Reljic of \"Malta Today\" states that cult B films are a realistic goal for Malta's burgeoning film industry. Genre films, B films that strictly adhere to genre limitations, can appeal to cult film fans: given their transgressive excesses, horror films are likely to become to cult films; films like \"Galaxy Quest\" (1999) highlight the importance of cult followings and fandom to science fiction; and authentic martial arts skills in Hong Kong action films can drive them to become cult favorites. Cult musicals can range from the traditional, such as \"Singin' in the Rain\" (1952), which appeal to cult audiences through nostalgia, camp, and spectacle, to the more non-traditional, such as \"Cry-Baby\" (1990), which parodies musicals, and \"Rocky Horror\", which uses a rock soundtrack. Romantic fairy tale \"The Princess Bride\" (1987) failed to attract audiences in its original release, as the studio did not know how to market it. The freedom and excitement associated with cars can be an important part of drawing cult film fans to genre films, and they can signify action and danger with more ambiguity than a gun. \"Ad Week\" writes that cult B films, when released on home video, market themselves and need only enough advertising to raise curiosity or nostalgia.\n\nAnimation can provide wide open vistas for stories; the French film \"Fantastic Planet\" (1972) explored ideas beyond the limits of traditional, live-action science fiction films. Phil Hoad of \"The Guardian\" identifies \"Akira\" (1988) as introducing violent, adult Japanese animation (known as anime) to the West and paving the way for later works. Anime, according to academic Brian Ruh, is not a cult genre, but the lack of individual fandoms inside anime fandom itself lends itself to a bleeding over of cult attention and can help spread works internationally. Anime, which is often highly franchised, provides its fans with alternative fictional canons and points of view that can drive fan activity. The \"Ghost in the Shell\" franchise, for example, provided Japanese fans with enough bonus material and spinoffs that it encouraged cult tendencies. Markets that did not support the sale of these materials saw less cult activity. Ralph Bakshi's career has been marked with controversy: \"Fritz the Cat\" (1972), the first animated film to be rated \"X\" by the MPAA, provoked outrage for its racial caricatures and graphic depictions of sex, and \"Coonskin\" (1975) was decried as racist. Bakshi recalls that older animators had tired of \"kid stuff\" and desired edgier work, whereas younger animators hated his work for \"destroying the Disney images\". Eventually, his work would be reassessed and cult followings, which include Quentin Tarantino and Robert Rodriguez, developed around several of his films. \"Heavy Metal\" (1981) faced similar denunciations from critics; Donald Liebenson of the \"Los Angeles Times\" cites the violence and sexual imagery as alienating critics, who did not know what to make of the film. It would go on to become a popular midnight movie and frequently bootlegged by fans, as licensing issues kept it from being released on video for many years.\n\nSensationalistic documentaries called mondo films replicate the most shocking and transgressive elements of exploitation films; they are usually modeled after \"sick films\" and cover similar subject matter. In \"The Cult Film Reader\", academics Mathijs and Mendik write that these documentaries often present non-Western societies as \"stereotypically mysterious, seductive, immoral, deceptive, barbaric or savage\". Though they can be interpreted as racist, Mathijs and Mendik state that they also \"exhibit a liberal attitude towards the breaking of cultural taboos\". Mondo films like \"Faces of Death\" mix real and fake footage freely, and they gain their cult following through the outrage and debate over authenticity that results. Like \"so bad it's good\" cult films, old propaganda and government hygiene films may be enjoyed ironically by more modern audiences for the camp value of the outdated themes and outlandish claims made about perceived social threats, such as drug use. Academic Barry K. Grant states that Frank Capra's \"Why We Fight\" World War II propaganda films are explicitly not cult, because they are \"slickly made and have proven their ability to persuade an audience.\" The sponsored film \"Mr. B Natural\" became a cult hit when it was broadcast on the satirical television show \"Mystery Science Theater 3000\"; cast member Trace Beaulieu cited these educational shorts as his favorite to mock on the show. Mark Jancovich states that cult audiences are drawn to these films because of their \"very banality or incoherence of their political positions\", unlike traditional cult films, which achieve popularity through auteurist radicalism.\n\nMark Shiel explains the rising popularity of cult films as an attempt by cinephiles and scholars to escape the oppressive conformity and mainstream appeal of even independent film, as well as a lack of condescension in both critics and the films; Academic Donna de Ville says it is a chance to subvert the dominance of academics and cinephiles. According to Xavier Mendik, \"academics have been really interested in cult movies for quite a while now.\" Mendik has sought to bring together academic interest and fandom through Cine-Excess, a film festival. I. Q. Hunter states that \"it's much easier to be a cultist now, but it is also rather more inconsequential.\" Citing the mainstream availability of \"Cannibal Holocaust\", Jeffrey Sconce rejects definitions of cult films based on controversy and excess, as they've now become meaningless. Cult films have influenced such diverse industries as cosmetics, music videos, and fashion. Cult films have shown up in less expected places; as a sign of his popularity, a bronze statue of Ed Wood has been proposed in his hometown, and \"L'Osservatore Romano\", the official newspaper of the Holy See, has courted controversy for its endorsement of cult films and pop culture. When cities attempt to renovate neighborhoods, fans have called attempts to demolish iconic settings from cult films \"cultural vandalism\". Cult films can also drive tourism, even when it is unwanted.\n\nAs far back as the 1970s, \"Attack of the Killer Tomatoes\" (1978) was designed specifically to be a cult film, and \"The Rocky Horror Picture Show\" was produced by 20th Century Fox, a major Hollywood studio. Over its decades-long release, \"Rocky Horror\" became the seventh highest grossing R-rated film when adjusted for inflation; Journalist Matt Singer has questioned whether \"Rocky Horror\"s popularity invalidates its cult status. Founded in 1974, Troma Entertainment, an independent studio, would become known for both its cult following and cult films. In the 1980s, Danny Peary's \"Cult Movies\" (1981) would influence director Edgar Wright and film critic Scott Tobias of \"The A.V. Club\". The rise of home video would have a mainstreaming effect on cult films and cultish behavior, though some collectors would be unlikely to self-identify as cult film fans. Film critic Joe Bob Briggs began reviewing drive-in theater and cult films, though he faced much criticism as an early advocate of exploitation and cult films. Briggs highlights the mainstreaming of cult films by pointing out the respectful obituaries that cult directors have received from formerly hostile publications and acceptance of politically incorrect films at mainstream film festivals. This acceptance is not universal, though, and some critics have resisted this mainstreaming of paracinema. Beginning in the 1990s, director Quentin Tarantino would have the greatest success in turning cult films mainstream. Tarantino later used his fame to champion obscure cult films that had influenced him and set up the short-lived Rolling Thunder Pictures, which distributed several of his favorite cult films. Tarantino's clout led Phil Hoad of \"The Guardian\" to call Tarantino the world's most influential director.\n\nAs major Hollywood studios and audiences both become savvy to cult films, productions once limited to cult appeal have instead become popular hits, and cult directors have become hot properties known for more mainstream and accessible films. Remarking on the popular trend of remaking cult films, Claude Brodesser-Akner of \"New York\" magazine states that Hollywood studios have been superstitiously hoping to recreate past successes rather than trading on nostalgia. Their popularity would bring some critics to proclaim the death of cult films now that they have finally become successful and mainstream, are too slick to attract a proper cult following, lack context, or are too easily found online. In response, David Church says that cult film fans have retreated to more obscure and difficult to find films, often using illegal distribution methods, which preserves the outlaw status of cult films. Virtual spaces, such as online forums and fan sites, replace the traditional fanzines and newsletters. Cult film fans consider themselves collectors, rather than consumers, as they associate consumers with mainstream, Hollywood audiences. This collecting can take the place of fetishization of a single film. Addressing concerns that DVDs have revoked the cult status of films like \"Rocky Horror\", academic Mikel J. Koven states that small scale screenings with friends and family can replace midnight showings. Koven also identifies television shows, such as \"Twin Peaks\", as retaining more traditional cult activities inside popular culture. David Lynch himself has not ruled out another television series, as studios have become reluctant to take chances on non-mainstream ideas. Despite this, the Alamo Drafthouse has capitalized on cult films and the surrounding culture through inspiration drawn from \"Rocky Horror\" and retro promotional gimmickry. They sell out their shows regularly and have acquired a cult following of their own.\n\nAcademic Bob Batchelor, writing in \"Cult Pop Culture\", states that the Internet has democratized cult culture and destroyed the line between cult and mainstream. Fans of even the most obscure films can communicate online with each other in vibrant communities. Although known for their big-budget blockbusters, Steven Spielberg and George Lucas have criticized the current Hollywood system of gambling everything on the opening weekend of these productions. Geoffrey Macnab of \"The Independent\" instead suggests that Hollywood look to capitalize on cult films, which have exploded in popularity on the Internet. The rise of social media has been a boon to cult films. Sites such as Twitter have displaced traditional venues for fandom and courted controversy from cultural critics who are unamused by campy cult films. After a clip from one of his films went viral, director-producer Roger Corman made a distribution deal with YouTube. Found footage which had originally been distributed as cult VHS collections eventually went viral on YouTube, which opened them to new generations of fans. Films such as \"Birdemic\" (2008) and \"The Room\" (2003) gained quick, massive popularity, as prominent members of social networking sites discussed them. Their rise as \"instant cult classics\" bypasses the years of obscurity that most cult films labor under. In response, critics have described the use of viral marketing as astroturfing and an attempt to manufacture cult films.\n\nI. Q. Hunter identifies a prefabricated cult film style which includes \"deliberately, insulting bad films\", \"slick exercises in dysfunction and alienation\", and mainstream films \"that sell themselves as worth obsessing over\". Writing for NPR, Scott Tobias states that Don Coscarelli, whose previous films effortlessly attracted cult followings, has drifted into this realm. Tobias criticizes Coscarelli as trying too hard to appeal to cult audiences and sacrificing internal consistency for calculated quirkiness. Influenced by the successful online hype of \"The Blair Witch Project\" (1999), other films have attempted to draw online cult fandom with the use of prefabricated cult appeal. \"Snakes on a Plane\" (2006) is an example that attracted massive attention from curious fans. Uniquely, its cult following preceded the film's release and included speculative parodies of what fans imagined the film might be. This reached the point of convergence culture when fan speculation began to impact on the film's production. Although it was proclaimed a cult film and major game-changer before it was released, it failed to win either mainstream audiences or maintain its cult following. In retrospect, critic Spencer Kornhaber would call it a serendipitous novelty and a footnote to a \"more naive era of the Internet\". However, it became influential in both marketing and titling. This trend of \"instant cult classics\" which are hailed yet fail to attain a lasting following is described by Matt Singer, who states that the phrase is an oxymoron.\n\nCult films are often approached in terms of auteur theory, which states that the director's creative vision drives a film. This has fallen out of favor in academia, creating a disconnect between cult film fans and critics. Matt Hills states that auteur theory can help to create cult films; fans that see a film as continuing a director's creative vision are likely to accept it as cult. According to academic Greg Taylor, auteur theory also helped to popularize cult films when middlebrow audiences found an accessible way to approach avant-garde film criticism. Auteur theory provided an alternative culture for cult film fans while carrying the weight of scholarship. By requiring repeated viewings and extensive knowledge of details, auteur theory naturally appealed to cult film fans. Taylor further states that this was instrumental in allowing cult films to break through to the mainstream. Academic Joe Tompkins states that this auteurism is often highlighted when mainstream success occurs. This may take the place of – and even ignore – political readings of the director. Cult films and directors may be celebrated for their transgressive content, daring, and independence, but Tompkins argues that mainstream recognition requires they be palatable to corporate interests who stand to gain much from the mainstreaming of cult film culture. While critics may champion revolutionary aspects of filmmaking and political interpretation, Hollywood studios and other corporate interests will instead highlight only the aspects that they wish to legitimize in their own films, such as sensational exploitation. Someone like George Romero, whose films are both transgressive and subversive, will have the transgressive aspects highlighted while the subversive aspects are ignored.\n\n", "id": "5645", "title": "Cult film"}
{"url": "https://en.wikipedia.org/wiki?curid=5646", "text": "Constantinople\n\nConstantinople (; ; ) was the capital city of the Roman/Byzantine Empire (330–1204 and 1261–1453), and also of the brief Latin (1204–1261), and the later Ottoman (1453–1923) empires. It was reinaugurated in 324 AD from ancient Byzantium as the new capital of the Roman Empire by Emperor Constantine the Great, after whom it was named, and dedicated on 11 May 330 AD.\n\nFrom the mid-5th century to the early 13th century, Constantinople was the largest and wealthiest city in Europe and it was instrumental in the advancement of Christianity during Roman and Byzantine times as the home of the Ecumenical Patriarch of Constantinople and as the guardian of Christendom's holiest relics such as the Crown of Thorns and the True Cross. After the final loss of its provinces in the early 15th century, the Byzantine Empire was reduced to just Constantinople and its environs, along with Morea in Greece, and the city eventually fell to the Ottomans after a month-long siege in 1453.\n\nConstantinople was famed for its massive and complex defences. Although besieged on numerous occasions by various peoples, the defences of Constantinople proved invulnerable for nearly nine hundred years before the city was taken in 1204 by the Crusader armies of the Fourth Crusade, and after it was liberated in 1261 by the Byzantine Emperor Michael VIII Palaiologos, a second and final time in 1453 when it was conquered by the Ottoman Sultan Mehmed II. The first wall of the city was erected by Constantine I, and surrounded the city on both land and sea fronts. Later, in the 5th century, the Praetorian Prefect Anthemius under the child emperor Theodosius II undertook the construction of the Theodosian Walls, which consisted of a double wall lying about to the west of the first wall and a moat with palisades in front. This formidable complex of defences was one of the most sophisticated of Antiquity and the city was built intentionally on seven hills as well as juxtaposed between the Golden Horn and the Sea of Marmara and thus presented an impregnable fortress enclosing magnificent palaces, domes, and towers, necessitated from being the gateway between two continents (Europe and Asia) and two seas (the Mediterranean and the Black Seas).\n\nThe city was also famed for its architectural masterpieces, such as the Greek Orthodox cathedral of Hagia Sophia, which served as the seat of the Ecumenical Patriarchate, the sacred Imperial Palace where the Emperors lived, the Galata Tower, the Hippodrome, the Golden Gate of the Land Walls, and the opulent aristocratic palaces lining the arcaded avenues and squares. The University of Constantinople was founded in the fifth century and contained numerous artistic and literary treasures before it was sacked in 1204 and 1453, including its vast Imperial Library which contained the remnants of the Library of Alexandria and had over 100,000 volumes of ancient texts.\n\nConstantinople never truly recovered from the devastation of the Fourth Crusade and the decades of misrule by the Latins. Although the city partially recovered in the early years after the restoration under the Palaiologos dynasty, the advent of the Ottomans and the subsequent loss of the Imperial territories until Constantinople became an enclave inside the fledgling Ottoman Empire rendered the city severely depopulated when it fell to the Ottoman Turks, whereafter it replaced Edirne (Adrianople) as the new capital of the Ottoman Empire.\n\nAccording to Pliny the Elder in his \"Natural History\", the first known name of a settlement on the site of Constantinople was \"Lygos,\" a settlement of likely Thracian origin founded between the 13th to 11th century BC. The site, according to the founding myth of the city, was abandoned by the time Greek settlers from the city-state of Megara founded \"Byzantium\" (, \"Byzántion\") in around 657 BC, across from the town of Chalcedon on the Asiatic side of the Bosphorus.\n\nThe origins of the name of \"Byzantion\", more commonly known by the later Latin \"Byzantium,\" are not entirely clear, though some suggest it is of Thraco-Illyrian origin. The founding myth of the city has it told that the settlement was named after the leader of the Megarian colonists, Byzas. The later Byzantines of Constantinople themselves would maintain that the city was named in honour of two men, Byzas and Antes, though this was more likely just a play on the word Byzantion.\n\nThe city was briefly renamed \"Augusta Antonina\" in the early 3rd century by the Emperor Septimius Severus (193–211), having razed the city to the ground in 196 AD for supporting a rival contender in the civil war and rebuilt, in honour of his son Antoninus, the later Emperor Caracalla. The name appears to have been quickly forgotten and abandoned, and the city reverted to Byzantium/Byzantion after either the assassination of Caracalla in 217 or, at the latest, the fall of the Severan dynasty in 235.\n\nByzantium took on the name of \"Konstantinoupolis\" (\"city of Constantine\", \"Constantinople\") after its re-foundation under Roman emperor Constantine I, who transferred the capital of the Roman Empire from Rome to Byzantium in 330 AD and designated his new capital officially as \"Nova Roma\" (Νέα Ῥώμη) 'New Rome'. During this time, the city was also called 'Second Rome', 'Eastern Rome', and \"Roma Constantinopolitana.\" As the city became the sole remaining capital of the Roman Empire after the fall of the West, and its wealth, population, and influence grew, the city also came to have a multitude of nicknames.\n\nAs the largest and wealthiest city in Europe during the 4th–13th centuries and a centre of culture and education of the Mediterranean basin, Constantinople came to be known by prestigious titles such as \"Basileuousa\" (Queen of Cities) and \"Megalopolis\" (the Great City) and was, in colloquial speech, commonly referred to as just \"Polis\" (η πόλη) 'the City' by Constantinopolitans and provincial Byzantines alike.\n\nIn the language of other peoples, Constantinople was referred to just as reverently. The medieval Vikings, who had contacts with the empire through their expansion in eastern Europe (Varangians) used the Old Norse name \"Miklagarðr\" (from \"mikill\" 'big' and \"garðr\" 'city'), and later \"Miklagard\" and \"Miklagarth\". In Arabic, the city was sometimes called \"Rūmiyyat al-kubra\" (Great City of the Romans) and in Persian as \"Takht-e Rum\" (Throne of the Romans).\n\nIn East and South Slavic languages, including in medieval Russia, Constantinople was referred to as \"Tsargrad\" (\"Царьград\") or \"Carigrad,\" 'City of the Caesar (Emperor)', from the Slavonic words \"tsar\" ('Caesar' or 'King') and \"grad\" ('city'). This was presumably a calque on a Greek phrase such as Βασιλέως Πόλις \"(Vasileos Polis),\" 'the city of the emperor [king]'.\n\nThe modern Turkish name for the city, \"İstanbul\", derives from the Greek phrase \"eis tin polin\" (), meaning \"into the city\" or \"to the city\". This name was used in Turkish alongside \"Kostantiniyye\", the more formal adaptation of the original \"Constantinople\", during the period of Ottoman rule, while western languages mostly continued to refer to the city as Constantinople until the early 20th century. In 1928, the Turkish alphabet was changed from Arabic script to Latin script. After that, as part of the 1920s Turkification movement, Turkey started to urge other countries to use Turkish names for Turkish cities, instead of other transliterations to Latin script that had been used in the Ottoman times. In time the city came to be known as Istanbul and its variations in most world languages.\n\nThe name \"Constantinople\" is still used by members of the Eastern Orthodox Church in the title of one of their most important leaders, the Orthodox patriarch based in the city, referred to as \"His Most Divine All-Holiness the Archbishop of Constantinople New Rome and Ecumenical Patriarch.\" In Greece today, the city is still called Konstantinoúpolis/Konstantinoúpoli (Κωνσταντινούπολη/Κωνσταντινούπολις) or simply just \"the City\" (Η Πόλη / Η Πόλις).\n\nConstantinople was founded by the Roman Emperor Constantine I (272–337 AD) in 324 on the site of an already-existing city, Byzantium, which was settled in the early days of Greek colonial expansion, in around 657 BC, by colonists of the city-state of Megara. This is the first major settlement that would develop on the site of later Constantinople, but the first known settlements was that of \"Lygos\", referred to in Pliny's Natural Histories, which was a village or town of suggested Thracian origin that was believed to have been founded in the 13th to 11th centuries BC. Apart from this, little is known about this initial settlement, except that it was abandoned by the time the Megarian colonists settled the site anew.\n\nThe city maintained independence as a city-state until it was annexed by Darius I in 512 BC into the Persian Empire, who saw the site as the optimal location to construct a pontoon bridge crossing into Europe as Byzantium was situated at the narrowest point in the Bosphorus strait. Persian rule lasted until 478 BC when as part of the Greek counterattack to the Second Persian Invasion of Greece, a Greek army led by the Spartan general Pausanias captured the city which remained an independent, yet subordinate, city under the Athenians, and later to the Spartans after 411 BC. A farsighted treaty with the emergent power of Rome in c.150 BC which stipulated tribute in exchange for independent status allowed it to enter Roman rule unscathed. This treaty would pay dividends retrospectively as Byzantium would maintain this independent status, and prosper under peace and stability in the Pax Romana, for nearly three centuries until the late 2nd century AD.\n\nByzantium was never a major influential city-state like that of Athens, Corinth, and Sparta, but the city enjoyed relative peace and steady growth as a prosperous trading city lent by its remarkable position. The site lay astride the land route from Europe to Asia and the seaway from the Black Sea to the Mediterranean, and had in the Golden Horn an excellent and spacious harbour. Already then, in Greek and early Roman times, Byzantium was famous for its strategic geographic position that made it difficult to besiege and capture, and its position at the crossroads of the Asiatic-European trade route over land and as the gateway between the Mediterranean and Black Seas made it too valuable of a settlement to abandon, as Emperor Septimius Severus later realized when he razed the city to the ground for supporting Pescennius Niger's claimancy. It was a move greatly criticized by the contemporary consul and historian Cassius Dio who said that Severus had destroyed \"a strong Roman outpost and a base of operations against the barbarians from Pontus and Asia\". He would later rebuild Byzantium towards the end of his reign, in which it would be briefly renamed \"Augusta Antonina\", fortifying it with a new city wall in his name, the Severan Wall.\n\nConstantine had altogether more colourful plans. Having restored the unity of the Empire, and, being in course of major governmental reforms as well as of sponsoring the consolidation of the Christian church, he was well aware that Rome was an unsatisfactory capital. Rome was too far from the frontiers, and hence from the armies and the imperial courts, and it offered an undesirable playground for disaffected politicians. Yet it had been the capital of the state for over a thousand years, and it might have seemed unthinkable to suggest that the capital be moved to a different location. Nevertheless, Constantine identified the site of Byzantium as the right place: a place where an emperor could sit, readily defended, with easy access to the Danube or the Euphrates frontiers, his court supplied from the rich gardens and sophisticated workshops of Roman Asia, his treasuries filled by the wealthiest provinces of the Empire.\n\nConstantinople was built over 6 years, and consecrated on 11 May 330. Constantine divided the expanded city, like Rome, into 14 regions, and ornamented it with public works worthy of an imperial metropolis. Yet, at first, Constantine's new Rome did not have all the dignities of old Rome. It possessed a proconsul, rather than an urban prefect. It had no praetors, tribunes, or quaestors. Although it did have senators, they held the title \"clarus\", not \"clarissimus\", like those of Rome. It also lacked the panoply of other administrative offices regulating the food supply, police, statues, temples, sewers, aqueducts, or other public works. The new programme of building was carried out in great haste: columns, marbles, doors, and tiles were taken wholesale from the temples of the empire and moved to the new city. In similar fashion, many of the greatest works of Greek and Roman art were soon to be seen in its squares and streets. The emperor stimulated private building by promising householders gifts of land from the imperial estates in Asiana and Pontica and on 18 May 332 he announced that, as in Rome, free distributions of food would be made to the citizens. At the time, the amount is said to have been 80,000 rations a day, doled out from 117 distribution points around the city.\n\nConstantine laid out a new square at the centre of old Byzantium, naming it the Augustaeum. The new senate-house (or Curia) was housed in a basilica on the east side. On the south side of the great square was erected the Great Palace of the Emperor with its imposing entrance, the Chalke, and its ceremonial suite known as the Palace of Daphne. Nearby was the vast Hippodrome for chariot-races, seating over 80,000 spectators, and the famed Baths of Zeuxippus. At the western entrance to the Augustaeum was the Milion, a vaulted monument from which distances were measured across the Eastern Roman Empire.\n\nFrom the Augustaeum led a great street, the Mese (Greek: Μέση [Οδός] lit. \"Middle [Street]\"), lined with colonnades. As it descended the First Hill of the city and climbed the Second Hill, it passed on the left the Praetorium or law-court. Then it passed through the oval Forum of Constantine where there was a second Senate-house and a high column with a statue of Constantine himself in the guise of Helios, crowned with a halo of seven rays and looking toward the rising sun. From there, the Mese passed on and through the Forum Tauri and then the Forum Bovis, and finally up the Seventh Hill (or Xerolophus) and through to the Golden Gate in the Constantinian Wall. After the construction of the Theodosian Walls in the early 5th century, it was extended to the new Golden Gate, reaching a total length of seven Roman miles.\n\nThe importance of Constantinople increased, but it was gradual. From the death of Constantine in 337 to the accession of Theodosius I, emperors had been resident only in the years 337-8, 347–51, 358–61, 368–69. Its status as a capital was recognized by the appointment of the first known Urban Prefect of the City Honoratus, who took office on 11 December 359 and until 361. The urban prefects had concurrent jurisdiction over three provinces each in the adjacent dioceses of Thrace (in which the city was located), Pontus and Asia comparable to the 100 mile extraordinary jurisdiction of the prefect of Rome. The emperor Valens who hated the city and spent only one year there nevertheless built the Palace of Hebdomon on the shore of the Propontis near the Golden Gate, probably for use when reviewing troops. All the emperors up to Zeno and Basiliscus were crowned and acclaimed at the Hebdomon. Theodosius I founded the Church of John the Baptist to house the skull of the saint (today preserved at the Topkapı Palace in Istanbul, Turkey), put up a memorial pillar to himself in the Forum of Taurus, and turned the ruined temple of Aphrodite into a coach house for the Praetorian Prefect; Arcadius built a new forum named after himself on the Mese, near the walls of Constantine.\n\nAfter the shock of the Battle of Adrianople in 378, in which the emperor Valens with the flower of the Roman armies was destroyed by the Visigoths within a few days' march, the city looked to its defences, and in 413–414, Theodosius II built the 18-metre (60-foot)-tall triple-wall fortifications, which were never to be breached until the coming of gunpowder. Theodosius also founded a University near the Forum of Taurus, on 27 February 425.\n\nUldin, a prince of the Huns, appeared on the Danube about this time and advanced into Thrace, but he was deserted by many of his followers, who joined with the Romans in driving their king back north of the river. Subsequent to this, new walls were built to defend the city, and the fleet on the Danube improved.\n\nIn due course, the barbarians overran the Western Roman Empire: Its emperors retreated to Ravenna, and it diminished to nothing. Thereafter, Constantinople became in truth the largest city of the Roman Empire and of the world. Emperors were no longer peripatetic between various court capitals and palaces. They remained in their palace in the Great City, and sent generals to command their armies. The wealth of the eastern Mediterranean and western Asia flowed into Constantinople.\n\nThe emperor Justinian I (527–565) was known for his successes in war, for his legal reforms and for his public works. It was from Constantinople that his expedition for the reconquest of the former Diocese of Africa set sail on or about 21 June 533. Before their departure, the ship of the commander Belisarius was anchored in front of the Imperial palace, and the Patriarch offered prayers for the success of the enterprise. After the victory, in 534, the Temple treasure of Jerusalem, looted by the Romans in 70 AD and taken to Carthage by the Vandals after their sack of Rome in 455, was brought to Constantinople and deposited for a time, perhaps in the Church of St. Polyeuctus, before being returned to Jerusalem in either the Church of the Resurrection or the New Church.\n\nChariot-racing had been important in Rome for centuries. In Constantinople, the hippodrome became over time increasingly a place of political significance. It was where (as a shadow of the popular elections of old Rome) the people by acclamation showed their approval of a new emperor, and also where they openly criticized the government, or clamoured for the removal of unpopular ministers. In the time of Justinian, public order in Constantinople became a critical political issue.\n\nThroughout the late Roman and early Byzantine periods, Christianity was resolving fundamental questions of identity, and the dispute between the orthodox and the monophysites became the cause of serious disorder, expressed through allegiance to the horse-racing parties of the Blues and the Greens. The partisans of the Blues and the Greens were said to affect untrimmed facial hair, head hair shaved at the front and grown long at the back, and wide-sleeved tunics tight at the wrist; and to form gangs to engage in night-time muggings and street violence. At last these disorders took the form of a major rebellion of 532, known as the \"Nika\" riots (from the battle-cry of \"Victory!\" of those involved).\n\nFires started by the Nika rioters consumed Constantine's basilica of Hagia Sophia (Holy Wisdom), the city's principal church, which lay to the north of the Augustaeum. Justinian commissioned Anthemius of Tralles and Isidore of Miletus to replace it with a new and incomparable Hagia Sophia. This was the great cathedral of the Orthodox Church, whose dome was said to be held aloft by God alone, and which was directly connected to the palace so that the imperial family could attend services without passing through the streets. The dedication took place on 26 December 537 in the presence of the emperor, who exclaimed, \"O Solomon, I have outdone thee!\" Hagia Sophia was served by 600 people including 80 priests, and cost 20,000 pounds of gold to build.\n\nJustinian also had Anthemius and Isidore demolish and replace the original Church of the Holy Apostles built by Constantine with a new church under the same dedication. This was designed in the form of an equal-armed cross with five domes, and ornamented with beautiful mosaics. This church was to remain the burial place of the Emperors from Constantine himself until the 11th century. When the city fell to the Turks in 1453, the church was demolished to make room for the tomb of Mehmet II the Conqueror. Justinian was also concerned with other aspects of the city's built environment, legislating against the abuse of laws prohibiting building within of the sea front, in order to protect the view.\n\nDuring Justinian I's reign, the city's population reached about 500,000 people. However, the social fabric of Constantinople was also damaged by the onset of the Plague of Justinian between 541–542 AD. It killed perhaps 40% of the city's inhabitants.\n\nIn the early 7th century, the Avars and later the Bulgars overwhelmed much of the Balkans, threatening Constantinople to attack from the west. Simultaneously, the Persian Sassanids overwhelmed the Prefecture of the East and penetrated deep into Anatolia. Heraclius, son to the exarch of Africa, set sail for the city and assumed the purple. He found the military situation so dire that he is said at first to have contemplated withdrawing the imperial capital to Carthage, but relented after the people of Constantinople begged him to stay. The citizens lost their right to free grain in 618 when Heraclius realised that the city no longer could be supplied from Egypt as a result of the Persian wars: the population dropped substantially in size as a result.\n\nWhile the city withstood a siege by the Sasanids and Avars in 626, Heraclius campaigned deep into Persian territory and briefly restored the \"status quo\" in 628, when the Persians surrendered all their conquests. However, further sieges followed in the course of attacks from the Arabs, a first from 674 to 678, and a second from 717 to 718. At this time the Theodosian Walls kept the city impregnable from the land, while a newly discovered incendiary substance known as \"Greek Fire\" allowed the Byzantine navy to destroy the Arab fleets and keep the city supplied. In the second siege, the Second ruler of Bulgaria, Khan Tervel (also called St. Triveliy) rendered decisive help, largely due to him the city was saved; he was called \"Saviour of Europe\".\n\nIn the 730s Leo III carried out extensive repairs of the Theodosian walls, which had been damaged by frequent and violent attacks; this work was financed by a special tax on all the subjects of the Empire.\n\nTheodora, widow of the Emperor Theophilus (died 842), acted as regent during the minority of her son Michael III, who was said to have been introduced to dissolute habits by her brother Bardas. When Michael assumed power in 856, he became known for excessive drunkenness, appeared in the hippodrome as a charioteer and burlesqued the religious processions of the clergy. He removed Theodora from the Great Palace to the Carian Palace and later to the monastery of Gastria, but, after the death of Bardas, she was released to live in the palace of St Mamas; she also had a rural residence at the Anthemian Palace, where Michael was assassinated in 867.\n\nIn 860, an attack was made on the city by a new principality set up a few years earlier at Kiev by Askold and Dir, two Varangian chiefs: Two hundred small vessels passed through the Bosporus and plundered the monasteries and other properties on the suburban Prince's Islands. Oryphas, the admiral of the Byzantine fleet, alerted the emperor Michael, who promptly put the invaders to flight; but the suddenness and savagery of the onslaught made a deep impression on the citizens.\n\nIn 980, the emperor Basil II received an unusual gift from Prince Vladimir of Kiev: 6,000 Varangian warriors, which Basil formed into a new bodyguard known as the Varangian Guard. They were known for their ferocity, honour, and loyalty. It is said that, in 1038, they were dispersed in winter quarters in the Thracesian theme when one of their number attempted to violate a countrywoman, but in the struggle she seized his sword and killed him; instead of taking revenge, however, his comrades applauded her conduct, compensated her with all his possessions, and exposed his body without burial as if he had committed suicide. However, following the death of an Emperor, they became known also for plunder in the Imperial palaces. Later in the 11th Century the Varangian Guard became dominated by Anglo-Saxons who preferred this way of life to subjugation by the new Norman kings of England.\n\nThe \"Book of the Eparch\", which dates to the 10th century, gives a detailed picture of the city's commercial life and its organization at that time. The corporations in which the tradesmen of Constantinople were organised were supervised by the Eparch, who regulated such matters as production, prices, import, and export. Each guild had its own monopoly, and tradesmen might not belong to more than one. It is an impressive testament to the strength of tradition how little these arrangements had changed since the office, then known by the Latin version of its title, had been set up in 330 to mirror the urban prefecture of Rome.\n\nIn the 9th and 10th centuries, Constantinople had a population of between 500,000 and 800,000.\n\nIn the 8th and 9th centuries, the iconoclast movement caused serious political unrest throughout the Empire. The emperor Leo III issued a decree in 726 against images, and ordered the destruction of a statue of Christ over one of the doors of the Chalke, an act that was fiercely resisted by the citizens. Constantine V convoked a church council in 754, which condemned the worship of images, after which many treasures were broken, burned, or painted over with depictions of trees, birds or animals: One source refers to the church of the Holy Virgin at Blachernae as having been transformed into a \"fruit store and aviary\". Following the death of her son Leo IV in 780, the empress Irene restored the veneration of images through the agency of the Second Council of Nicaea in 787.\n\nThe iconoclast controversy returned in the early 9th century, only to be resolved once more in 843 during the regency of Empress Theodora, who restored the icons. These controversies contributed to the deterioration of relations between the Western and the Eastern Churches.\n\nIn the late 11th century catastrophe struck with the unexpected and calamitous defeat of the imperial armies at the Battle of Manzikert in Armenia in 1071. The Emperor Romanus Diogenes was captured. The peace terms demanded by Alp Arslan, sultan of the Seljuk Turks, were not excessive, and Romanus accepted them. On his release, however, Romanus found that enemies had placed their own candidate on the throne in his absence; he surrendered to them and suffered death by torture, and the new ruler, Michael VII Ducas, refused to honour the treaty. In response, the Turks began to move into Anatolia in 1073. The collapse of the old defensive system meant that they met no opposition, and the empire's resources were distracted and squandered in a series of civil wars. Thousands of Turkoman tribesmen crossed the unguarded frontier and moved into Anatolia. By 1080, a huge area had been lost to the Empire, and the Turks were within striking distance of Constantinople.\n\nUnder the Comnenian dynasty (1081–1185), Byzantium staged a remarkable recovery. In 1090–91, the nomadic Pechenegs reached the walls of Constantinople, where Emperor Alexius I with the aid of the Kipchaks annihilated their army. In response to a call for aid from Alexius, the First Crusade assembled at Constantinople in 1096, but declining to put itself under Byzantine command set out for Jerusalem on its own account. John II built the monastery of the Pantocrator (Almighty) with a hospital for the poor of 50 beds.\n\nWith the restoration of firm central government, the empire became fabulously wealthy. The population was rising (estimates for Constantinople in the 12th century vary from some 100,000 to 500,000), and towns and cities across the realm flourished. Meanwhile, the volume of money in circulation dramatically increased. This was reflected in Constantinople by the construction of the Blachernae palace, the creation of brilliant new works of art, and general prosperity at this time: an increase in trade, made possible by the growth of the Italian city-states, may have helped the growth of the economy. It is certain that the Venetians and others were active traders in Constantinople, making a living out of shipping goods between the Crusader Kingdoms of Outremer and the West, while also trading extensively with Byzantium and Egypt. The Venetians had factories on the north side of the Golden Horn, and large numbers of westerners were present in the city throughout the 12th century. Toward the end of Manuel I Komnenos's reign, the number of foreigners in the city reached about 60,000–80,000 people out of a total population of about 400,000 people. In 1171, Constantinople also contained a small community of 2,500 Jews. In 1182, all Latin (Western European) inhabitants of Constantinople were massacred.\n\nIn artistic terms, the 12th century was a very productive period. There was a revival in the mosaic art, for example: Mosaics became more realistic and vivid, with an increased emphasis on depicting three-dimensional forms. There was an increased demand for art, with more people having access to the necessary wealth to commission and pay for such work. According to N.H. Baynes (\"Byzantium, An Introduction to East Roman Civilization\"):\n\nOn 25 July 1197, Constantinople was struck by a severe fire which burned the Latin Quarter and the area around the Gate of the Droungarios () on the Golden Horn. Nevertheless, the destruction wrought by the 1197 fire paled in comparison with that brought by the Crusaders. In the course of a plot between Philip of Swabia, Boniface of Montferrat and the Doge of Venice, the Fourth Crusade was, despite papal excommunication, diverted in 1203 against Constantinople, ostensibly promoting the claims of Alexius, son of the deposed emperor Isaac. The reigning emperor Alexius III had made no preparation. The Crusaders occupied Galata, broke the defensive chain protecting the Golden Horn and entered the harbour, where on 27 July they breached the sea walls: Alexius III fled. But the new Alexius IV found the Treasury inadequate, and was unable to make good the rewards he had promised to his western allies. Tension between the citizens and the Latin soldiers increased. In January 1204, the \"protovestiarius\" Alexius Murzuphlus provoked a riot, it is presumed, to intimidate Alexius IV, but whose only result was the destruction of the great statue of Athena, the work of Phidias, which stood in the principal forum facing west.\n\nIn February, the people rose again: Alexius IV was imprisoned and executed, and Murzuphlus took the purple as Alexius V. He made some attempt to repair the walls and organise the citizenry, but there had been no opportunity to bring in troops from the provinces and the guards were demoralised by the revolution. An attack by the Crusaders on 6 April failed, but a second from the Golden Horn on 12 April succeeded, and the invaders poured in. Alexius V fled. The Senate met in Hagia Sophia and offered the crown to Theodore Lascaris, who had married into the Angelid family, but it was too late. He came out with the Patriarch to the Golden Milestone before the Great Palace and addressed the Varangian Guard. Then the two of them slipped away with many of the nobility and embarked for Asia. By the next day the Doge and the leading Franks were installed in the Great Palace, and the city was given over to pillage for three days.\n\nSir Steven Runciman, historian of the Crusades, wrote that the sack of Constantinople is “unparalleled in history”.\n\nFor the next half-century, Constantinople was the seat of the Latin Empire. Under the rulers of the Latin Empire, the city declined, both in population and the condition of its buildings. Alice-Mary Talbot cites an estimated population for Constantinople of 400,000 inhabitants; after the destruction wrought by the Crusaders on the city, about one third were homeless, and numerous courtiers, nobility, and higher clergy, followed various leading personages into exile. \"As a result Constantinople became seriously depopulated,\" Talbot concludes.\n\nThe Latins took over at least 20 churches and 13 monasteries, most prominently the Hagia Sophia, which became the cathedral of the Latin Patriarch of Constantinople. It is to these that E.H. Swift attributed the construction of a series of flying buttresses to shore up the walls of the church, which had been weakened over the centuries by earthquake tremors. However, this act of maintenance is an exception: for the most part, the Latin occupiers were too few to maintain all of the buildings, either secular and sacred, and many became targets for vandalism or dismantling. Bronze and lead were removed from the roofs of abandoned buildings and melted down and sold to provide money to the chronically under-funded Empire for defense and to support the court; Deno John Geanokoplos writes that \"it may well be that a division is suggested here: Latin laymen stripped secular buildings, ecclesiastics, the churches.\" Buildings were not the only targets of officials looking to raise funds for the impoverished Latin Empire: the monumental sculptures which adorned the Hippodrome and fora of the city were pulled down and melted for coinage. \"Among the masterpieces destroyed, writes Talbot, \"were a Herakles attributed to the fourth-century B.C. sculptor Lysippos, and monumental figures of Hera, Paris, and Helen.\"\n\nThe Nicaean emperor John III Vatatzes reportedly saved several churches from being dismantled for their valuable building materials; by sending money to the Latins \"to buy them off\" (\"exonesamenos\"), he prevented the destruction of several churches. According to Talbot, these included the churches of Blachernae, Rouphinianai, and St. Michael at Anaplous. He also granted funds for the restoration of the Church of the Holy Apostles, which had been seriously damaged in an earthquake.\n\nThe Byzantine nobility scattered, many going to Nicaea, where Theodore Lascaris set up an imperial court, or to Epirus, where Theodore Angelus did the same; others fled to Trebizond, where one of the Comneni had already with Georgian support established an independent seat of empire. Nicaea and Epirus both vied for the imperial title, and tried to recover Constantinople. In 1261, Constantinople was captured from its last Latin ruler, Baldwin II, by the forces of the Nicaean emperor Michael VIII Palaiologos.\n\nAlthough Constantinople was retaken by Michael VIII Palaiologos, the Empire had lost many of its key economic resources, and struggled to survive. The palace of Blachernae in the north-west of the city became the main Imperial residence, with the old Great Palace on the shores of the Bosporus going into decline. When Michael VIII captured the city, its population was 35,000 people, but, by the end of his reign, he had succeeded in increasing the population to about 70,000 people. The Emperor achieved this by summoning former residents having fled the city when the crusaders captured it, and by relocating Greeks from the recently reconquered Peloponnese to the capital. In 1347, the Black Death spread to Constantinople. In 1453, when the Ottoman Turks captured the city, it contained approximately 50,000 people.\n\nConstantinople was conquered by the Ottoman Empire on 29 May 1453. The Ottomans were commanded by 22-year-old Ottoman Sultan Mehmed II. The conquest of Constantinople followed a seven-week siege that had begun on 6 April 1453.\n\nThe Christian Orthodox city of Constantinople was now under Ottoman control. When Mehmed II finally entered Constantinople through what is now known as the Topkapi Gate, he immediately rode his horse to the Hagia Sophia, where he ordered his soldiers to stop hacking at the marbles and 'be satisfied with the booty and captives; as for all the buildings, they belonged to him'. He ordered that an imam meet him there in order to chant the adhan thus transforming the Orthodox cathedral into a Muslim mosque, solidifying Islamic rule in Constantinople.\n\nMehmed’s main concern with Constantinople had to do with rebuilding the city’s defenses and population. Building projects were commenced immediately after the conquest, which included the repair of the walls, construction of the citadel, and building a new palace. Mehmed issued orders across his empire that Muslims, Christians, and Jews should resettle the city; he demanded that five thousand households needed to be transferred to Constantinople by September. From all over the Islamic empire, prisoners of war and deported people were sent to the city: these people were called \"Sürgün\" in Turkish (). Two centuries later, Ottoman traveler Evliya Çelebi gave a list of groups introduced into the city with their respective origins. Even today, many quarters of Istanbul, such as Aksaray, Çarşamba, bear the names of the places of origin of their inhabitants. However, many people escaped again from the city, and there were several outbreaks of plague, so that in 1459 Mehmet allowed the deported Greeks to come back to the city.\n\nConstantinople was the largest and richest urban center in the Eastern Mediterranean Sea during the late Eastern Roman Empire, mostly as a result of its strategic position commanding the trade routes between the Aegean Sea and the Black Sea. It would remain the capital of the eastern, Greek-speaking empire for over a thousand years. At its peak, roughly corresponding to the Middle Ages, it was the richest and largest European city, exerting a powerful cultural pull and dominating economic life in the Mediterranean. Visitors and merchants were especially struck by the beautiful monasteries and churches of the city, in particular, Hagia Sophia, or the Church of Holy Wisdom: A Russian 14th-century traveler, Stephen of Novgorod, wrote, \"As for Hagia Sophia, the human mind can neither tell it nor make description of it.\"\n\nIt was especially important for preserving in its libraries manuscripts of Greek and Latin authors throughout a period when instability and disorder caused their mass-destruction in western Europe and north Africa: On the city's fall, thousands of these were brought by refugees to Italy, and played a key part in stimulating the Renaissance, and the transition to the modern world. The cumulative influence of the city on the west, over the many centuries of its existence, is incalculable. In terms of technology, art and culture, as well as sheer size, Constantinople was without parallel anywhere in Europe for a thousand years.\n\nArmenians, Syrians, Slavs and Georgians were part of the Byzantine social hierarchy.\n\nThe city provided a defence for the eastern provinces of the old Roman Empire against the barbarian invasions of the 5th century. The 18-meter-tall walls built by Theodosius II were, in essence, impregnable to the barbarians coming from south of the Danube river, who found easier targets to the west rather than the richer provinces to the east in Asia. From the 5th century, the city was also protected by the Anastasian Wall, a 60-kilometer chain of walls across the Thracian peninsula. Many scholars argue that these sophisticated fortifications allowed the east to develop relatively unmolested while Ancient Rome and the west collapsed. With the emergence of Christianity and the rise of Islam, Constantinople became the last bastion of Christian Europe, standing at the fore of Islamic expansion, and repelling its influence. As the Byzantine Empire was situated in-between the Islamic world and the Christian west, so did Constantinople act as Europe’s first line-of-defence against Arab advances in the 7th and 8th centuries. The city, and the Empire, would ultimately fall to the Ottomans by 1453, but its enduring legacy had provided Europe centuries of resurgence following the collapse of Rome.\n\nConstantinople's fame was such that it was described even in contemporary Chinese histories, the \"Old\" and \"New Book of Tang\", which mentioned its massive walls and gates as well as a purported clepsydra mounted with a golden statue of a man. The Chinese histories even related how the city had been besieged in the 7th century by Muawiyah I and how he exacted tribute in a peace settlement.\n\nThe Byzantine Empire used Roman and Greek architectural models and styles to create its own unique type of architecture. The influence of Byzantine architecture and art can be seen in the copies taken from it throughout Europe. Particular examples include St Mark's Basilica in Venice, the basilicas of Ravenna, and many churches throughout the Slavic East. Also, alone in Europe until the 13th-century Italian florin, the Empire continued to produce sound gold coinage, the solidus of Diocletian becoming the bezant prized throughout the Middle Ages. Its city walls were much imitated (for example, see Caernarfon Castle) and its urban infrastructure was moreover a marvel throughout the Middle Ages, keeping alive the art, skill and technical expertise of the Roman Empire. In the Ottoman period Islamic architecture and symbolism were used.\n\nConstantine's foundation gave prestige to the Bishop of Constantinople, who eventually came to be known as the Ecumenical Patriarch, and made it a prime center of Christianity alongside Rome. This contributed to cultural and theological differences between Eastern and Western Christianity eventually leading to the Great Schism that divided Western Catholicism from Eastern Orthodoxy from 1054 onwards. Constantinople is also of great religious importance to Islam, as the conquest of Constantinople is one of the signs of the End time in Islam.\n\n\n\n\n", "id": "5646", "title": "Constantinople"}
{"url": "https://en.wikipedia.org/wiki?curid=5647", "text": "Columbus\n\nColumbus is a Latinized version of the Italian surname \"\"Colombo\"\". It most commonly refers to:\n\n\nColumbus may also refer to:\n\n\nIn the United States, Columbus may refer to:\n\n\n\n", "id": "5647", "title": "Columbus"}
{"url": "https://en.wikipedia.org/wiki?curid=5648", "text": "Cornwall\n\nCornwall ( or ; ) is a ceremonial county and unitary authority area of England within the United Kingdom. It is bordered to the north and west by the Celtic Sea, to the south by the English Channel, and to the east by the county of Devon, over the River Tamar. Cornwall has a population of and covers an area of . The administrative centre, and only city in Cornwall, is Truro, although the town of Falmouth has the largest population for a civil parish and the conurbation of Camborne, Pool and Redruth has the highest total population.\n\nCornwall forms the westernmost part of the south-west peninsula of the island of Great Britain, and a large part of the Cornubian batholith is within Cornwall. This area was first inhabited in the Palaeolithic and Mesolithic periods. It continued to be occupied by Neolithic and then Bronze Age peoples, and later (in the Iron Age) by Brythons with distinctive cultural relations to neighbouring Wales and Brittany. There is little evidence that Roman rule was effective west of Exeter and few Roman remains have been found. Cornwall was the home of a division of the Dumnonii tribe – whose tribal centre was in the modern county of Devon – known as the Cornovii, separated from the Brythons of Wales after the Battle of Deorham, often coming into conflict with the expanding kingdom of Wessex before King Athelstan in AD 936 set the boundary between English and Cornish at the high water mark of the eastern bank of the River Tamar. From the early Middle Ages, British language and culture was apparently shared by Brythons trading across both sides of the Channel, evidenced by the corresponding high medieval Breton kingdoms of Domnonée and Cornouaille and the Celtic Christianity common to both territories.\n\nHistorically tin mining was important in the Cornish economy, becoming increasingly significant during the High Middle Ages and expanding greatly during the 19th century when rich copper mines were also in production. In the mid-19th century, however, the tin and copper mines entered a period of decline. Subsequently, china clay extraction became more important and metal mining had virtually ended by the 1990s. Traditionally, fishing (particularly of pilchards) and agriculture (notably dairy products and vegetables) were the other important sectors of the economy. Railways led to a growth of tourism in the 20th century; however, Cornwall's economy struggled after the decline of the mining and fishing industries. The area is noted for its wild moorland landscapes, its long and varied coastline, its attractive villages, its many place-names derived from the Cornish language, and its very mild climate. Extensive stretches of Cornwall's coastline, and Bodmin Moor, are protected as an Area of Outstanding Natural Beauty.\n\nCornwall is the homeland of the Cornish people and is recognised as one of the Celtic nations, retaining a distinct cultural identity that reflects its history. Some people question the present constitutional status of Cornwall, and a nationalist movement seeks greater autonomy within the United Kingdom in the form of a devolved legislative Cornish Assembly. On 24 April 2014 it was announced that Cornish people will be granted minority status under the European Framework Convention for the Protection of National Minorities.\n\nThe modern English name \"Cornwall\" derives from the concatenation of two ancient demonyms from different linguistic traditions:\n\nCorn- records the native Brythonic tribe, the \"Cornovii\" (\"peninsula people\"). The Celtic word \"\"kernou\"\" (\"horn\" or \"headland\") is cognate with the English word \"horn\" (both deriving from the Proto-Indo-European *ker-).\n-wall derives from the Old English exonym \"w(e)alh\", meaning \"foreigner\" or \"Roman\" (i.e. a Welshman).\n\nThe Ravenna Cosmography first mentions a city named \"Purocoronavis\" in the locality. This is thought to be a Latinized rendering of \"Duro-cornov-ium\", meaning 'fort of the Cornovii'. The exact location of Durocornovium is disputed, with Tintagel and Carn Brea suggested as possible sites.\nIn later times, Cornwall was known to the Anglo-Saxons as \"West Wales\" to distinguish it from \"North Wales\" (the modern nation of Wales). The name appears in the \"Anglo-Saxon Chronicle\" in 891 as \"On Corn walum\". In the Domesday Book it was referred to as \"Cornualia\" and in c. 1198 as \"Cornwal\".\n\nOther names for the county include a latinisation of the name as \"Cornubia\" (first appears in a mid-9th-century deed purporting to be a copy of one dating from c. 705), and as \"Cornugallia\" in 1086.\n\nThe present human history of Cornwall begins with the reoccupation of Britain after the last Ice Age. The area now known as Cornwall was first inhabited in the Palaeolithic and Mesolithic periods. It continued to be occupied by Neolithic and then Bronze Age people. According to John T. Koch and others, Cornwall in the Late Bronze Age was part of a maritime trading-networked culture called the Atlantic Bronze Age, in modern-day Ireland, England, France, Spain and Portugal. During the British Iron Age, Cornwall, like all of Britain south of the Firth of Forth, was inhabited by a Celtic people known as the Britons with distinctive cultural relations to neighbouring Wales and Brittany. The Common Brittonic spoken at the time eventually developed into several distinct tongues, including Cornish.\n\nThe first account of Cornwall comes from the 1st century BC Sicilian Greek historian Diodorus Siculus, supposedly quoting or paraphrasing the 4th-century BCE geographer Pytheas, who had sailed to Britain:\nThe identity of these merchants is unknown. It has been theorised that they were Phoenicians, but there is no evidence for this. Professor Timothy Champion, discussing Diodorus Siculus's comments on the tin trade, states that \"Diodorus never actually says that the Phoenicians sailed to Cornwall. In fact, he says quite the opposite: the production of Cornish tin was in the hands of the natives of Cornwall, and its transport to the Mediterranean was organised by local merchants, by sea and then over land through France, well outside Phoenician control.\" (For further discussion of tin mining see the section on the economy below.)\n\nThere is little evidence that Roman rule was effective west of Exeter in Devon and few Roman remains have been found. However, after 410, Cornwall appears to have reverted to rule by Romano-Celtic chieftains of the Cornovii tribe as part of Dumnonia including one Marcus Cunomorus with at least one significant power base at Tintagel. 'King' Mark of Cornwall is a semi-historical figure known from Welsh literature, the Matter of Britain, and in particular, the later Norman-Breton medieval romance of Tristan and Yseult where he is regarded as a close kinsman of King Arthur; himself usually considered to be born of the Cornish people in folklore traditions derived from Geoffrey of Monmouth's \"Historia Regum Britanniae\".\nArchaeology supports ecclesiastical, literary and legendary evidence for some relative economic stability and close cultural ties between the sub-Roman Westcountry, South Wales, Brittany and Ireland through the fifth and sixth centuries.\n\nThe Battle of Deorham in 577 saw the separation of Dumnonia (and therefore Cornwall) from Wales, following which the Dumnonii often came into conflict with the expanding English kingdom of Wessex. The \"Annales Cambriae\" report that in 722 AD the Britons of Cornwall won a battle at \"Hehil\". It seems likely that the enemy the Cornish fought was a West Saxon force, as evidenced by the naming of King Ine of Wessex and his kinsman Nonna in reference to an earlier Battle of Lining in 710.\n\nThe \"Anglo-Saxon Chronicle\" stated in 815 (adjusted date) \"and in this year king Ecgbryht raided in Cornwall from east to west.\" and thenceforth apparently held it as a ducatus or dukedom annexed to his regnum or kingdom of Wessex, but not wholly incorporated with it. The \"Anglo-Saxon Chronicle\" states that in 825 (adjusted date) a battle took place between the Wealas (Cornish) and the Defnas (men of Devon) at Gafulforda. In the same year Ecgbert, as a later document expresses it, \"disposed of their territory as it seemed fit to him, giving a tenth part of it to God.\" In other words, he incorporated Cornwall ecclesiastically with the West Saxon diocese of Sherborne, and endowed Eahlstan, his fighting bishop, who took part in the campaign, with an extensive Cornish estate consisting of Callington and Lawhitton, both in the Tamar valley, and Pawton near Padstow.\n\nIn 838, the Cornish and their Danish allies were defeated by Egbert in the Battle of Hingston Down at Hengestesdune (probably Hingston Down in Cornwall). In 875, the last recorded king of Cornwall, Dumgarth, is said to have drowned. Around the 880s, Anglo-Saxons from Wessex had established modest land holdings in the eastern part of Cornwall; notably Alfred the Great who had acquired a few estates. William of Malmesbury, writing around 1120, says that King Athelstan of England (924–939) fixed the boundary between English and Cornish people at the east bank of the River Tamar.\n\nOne interpretation of the Domesday Book is that by this time the native Cornish landowning class had been almost completely dispossessed and replaced by English landowners, particularly Harold Godwinson himself. However, the Bodmin manumissions show that two leading Cornish figures nominally had Saxon names, but these were both glossed with native Cornish names. Naming evidence cited by medievalist Edith Ditmas suggests that many post-Conquest landowners in Cornwall were Breton allies of the Normans and further proposed this period for the early composition of the Tristan and Iseult cycle by poets such as Beroul from a pre-existing shared Brittonic oral tradition.\n\nSoon after the Norman conquest most of the land was transferred to the new Breton–Norman aristocracy, with the lion's share going to Robert, Count of Mortain, half-brother of King William and the largest landholder in England after the king with his stronghold at Trematon Castle near the mouth of the Tamar. Cornwall and Devon west of Dartmoor showed a very different type of settlement pattern from that of Saxon Wessex and places continued, even after 1066, to be named in the Celtic Cornish tradition with Saxon architecture being uncommon.\n\nSubsequently, however, Norman absentee landlords became replaced by a new Cornu-Norman elite including scholars such as Richard Rufus of Cornwall. These families eventually became the new ruling class of Cornwall (typically speaking Norman French, Breton-Cornish, Latin and eventually English), many becoming involved in the operation of the Stannary Parliament system, Earldom and eventually the Duchy. The Cornish language continued to be spoken and it acquired a number of characteristics establishing its identity as a separate language from Breton.\n\nCornish piracy was active during the Elizabethan era on the west coast of Britain.\n\nMany place names in Cornwall are associated with Christian missionaries described as coming from Ireland and Wales in the 5th century AD and usually called saints (\"See\" List of Cornish saints). The historicity of some of these missionaries is problematic. The patron saint of Wendron Parish Church, \"Saint Wendrona\" is another example. and it has been pointed out by Canon Doble that it was customary in the Middle Ages to ascribe such geographical origins to saints. Some of these saints are not included in the early lists of saints.\n\nSaint Piran, after whom Perranporth is named, is generally regarded as the patron saint of Cornwall. However, in early Norman times it is likely that Saint Michael the Archangel was recognised as the patron saint and is still recognised by the Anglican Church as the \"Protector of Cornwall\". The title has also been claimed for Saint Petroc who was patron of the Cornish diocese prior to the Normans.\n\nThe church in Cornwall until the time of Athelstan of Wessex observed more or less orthodox practices, being completely separate from the Anglo-Saxon church until then (and perhaps later). The See of Cornwall continued until much later: Bishop Conan apparently in place previously, but (re-?) consecrated in 931 AD by Athelstan. However, it is unclear whether he was the sole Bishop for Cornwall or the leading Bishop in the area. The situation in Cornwall may have been somewhat similar to Wales where each major religious house corresponded to a cantref (this has the same meaning as Cornish keverang) both being under the supervision of a Bishop. However, if this was so the status of keverangow before the time of King Athelstan is not recorded. However, it can be inferred from the districts included at this period that the minimum number would be three: Triggshire; Wivelshire; and the remaining area. Penwith, Kerrier, Pydar and Powder meet at a central point (Scorrier) which some have believed indicates a fourfold division imposed by Athelstan on a sub-kingdom.\n\nThe whole of Cornwall was in this period in the Archdeaconry of Cornwall within the Diocese of Exeter. From 1267 the archdeacons had a house at Glasney near Penryn. Their duties were to visit and inspect each parish annually and to execute the bishop's orders. Archdeacon Roland is recorded in the Domesday Book of 1086 as having land holdings in Cornwall but he was not Archdeacon of Cornwall, just an archdeacon in the Diocese of Exeter. In the episcopate of William Warelwast (1107–37) the first Archdeacon of Cornwall was appointed (possibly Hugo de Auco). Most of the parish churches in Cornwall in Norman times were not in the larger settlements, and the medieval towns which developed thereafter usually had only a chapel of ease with the right of burial remaining at the ancient parish church. Over a hundred holy wells exist in Cornwall, each associated with a particular saint, though not always the same one as the dedication of the church.\n\nVarious kinds of religious houses existed in mediaeval Cornwall though none of them were nunneries; the benefices of the parishes were in many cases appropriated to religious houses within Cornwall or elsewhere in England or France.\n\nIn the 16th century there was some violent resistance to the replacement of Catholicism with Protestantism in the Prayer Book Rebellion. In 1548 the college at Glasney, a centre of learning and study established by the Bishop of Exeter, had been closed and looted (many manuscripts and documents were destroyed) which aroused resentment among the Cornish. They, among other things, objected to the English language Book of Common Prayer, protesting that the English language was still unknown to many at the time. The Prayer Book Rebellion was a cultural and social disaster for Cornwall; the reprisals taken by the forces of the Crown have been estimated to account for 10–11% of the civilian population of Cornwall. Culturally speaking, it saw the beginning of the slow decline of the Cornish language.\n\nFrom that time Christianity in Cornwall was in the main within the Church of England and subject to the national events which affected it in the next century and a half. Roman Catholicism never became extinct, though openly practised by very few; there were some converts to Puritanism, Anabaptism and Quakerism in certain areas though they suffered intermittent persecution which more or less came to an end in the reign of William and Mary. During the 18th century Cornish Anglicanism was very much in the same state as Anglicanism in most of England. Wesleyan Methodist missions began during John Wesley's lifetime and had great success over a long period during which Methodism itself divided into a number of sects and established a definite separation from the Church of England.\n\nFrom the early 19th to the mid-20th century Methodism was the leading form of Christianity in Cornwall but it is now in decline. The Church of England was in the majority from the reign of Queen Elizabeth until the Methodist revival of the 19th century: before the Wesleyan missions dissenters were very few in Cornwall. The county remained within the Diocese of Exeter until 1876 when the Anglican Diocese of Truro was created (the first Bishop was appointed in 1877). Roman Catholicism was virtually extinct in Cornwall after the 17th century except for a few families such as the Arundells of Lanherne. From the mid-19th century the church reestablished episcopal sees in England, one of these being at Plymouth. Since then immigration to Cornwall has brought more Roman Catholics into the population.\n\nCornwall forms the tip of the south-west peninsula of the island of Great Britain, and is therefore exposed to the full force of the prevailing winds that blow in from the Atlantic Ocean. The coastline is composed mainly of resistant rocks that give rise in many places to impressive cliffs. Cornwall has a border with only one other county, Devon, which is formed almost entirely by the River Tamar and (to the north) by the Marsland Valley.\n\nThe north and south coasts have different characteristics. The north coast on the Celtic Sea, part of the Atlantic Ocean, is more exposed and therefore has a wilder nature. The prosaically named \"High Cliff\", between Boscastle and St Gennys, is the highest sheer-drop cliff in Cornwall at . However, there are also many extensive stretches of fine golden sand which form the beaches that are so important to the tourist industry, such as those at Bude, Polzeath, Watergate Bay, Perranporth, Porthtowan, Fistral Beach, Newquay, St Agnes, St Ives, and on the south coast Gyllyngvase beach in Falmouth and the large beach at Praa Sands further to the south west. There are two river estuaries on the north coast: Hayle Estuary and the estuary of the River Camel, which provides Padstow and Rock with a safe harbour. The seaside town of Newlyn is a popular holiday destination, as it is one of the last remaining traditional Cornish fishing ports, with views reaching over Mount's Bay.\nThe south coast, dubbed the \"Cornish Riviera\", is more sheltered and there are several broad estuaries offering safe anchorages, such as at Falmouth and Fowey. Beaches on the south coast usually consist of coarser sand and shingle, interspersed with rocky sections of wave-cut platform. Also on the south coast, the picturesque fishing village of Polperro, at the mouth of the Pol River, and the fishing port of Looe on the River Looe are both popular with tourists.\n\nThe interior of the county consists of a roughly east–west spine of infertile and exposed upland, with a series of granite intrusions, such as Bodmin Moor, which contains the highest land within Cornwall. From east to west, and with approximately descending altitude, these are Bodmin Moor, Hensbarrow north of St Austell, Carnmenellis to the south of Camborne, and the Penwith or Land's End peninsula. These intrusions are the central part of the granite outcrops that form the exposed parts of the Cornubian batholith of south-west Britain, which also includes Dartmoor to the east in Devon and the Isles of Scilly to the west, the latter now being partially submerged.\nThe intrusion of the granite into the surrounding sedimentary rocks gave rise to extensive metamorphism and mineralisation, and this led to Cornwall being one of the most important mining areas in Europe until the early 20th century. It is thought tin was mined here as early as the Bronze Age, and copper, lead, zinc and silver have all been mined in Cornwall. Alteration of the granite also gave rise to extensive deposits of China Clay, especially in the area to the north of St Austell, and the extraction of this remains an important industry.\n\nThe uplands are surrounded by more fertile, mainly pastoral farmland. Near the south coast, deep wooded valleys provide sheltered conditions for flora that like shade and a moist, mild climate. These areas lie mainly on Devonian sandstone and slate. The north east of Cornwall lies on Carboniferous rocks known as the Culm Measures. In places these have been subjected to severe folding, as can be seen on the north coast near Crackington Haven and in several other locations.\n\nThe geology of the Lizard peninsula is unusual, in that it is mainland Britain's only example of an ophiolite, a section of oceanic crust now found on land. Much of the peninsula consists of the dark green and red Precambrian serpentinite, which forms spectacular cliffs, notably at Kynance Cove, and carved and polished serpentine ornaments are sold in local gift shops. This ultramafic rock also forms a very infertile soil which covers the flat and marshy heaths of the interior of the peninsula. This is home to rare plants, such as the Cornish Heath, which has been adopted as the county flower.\n\nCornwall has varied habitats including terrestrial and marine ecosystems. One noted species in decline locally is the Reindeer lichen, which species has been made a priority for protection under the national UK Biodiversity Action Plan.\n\nBotanists divide Cornwall and Scilly into two vice-counties: West (1) and East (2). The standard flora is by F. H. Davey \"Flora of Cornwall\" (1909). Davey was assisted by A. O. Hume and he thanks Hume, his companion on excursions in Cornwall and Devon, and for help in the compilation of that Flora, publication of which was financed by him.\n\nCornwall has a temperate Oceanic climate (Köppen climate classification: Cfb) and has the mildest and sunniest climate in the United Kingdom, as a result of its southerly latitude and the influence of the Gulf Stream. The average annual temperature in Cornwall ranges from on the Isles of Scilly to in the central uplands. Winters are amongst the warmest in the country due to the southerly latitude and moderating effects of the warm ocean currents, and frost and snow are very rare at the coast and are also rare in the central upland areas. Summers are however not as warm as in other parts of southern England. The surrounding sea and its southwesterly position mean that Cornwall's weather can be relatively changeable.\n\nCornwall is one of the sunniest areas in the UK, with over 1541 hours of sunshine per year, with the highest average of 7.6 hours of sunshine per day in July. The moist, mild air coming from the south west brings higher amounts of rainfall than in eastern Great Britain, at per year, however not as much as in more northern areas of the west coast. The Isles of Scilly, for example, where there are on average less than two days of air frost per year, is the only area in the UK to be in the USDA Hardiness zone 10. In Scilly there is on average less than one day of air temperature exceeding 30 °C per year and it is in the AHS Heat Zone 1. Extreme temperatures in Cornwall are particularly rare; however, extreme weather in the form of storms and floods is common.\n\nWith the exception of the Isles of Scilly, Cornwall is governed by a unitary authority, Cornwall Council, based in Truro. The Crown Court is based at the Courts of Justice in Truro. Magistrates' Courts are found in Truro (but at a different location to the Crown Court), Bodmin, Penzance and Liskeard.\n\nThe Isles of Scilly form part of the ceremonial county of Cornwall and have, at times, been served by the same county administration. Since 1890 they have been administered by their own unitary authority, the Council of the Isles of Scilly. They are grouped with Cornwall for other administrative purposes, such as the National Health Service and Devon and Cornwall Police.\n\nBefore reorganisation on 1 April 2009, council functions throughout the rest of Cornwall were organised on a two-tier basis, with a county council and district councils for its six districts, Caradon, Carrick, Kerrier, North Cornwall, Penwith, and Restormel. While projected to streamline services, cut red tape and save around £17 million a year, the reorganisation was met with wide opposition, with a poll in 2008 giving a result of 89% disapproval from Cornish residents.\n\nThe first elections for the unitary authority were held on 4 June 2009. The council has 123 seats; the largest party (in 2015) is the Liberal Democrats with 37 seats (37), followed by the Independents, 36 seats (23), Conservative Party with 31 seats (46) Labour Party 8 seats (1), UKIP 6 seats (0), Mebyon Kernow with 4 seats (6), Greens 1 seat, (0). (Number of seats prior to 2013 election in brackets.)\n\nBefore the creation of the unitary council, the former county council had 82 seats, the majority of which were held by the Liberal Democrats, elected at the 2005 county council elections. The six former districts had a total of 249 council seats, and the groups with greatest numbers of councillors were Liberal Democrats, Conservatives, and Independents.\n\nFollowing a review by the Boundary Commission for England taking effect at the 2010 general election, Cornwall is divided into six county constituencies to elect MPs to the House of Commons of the United Kingdom.\n\nBefore the 2010 boundary changes Cornwall had five constituencies all of which were won by Liberal Democrats in the 2005 general election. At the 2010 general election Liberal Democrat candidates won three constituencies and Conservative candidates won three constituencies (\"see also 2010 United Kingdom general election result in Cornwall\"). At the 2015 general election all six Cornish seats were won by Conservative candidates.\n\nUntil 1832, Cornwall had 44 MPs – more than any other county – reflecting the importance of tin to the Crown. Most of the increase in numbers of MPs came between 1529 and 1584 after which there was no change until 1832.\n\nCornish nationalists have organised into two political parties: Mebyon Kernow, formed in 1951, and the Cornish Nationalist Party. In addition to the political parties, there are various interest groups such as the Revived Cornish Stannary Parliament and the Celtic League. The Cornish Constitutional Convention was formed in 2000 as a cross-party organisation including representatives from the private, public and voluntary sectors to campaign for the creation of a Cornish Assembly, along the lines of the National Assembly for Wales, Northern Ireland Assembly and the Scottish Parliament. Between 5 March 2000 and December 2001, the campaign collected the signatures of 41,650 Cornish residents endorsing the call for a devolved assembly, along with 8,896 signatories from outside Cornwall. The resulting petition was presented to the Prime Minister, Tony Blair. The Liberal Democrats recognise Cornwall's claims for greater autonomy, as do the Liberal Party.\n\nAn additional political issue is the recognition of the Cornish people as a minority.\n\nCornwall is recognised by several organisations, including the Cornish nationalist party Mebyon Kernow, the Celtic League and the International Celtic Congress, as one of the six Celtic nations, alongside Brittany, Ireland, the Isle of Man, Scotland and Wales. Alongside Asturias and Galicia, Cornwall is also recognised as one of the eight Celtic nations by the Isle of Man Government and the Welsh Government. Cornwall is represented, as one of the Celtic nations, at the \"Festival Interceltique de Lorient\", an annual celebration of Celtic culture held in Brittany.\n\nCornwall Council consider Cornwall's unique cultural heritage and distinctiveness to be one of the area's major assets. They see Cornwall's language, landscape, Celtic identity, political history, patterns of settlement, maritime tradition, industrial heritage, and non-conformist tradition, to be among the features making up its \"distinctive\" culture. However, it is uncertain how many of the people living in Cornwall consider themselves to be Cornish; results from different surveys (including the national census) have varied. In the 2001 census, 7 percent of people in Cornwall identified themselves as Cornish, rather than British or English. However, activists have argued that this underestimated the true number as there was no explicit \"Cornish\" option included in the official census form. Subsequent surveys have suggested that as many as 44 percent identify as Cornish. Many people in Cornwall say that this issue would be resolved if a Cornish option became available on the census. The question and content recommendations for the 2011 Census provided an explanation of the process of selecting an ethnic identity which is relevant to the understanding of the often quoted figure of 37,000 who claim Cornish identity.\n\nOn 24 April 2014 it was announced that Cornish people would be granted minority status under the European Framework Convention for the Protection of National Minorities.\n\nCornwall's only city, and the home of the council headquarters, is Truro. Nearby Falmouth is notable as a port. St Just in Penwith is the westernmost town in England, though the same claim has been made for Penzance, which is larger. St Ives and Padstow are today small vessel ports with a major tourism and leisure sector in their economies. Newquay on the north coast is famous for its beaches and is a popular surfing destination, as is Bude further north. St Austell is the county's largest town and more populous than the capital Truro; it was the centre of the china clay industry in Cornwall. Redruth and Camborne form the largest urban area in Cornwall, and both towns were significant as centres of the global tin mining industry in the 19th century (nearby copper mines were also very productive during that period).\n\nCornwall borders the county of Devon at the River Tamar. Major road links between Cornwall and the rest of Great Britain are the A38 which crosses the Tamar at Plymouth via the Tamar Bridge and the town of Saltash, the A39 road (Atlantic Highway) from Barnstaple, passing through North Cornwall to end in Falmouth, and the A30 which crosses the border south of Launceston crosses Bodmin Moor and connects Bodmin and Truro. Torpoint Ferry links Plymouth with Torpoint on the opposite side of the Hamoaze. A rail bridge, the Royal Albert Bridge, built by Isambard Kingdom Brunel (1859) provides the only other major transport link. The major city of Plymouth, a large urban centre closest to east Cornwall is an important location for services such as hospitals, department stores, road and rail transport, and cultural venues.\n\nNewquay Cornwall International Airport provides an airlink to the rest of the UK, Ireland and Europe.\n\nCardiff and Swansea, across the Bristol Channel, have at some times in the past been connected to Cornwall by ferry, but these do not operate currently.\n\nThe Isles of Scilly are served by ferry (from Penzance) and by aeroplane, having its own airport — St Mary's Airport. There are regular flights between St Mary's and Land's End Airport, near St Just, and Newquay Airport; during the summer season, a service also exist between St Mary's and Exeter International Airport, in Devon.\n\nSaint Piran's Flag is regarded by many as the national flag of Cornwall, and an emblem of the Cornish people; and by others as the county flag. The banner of Saint Piran is a white cross on a black background (in terms of heraldry 'sable, a cross argent'). Saint Piran is supposed to have adopted these two colours from seeing the white tin in the black coals and ashes during his supposed discovery of tin. Davies Gilbert in 1826 described it as anciently the flag of St Piran and the banner of Cornwall, and another history of 1880 said that: \"The white cross of St. Piran was the ancient banner of the Cornish people.\" The Cornish flag is an exact reverse of the former Breton national flag (black cross) and is known by the same name \"Kroaz Du\".\n\nThere are also claims that the patron saint of Cornwall is Saint Michael or Saint Petroc, but Saint Piran is by far the most popular of the three and his emblem is internationally recognised as the flag of Cornwall. St Piran's Day (5 March) is celebrated by the Cornish diaspora around the world.\n\nFor the heraldry of Cornwall see:\n\nCornwall is one of the poorest parts of the United Kingdom in terms of per capita GDP and average household incomes. At the same time, parts of the county, especially on the coast, have high house prices, driven up by demand from relatively wealthy retired people and second-home owners. The GVA per head was 65% of the UK average for 2004. The GDP per head for Cornwall and the Isles of Scilly was 79.2% of the EU-27 average for 2004, the UK per head average was 123.0%. In 2011, the latest available figures, Cornwall's (including the Isles of Scilly) measure of wealth was 64% of the European average per capita.\n\nHistorically mining of tin (and later also of copper) was important in the Cornish economy. The first reference to this appears to be by Pytheas: \"see above\". Julius Caesar was the last classical writer to mention the tin trade, which appears to have declined during the Roman occupation. The tin trade revived in the Middle Ages and its importance to the Kings of England resulted in certain privileges being granted to the tinners; the Cornish Rebellion of 1497 is attributed to grievances of the tin miners. In the mid-19th century, however, the tin trade again fell into decline. Other primary industries that have declined since the 1960s include china clay production, fishing and farming.\n\nToday, the Cornish economy depends heavily on its tourist industry, which makes up around a quarter of the economy. The official measures of deprivation and poverty at district and 'sub-ward' level show that there is great variation in poverty and prosperity in Cornwall with some areas among the poorest in England and others among the top half in prosperity. For example, the ranking of 32,482 sub-wards in England in the index of multiple deprivation (2006) ranged from 819th (part of Penzance East) to 30,899th (part of Saltash Burraton in Caradon), where the lower number represents the greater deprivation.\n\nCornwall is one of two UK areas designated as 'less developed regions' which qualify for Cohesion\nPolicy grants from the European Union. It was granted Objective 1 status by the European Commission for 2000 to 2006, followed by further rounds of funding known as 'Convergence Funding' from 2007 to 2013 and 'Growth Programme' for 2014 to 2020.\n\nTourism is estimated to contribute up to 24% of Cornwall's gross domestic product. In 2011 Tourism brought £1.85 billion into the Cornish economy. Cornwall's unique culture, spectacular landscape and mild climate make it a popular tourist destination, despite being somewhat distant from the United Kingdom's main centres of population. Surrounded on three sides by the English Channel and Celtic Sea, Cornwall has many miles of beaches and cliffs; the South West Coast Path follows a complete circuit of both coasts. Other tourist attractions include moorland, country gardens, museums, historic and prehistoric sites, and wooded valleys. Five million tourists visit Cornwall each year, mostly drawn from within the UK. Visitors to Cornwall are served by airports at Newquay and Exeter, whilst private jets, charters and helicopters are also served by Perranporth airfield; nightsleeper and daily rail services run between Cornwall, London and other regions of the UK. Cornwall has a tourism-based seasonal economy.\n\nNewquay and Porthtowan are popular destinations for surfers. In recent years, the Eden Project near St Austell has been a major financial success, drawing one in eight of Cornwall's visitors.\n\nCornwall is the landing point for one of the world's fastest high-speed transatlantic fibre optic cables, making Cornwall an important hub within Europe's Internet infrastructure. The Superfast Cornwall project completed in 2015, and saw 95% of Cornish houses and businesses connected to a fibre-based broadband network, with over 90% of properties able to connect with speeds above 24Mbit/s.\n\nOther industries are fishing, although this has been significantly re-structured by EU fishing policies (the Southwest Handline Fishermen's Association has started to revive the fishing industry), and agriculture, which has also declined significantly. Mining of tin and copper was also an industry, but today the derelict mine workings survive only as a World Heritage Site. However, the Camborne School of Mines, which was relocated to Penryn in 2004, is still a world centre of excellence in the field of mining and applied geology and the grant of World Heritage status has attracted funding for conservation and heritage tourism. China clay extraction has also been an important industry in the St Austell area, but this sector has been in decline, and this, coupled with increased mechanisation, has led to a decrease in employment in this sector, although the industry still employs around 2,133 people in Cornwall, and generates over £80 Million to the local economy\n\nCornwall's population was 537,400 at the last census, with a population density of 144 people per square kilometre, ranking it 40th and 41st respectively among the 47 counties of England. Cornwall's population was 95.7% White British and has a relatively high level of population growth. At 11.2% in the 1980s and 5.3% in the 1990s, it had the fifth-highest population growth rate of the English counties. The natural change has been a small population decline, and the population increase is due to inward migration into Cornwall. According to the 1991 census, the population was 469,800.\n\nCornwall has a relatively high retired population, with 22.9% of pensionable age, compared with 20.3% for the United Kingdom as a whole. This may be due to a combination of Cornwall's rural and coastal geography increasing its popularity as a retirement location, and outward migration of younger residents to more economically diverse areas.\n\nCornwall has a comprehensive education system, with 31 state and eight independent secondary schools. There are three further education colleges: Truro and Penwith College, Cornwall College and Callywith College which is due to open in September 2017. The Isles of Scilly only has one school while the former Restormel district has the highest school population, and school year sizes are around 200, with none above 270.\n\nHigher education is provided by Falmouth University, the University of Exeter (including Camborne School of Mines), the Combined Universities in Cornwall, and by Truro College, Penwith College (which combined in 2008 to make Truro and Penwith College) and Cornwall College.\n\nEnglish is the main language used in Cornwall, although the revived Cornish language may be seen on road signs and is spoken fluently by a small minority of people.\n\nThe Cornish language is a language from the Brythonic branch of the Celtic language family, closely related to the other Brythonic languages of Welsh and Breton, and less so to the Goidelic languages of Irish, Scots Gaelic and Manx. The language continued to function visibly as a community language in parts of Cornwall until the late 18th century, and it was claimed in 2011 that the last native speaker did not die until 1914.\n\nThere has been a revival of the language since Henry Jenner's \"Handbook of the Cornish Language\" was published in 1904. A study in 2000 suggested that there were around 300 people who spoke Cornish fluently. Cornish, however, had no legal status in the UK until 2002. Nevertheless, the language is taught in about twelve primary schools, and occasionally used in religious and civic ceremonies. In 2002 Cornish was officially recognised as a UK minority language and in 2005 it received limited Government funding. A Standard Written Form was agreed in 2008.\n\nSeveral Cornish mining words are used in English language mining terminology, such as costean, gossan, gunnies, kibbal, kieve and vug.\n\nIn the 2010–15 Parliament of the United Kingdom, four Cornish MPs, Andrew George, MP for St Ives, Dan Rogerson, MP for North Cornwall, Steve Gilbert, MP for St Austell and Newquay, and Sarah Newton, MP for Truro and Falmouth repeated their Parliamentary oaths in Cornish.\n\nSince the 19th century, Cornwall, with its unspoilt maritime scenery and strong light, has sustained a vibrant visual art scene of international renown. Artistic activity within Cornwall was initially centred on the art-colony of Newlyn, most active at the turn of the 20th century. This Newlyn School is associated with the names of Stanhope Forbes, Elizabeth Forbes, Norman Garstin and Lamorna Birch. Modernist writers such as D. H. Lawrence and Virginia Woolf lived in Cornwall between the wars, and Ben Nicholson, the painter, having visited in the 1920s came to live in St Ives with his then wife, the sculptor Barbara Hepworth, at the outbreak of the second world war. They were later joined by the Russian emigrant Naum Gabo, and other artists. These included Peter Lanyon, Terry Frost, Patrick Heron, Bryan Wynter and Roger Hilton. St Ives also houses the Leach Pottery, where Bernard Leach, and his followers championed Japanese inspired studio pottery. Much of this modernist work can be seen in Tate St Ives. The Newlyn Society and Penwith Society of Arts continue to be active, and contemporary visual art is documented in a dedicated online journal.\n\nCornwall has a full and vibrant folk music tradition which has survived into the present and is well known for its unusual folk survivals such as Mummers Plays, the Furry Dance in Helston played by the famous Helston Town Band, and Obby Oss in Padstow.\n\nNewlyn is home to a food and music festival which hosts live music, cooking demonstrations, and displays of locally caught fish.\n\nAs in other former mining districts of Britain, male voice choirs and Brass Bands, e.g. \"Brass on the Grass\" concerts during the summer at Constantine, are still very popular in Cornwall: Cornwall also has around 40 brass bands, including the six-times National Champions of Great Britain, Camborne Youth Band, and the bands of Lanner and St Dennis.\n\nCornish players are regular participants in inter-Celtic festivals, and Cornwall itself has several lively inter-Celtic festivals such as Perranporth's Lowender Peran folk festival.\n\nOn a more modern note, contemporary musician Richard D. James (also known as Aphex Twin) grew up in Cornwall, as did Luke Vibert and Alex Parks, winner of Fame Academy 2003. Roger Taylor, the drummer from the band Queen was also raised in the county, and currently lives not far from Falmouth. The American singer-songwriter Tori Amos now resides predominantly in North Cornwall not far from Bude with her family. The lutenist, lutarist, composer and festival director Ben Salfield lives in Truro.\n\nCornwall's rich heritage and dramatic landscape have inspired writers since at least the 19th century.\n\nSir Arthur Quiller-Couch, author of many novels and works of literary criticism, lived in Fowey: his novels are mainly set in Cornwall. Daphne du Maurier lived at Menabilly near Fowey and many of her novels had Cornish settings, including \"Rebecca\", \"Jamaica Inn\", \"Frenchman's Creek\", \"My Cousin Rachel\", and \"The House on the Strand\". She is also noted for writing \"Vanishing Cornwall\". Cornwall provided the inspiration for \"The Birds\", one of her terrifying series of short stories, made famous as a film by Alfred Hitchcock. \n\nMedieval Cornwall is the setting of the trilogy by Monica Furlong, \"Wise Child\", \"Juniper\", and \"Colman\", as well as part of Charles Kingsley's \"Hereward the Wake\".\n\nConan Doyle's \"The Adventure of the Devil's Foot\" featuring Sherlock Holmes is set in Cornwall. Winston Graham's series \"Poldark\", Kate Tremayne's Adam Loveday series, Susan Cooper's novels \"Over Sea, Under Stone\" and \"Greenwitch\", and Mary Wesley's \"The Camomile Lawn\" are all set in Cornwall. Writing under the pseudonym of Alexander Kent, Douglas Reeman sets parts of his Richard Bolitho and Adam Bolitho series in the Cornwall of the late 18th and the early 19th centuries, particularly in Falmouth.\n\nHammond Innes's novel, \"The Killer Mine\"; Charles de Lint's novel \"The Little Country\"; and Chapters 24 and 25 of J. K. Rowling's \"Harry Potter and the Deathly Hallows\" take place in Cornwall (the Harry Potter story at Shell Cottage, which is on the beach outside the fictional village of Tinworth in Cornwall).\n\nDavid Cornwell, who writes espionage novels under the name John le Carré, lives and writes in Cornwall. Nobel Prize-winning novelist William Golding was born in St Columb Minor in 1911, and returned to live near Truro from 1985 until his death in 1993. D. H. Lawrence spent a short time living in Cornwall. Rosamunde Pilcher grew up in Cornwall, and several of her books take place there.\n\nThe late Poet Laureate Sir John Betjeman was famously fond of Cornwall and it featured prominently in his poetry. He is buried in the churchyard at St Enodoc's Church, Trebetherick.\nCharles Causley, the poet, was born in Launceston and is perhaps the best known of Cornish poets. Jack Clemo and the scholar A. L. Rowse were also notable Cornishmen known for their poetry; The Rev. R. S. Hawker of Morwenstow wrote some poetry which was very popular in the Victorian period. The Scottish poet W. S. Graham lived in West Cornwall from 1944 until his death in 1986.\n\nThe poet Laurence Binyon wrote \"For the Fallen\" (first published in 1914) while sitting on the cliffs between Pentire Point and The Rumps and a stone plaque was erected in 2001 to commemorate the fact. The plaque bears the inscription \"FOR THE FALLEN / Composed on these cliffs, 1914\". The plaque also bears below this the fourth stanza (sometimes referred to as \"The Ode\") of the poem:\n\nCornwall produced a substantial number of passion plays such as the Ordinalia during the Middle Ages. Many are still extant, and provide valuable information about the Cornish language. See also Cornish literature\n\nProlific writer Colin Wilson, best known for his debut work \"The Outsider\" (1956) and for \"The Mind Parasites\" (1967), lived in Gorran Haven, a small village on the southern Cornish coast. The writer D. M. Thomas was born in Redruth but lived and worked in Australia and the United States before returning to his native Cornwall. He has written novels, poetry, and other works, including translations from Russian.\n\nThomas Hardy's drama \"The Queen of Cornwall\" (1923) is a version of the Tristan story; the second act of Richard Wagner's opera \"Tristan und Isolde\" takes place in Cornwall, as do Gilbert and Sullivan's operettas \"The Pirates of Penzance\" and \"Ruddigore\". A level of \"\", a game dealing with Arthurian Legend, takes place in Cornwall at a museum above King Arthur's tomb.\n\nThe fairy tale Jack the Giant Killer takes place in Cornwall.\n\nAs its population is comparatively small, and largely rural, Cornwall's contribution to British national sport in the United Kingdom has been limited; the county's greatest successes have come in fencing. In 2014, half of the men's GB team fenced for Truro Fencing Club, and 3 Truro fencers appeared at the 2012 Olympics. Truro, all of the towns and some villages have football clubs belonging to the Cornwall County Football Association, and the Cornwall County Cricket Club plays as one of the minor counties of English cricket. Viewed as an \"important identifier of ethnic affiliation\", rugby union has become a sport strongly tied to notions of Cornishness. and since the 20th century, rugby union in Cornwall has emerged as one of the most popular spectator and team sports in Cornwall (perhaps the most popular), with professional Cornish rugby footballers being described as a \"formidable force\", \"naturally independent, both in thought and deed, yet paradoxically staunch English patriots whose top players have represented England with pride and passion\". In 1985, sports journalist Alan Gibson made a direct connection between love of rugby in Cornwall and the ancient parish games of hurling and wrestling that existed for centuries before rugby officially began. Among Cornwall's native sports are a distinctive form of Celtic wrestling related to Breton wrestling, and Cornish hurling, a kind of mediaeval football played with a silver ball (distinct from Irish Hurling). Cornish Wrestling is Cornwall's oldest sport and as Cornwall's native tradition it has travelled the world to places like Victoria, Australia and Grass Valley, California following the miners and gold rushes. Cornish hurling now takes place at St. Columb Major, St Ives, and less frequently at Bodmin. Cornwall is also one of the few places in England where shinty is played; Cornwall Shinty Club was set up in 2012 after the sport was extinct for centuries in the county. English Shinty Association is based in Penryn.\n\nDue to its long coastline, various maritime sports are popular in Cornwall, notably sailing and surfing. International events in both are held in Cornwall. Cornwall hosted the Inter-Celtic Watersports Festival in 2006. Surfing in particular is very popular, as locations such as Bude and Newquay offer some of the best surf in the UK. Pilot gig rowing has been popular for many years and the World championships takes place annually on the Isles of Scilly. On 2 September 2007, 300 surfers at Polzeath beach set a new world record for the highest number of surfers riding the same wave as part of the Global Surf Challenge and part of a project called Earthwave to raise awareness about global warming.\n\nCornwall has a strong culinary heritage. Surrounded on three sides by the sea amid fertile fishing grounds, Cornwall naturally has fresh seafood readily available; Newlyn is the largest fishing port in the UK by value of fish landed, and is known for its wide range of restaurants. Television chef Rick Stein has long operated a fish restaurant in Padstow for this reason, and Jamie Oliver chose to open his second restaurant, Fifteen, in Watergate Bay near Newquay. MasterChef host and founder of Smiths of Smithfield, John Torode, in 2007 purchased Seiners in Perranporth. One famous local fish dish is Stargazy pie, a fish-based pie in which the heads of the fish stick through the piecrust, as though \"star-gazing\". The pie is cooked as part of traditional celebrations for Tom Bawcock's Eve, but is not generally eaten at any other time.\nCornwall is perhaps best known though for its pasties, a savoury dish made with pastry. Today's pasties usually contain a filling of beef steak, onion, potato and swede with salt and white pepper, but historically pasties had a variety of different fillings. \"Turmut, 'tates and mate\" (i.e. \"Turnip, potatoes and meat\", turnip being the Cornish and Scottish term for swede, itself an abbreviation of 'Swedish Turnip', the British term for rutabaga) describes a filling once very common. For instance, the licky pasty contained mostly leeks, and the herb pasty contained watercress, parsley, and shallots. Pasties are often locally referred to as \"oggies\". Historically, pasties were also often made with sweet fillings such as jam, apple and blackberry, plums or cherries.\nThe wet climate and relatively poor soil of Cornwall make it unsuitable for growing many arable crops. However, it is ideal for growing the rich grass required for dairying, leading to the production of Cornwall's other famous export, clotted cream. This forms the basis for many local specialities including Cornish fudge and Cornish ice cream. Cornish clotted cream has Protected Geographical Status under EU law, and cannot be made anywhere else. Its principal manufacturer is A. E. Rodda & Son of Scorrier.\n\nLocal cakes and desserts include Saffron cake, Cornish heavy (\"hevva\") cake, Cornish fairings biscuits, figgy 'obbin, Cream tea and whortleberry pie.\n\nThere are also many types of beers brewed in Cornwall – those produced by Sharp's Brewery, Skinner's Brewery, Keltek Brewery and St Austell Brewery are the best-known – including stouts, ales and other beer types. There is some small scale production of wine, mead and cider.\n\n\n\n\n", "id": "5648", "title": "Cornwall"}
{"url": "https://en.wikipedia.org/wiki?curid=5649", "text": "Constitutional monarchy\n\nA constitutional monarchy (also known as a parliamentary monarchy) is a form of monarchy in which the sovereign exercises their authorities in accordance with a written or unwritten constitution. Constitutional monarchy differs from absolute monarchy (in which a monarch holds absolute power), in that constitutional monarchs are bound to exercise their powers and authorities within the limits prescribed within an established legal framework. Constitutional monarchies range from countries such as Morocco, where the constitution grants substantial discretionary powers to the sovereign, to countries such as Sweden or Denmark where the monarch retains very few formal authorities.\n\nA constitutional monarchy may refer to a system in which the monarch acts as a non-party political head of state under the constitution, whether written or unwritten. While most monarchs may hold formal authority and the government may legally operate in the monarch's name, in the form typical in Europe the monarch no longer personally sets public policy or chooses political leaders. Political scientist Vernon Bogdanor, paraphrasing Thomas Macaulay, has defined a constitutional monarch as \"a sovereign who reigns but does not rule\". \n\nIn addition to acting as a visible symbol of national unity, a constitutional monarch may hold formal powers such as dissolving parliament or giving royal assent to legislation. However, the exercise of such powers is largely strictly in accordance with either written constitutional principles or unwritten constitutional conventions, rather than any personal political preference imposed by the sovereign. In \"The English Constitution\", British political theorist Walter Bagehot identified three main political rights which a constitutional monarch may freely exercise: the right to be consulted, the right to encourage, and the right to warn. Many constitutional monarchies still retain significant authorities or political influence however, such as through certain reserve powers, and may also play an important political role.\n\nThe United Kingdom and the other Commonwealth realms are all constitutional monarchies in the Westminster tradition of constitutional governance. Three states – Malaysia, Cambodia and the Holy See – are elective monarchies, wherein the ruler is periodically selected by a small electoral college.\n\nThe oldest constitutional monarchy dating back to ancient times was that of the Hittites. They were an ancient Anatolian people that lived during the Bronze Age whose king or queen had to share their authority with an assembly, called \"Panku\", equivalent to a modern-day deliberative assembly of a legislature. These were scattered noble families that worked as representatives of their subjects in an adjutant or subaltern federal-type landscape.\n\nThe country to move from an absolute monarchy to a constitutional monarchy was Bhutan, between 2007 and 2008 \"(see Politics of Bhutan, Constitution of Bhutan and Bhutanese democracy)\". \n\nIn the Kingdom of England, the Glorious Revolution of 1688 led to a constitutional monarchy restricted by laws such as the Bill of Rights 1689 and the Act of Settlement 1701, although limits on the power of the monarch ('a limited monarchy') are much older than that (see Magna Carta). At the same time, in Scotland the Convention of Estates enacted the Claim of Right Act 1689, which placed similar limits on the Scottish monarchy.\n\nAlthough Queen Anne was the last monarch to veto an Act of Parliament when in 1707 she blocked the Scottish Militia Bill, Hanoverian monarchs continued to selectively dictate government policies. For instance George III constantly blocked Catholic Emancipation, eventually precipitating the resignation of William Pitt the Younger as Prime Minister in 1801. The sovereign's influence on the choice of Prime Minister gradually declined over this period, William IV being the last monarch to dismiss a Prime Minister, when in 1834 he removed Lord Melbourne as a result of Melbourne's choice of Lord John Russell as Leader of the House of Commons. Queen Victoria was the last monarch to exercise real personal power but this diminished over the course of her reign. In 1839 she became the last sovereign to keep a Prime Minister in power against the will of Parliament when the Bedchamber crisis resulted in the retention of Lord Melbourne's administration. By the end of her reign, however, she could do nothing to block the unacceptable (to her) premierships of William Gladstone, although she still exercised power in appointments to the Cabinet, for example in 1886 preventing Gladstone's choice of Hugh Childers as War Secretary in favor of Sir Henry Campbell-Bannerman.\n\nToday, the role of the British monarch is by convention effectively ceremonial. Instead, the British Parliament and the Government – chiefly in the office of Prime Minister – exercise their powers under 'Royal (or Crown) Prerogative': on behalf of the monarch and through powers still formally possessed by the Monarch.\n\nNo person may accept significant public office without swearing an oath of allegiance to the Queen. With few exceptions, the monarch is bound by constitutional convention to act on the advice of the Government.\n\nConstitutional monarchy originated in continental Europe, with Poland developing the first constitution for a monarchy with the Constitution of May 3, 1791; it was the third constitution in the world just after the first republican Constitution of the United States. Constitutional monarchy also occurred briefly in the early years of the French Revolution, but much more widely afterwards. Napoleon Bonaparte is considered the first monarch proclaiming \"himself\" as an embodiment of the nation, rather than as a divinely-appointed ruler; this interpretation of monarchy is germane to continental constitutional monarchies. German philosopher Georg Wilhelm Friedrich Hegel, in his work \"Elements of the Philosophy of Right\" (1820), gave the concept a philosophical justification that concurred with evolving contemporary political theory and the Protestant Christian view of natural law. Hegel's forecast of a constitutional monarch with very limited powers whose function is to embody the national character and provide constitutional continuity in times of emergency was reflected in the development of constitutional monarchies in Europe and Japan.\n\nAs originally conceived, a constitutional monarch was head of the executive branch and quite a powerful figure even though his or her power was limited by the constitution and the elected parliament. Some of the framers of the US Constitution may have envisioned the president as an elected constitutional monarch, as the term was then understood, following Montesquieu's account of the separation of powers.\n\nThe present-day concept of a constitutional monarchy developed in the United Kingdom, where the democratically elected parliaments, and their leader, the prime minister, exercise power, with the monarchs having ceded power and remaining as a titular position. In many cases the monarchs, while still at the very top of the political and social hierarchy, were given the status of \"servants of the people\" to reflect the new, egalitarian position. In the course of France's July Monarchy, Louis-Philippe I was styled \"King of the French\" rather than \"King of France.\"\n\nFollowing the Unification of Germany, Otto von Bismarck rejected the British model. In the constitutional monarchy established under the Constitution of the German Empire which Bismarck inspired, the Kaiser retained considerable actual executive power, while the Imperial Chancellor needed no parliamentary vote of confidence and ruled solely by the imperial mandate. However this model of constitutional monarchy was discredited and abolished following Germany's defeat in the First World War. Later, Fascist Italy could also be considered as a constitutional monarchy, in that there was a king as the titular head of state while actual power was held by Benito Mussolini under a constitution. This eventually discredited the Italian monarchy and led to its abolition in 1946. After the Second World War, surviving European monarchies almost invariably adopted some variant of the constitutional monarchy model originally developed in Britain.\n\nNowadays a parliamentary democracy that is a constitutional monarchy is considered to differ from one that is a republic only in detail rather than in substance. In both cases, the titular head of state—monarch or president—serves the traditional role of embodying and representing the nation, while the government is carried on by a cabinet composed predominantly of elected Members of Parliament.\n\nHowever, three important factors distinguish monarchies such as the United Kingdom from systems where greater power might otherwise rest with Parliament. These are: the Royal Prerogative under which the monarch may exercise power under certain very limited circumstances; Sovereign Immunity under which the monarch may \"do no wrong\" under the law because the responsible government is instead deemed accountable; and the monarch may not be subject to the same taxation or property use restrictions as most citizens. Other privileges may be nominal or ceremonial (e.g., where the executive, judiciary, police or armed forces act on the authority of or owe allegiance to the Crown).\n\nToday slightly more than a quarter of constitutional monarchies are Western European countries, including the United Kingdom, the Netherlands, Belgium, Norway, Denmark, Spain, Luxembourg, Monaco, Liechtenstein and Sweden. However, the two most populous constitutional monarchies in the world are in Asia: Japan and Thailand. In these countries the prime minister holds the day-to-day powers of governance, while the monarch retains residual (but not always insignificant) powers. The powers of the monarch differ between countries. In Denmark and in Belgium, for example, the Monarch formally appoints a representative to preside over the creation of a coalition government following a parliamentary election, while in Norway the King chairs special meetings of the cabinet.\n\nIn nearly all cases, the monarch is still the nominal chief executive, but is bound by convention to act on the advice of the Cabinet. Only a few monarchies (most notably Japan and Sweden) have amended their constitutions so that the monarch is no longer even the nominal chief executive.\n\nThere are sixteen constitutional monarchies under Queen Elizabeth II, which are known as Commonwealth realms. Unlike some of their continental European counterparts, the Monarch and her Governors-General in the Commonwealth realms hold significant \"reserve\" or \"prerogative\" powers, to be wielded in times of extreme emergency or constitutional crises, usually to uphold parliamentary government. An instance of a Governor-General exercising such power occurred during the 1975 Australian constitutional crisis, when the Australian Prime Minister, Gough Whitlam, was dismissed by the Governor-General. The Australian senate had threatened to block the Government's budget by refusing to pass the necessary appropriation bills. On November 11, 1975, Whitlam intended to call a half-Senate election in an attempt to break the deadlock. When he sought the Governor-General's approval of the election, the Governor-General instead dismissed him as Prime Minister, and shortly thereafter installed leader of the opposition Malcolm Fraser in his place. Acting quickly before all parliamentarians became aware of the change of government, Fraser and his allies secured passage of the appropriation bills, and the Governor-General dissolved Parliament for a double dissolution election. Fraser and his government were returned with a massive majority. This led to much speculation among Whitlam's supporters as to whether this use of the Governor-General's reserve powers was appropriate, and whether Australia should become a republic. Among supporters of constitutional monarchy, however, the experience confirmed the value of the monarchy as a source of checks and balances against elected politicians who might seek powers in excess of those conferred by the constitution, and ultimately as a safeguard against dictatorship.\n\nIn Thailand's constitutional monarchy, the monarch is recognized as the Head of State, Head of the Armed Forces, Upholder of the Buddhist Religion, and Defender of the Faith. The former King, Bhumibol Adulyadej, was the longest reigning monarch in the world and in all of Thailand's history, prior to passing away on 13 October 2016. Bhumibol has reigned through several political changes in the Thai government. He has played an influential role in each incident, often acting as mediator between disputing political opponents. (See Bhumibol's role in Thai Politics.) Among the powers retained by the monarch under the constitution, lèse majesté protects the image of the monarch and enables him to play a role in politics. It carries strict criminal penalties for violators. Generally, the Thai people are reverent of Bhumibol. Much of his social influence arises from this reverence and from the socio-economic improvement efforts undertaken by the royal family.\n\nIn both the United Kingdom and elsewhere, a frequent debate centers on when it is appropriate for a monarch to use his or her political powers. When a monarch does act, political controversy can often ensue, partially because the neutrality of the crown is seen to be compromised in favor of a partisan goal, while some political scientists champion the idea of an \"interventionist monarch\" as a check against possible illegal action by politicians. For instance, the monarch of the United Kingdom can theoretically exercise an absolute veto over legislation by withholding royal assent. However, no monarch has done so since 1708, and it is widely believed that this and many of the monarch's other political powers are lapsed powers.\n\nThere are currently 44 monarchies, and most of them are constitutional monarchies.\n\n\n\n\n", "id": "5649", "title": "Constitutional monarchy"}
{"url": "https://en.wikipedia.org/wiki?curid=5653", "text": "Clarke's three laws\n\nBritish science fiction writer Arthur C. Clarke formulated three adages that are known as Clarke's three laws, of which the third law is the best known and most widely cited:\n\n\nClarke's first law was proposed by Clarke in the essay \"Hazards of Prophecy: The Failure of Imagination\", in \"Profiles of the Future\" (1962).\n\nThe second law is offered as a simple observation in the same essay. Its status as Clarke's second law was conferred by others. In a 1973 revision of \"Profiles of the Future\", Clarke acknowledged the second law and proposed the third. \"As three laws were good enough for Newton, I have modestly decided to stop there\".\n\nThe third law is the best known and most widely cited, and appears in Clarke's 1973 revision of his essay \"Hazards of Prophecy: The Failure of Imagination.\" It echoes a statement in a 1942 story by Leigh Brackett: \"Witchcraft to the ignorant, … simple science to the learned\". An earlier example of this sentiment may be found in \"Wild Talents\" (1932) by Charles Fort: \"...a performance that may some day be considered understandable, but that, in these primitive times, so transcends what is said to be the known that it is what I mean by magic.\"\n\nClarke gave an example of the third law when he said that while he \"would have believed anyone who told him back in 1962 that there would one day exist a book-sized object capable of holding the content of an entire library, he would never have accepted that the same device could find a page or word in a second and then convert it into any typeface and size from Albertus Extra Bold to Zurich Calligraphic\", referring to his memory of \"seeing and hearing Lynotype machines which slowly converted ‘molten lead into front pages that required two men to lift them’\".\n\nA fourth law has been proposed for the canon, despite Clarke's declared intention of not going one better than Newton. Geoff Holder quotes: \"For every expert, there is an equal and opposite expert,\" which is part of American economist Thomas Sowell's \n\"For every expert, there is an equal and opposite expert, but for every fact there is not necessarily an equal and opposite fact\", from his 1995 book \"The Vision of the Anointed\". \n\nThe third law has inspired many snowclones and other variations:\n\n\nA of the third law is\n\n\nThe third law has been:\n\n\n\n", "id": "5653", "title": "Clarke's three laws"}
{"url": "https://en.wikipedia.org/wiki?curid=5654", "text": "Caspar David Friedrich\n\nCaspar David Friedrich (5 September 1774 – 7 May 1840) was a 19th-century German Romantic landscape painter, generally considered the most important German artist of his generation. He is best known for his mid-period allegorical landscapes which typically feature contemplative figures silhouetted against night skies, morning mists, barren trees or Gothic or megalithic ruins. His primary interest as an artist was the contemplation of nature, and his often symbolic and anti-classical work seeks to convey a subjective, emotional response to the natural world. Friedrich's paintings characteristically set a human presence in diminished perspective amid expansive landscapes, reducing the figures to a scale that, according to the art historian Christopher John Murray, directs \"the viewer's gaze towards their metaphysical dimension\".\n\nFriedrich was born in the Pomeranian town of Greifswald at the Baltic Sea, where he began his studies in art as a young man. He studied in Copenhagen until 1798, before settling in Dresden. He came of age during a period when, across Europe, a growing disillusionment with materialistic society was giving rise to a new appreciation of spirituality. This shift in ideals was often expressed through a reevaluation of the natural world, as artists such as Friedrich, J. M. W. Turner (1775–1851) and John Constable (1776–1837) sought to depict nature as a \"divine creation, to be set against the artifice of human civilization\".\n\nFriedrich's work brought him renown early in his career, and contemporaries such as the French sculptor David d'Angers (1788–1856) spoke of him as a man who had discovered \"the tragedy of landscape\". Nevertheless, his work fell from favour during his later years, and he died in obscurity, and in the words of the art historian Philip B. Miller, \"half mad\". As Germany moved towards modernisation in the late 19th century, a new sense of urgency characterised its art, and Friedrich's contemplative depictions of stillness came to be seen as the products of a bygone age. The early 20th century brought a renewed appreciation of his work, beginning in 1906 with an exhibition of thirty-two of his paintings and sculptures in Berlin. By the 1920s his paintings had been discovered by the Expressionists, and in the 1930s and early 1940s Surrealists and Existentialists frequently drew ideas from his work. The rise of Nazism in the early 1930s again saw a resurgence in Friedrich's popularity, but this was followed by a sharp decline as his paintings were, by association with the Nazi movement, interpreted as having a nationalistic aspect. It was not until the late 1970s that Friedrich regained his reputation as an icon of the German Romantic movement and a painter of international importance.\n\nCaspar David Friedrich was born on 5 September 1774, in Greifswald, Swedish Pomerania, on the Baltic coast of Germany. The sixth of ten children, he was brought up in the strict Lutheran creed of his father Adolf Gottlieb Friedrich, a candle-maker and soap boiler. Records of the family's financial circumstances are contradictory; while some sources indicate the children were privately tutored, others record that they were raised in relative poverty. Caspar David was familiar with death from an early age. His mother, Sophie Dorothea Bechly, died in 1781 when he was just seven. A year later, his sister Elisabeth died, while a second sister, Maria, succumbed to typhus in 1791. Arguably the greatest tragedy of his childhood happened in 1787 when his brother Johann Christoffer died: at the age of thirteen, Caspar David witnessed his younger brother fall through the ice of a frozen lake, and drown. Some accounts suggest that Johann Christoffer perished while trying to rescue Caspar David, who was also in danger on the ice.\n\nFriedrich began his formal study of art in 1790 as a private student of artist Johann Gottfried Quistorp at the University of Greifswald in his home city, at which the art department is now named \"Caspar-David-Friedrich-Institut\" in his honour. Quistorp took his students on outdoor drawing excursions; as a result, Friedrich was encouraged to sketch from life at an early age. Through Quistorp, Friedrich met and was subsequently influenced by the theologian Ludwig Gotthard Kosegarten, who taught that nature was a revelation of God. Quistorp introduced Friedrich to the work of the German 17th-century artist Adam Elsheimer, whose works often included religious subjects dominated by landscape, and nocturnal subjects. During this period he also studied literature and aesthetics with Swedish professor Thomas Thorild. Four years later Friedrich entered the prestigious Academy of Copenhagen, where he began his education by making copies of casts from antique sculptures before proceeding to drawing from life. Living in Copenhagen afforded the young painter access to the Royal Picture Gallery's collection of 17th-century Dutch landscape painting. At the Academy he studied under teachers such as Christian August Lorentzen and the landscape painter Jens Juel. These artists were inspired by the \"Sturm und Drang\" movement and represented a midpoint between the dramatic intensity and expressive manner of the budding Romantic aesthetic and the waning neo-classical ideal. Mood was paramount, and influence was drawn from such sources as the Icelandic legend of Edda, the poems of Ossian and Norse mythology.\n\nFriedrich settled permanently in Dresden in 1798. During this early period, he experimented in printmaking with etchings and designs for woodcuts which his furniture-maker brother cut. By 1804 he had produced 18 etchings and four woodcuts; they were apparently made in small numbers and only distributed to friends. Despite these forays into other media, he gravitated toward working primarily with ink, watercolour and sepias. With the exception of a few early pieces, such as \"\" (1797), he did not work extensively with oils until his reputation was more established. Landscapes were his preferred subject, inspired by frequent trips, beginning in 1801, to the Baltic coast, Bohemia, the Krkonoše and the Harz Mountains. Mostly based on the landscapes of northern Germany, his paintings depict woods, hills, harbors, morning mists and other light effects based on a close observation of nature. These works were modeled on sketches and studies of scenic spots, such as the cliffs on Rügen, the surroundings of Dresden and the river Elbe. He executed his studies almost exclusively in pencil, even providing topographical information, yet the subtle atmospheric effects characteristic of Friedrich's mid-period paintings were rendered from memory. These effects took their strength from the depiction of light, and of the illumination of sun and moon on clouds and water: optical phenomena peculiar to the Baltic coast that had never before been painted with such an emphasis.\n\nFriedrich established his reputation as an artist when he won a prize in 1805 at the Weimar competition organised by the writer, poet, and dramatist Johann Wolfgang von Goethe. At the time, the Weimar competition tended to draw mediocre and now long-forgotten artists presenting derivative mixtures of neo-classical and pseudo-Greek styles. The poor quality of the entries began to prove damaging to Goethe's reputation, so when Friedrich entered two sepia drawings—\"Procession at Dawn\" and \"Fisher-Folk by the Sea\"—the poet responded enthusiastically and wrote, \"We must praise the artist's resourcefulness in this picture fairly. The drawing is well done, the procession is ingenious and appropriate... his treatment combines a great deal of firmness, diligence and neatness... the ingenious watercolour... is also worthy of praise.\"\n\nFriedrich completed the first of his major paintings in 1807, at the age of 34. \"The Cross in the Mountains\", today known as the \"Tetschen Altar\" (Galerie Neue Meister, Dresden), is an altarpiece panel commissioned by the Countess of Thun for her family's chapel in Tetschen, Bohemia. It was to be one of the few commissions the artist received. The altar panel depicts a \"Gipfelkreuz,\" or a gilded cross, in profile at the top of a mountain, alone, and surrounded by German and Austrian pine trees. The cross reaches the highest point in the pictorial plane but is presented from an oblique and a distant viewpoint. Nature dominates the scene and for the first time in Christian art, an altarpiece showcases a landscape. According to the art historian Linda Siegel, the design of the altarpiece is the \"logical climax of many earlier drawings of his which depicted a cross in nature's world.\"\n\nThe work was first exhibited on Christmas Day, 1808. Although it was generally coldly received, it was nevertheless Friedrich's first painting to receive wide publicity. The artist's friends publicly defended the work, while art critic Basilius von Ramdohr published a lengthy article rejecting Friedrich's use of landscape in such a context; he wrote that it would be \"a veritable presumption, if landscape painting were to sneak into the church and creep onto the altar\". Ramdohr fundamentally challenged the concept that pure landscape painting could convey explicit meaning. Friedrich responded with a programme describing his intentions. In his 1809 commentary on the painting, he compared the rays of the evening sun to the light of the Holy Father. The sinking of the sun suggests that the era when God revealed himself directly to man has passed. This statement marked the only time Friedrich recorded a detailed interpretation of his own work.\n\nFriedrich was elected a member of the Berlin Academy in 1810 following the purchase of two of his paintings by the Prussian Crown Prince. Yet in 1816, he sought to distance himself from Prussian authority, and that June applied for Saxon citizenship. The move was unexpected by his friends, as the Saxon government of the time was pro-French, while Friedrich's paintings to date were seen as generally patriotic and distinctly anti-French. Nevertheless, with the aid of his Dresden-based friend Graf Vitzthum von Eckstädt, Friedrich attained not only citizenship, but in 1818, a place in the Saxon Academy as a member with a yearly dividend of 150 thalers. Although he hoped to receive a full Professorship, it was never awarded him as, according to the German Library of Information, \"it was felt that his painting was too personal, his point of view too individual to serve as a fruitful example to students.\" Politics too may have played a role in the stalling of his career: Friedrich's decidedly Germanic choice of subject and costuming frequently clashed with the prevailing pro-French attitudes of the time.\n\nOn 21 January 1818, Friedrich married Caroline Bommer, the twenty-five-year-old daughter of a dyer from Dresden. The couple had three children, with their first, Emma, arriving in 1820. Physiologist and painter Carl Gustav Carus notes in his biographical essays that marriage did not impact significantly on either Friedrich's life or personality, yet his canvasses from this period, including \"Chalk Cliffs on Rügen\"—painted after his honeymoon—display a new sense of levity, while his palette is brighter and less austere. Human figures appear with increasing frequency in the paintings of this period, which Siegel interprets as a reflection that \"the importance of human life, particularly his family, now occupies his thoughts more and more, and his friends, his wife, and his townspeople appear as frequent subjects in his art.\"\n\nAround this time, the artist found support from two sources in Russia. In 1820, Grand Duke Nikolai Pavlovich, at the behest of his wife Alexandra Feodorovna, visited Friedrich's studio and returned to Saint Petersburg with a number of his paintings. The exchange marked the beginning of a patronage that continued for many years. Not long thereafter, the poet Vasily Zhukovsky, tutor to Alexander II, met Friedrich in 1821 and found in him a kindred spirit. For decades Zhukovsky helped Friedrich both by purchasing his work himself and by recommending his art to the royal family; his assistance toward the end of Friedrich's career proved invaluable to the ailing and impoverished artist. Zhukovsky remarked that his friend's paintings \"please us by their precision, each of them awakening a memory in our mind.\"\n\nFriedrich was acquainted with Philipp Otto Runge (1777–1810), another leading German painter of the Romantic period. He was also a friend of Georg Friedrich Kersting (1785–1847), who painted him at work in his unadorned studio, and of the Norwegian painter Johan Christian Clausen Dahl (1788–1857). Dahl was close to Friedrich during the artist's final years, and he expressed dismay that to the art-buying public, Friedrich's pictures were only \"curiosities\". While the poet Zhukovsky appreciated Friedrich's psychological themes, Dahl praised the descriptive quality of Friedrich's landscapes, commenting that \"artists and connoisseurs saw in Friedrich's art only a kind of mystic, because they themselves were only looking out for the mystic... They did not see Friedrich's faithful and conscientious study of nature in everything he represented\".\n\nDuring this period Friedrich frequently sketched memorial monuments and sculptures for mausoleums, reflecting his obsession with death and the afterlife; he even created designs for some of the funerary art in Dresden's cemeteries. Some of these works were lost in the fire that destroyed Munich's Glass Palace (1931) and later in the 1945 bombing of Dresden.\n\nFriedrich's reputation steadily declined over the final fifteen years of his life. As the ideals of early Romanticism passed from fashion, he came to be viewed as an eccentric and melancholy character, out of touch with the times. Gradually his patrons fell away. By 1820, he was living as a recluse and was described by friends as the \"most solitary of the solitary\". Towards the end of his life he lived in relative poverty and was increasingly dependent on the charity of friends. He became isolated and spent long periods of the day and night walking alone through woods and fields, often beginning his strolls before sunrise.\n\nIn June 1835, Friedrich suffered his first stroke, which left him with minor limb paralysis and greatly reduced his ability to paint. As a result, he was unable to work in oil; instead he was limited to watercolour, sepia and reworking older compositions.\n\nAlthough his vision remained strong, he had lost the full strength of his hand. Yet he was able to produce a final 'black painting', \"Seashore by Moonlight\" (1835–36), described by Vaughan as the \"darkest of all his shorelines, in which richness of tonality compensates for the lack of his former finesse\".\n\nSymbols of death appeared in his other work from this period. Soon after his stroke, the Russian royal family purchased a number of his earlier works, and the proceeds allowed him to travel to Teplitz—in today's Czech Republic—to recover.\n\nDuring the mid-1830s, Friedrich began a series of portraits and he returned to observing himself in nature. As the art historian William Vaughan has observed, however, \"He can see himself as a man greatly changed. He is no longer the upright, supportive figure that appeared in \"Two Men Contemplating the Moon\" in 1819. He is old and stiff... he moves with a stoop\".\n\nBy 1838, he was capable only of working in a small format. He and his family were living in poverty and grew increasingly dependent for support on the charity of friends.\n\nFriedrich died in Dresden on 7 May 1840, and was buried in Dresden's Trinitatis-Friedhof (Trinity Cemetery) east of the city centre (the entrance to which he had painted some 15 years earlier). The simple flat gravestone lies north-west of the central roundel within the main avenue.\n\nBy the time of his death, his reputation and fame were waning, and his passing was little noticed within the artistic community. His artwork had certainly been acknowledged during his lifetime, but not widely. While the close study of landscape and an emphasis on the spiritual elements of nature were commonplace in contemporary art, his work was too original and personal to be well understood. By 1838, his work no longer sold or received attention from critics; the Romantic movement had been moving away from the early idealism that the artist had helped found.\n\nAfter his death, Carl Gustav Carus wrote a series of articles which paid tribute to Friedrich's transformation of the conventions of landscape painting. However, Carus' articles placed Friedrich firmly in his time, and did not place the artist within a continuing tradition. Only one of his paintings had been reproduced as a print, and that was produced in very few copies.\n\nThe visualisation and portrayal of landscape in an entirely new manner was Friedrich's key innovation. He sought not just to explore the blissful enjoyment of a beautiful view, as in the classic conception, but rather to examine an instant of sublimity, a reunion with the spiritual self through the contemplation of nature. Friedrich was instrumental in transforming landscape in art from a backdrop subordinated to human drama to a self-contained emotive subject. Friedrich's paintings commonly employed the \"Rückenfigur\"—a person seen from behind, contemplating the view. The viewer is encouraged to place himself in the position of the \"Rückenfigur\", by which means he experiences the sublime potential of nature, understanding that the scene is as perceived and idealised by a human. Friedrich created the notion of a landscape full of romantic feeling—\"die romantische Stimmungslandschaft\". His art details a wide range of geographical features, such as rock coasts, forests, and mountain scenes. He often used the landscape to express religious themes. During his time, most of the best-known paintings were viewed as expressions of a religious mysticism.\n\nFriedrich said, \"The artist should paint not only what he sees before him, but also what he sees within him. If, however, he sees nothing within him, then he should also refrain from painting that which he sees before him. Otherwise, his pictures will be like those folding screens behind which one expects to find only the sick or the dead.\" Expansive skies, storms, mist, forests, ruins and crosses bearing witness to the presence of God are frequent elements in Friedrich's landscapes. Though death finds symbolic expression in boats that move away from shore—a Charon-like motif—and in the poplar tree, it is referenced more directly in paintings like \"The Abbey in the Oakwood\" (1808–10), in which monks carry a coffin past an open grave, toward a cross, and through the portal of a church in ruins.\n\nHe was one of the first artists to portray winter landscapes in which the land is rendered as stark and dead. Friedrich's winter scenes are solemn and still—according to the art historian Hermann Beenken, Friedrich painted winter scenes in which \"no man has yet set his foot. The theme of nearly all the older winter pictures had been less winter itself than life in winter. In the 16th and 17th centuries, it was thought impossible to leave out such motifs as the crowd of skaters, the wanderer... It was Friedrich who first felt the wholly detached and distinctive features of a natural life. Instead of many tones, he sought the one; and so, in his landscape, he subordinated the composite chord into one single basic note\".\n\nBare oak trees and tree stumps, such as those in \"\" (c. 1822), \"\" (c. 1833), and \"Willow Bush under a Setting Sun\" (c. 1835), are recurring elements of Friedrich's paintings, symbolizing death. Countering the sense of despair are Friedrich's symbols for redemption: the cross and the clearing sky promise eternal life, and the slender moon suggests hope and the growing closeness of Christ. In his paintings of the sea, anchors often appear on the shore, also indicating a spiritual hope. German literature scholar Alice Kuzniar finds in Friedrich's painting a temporality—an evocation of the passage of time—that is rarely highlighted in the visual arts. For example, in \"The Abbey in the Oakwood\", the movement of the monks away from the open grave and toward the cross and the horizon imparts Friedrich's message that the final destination of man's life lies beyond the grave.\n\nWith dawn and dusk constituting prominent themes of his landscapes, Friedrich's own later years were characterized by a growing pessimism. His work becomes darker, revealing a fearsome monumentality. \"The Wreck of the Hope\"—also known as \"The Polar Sea\" or \"The Sea of Ice\" (1823–24)—perhaps best summarizes Friedrich's ideas and aims at this point, though in such a radical way that the painting was not well received. Completed in 1824, it depicted a grim subject, a shipwreck in the Arctic Ocean; \"the image he produced, with its grinding slabs of travertine-colored floe ice chewing up a wooden ship, goes beyond documentary into allegory: the frail bark of human aspiration crushed by the world's immense and glacial indifference.\"\n\nFriedrich's written commentary on aesthetics was limited to a collection of aphorisms set down in 1830, in which he explained the need for the artist to match natural observation with an introspective scrutiny of his own personality. His best-known remark advises the artist to \"close your bodily eye so that you may see your picture first with the spiritual eye. Then bring to the light of day that which you have seen in the darkness so that it may react upon others from the outside inwards.\" He rejected the overreaching portrayals of nature in its \"totality\", as found in the work of contemporary painters like Adrian Ludwig Richter (1803–84) and Joseph Anton Koch (1768–1839).\n\nBoth Friedrich's life and art have at times been perceived by some to have been marked with an overwhelming sense of loneliness. Art historians and some of his contemporaries attribute such interpretations to the losses suffered during his youth to the bleak outlook of his adulthood, while Friedrich's pale and withdrawn appearance helped reinforce the popular notion of the \"taciturn man from the North\".\n\nFriedrich suffered depressive episodes in 1799, 1803–1805, c.1813, in 1816 and between 1824 and 1826. There are noticeable thematic shifts in the works he produced during these episodes, which see the emergence of such motifs and symbols as vultures, owls, graveyards and ruins. From 1826 these motifs became a permanent feature of his output, while his use of color became more dark and muted. Carus wrote in 1929 that Friedrich \"is surrounded by a thick, gloomy cloud of spiritual uncertainty\", though the noted art historian and curator Hubertus Gassner disagrees with such notions, seeing in Friedrich's work a positive and life-affirming subtext inspired by Freemasonry and religion.\n\nReflecting Friedrich's patriotism and resentment during the 1813 French occupation of the dominion of Pomerania, motifs from German folklore became increasingly prominent in his work. An anti-French German nationalist, Friedrich used motifs from his native landscape to celebrate Germanic culture, customs and mythology. He was impressed by the anti-Napoleonic poetry of Ernst Moritz Arndt and Theodor Körner, and the patriotic literature of Adam Müller and Heinrich von Kleist. Moved by the deaths of three friends killed in battle against France, as well as by Kleist's 1808 drama \"Die Hermannsschlacht\", Friedrich undertook a number of paintings in which he intended to convey political symbols solely by means of the landscape—a first in the history of art.\n\nIn \"\" (1812), a dilapidated monument inscribed \"Arminius\" invokes the Germanic chieftain, a symbol of nationalism, while the four tombs of fallen heroes are slightly ajar, freeing their spirits for eternity. Two French soldiers appear as small figures before a cave, lower and deep in a grotto surrounded by rock, as if farther from heaven. A second political painting, \"\" (c. 1813), depicts a lost French soldier dwarfed by a dense forest, while on a tree stump a raven is perched—a prophet of doom, symbolizing the anticipated defeat of France.\n\nAlongside other Romantic painters, Friedrich helped position landscape painting as a major genre within Western art. Of his contemporaries, Friedrich's style most influenced the painting of Johan Christian Dahl (1788–1857). Among later generations, Arnold Böcklin (1827–1901) was strongly influenced by his work, and the substantial presence of Friedrich's works in Russian collections influenced many Russian painters, in particular Arkhip Kuindzhi (c. 1842–1910) and Ivan Shishkin (1832–98). Friedrich's spirituality anticipated American painters such as Albert Pinkham Ryder (1847–1917), Ralph Blakelock (1847–1919), the painters of the Hudson River School and the New England Luminists.\n\nAt the turn of the 20th century, Friedrich was rediscovered by the Norwegian art historian Andreas Aubert (1851–1913), whose writing initiated modern Friedrich scholarship, and by the Symbolist painters, who valued his visionary and allegorical landscapes. The Norwegian Symbolist Edvard Munch (1863–1944) would have seen Friedrich's work during a visit to Berlin in the 1880s. Munch's 1899 print \"The Lonely Ones\" echoes Friedrich's \"Rückenfigur (back figure)\", although in Munch's work the focus has shifted away from the broad landscape and toward the sense of dislocation between the two melancholy figures in the foreground.\n\nFriedrich's landscapes exercised a strong influence on the work of German artist Max Ernst (1891–1976), and as a result other Surrealists came to view Friedrich as a precursor to their movement. In 1934, the Belgian painter René Magritte (1898–1967) paid tribute in his work \"The Human Condition\", which directly echoes motifs from Friedrich's art in its questioning of perception and the role of the viewer. A few years later, the Surrealist journal \"Minotaure\" featured Friedrich in a 1939 article by critic Marie Landsberger, thereby exposing his work to a far wider circle of artists. The influence of \"The Wreck of Hope\" (or \"The Sea of Ice\") is evident in the 1940–41 painting \"Totes Meer\" by Paul Nash (1889–1946), a fervent admirer of Ernst. Friedrich's work has been cited as an inspiration by other major 20th-century artists, including Mark Rothko (1903–70), Gerhard Richter (b. 1932), Gotthard Graubner and Anselm Kiefer (b. 1945). Friedrich's Romantic paintings have also been singled out by writer Samuel Beckett (1906–89), who, standing before \"Man and Woman Contemplating the Moon\", said \"This was the source of \"Waiting for Godot\", you know.\"\n\nIn his 1961 article \"The Abstract Sublime\", originally published in ARTnews, the art historian Robert Rosenblum drew comparisons between the Romantic landscape paintings of both Friedrich and Turner with the Abstract Expressionist paintings of Mark Rothko. Rosenblum specifically describes Friedrich's 1809 painting \"The Monk by the Sea\", Turner's \"The Evening Star\" and Rothko's 1954 \"Light, Earth and Blue\" as revealing affinities of vision and feeling. According to Rosenblum, \"Rothko, like Friedrich and Turner, places us on the threshold of those shapeless infinities discussed by the aestheticians of the Sublime. The tiny monk in the Friedrich and the fisher in the Turner establish a poignant contrast between the infinite vastness of a pantheistic God and the infinite smallness of His creatures. In the abstract language of Rothko, such literal detail—a bridge of empathy between the real spectator and the presentation of a transcendental landscape—is no longer necessary; we ourselves are the monk before the sea, standing silently and contemplatively before these huge and soundless pictures as if we were looking at a sunset or a moonlit night.\"\n\nUntil 1890, and especially after his friends had died, Friedrich's work lay in near-oblivion for decades. Yet, by 1890, the symbolism in his work began to ring true with the artistic mood of the day, especially in central Europe. However, despite a renewed interest and an acknowledgment of his originality, his lack of regard for \"painterly effect\" and thinly rendered surfaces jarred with the theories of the time.\n\nDuring the 1930s, Friedrich's work was used in the promotion of Nazi ideology, which attempted to fit the Romantic artist within the nationalistic \"Blut und Boden\". It took decades for Friedrich's reputation to recover from this association with Nazism. His reliance on symbolism and the fact that his work fell outside the narrow definitions of modernism contributed to his fall from favour. In 1949, art historian Kenneth Clark wrote that Friedrich \"worked in the frigid technique of his time, which could hardly inspire a school of modern painting\", and suggested that the artist was trying to express in painting what is best left to poetry. Clark's dismissal of Friedrich reflected the damage the artist's reputation sustained during the late 1930s.\n\nFriedrich's reputation suffered further damage when his imagery was adopted by a number of Hollywood directors, such as Walt Disney, built on the work of such German cinema masters as Fritz Lang and F. W. Murnau, within the horror and fantasy genres. His rehabilitation was slow, but enhanced through the writings of such critics and scholars as Werner Hofmann, Helmut Börsch-Supan and Sigrid Hinz, who successfully rejected and rebutted the political associations ascribed to his work, and placed it within a purely art-historical context. By the 1970s, he was again being exhibited in major galleries across the world, as he found favour with a new generation of critics and art historians.\n\nToday, his international reputation is well established. He is a national icon in his native Germany, and highly regarded by art historians and art connoisseurs across the Western World. He is generally viewed as a figure of great psychological complexity, and according to Vaughan, \"a believer who struggled with doubt, a celebrator of beauty haunted by darkness. In the end, he transcends interpretation, reaching across cultures through the compelling appeal of his imagery. He has truly emerged as a butterfly—hopefully one that will never again disappear from our sight\".\n\nFriedrich was a prolific artist who produced more than 500 attributed works. In line with the Romantic ideals of his time, he intended his paintings to function as pure aesthetic statements, so he was cautious that the titles given to his work were not overly descriptive or evocative. It is likely that some of today's more literal titles, such as \"The Stages of Life\", were not given by the artist himself, but were instead adopted during one of the revivals of interest in Friedrich. Complications arise when dating Friedrich's work, in part because he often did not directly name or date his canvases. He kept a carefully detailed notebook on his output, however, which has been used by scholars to tie paintings to their completion dates.\n\n\n", "id": "5654", "title": "Caspar David Friedrich"}
{"url": "https://en.wikipedia.org/wiki?curid=5655", "text": "Courtney Love\n\nCourtney Michelle Love (\"née\" Harrison; born July 9, 1964) is an American singer, actress, writer, and visual artist. Prolific in the punk and grunge scenes of the 1990s, Love has enjoyed a career that spans four decades. She rose to prominence as the frontwoman of the alternative rock band Hole, which she formed in 1989. Love has drawn public attention for her uninhibited live performances and confrontational lyrics, as well as her highly publicized personal life following her marriage to Kurt Cobain.\n\nThe daughter of Hank Harrison and psychotherapist Linda Carroll, Love had an itinerant early life. She spent her formative years in San Francisco and Portland, Oregon, where she was in a series of short-lived bands before being cast in two films by British director Alex Cox. After forming Hole in 1989, she received substantial attention from underground rock press for the group's debut album, produced by Kim Gordon. Hole's second release, \"Live Through This\" (1994), gave her high-profile renown with critical accolades and multi-platinum sales. In 1995, Love returned to acting, earning a Golden Globe Award nomination for her performance as Althea Leasure in Miloš Forman's \"The People vs. Larry Flynt\" (1996), which established her as a mainstream actress. The following year, she saw further mainstream success with the release of Hole's third album, \"Celebrity Skin\" (1998), which was nominated for multiple Grammy Awards.\n\nLove continued to work as an actress into the early 2000s, appearing in big-budget pictures such as \"Man on the Moon\" (1999) and \"Trapped\" (2002), before releasing her first solo album, \"America's Sweetheart\", in 2004. The next few years were marked by publicity surrounding Love's legal troubles and drug addiction, which resulted in a mandatory lockdown rehabilitation sentence in 2005 while she was in the process of writing a second planned solo album. That project became \"Nobody's Daughter\", which was released in 2010 as a Hole album but without any other members of the original lineup. Between 2014 and 2015, she released two solo singles and returned to acting in the network series \"Sons of Anarchy\" and \"Empire\".\n\nLove has also had endeavors in writing, co-creating and co-authoring three volumes of a manga, \"Princess Ai\", between 2004 and 2006, as well as a memoir, \"\" (2006). In 2012, she premiered an exhibit of mixed media visual art titled \"And She's Not Even Pretty\".\n\nLove was born Courtney Michelle Harrison on July 9, 1964 in San Francisco, California, the daughter of Linda Carroll (\"née\" Risi) and Hank Harrison, a publisher and road manager for the Grateful Dead. Love's godfather is the founding Grateful Dead bassist Phil Lesh. Her mother, who was adopted as a child, was later revealed to be the biological daughter of novelist Paula Fox. Love's great-grandmother was screenwriter Elsie Fox. Love is of Cuban, English, German, Irish, and Welsh descent.\n\nLove spent her early years in the Haight-Ashbury district of San Francisco until her parents' 1969 divorce, after which her father's custody was withdrawn when her mother alleged that he had fed LSD to her as a toddler, which he denied. Love's mother, who was studying to be a psychologist, had her in therapy by the age of two. In 1970, her mother moved the family to the rural community of Marcola, Oregon, where they lived along the Mohawk River, while her mother completed her degree at the University of Oregon. She described her parents' household as being full of \"hairy, wangly-ass hippies running around naked [doing] Gestalt therapy. My mom was also adamant about a gender-free household: no dresses, no patent leather shoes, no canopy beds, nothing.\" Love was legally adopted by her then-stepfather, Frank Rodriguez, with whom her mother had Love's two half-sisters, Jaimee and Nicole; another brother, Joshua, was adopted at three years old, from an African American family; and a half-brother died in infancy of a heart defect when Love was ten. Love attended a Montessori school in Eugene, where she struggled academically and had trouble making friends. At age nine, a psychologist noted that she exhibited signs of autism.\n\nIn 1972, Love's mother divorced Rodriguez, remarried, and moved the family to New Zealand. There, she enrolled Love at Nelson College for Girls, from which Love was eventually expelled. Love's mother sent her back to the United States in 1973, where she was raised in Portland, Oregon by her former stepfather and other family friends. During this time, her mother gave birth to two of Love's other half-brothers, Tobias and Daniel. At age fourteen, Love was arrested for shoplifting a T-shirt from a Woolworth's, and was sent to Hillcrest Correctional Facility, a juvenile hall in Salem, Oregon. She was then placed in foster care until she became legally emancipated at age sixteen. She supported herself by working illegally as a topless dancer at Mary's Club in downtown Portland adopting the last name \"Love\" to conceal her identity; she later adopted \"Love\" as her surname. She also worked various odd jobs, including picking berries at a farm in Troutdale, Oregon, and as a disc jockey. During this time, she enrolled at Portland State University, studying English and philosophy. Love has said that she \"didn't have a lot of social skills\", and that she learned them while frequenting gay clubs in Portland.\n\nIn 1981, Love was granted a small trust fund that had been left by her adoptive grandparents, which she used to travel to Dublin, Ireland, where her biological father was living. While there, she enrolled in courses at Trinity College, studying theology for two semesters. She would later receive honorary patronage from Trinity's University Philosophical Society in 2010. In the United Kingdom, she became acquainted with musician Julian Cope and his band, The Teardrop Explodes, in Liverpool and briefly lived in his house. \"They kind of took me in\", she recalled. \"I was sort of a mascot; I would get them coffee or tea during rehearsal.\" In Cope's autobiography \"Head-On\", Love is referred to as \"the adolescent\". After spending a year abroad, Love returned to Portland: \"I thought that [going to the United Kingdom] was my peak life experience\", she said in 2011. \"Nothing else will happen to me again.\" In 1983, she took short-lived jobs working as an erotic dancer in Japan and later Taiwan, but was deported after the club was shut down by the government.\n\nLove began several music projects in the 1980s, first forming Sugar Babylon (later Sugar Babydoll) in Portland with her friends Ursula Wehr and Robin Barbur. In 1982, Love attended a Faith No More concert in San Francisco and convinced the members to let her join as a singer. The group recorded material with Love as a vocalist, but she was subsequently kicked out of the band. According to the Faith No More keyboardist Roddy Bottum, who remained Love's friend in the years after, the band wanted a \"male energy\".\n\nShe later formed the Pagan Babies with friend Kat Bjelland, whom she met at the Satyricon club in Portland in 1984. As Love later reflected, \"The best thing that ever happened to me in a way, was Kat.\" Love asked Bjelland to start a band with her as a guitarist, and the two moved to San Francisco in June 1985, where they recruited bassist Jennifer Finch and drummer Janis Tanaka. According to Bjelland, \"[Courtney] didn't play an instrument at the time\" aside from keyboards, so Bjelland would transcribe Love's musical ideas on guitar for her. The group played several house shows and recorded one 4-track demo before disbanding in late 1985. After Pagan Babies, Love moved to Minneapolis, where Bjelland had formed the group Babes in Toyland, and briefly worked as a concert promoter before returning to California.\n\nDeciding to shift her focus to acting, Love enrolled at the San Francisco Art Institute and studied film with George Kuchar. Love featured in one of his short films, titled \"Club Vatican\". In 1985 she submitted an audition tape for the role of Nancy Spungen in the Sid Vicious biopic \"Sid and Nancy\" (1986), and was given a minor supporting role by director Alex Cox. After filming \"Sid and Nancy\" in New York City, she worked at a peep show in Times Square and squatted at the ABC No Rio social center and Pyramid Club in the East Village. The same year, Cox cast her in a leading role in his film \"Straight to Hell\" (1987), a spaghetti western starring Joe Strummer and Grace Jones filmed in Spain in 1986. The film caught the attention of Andy Warhol, who featured Love in an episode of \"Andy Warhol's Fifteen Minutes\" with Robbie Nevil in a segment titled \"C'est la Vie\". She also had a part in the 1988 Ramones music video for \"I Wanna Be Sedated\", appearing as a bride among dozens of party guests.\n\nIn 1988, Love aborted her acting career and left New York, returning to the West Coast, citing the \"celebutante\" fame she'd attained as the central reason. \"I hated it\", she recalled. \"It was misery itself.\" She returned to stripping in the small town of McMinnville, Oregon, where she was recognized by customers at the bar. This prompted Love to go into isolation, so she relocated to Anchorage, Alaska. \"I decided to move to Alaska because I needed to get my shit together and learn how to work\", Love said in retrospect. \"So I went on this sort of vision quest. I got rid of all my earthly possessions. I had my bad little strip clothes and some big sweaters, and I moved into a trailer with a bunch of other strippers.\"\n\nAt the end of 1988, Love taught herself to play guitar and relocated to Los Angeles, where she placed an ad in a local music zine: \"I want to start a band. My influences are Big Black, Sonic Youth, and Fleetwood Mac.\" Love recruited lead guitarist Eric Erlandson; Lisa Roberts, her neighbor, as bassist; and drummer Caroline Rue, whom she met at a Gwar concert. Love named the band Hole after a line from Euripides' \"Medea\" (\"There is a hole that pierces right through me\") as well as a conversation she had had with her mother, in which she told her that she couldn't live her life \"with a hole running through her\".\n\nLove continued to work at strip clubs in the band's formative stages, saving money to purchase backline equipment and a touring van, and rehearsed at a studio in Hollywood that was loaned to her by the Red Hot Chili Peppers. Hole played their first show in November 1989 at Raji's, a rock club in central Hollywood. The band's debut single, \"Retard Girl\", was issued in April 1990 through the Long Beach indie label Sympathy for the Record Industry, and was given airtime by Rodney Bingenheimer's show on local rock station KROQ. That fall, the band appeared on the cover of \"Flipside\", a Los Angeles-based punk fanzine. In early 1991, the band released their second single, \"Dicknail\", through Sub Pop Records.\n\nWith no wave, noise rock and grindcore bands being major influences on Love, Hole's first studio album, \"Pretty on the Inside\", captured a particularly abrasive sound and contained disturbing lyrics, described by \"Q\" magazine as \"confrontational [and] genuinely uninhibited\". The record was released in September 1991 on Caroline Records, produced by Kim Gordon of Sonic Youth with assistant production from Gumball's Don Fleming; Love and Gordon had initially met when Hole opened for Sonic Youth during their promotional tour for \"Goo\" at the Whisky a Go Go in November 1990. In early 1991, Love sent Gordon a personal letter asking her to produce the record for the band, to which she agreed. Though Love would later say it was \"unlistenable\" and \"[un]melodic\", the album received generally positive critical reception from indie and punk rock critics and was labeled one of the twenty best albums of the year by \"Spin\" magazine. It also gained a following in the United Kingdom, charting at 59 on the UK Albums Chart, and its lead single, \"Teenage Whore\", entered the country's indie chart at number one. The underlying feminist slant of the album's songs led many to mistakenly tag the band as being part of the riot grrrl movement, a movement that Love did not associate with. The band toured in support of the record, headlining with Mudhoney in Europe; in the United States, they opened for The Smashing Pumpkins, and performed at CBGB in New York City.\n\nAfter the release of \"Pretty on the Inside\", Love began dating Kurt Cobain and became pregnant. During Love's pregnancy, Hole recorded a cover of \"Over the Edge\" for a Wipers tribute album, and recorded their fourth single, \"Beautiful Son\", which was released in April 1993. Love and Cobain married in February 1992 and, after the birth of their daughter Frances Bean Cobain, relocated to Carnation, Washington and then to Seattle. On September 8, 1993, Love and Cobain made their only public performance together at the Rock Against Rape benefit in Hollywood, performing two acoustic duets of \"Pennyroyal Tea\" and \"Where Did You Sleep Last Night\". Love also performed electric versions of two new Hole songs, \"Doll Parts\" and \"Miss World\", both written for the band's upcoming second album.\n\nIn October 1993, Hole recorded their second album, \"Live Through This\", in Atlanta. The album featured a new lineup with bassist Kristen Pfaff and drummer Patty Schemel. \"Live Through This\" was released on Geffen's subsidiary label DGC in April 1994, four days after Love's husband, Cobain, was found dead after having committed suicide in their Seattle home. Two months later, in June 1994, bassist Kristen Pfaff died of a heroin overdose, and Love recruited Melissa Auf der Maur for the band's impending tour. Love, who was rarely seen in public in the months preceding the tour, split time between two Washington homes, Atlanta, the Paramount Hotel in New York City, and the Namgyal Buddhist Monastery in New York.\n\n\"Live Through This\" was a commercial and critical success, hitting platinum RIAA certification in April 1995 and receiving numerous critical accolades. The success of the record combined with Cobain's suicide resulted in a high level of publicity for Love, and she was featured on Barbara Walters' \"10 Most Fascinating People\" in 1995. At Hole's performance on August 26, 1994 at the Reading Festival— Love's first public performance following her husband's death— she appeared onstage with outstretched arms, mimicking crucifixion. A MTV review of their performance referred to it as \"by turns macabre, frightening and inspirational.\" John Peel wrote in \"The Guardian\" that Love's disheveled appearance \"would have drawn whistles of astonishment in Bedlam\", and that her performance \"verged on the heroic ... Love steered her band through a set which dared you to pity either her recent history or that of the band ... the band teetered on the edge of chaos, generating a tension which I cannot remember having felt before from any stage.\"\n\nThe band performed a series of riotous concerts during the tour, with Love frequently appearing hysterical onstage, flashing crowds, stage diving, and getting into fights with audience members. One journalist reported that at the band's show in Boston in December 1994, \"Love interrupted the music and talked about her deceased husband Kurt Cobain, and also broke out into Tourette syndrome-like rants. The music was great, but the raving was vulgar and offensive, and prompted some of the audience to shout back at her.\" In retrospect, Love said she \"couldn't remember much\" of the shows as she was using drugs heavily at the time. On Valentine's Day 1995, Hole performed a well-reviewed acoustic set on \"MTV Unplugged\" at the Brooklyn Academy of Music, and they continued to tour late into the year, concluding their world tour with an appearance at the 1995 \"MTV Video Music Awards\", where they were nominated for Best Alternative Video for \"Doll Parts\".\n\nAfter Hole's world tour concluded in 1996, Love made a return to acting, first in small roles in the Jean-Michel Basquiat biopic \"Basquiat\" and the drama \"Feeling Minnesota\" (1996), before landing the co-starring role of Larry Flynt's wife Althea in Miloš Forman's critically acclaimed 1996 film \"The People vs. Larry Flynt\". Despite Columbia Pictures' reluctance to hire Love due to her troubled past, she received critical acclaim for her performance in the film after its release in December 1996, earning a Golden Globe nomination for Best Actress, and a New York Film Critics Circle Award for Best Supporting Actress. Critic Roger Ebert called her work in the film \"quite a performance; Love proves she is not a rock star pretending to act, but a true actress.\" She won several other awards from various film critic associations for the film. During this time, she also modeled for Versace advertisements and appeared in Vogue Italia.\n\nIn late 1997, Hole released a compilation album, \"My Body, the Hand Grenade\", featuring rare singles and B-sides, and an EP titled \"The First Session\" which consisted of the band's first recording session in 1990. In September 1998, Hole released their third studio album, \"Celebrity Skin\", which marked something of a transformation for Love, featuring a stark power pop sound as opposed to the group's earlier punk rock influences. Love divulged her ambition of making an album where \"art meets commerce ... there are no compromises made, it has commercial appeal, and it sticks to [our] original vision.\" She said she was influenced by Neil Young, Fleetwood Mac, and My Bloody Valentine when writing the album. The Smashing Pumpkins' Billy Corgan helped co-write several songs on the album. \"Celebrity Skin\" was well received by critics; \"Rolling Stone\" called the album \"accessible, fiery and intimate—often at the same time ... a basic guitar record that's anything but basic\". \"Celebrity Skin\" went multi-platinum, and topped \"Best of Year\" lists at \"Spin\" and the \"The Village Voice\". The album garnered the band their only No. 1 hit single on the Modern Rock Tracks chart with the title track \"Celebrity Skin\". The band promoted the album through MTV performances and at the 1998 Billboard Music Awards. Hole toured with Marilyn Manson on the Beautiful Monsters Tour in 1999, but dropped out of the tour nine dates in after a dispute over production costs between Love and Manson; Hole resumed touring with Imperial Teen.\n\nBefore the release and promotion of \"Celebrity Skin\", Love and Fender designed a low-priced Squier brand guitar, called Vista Venus. The instrument featured a shape inspired by Mercury, a little-known independent guitar manufacturer, Stratocaster, and Rickenbacker's solid body guitars and had a single-coil and a humbucker pickup, and was available in 6-string and 12-string versions. In an early 1999 interview, Love said about the Venus: \"I wanted a guitar that sounded really warm and pop, but which required just one box to go dirty ... And something that could also be your first band guitar. I didn't want it all teched out. I wanted it real simple, with just one pickup switch.\" In 1999, Love was awarded an Orville H. Gibson award for Best Female Rock Guitarist. During this time, she also starred opposite Jim Carrey as his longtime partner Lynne Margulies in the Andy Kaufman biopic \"Man on the Moon\" (1999), which was followed with a role as William S. Burroughs's wife Joan Vollmer in \"Beat\" (2000) alongside Kiefer Sutherland.\n\nAfter touring for \"Celebrity Skin\" finished, Auf der Maur left the band to tour with The Smashing Pumpkins; Hole's touring drummer Samantha Maloney left soon after. Love and Erlandson released the single \"Be A Man\"—an outtake from the \"Celebrity Skin\" sessions—for the soundtrack of the Oliver Stone film \"Any Given Sunday\" (1999). The group became dormant in the following two years, and Love starred in several more films, including in \"Julie Johnson\" (2001) as Lili Taylor's lesbian lover, for which she won an Outstanding Actress award at L.A.'s Outfest, and in the thriller \"Trapped\" (2002), alongside Kevin Bacon and Charlize Theron. In May 2002, Hole officially announced their breakup amid continuing litigation with Universal Music Group over their record contract.\n\nWith Hole in disarray, Love began a \"punk rock femme supergroup\" called Bastard during late 2001, enlisting Schemel, Veruca Salt co-frontwoman Louise Post, and bassist Gina Crosley. Though a demo was completed, the project never reached fruition.\nIn 2002, Love began composing an album with songwriter Linda Perry of 4 Non Blondes, titled \"America's Sweetheart\", also reuniting with drummer Patty Schemel. Love signed with Virgin Records to release it, and began recording it in France in 2003. \"America's Sweetheart\" was released in February 2004, and received mixed reviews from critics. Charles Aaron of \"Spin\" called it a \"jaw-dropping act of artistic will and a fiery, proper follow-up to 1994's \"Live Through This\"\" and awarded it eight out of ten stars, while \"The Village Voice\" said: \"[Love is] willing to act out the dream of every teenage brat who ever wanted to have a glamorous, high-profile hissyfit, and she turns those egocentric nervous breakdowns into art. Sure, the art becomes less compelling when you've been pulling the same stunts for a decade. But, honestly, is there anybody out there who fucks up better?\" The album sold less than 100,000 copies. Love has publicly expressed her regret over the record several times, reasoning that her drug issues at the time were to blame: \"\"America's Sweetheart\" was my one true piece of shit. It has no cohesive thread. I just hate it\", she said in a 2014 interview. Shortly after the record was released, Love told Kurt Loder on \"TRL\": \"I cannot exist as a solo artist. It's a joke.\"\n\nLove also collaborated on a semi-autobiographical manga titled \"Princess Ai\" (Japanese: プリンセス·アイ物語), which she co-wrote with Stu Levy. The manga was illustrated by Misaho Kujiradou and Ai Yazawa, and was released in three volumes in both the United States and Japan between 2004 and 2006.\n\nIn 2005, Love was ordered into lockdown rehab by a California judge after a series of legal issues and controlled substance charges. After her release in 2006, she published a memoir, \"\", and started recording what would become her second solo album, \"How Dirty Girls Get Clean\", collaborating again with Perry and Smashing Pumpkins vocalist/guitarist Billy Corgan in the writing and recording. Love had written several songs, including an anti-cocaine song titled \"Loser Dust\", during her time in rehab in 2005. She told \"Billboard\": \"My hand-eye coordination was so bad [after the drug use], I didn't even know chords anymore. It was like my fingers were frozen. And I wasn't allowed to make noise [in rehab] ... I never thought I would work again.\" Some tracks and demos from the album (initially planned for release in 2008) were leaked on the internet in 2006, and a documentary entitled \"The Return of Courtney Love\", detailing the making of the album, aired on the British television network More4 in the fall of that year. A rough acoustic version of \"Never Go Hungry Again\", recorded during an interview for \"The Times\" in November, was also released. Incomplete audio clips of the song \"Samantha\", originating from an interview with NPR, were also distributed on the internet in 2007.\nIn June 2009, \"NME\" published an article detailing Love's plan to reunite Hole and release a new album, \"Nobody's Daughter\". In response, former Hole guitarist Eric Erlandson stated in \"Spin\" magazine that contractually no reunion could take place without his involvement; therefore \"Nobody's Daughter\" would remain Love's solo record, as opposed to a \"Hole\" record. Love responded to Erlandson's comments in a Twitter post, claiming \"he's out of his mind, Hole is my band, my name, and my Trademark\". \"Nobody's Daughter\" was released worldwide as a Hole album on April 27, 2010. For the new line-up, Love recruited guitarist Micko Larkin, Shawn Dailey (bass guitar), and Stu Fisher (drums, percussion). \"Nobody's Daughter\" featured material written and recorded for Love's unfinished solo album, \"How Dirty Girls Get Clean\", including \"Pacific Coast Highway\", \"Letter to God\", \"Samantha\", and \"Never Go Hungry\", although they were re-produced in the studio with Larkin and engineer Michael Beinhorn. The album's subject matter was largely centered on Love's tumultuous life between 2003 and 2007, and featured a polished folk rock sound, and more acoustic guitar work than previous Hole albums.\n\nThe first single from \"Nobody's Daughter\" was \"Skinny Little Bitch\", released in promotion of the album in March 2010. The album received mixed reviews. Robert Sheffield of \"Rolling Stone\" gave the album three out of five stars, saying that Love \"worked hard on these songs, instead of just babbling a bunch of druggy bullshit and assuming people would buy it, the way she did on her 2004 flop, \"America's Sweetheart\"\". Sal Cinquemani of \"Slant Magazine\" also gave the album three out of five stars, saying, \"It's Marianne Faithfull's substance-ravaged voice that comes to mind most often while listening to songs like 'Honey' and 'For Once in Your Life'. The latter track is, in fact, one of Love's most raw and vulnerable vocal performances to date ... the song offers a rare glimpse into the mind of a woman who, for the last 15 years, has been as famous for being a rock star as she's been for being a victim.\" Love and the band toured internationally from 2010 into late 2012 promoting the record, after which she dropped the Hole name and returned to a solo career.\n\nIn May 2012, Love debuted an art collection at Fred Torres Collaborations in New York titled \"\"And She's Not Even Pretty\"\", which contained over forty drawings and paintings by Love composed in ink, colored pencil, pastels, and watercolors. Later in the year, she collaborated with Michael Stipe on the track \"Rio Grande\" for Johnny Depp's sea shanty album \"\", and in 2013, co-wrote and contributed vocals on \"Rat A Tat\" from Fall Out Boy's album \"Save Rock and Roll\"; she also appeared in the music video for the track. After solo performances in December 2012 and January 2013, Love appeared in advertisements for Yves Saint Laurent alongside Kim Gordon and Ariel Pink. Love completed a solo tour of North America in mid-2013, which was purported to be in promotion of an upcoming solo album; however, it was ultimately dubbed a \"greatest hits\" tour, and featured songs from Love's and Hole's back catalogue. Love told \"Billboard\" at the time that she had recorded eight songs in the studio. \"[These songs] are not my usual (style)\", Love said. \"I don't have any Fleetwood Mac references on it. Usually I always have a Fleetwood Mac reference as well as having, like, Big Black references. These are very unique songs that sort of magically happened.\"\n\nOn April 22, 2014, Love debuted the song \"You Know My Name\" on BBC Radio 6 to promote her tour of the United Kingdom. It was released as a double A-side single with the song \"Wedding Day\" on May 4, 2014, on her own label Cherry Forever Records via Kobalt Label Services. The tracks were produced by Michael Beinhorn, and feature Tommy Lee on drums. In an interview with the BBC, Love revealed that she and former Hole guitarist Eric Erlandson had reconciled, and had been rehearsing new material together, along with former bassist Melissa Auf der Maur and drummer Patty Schemel, though she did not confirm a reunion of the band. On May 1, 2014, in an interview with \"Pitchfork\", Love commented further on the possibility of Hole reuniting, saying:\n\"I'm not going to commit to it happening, because we want an element of surprise. There's a lot of \"i\"s to be dotted and \"t\"s to be crossed.\"\n\nIn 2014, Love was cast in several television series in supporting parts, including the FX series \"Sons of Anarchy\", \"Revenge\", and Lee Daniels' network series \"Empire\" in a recurring guest role as Elle Dallas. The track \"Walk Out on Me\" featuring Love was included on the \"\" album, which debuted at number 1 on the Billboard 200. Alexis Petridis of \"The Guardian\" praised the track, saying: \"The idea of Courtney Love singing a ballad with a group of gospel singers seems faintly terrifying ... the reality is brilliant. Love's voice fits the careworn lyrics, effortlessly summoning the kind of ravaged darkness that Lana Del Rey nearly ruptures herself trying to conjure up.\"\n\nIn addition to television acting, Love collaborated with theater producer Todd Almond, starring in \"Kansas City Choir Boy\", a collaborative \"pop opera\" which showed at the Manhattan arts center Here during their annual \"Prototype\" festival in January 2015. The show toured later in the year, with performances in Boston and Los Angeles. In early 2015, Love joined Lana Del Rey on her Endless Summer Tour, performing as an opener on the tour's eight West Coast shows. During her tenure on Del Rey's tour, Love debuted a new single, \"Miss Narcissist\", released on Wavves' independent label Ghost Ramp. She also was cast in a supporting role in James Franco's film \"The Long Home\", based on William Gay's novel of the same name, marking her first film role in over ten years. In January 2016, Love released a clothing line in collaboration with Sophia Amoruso titled \"Love, Courtney\", featuring eighteen pieces reflecting Love's style over the course of her career.\n\nIn November 2016, Love began filming the pilot for \"A Midsummer's Nightmare\", a Shakespeare anthology series adapted for \"Lifetime\", in Vancouver, British Columbia. In early 2017, it was announced in \"Variety\" that Love had been cast as Kitty Menendez in \"The Menendez Brothers\", a biopic television film based on the lives of Lyle and Erik Menendez.\n\nLove has been candid about her diverse musical influences, the earliest being Patti Smith and The Pretenders, whom she discovered while in juvenile hall. As a teenager, she named Flipper, Kate Bush, Soft Cell, Joni Mitchell, Lou Reed, and Dead Kennedys among her favorite artists, as well as several new wave and post-punk bands, such as Echo and the Bunnymen, The Smiths, Siouxsie and the Banshees, Television, Bauhaus, and Joy Division. While in Ireland at age fifteen, she saw The Virgin Prunes perform live in Dublin, an event she credited as being a pivotal influence. Her varying genre interests were illustrated in a 1991 interview with \"Flipside\", in which she stated: \"There's a part of me that wants to have a grindcore band and another that wants to have a Raspberries-type pop band\". Love also embraced the influence of experimental artists and punk rock groups, including Sonic Youth, Swans, Big Black, Diamanda Galás, the Germs, and The Stooges. While writing \"Celebrity Skin\", Love was mainly influenced by Neil Young and My Bloody Valentine. She also cited her contemporary PJ Harvey as an influence, saying, \"The one rock star that makes me know I'm shit is Polly Harvey. I'm nothing next to the purity that she experiences.\"\n\n\"Spin\"s October 1991 review of Hole's first album noted Love's layering of harsh and abrasive riffs buried more sophisticated musical arrangements. In 1998, Love stated that Hole had \"always been a pop band. We always had a subtext of pop. I always talked about it, if you go back ... what'll sound like some weird Sonic Youth tuning back then to you was sounding like the Raspberries to me, in my demented pop framework\". Love's lyrical content is composed from a female's point of view, and her lyrics have been described as \"literate and mordant\" and noted by scholars for \"articulating a third-wave feminist consciousness\". According to a 2014 interview, lyrics have remained the most important component of songwriting for Love: \"I want it to look just as good on the page as it would if it was in a poetry book\". A great deal of her songwriting has been diaristic in nature. Common themes present in Love's songs during her early career included body image, rape, suicide, conformity, elitism, pregnancy, prostitution, and death. In a 1991 interview with Everett True, Love said: \"I try to place [beautiful imagery] next to fucked up imagery, because that's how I view things ... I sometimes feel that no one's taken the time to write about certain things in rock, that there's a certain female point of view that's never been given space\". \nCritics have noted that Love's later musical work is more lyrically introspective. \"Celebrity Skin\" and \"America's Sweetheart\" are lyrically centered on celebrity life, Hollywood, and drug addiction, while continuing Love's interest in vanity and body image. \"Nobody's Daughter\" was lyrically reflective of Love's past relationships and her struggle for sobriety, with the majority of its lyrics written while she was in rehab in 2006.\n\nLiterature and poetry have often been a major influence on her writing; Love said she had \"always wanted to be a poet, but there was no money in it\". She has named the works of T.S. Eliot and Charles Baudelaire as influential, and referenced works by Dante Rossetti, William Shakespeare, Rudyard Kipling, and Anne Sexton in her lyrics.\n\nLove possesses a contralto vocal range, and her vocal style has been described as \"raw and distinctive\". According to Love, she never wanted to be a singer, but rather aspired to be a skilled guitarist: \"I'm such a lazy bastard though that I never did that\", she said. \"I was always the only person with the nerve to sing, and so I got stuck with it\". She has been regularly noted by critics for her husky vocals as well as her \"banshee[-like]\" screaming abilities. Her vocals have been compared to those of Johnny Rotten, and David Fricke of \"Rolling Stone\" described them as \"lung-busting\" and \"a corrosive, lunatic wail\". Upon the release of Hole's 2010 album, \"Nobody's Daughter\", Amanda Petrusich of \"Pitchfork\" compared Love's raspy, unpolished vocals to those of Bob Dylan.\n\nLove has played a variety of Fender guitars throughout her career, including a Jaguar and a vintage 1965 Jazzmaster; the latter was purchased by the Hard Rock Cafe and is on display in New York City. Love is seen playing her Jazzmaster in the music video for \"Miss World\". Earlier in Hole's career, between 1989 and 1991, Love primarily played a Rickenbacker 425 because she \"preferred the 3/4 neck\", but she destroyed the guitar onstage at a 1991 concert opening for The Smashing Pumpkins. In the mid-1990s, she often played a guitar made by Mercury, an obscure company that manufactured custom guitars, as well as a Univox Hi-Flier. Fender's Vista Venus, designed by Love in 1998, was partially inspired by Rickenbacker guitars as well as her Mercury. During her 2010 and more recent tours, Love has played a Rickenbacker 360 onstage. She has referred to herself as \"a shit guitar player\", further commenting in a 2014 interview: \"I can still write a song, but [the guitar playing] sounds like shit ... I used to be a good rhythm player but I am no longer dependable.\" Love's setup has included Fender tube gear, Matchless, Ampeg, Silvertone and a solid-state 1976 Randall Commander.\n\nLove's candidness concerning her struggles with drug addiction and legal issues has made her subject to significant media interest over the course of her career. She has openly discussed the substance abuse problems she has had throughout her life. She became addicted to heroin in the early 1990s, and her addiction was placed in the media spotlight in 1992 when \"Vanity Fair\" published an article by journalist Lynn Hirschberg which stated that Love was addicted to heroin during her pregnancy. This resulted in the custody of Love and Cobain's newborn daughter, Frances, being temporarily awarded to Love's sister. Love claimed she was misquoted in the piece, and asserted that she had immediately quit using the drug during her first trimester after she discovered she was pregnant.\n\nAfter the suicide of her husband Kurt Cobain in 1994, public and media interest in Love heightened, with many journalists labeling her the \"Yoko Ono\" of Generation X. Her erratic onstage behavior and suspected drug problems during Hole's 1994–1995 world tour only exacerbated the media's critical observations of her. On July 4, 1995, at the Lollapalooza Festival in George, Washington, Love punched musician Kathleen Hanna in the face after alleging she had made a joke about her daughter. Love was charged with assault, to which she pleaded guilty, and was sentenced to anger management classes. The same year, she was arrested in Melbourne for disrupting a Qantas Airways flight after getting into an argument with a stewardess.\n\nIn 1996, Love went through rehabilitation and quit using heroin at the insistence of director Miloš Forman, who cast her in a leading role in \"The People vs. Larry Flynt\". She was ordered to take multiple urine tests under the supervision of Columbia Pictures while filming the movie, and passed all of them. During this period, and after the release of Hole's \"Celebrity Skin\", Love maintained a more polished public image, though she attracted public attention after punching \"Los Angeles Times\" journalist Belissa Cohen in the face at a party; the suit was settled out of court for an undisclosed sum. In February 2003, Love was banned from Virgin Airlines by founder Richard Branson after being arrested at Heathrow Airport for disrupting a flight. In October of that year, in the midst of what Love would later admit was a serious cocaine and prescription drug addiction, she was arrested in Los Angeles after breaking several windows of her producer and then-boyfriend James Barber's home, and was charged with being under the influence of a controlled substance; the ordeal resulted in her losing custody of her daughter. On March 18, 2004, Love was arrested in New York City for allegedly striking a 24-year-old male fan with a microphone stand at a concert at the Bowery Ballroom. Four days later, on March 22, Love called in multiple times to \"The Howard Stern Show\", making various claims and speaking erratically; in broadcast conversations with Stern, she claimed that the incident with the fan had not occurred, and that actress Natasha Lyonne, who was at the concert, was told by the alleged victim that he had been paid $10,000 to file a false claim leading to Love's arrest. She pleaded guilty to disorderly conduct in October 2004.\n\nOn July 9, 2004, Love's 40th birthday, she was arrested for failing to make a court appearance and taken to Bellevue Hospital, allegedly incoherent, where she was put on a 72-hour watch. According to police, she was believed to be a potential \"danger to herself\", but was deemed mentally sound and released to a rehab facility two days later. In 2005 and 2006, after making several public appearances clearly intoxicated (on the \"Late Show with David Letterman\" and the \"Comedy Central Roast\" of Pamela Anderson) and suffering drug-related arrests and probation violations, Love was sentenced to six months in lockdown rehab due to struggles with prescription drugs and cocaine. She has stated she has been sober since 2007. In a 2011 interview, she said: \"I've been maligned as this drug freak for years, [but] that's not the way I live anymore.\"\n\nIn 2009, fashion designer Dawn Simorangkir brought a libel suit against Love concerning a defamatory post Love made on her Twitter account, which was settled for $450,000. Six years later, Simorangkir filed another lawsuit against Love for further defamatory Twitter posts, and Love paid a further $350,000 in recompense. A similar suit was brought against Love by her former attorney Rhonda Holmes in 2014, who also accused Love of online defamation, seeking $8 million in damages. It was the first case of alleged Twitter-based libel in U.S. history to make it to trial. The jury, however, found in Love's favor.\n\nShe was briefly married to James Moreland (vocalist of The Leaving Trains) in 1989 for several months, but has said that Moreland was a transvestite and that their marriage was \"a joke\", ending in an annulment filed by Love. After forming Hole in 1989, Love and bandmate Eric Erlandson had a romantic relationship for over a year, and she also briefly dated Billy Corgan in 1991, with whom she has maintained a volatile friendship over the years.\n\nHer most documented romantic relationship was with Kurt Cobain. It is uncertain when they first met; according to Love, she first met Cobain at a Dharma Bums show in Portland where she was doing a spoken-word performance. According to Michael Azerrad, the two met at the Satyricon nightclub in Portland in 1989, though Cobain biographer Charles Cross stated the date was actually February 12, 1990, and that Cobain playfully wrestled Love to the floor after she commented to him in passing that he looked like Dave Pirner of Soul Asylum. Love's bandmate Eric Erlandson said that both he and Love were formally introduced to Cobain in a parking lot after a Butthole Surfers concert at the Hollywood Palladium in 1991. The two later became reacquainted through Jennifer Finch, one of Love's longtime friends and former bandmates.\nLove and Cobain began dating in the fall of 1991, and were married on Waikiki Beach in Honolulu, Hawaii on February 24, 1992. Love wore a satin and lace dress once owned by actress Frances Farmer, and Cobain wore green pajamas. Six months later, on August 18, the couple's only child, a daughter, Frances Bean Cobain, was born. In April 1994, Cobain died of a self-inflicted gunshot wound in their Seattle home while Love was in rehab in Los Angeles. During their marriage, and after Cobain's death, Love became something of a hate-figure among some of Cobain's fans. In reflecting on their relationship, Love said: \"I think that it looked like it was headed for doom, but it didn't feel like it was headed for doom on a daily basis. We went mountain biking; we would go camping. We were damn normal.\" After his cremation, Love divided portions of Cobain's ashes; she kept some in a teddy bear and some in an urn. Another portion of his ashes was taken by Love to the Namgyal Buddhist Monastery in Ithaca, New York in 1994, where they were ceremonially blessed by Buddhist monks and mixed into clay which was made into memorial sculptures.\n\nBetween 1996 and 1999, Love dated her \"The People vs. Larry Flynt\" co-star Edward Norton, and was also linked to comedian Steve Coogan in the early 2000s.\n\nLove has practiced several religions, including Catholicism, Episcopalianism and New Age religions, but has said that Buddhism is the \"most transcendent\" path for her. She has studied and practiced both Tibetan and Nichiren Buddhism since 1989.\n\nLove is a supporter of the Democratic Party, and in 2016 she endorsed Hillary Clinton's campaign for the US presidency. In 2000, Love gave a speech at the Million Mom March to advocate stricter gun control laws in the United States, calling the country's gun laws \"nihilistic and barbaric\", and urging stringent registration of guns, licensing of gun owners, and thorough evaluation of legal and mental health records. Love has also consistently advocated for LGBT rights, and identifies as a feminist.\n\nIn 2000, Love publicly advocated for reform of the record industry in a personal letter published by \"Salon\". In the letter, Love said: \"It's not piracy when kids swap music over the Internet using Napster or Gnutella or Freenet or iMesh or beaming their CDs into a My.MP3.com or MyPlay.com music locker. It's piracy when those guys that run those companies make side deals with the cartel lawyers and label heads so that they can be 'the labels' friend', and not the artists\". In a subsequent interview with Carrie Fisher, Love said that she was interested in starting a union for recording artists, and also discussed race relations in the music industry, advocating for record companies to \"put money back into the black community [whom] white people have been stealing from for years.\" She also cited Limp Bizkit's Fred Durst as an example of \"a white guy [getting] to express a black man's rage with all the privileges of [being] a white guy\".\n\nIn 1993, Love and husband Kurt Cobain performed an acoustic set together at the Rock Against Rape benefit in Los Angeles, which raised awareness and provided resources for victims of sexual abuse. Love has also contributed to amfAR's AIDS research benefits and held live musical performances at their events. In 2009, Love performed a benefit concert for the RED Campaign at Carnegie Hall alongside Laurie Anderson, Rufus Wainwright, and Scarlett Johansson, with proceeds going to AIDS research. In May 2011, she attended Mariska Hargitay's Joyful Heart Foundation event for victims of child abuse, rape, and domestic violence, donating six of her husband Kurt Cobain's personal vinyl records for auction.\n\nLove has also participated with LGBT youth charities, specifically with the Los Angeles Gay and Lesbian Center, where she has taken part in performances at the center's \"An Evening with Women\" events. The proceeds of the event help provide food and shelter for homeless youth; services for seniors; legal assistance; domestic violence services; health and mental health services, and cultural arts programs. Love participated with Linda Perry for the event again in 2012, relating her experiences as a nomadic teenager: This really resonates with me, [because] I was a kid from Oregon, and I came to Hollywood like a lot of people do, and you know, what happens is that we end up on the street ... and if you're gay, or lesbian, or transgendered— the more \"outside\" you are, the more screwed you are in a lot of ways ... Seven thousand kids in Los Angeles a year go out on the street, and forty percent of those kids are gay, lesbian, or transgendered. They come out to their parents, and become homeless. [The charity helps them] get sent to the right foster care, they can get medical help, food, clothing ... and for whatever reason, I don't really know why, but gay men have a lot of foundations, I've played many of them— but the lesbian side of it doesn't have as much money and/or donors, so we're excited that this has grown to cover women and women's affairs.\n\nLove has had a significant impact on female-fronted alternative acts and performers. She has been cited as a particular influence on young female instrumentalists, once infamously proclaiming: \"I want every girl in the world to pick up a guitar and start screaming.\" \"I strap on that motherfucking guitar and you cannot fuck with me. That's my feeling,\" she said. In \"The Electric Guitar: A History of an American Icon\", it is noted that, Having sold over 3 million records in the United States alone, Hole became one of the most successful rock bands of all time fronted by a woman. In 2015, the \"Phoenix New Times\" declared Love the number one greatest female rock star of all time, writing: \"To build a perfect rock star, there are several crucial ingredients: musical talent, physical attractiveness, tumultuous relationships, substance abuse, and public meltdowns, just to name a few. These days, Love seems to have rebounded from her epic tailspin and has leveled out in a slightly more normal manner, but there's no doubt that her life to date is the type of story people wouldn't believe in a novel or a movie.\"\n\nAmong the alternative musicians who have cited Love as an influence are Scout Niblett; Brody Dalle of The Distillers; Dee Dee Penny of Dum Dum Girls; and Nine Black Alps. Contemporary female pop artists Lana Del Rey, Avril Lavigne, Tove Lo, and Sky Ferreira have also cited Love as an influence. Love has frequently been recognized as the most high-profile contributor of feminist music during the 1990s, and for \"subverting [the] mainstream expectations of how a woman should look, act, and sound.\" According to music journalist Maria Raha, \"Hole was the highest-profile female-fronted band of the '90s to openly and directly sing about feminism.\" Patti Smith, a major influence of Love's, also praised her, saying: \"I hate genderizing things ... [but] when I heard Hole, I was amazed to hear a girl sing like that. Janis Joplin was her own thing; she was into Big Mama Thornton and Bessie Smith. But what Courtney Love does, I'd never heard a girl do that.\"\n\nShe has also been noted as a gay icon since the mid-1990s, and has jokingly referred to her fanbase as consisting of \"females, gay guys, and a few advanced, evolved heterosexual men.\" Love's aesthetic image, particularly in the early 1990s, also became influential, and was dubbed \"kinderwhore\" by critics and media. The subversive fashion mainly consisted of vintage babydoll dresses accompanied by smeared makeup and red lipstick; MTV reporter Kurt Loder described Love as looking like \"a debauched rag doll\" onstage. Love later said she had been influenced by the fashion of Chrissy Amphlett of the Divinyls.\n\nLove has been depicted in popular culture across various mediums: Artist Barbara Kruger used one of Love's quotes on her New York City bus project, and the indie pop punk band The Muffs named their second album \"Blonder and Blonder\" (1995) after a quote by Love, while a recording of her talking about a stolen dress appears as the track \"Love\" on the band's 2000 compilation album \"Hamburger\". She was also the basis of an eponymous character in Michael Hornburg's novel \"Bongwater\" (1995), which would be made into a film of the same name in 1998; the novel and film, set in Portland, Oregon, are based on Hornburg's teenage years living there, where he had known her. There is also a band named after her.\n\n\n\n\nBiographical guides\n\nPublished interviews and profiles\n\nResearch resources\n", "id": "5655", "title": "Courtney Love"}
{"url": "https://en.wikipedia.org/wiki?curid=5657", "text": "Cow (disambiguation)\n\nCow is the nickname for cattle, and the name of adult female cattle.\n\nCow, cows or COW may also refer to:\n\n\n\n\n\n\n\n", "id": "5657", "title": "Cow (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=5658", "text": "Cannibalism\n\nCannibalism is the act or practice of humans eating the flesh or internal organs of other human beings. A person who practices cannibalism is called a cannibal. The expression \"cannibalism\" has been extended into zoology to mean one individual of a species consuming all or part of another individual of the same species as food, including sexual cannibalism.\n\nThe Island Carib people of the Lesser Antilles, from whom the word cannibalism derives, acquired a long-standing reputation as cannibals following the recording of their legends in the 17th century. Some controversy exists over the accuracy of these legends and the prevalence of actual cannibalism in the culture. Cannibalism was widespread in the past among humans in many parts of the world, continuing into the 19th century in some isolated South Pacific cultures, and to the present day in parts of tropical Africa. Cannibalism was practiced in New Guinea and in parts of the Solomon Islands, and flesh markets existed in some parts of Melanesia. Fiji was once known as the \"Cannibal Isles\". Cannibalism has been well documented around the world, from Fiji to the Amazon Basin to the Congo to Māori New Zealand. Neanderthals are believed to have practiced cannibalism, and Neanderthals may have been eaten by anatomically modern humans.\n\nCannibalism has recently been both practiced and fiercely condemned in several wars, especially in Liberia and Congo. It is still practiced in Papua New Guinea as of 2012 for cultic reasons and in ritual and in war in various Melanesian tribes. Cannibalism has been said to test the bounds of cultural relativism as it challenges anthropologists \"to define what is or is not beyond the pale of acceptable human behavior.\"\n\nCannibalism has been occasionally practiced as a last resort by people suffering from famine, including in modern times. Famous examples include the ill-fated Westward expedition of the Donner Party (1846-7) and, more recently, the crash of Uruguayan Air Force Flight 571 (1972), after which some survivors ate the bodies of dead passengers. Also, some mentally ill people obsess about eating others and actually do so, such as Jeffrey Dahmer and Albert Fish. There is resistance to formally labeling cannibalism as a mental disorder.\n\nCannibalism derives from \"Caníbales\", the Spanish name for the Caribs, a West Indies tribe that formerly practiced cannibalism, from Spanish \"canibal\" or \"caribal\", \"a savage\". It is also called \"anthropophagy\".\n\nIn some societies, especially tribal societies, cannibalism is a cultural norm. Consumption of a person from within the same community is called endocannibalism; ritual cannibalism of the recently deceased can be part of the grieving process or a way of guiding the souls of the dead into the bodies of living descendants. \nExocannibalism is the consumption of a person from outside the community, usually as a celebration of victory against a rival tribe. Both types of cannibalism can also be fueled by the belief that eating a person's flesh or internal organs will endow the cannibal with some of the characteristics of the deceased.\n\nIn most parts of the world, cannibalism is not a societal norm, but is sometimes resorted to in situations of extreme necessity. The survivors of the shipwrecks of the \"Essex\" and \"Méduse\" in the 19th century are said to have engaged in cannibalism, as did the members of Franklin's lost expedition and the Donner Party. Such cases generally involve necro-cannibalism (eating the corpse of someone who is already dead) as opposed to homicidal cannibalism (killing someone for food). In English law, the latter is always considered a crime, even in the most trying circumstances. The case of \"R v Dudley and Stephens\", in which two men were found guilty of murder for killing and eating a cabin boy while adrift at sea in a lifeboat, set the precedent that necessity is no defence to a charge of murder.\n\nThere are numerous examples of murderers consuming their victims, often deriving some degree of sexual satisfaction from the act of cannibalism. Notable examples include Albert Fish, Issei Sagawa and Jeffrey Dahmer. These individuals are usually considered to be mentally ill, although the compulsion to eat human flesh is not formally listed as a mental disorder in the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM). Cases of autophagia, or self-cannibalism, have also been reported.\n\nIn pre-modern medicine, the explanation given by the now-discredited theory of humorism for cannibalism was that it came about within a black acrimonious humor, which, being lodged in the linings of the ventricle, produced the voracity for human flesh.\n\nA well-known case of mortuary cannibalism is that of the Fore tribe in New Guinea, which resulted in the spread of the prion disease kuru. Although the Fore's mortuary cannibalism was well documented, the practice had ceased before the cause of the disease was recognized. However, some scholars argue that although post-mortem dismemberment was the practice during funeral rites, cannibalism was not. Marvin Harris theorizes that it happened during a famine period coincident with the arrival of Europeans and was rationalized as a religious rite.\n\nIn 2003, a publication in \"Science\" received a large amount of press attention when it suggested that early humans may have practiced extensive cannibalism. According to this research, genetic markers commonly found in modern humans worldwide suggest that today many people carry a gene that evolved as protection against the brain diseases that can be spread by consuming human brain tissue. A 2006 reanalysis of the data questioned this hypothesis, as it claimed to have found a data collection bias, which led to an erroneous conclusion. This claimed bias came from incidents of cannibalism used in the analysis not being due to local cultures, but having been carried out by explorers, stranded seafarers or escaped convicts. The original authors published a subsequent paper in 2008 defending their conclusions.\n\nCannibalism features in the folklore and legends of many cultures and is most often attributed to evil characters or as extreme retribution for some wrong. Examples include the witch in \"Hansel and Gretel\", Lamia of Greek mythology and Baba Yaga of Slavic folklore.\n\nA number of stories in Greek mythology involve cannibalism, in particular cannibalism of close family members, e.g., the stories of Thyestes, Tereus and especially Cronus, who was Saturn in the Roman pantheon. The story of Tantalus also parallels this.\n\nThe wendigo is a creature appearing in the legends of the Algonquian people. It is thought of variously as a malevolent cannibalistic spirit that could possess humans or a monster that humans could physically transform into. Those who indulged in cannibalism were at particular risk, and the legend appears to have reinforced this practice as taboo.\n\nWilliam Arens, author of \"The Man-Eating Myth: Anthropology and Anthropophagy\", questions the credibility of reports of cannibalism and argues that the description by one group of people of another people as cannibals is a consistent and demonstrable ideological and rhetorical device to establish perceived cultural superiority. Arens bases his thesis on a detailed analysis of numerous \"classic\" cases of cultural cannibalism cited by explorers, missionaries, and anthropologists. He asserts that many were steeped in racism, unsubstantiated, or based on second-hand or hearsay evidence.\n\nAccusations of cannibalism helped characterize indigenous peoples as \"uncivilized,\" \"primitive,\" or even \"inhuman.\" These assertions promoted the use of military force as a means of \"civilizing\" and \"pacifying\" the \"savages\". The Spanish conquest of the Aztec empire and its earlier conquests in the Caribbean where there were widespread reports of cannibalism, justifying the conquest. Cannibals were exempt from Queen Isabella's prohibition on enslaving the indigenous. Another example of the sensationalism of cannibalism and its connection to imperialism was in the Japan's 1874 expedition to Taiwan. As Eskildsen describes, there was an exaggeration of cannibalism by Taiwanese aboriginals in Japan's popular media such as newspapers and illustrations at the time.\n\nAmong modern humans, cannibalism has been practiced by various groups. It was practiced by humans in Prehistoric Europe, Mesoamerica South America, among Iroquoian peoples in North America, Maori in New Zealand, the Solomon Islands, parts of West Africa and Central Africa, some of the islands of Polynesia, New Guinea, Sumatra, and Fiji. Evidence of cannibalism has been found in ruins associated with the Anasazi culture of the Southwestern United States as well as (at Cowboy Wash in Colorado).\n\nThere is evidence, both archaeological and genetic, that cannibalism has been practiced for tens of thousands of years. Human bones that have been \"de-fleshed\" by other humans go back 600,000 years. The oldest \"Homo sapiens\" bones (from Ethiopia) show signs of this as well. Some anthropologists, such as Tim White, suggest that ritual cannibalism was common in human societies prior to the beginning of the Upper Paleolithic period. This theory is based on the large amount of \"butchered human\" bones found in Neanderthal and other Lower/Middle Paleolithic sites. Cannibalism in the Lower and Middle Paleolithic may have occurred because of food shortages. It has been also suggested that removing dead bodies through ritual cannibalism might have been a means of predator control, aiming to eliminate predators' and scavengers' access to hominid (and early human) bodies. Jim Corbett proposed that after major epidemics, when human corpses are easily accessible to predators, there are more cases of man-eating leopards, so removing dead bodies through ritual cannibalism (before the cultural traditions of burying and burning bodies appeared in human history) might have had practical reasons for hominids and early humans to control predation.\n\nIn Gough's Cave, England, remains of human bones and skulls, around 15,000 years old, suggest that cannibalism took place amongst the people living in or visiting the cave, and that they may have used human skulls as drinking vessels.\n\nResearchers have found physical evidence of cannibalism in ancient times. In 2001, archaeologists at the University of Bristol found evidence of Iron Age cannibalism in Gloucestershire. Cannibalism was practiced as recently as 2000 years ago in Great Britain.\n\nCannibalism is mentioned many times in early history and literature. Cannibalism was reported by Flavius Josephus during the siege of Jerusalem by Rome in 70 AD, and according to Appian, during the Roman Siege of Numantia in the 2nd century BC, the population of Numantia was reduced to cannibalism and suicide.\n\nSt. Jerome, in his letter \"Against Jovinianus\", discusses how people come to their present condition as a result of their heritage, and he then lists several examples of peoples and their customs. In the list, he mentions that he has heard that Atticoti eat human flesh and that Massagetae and \"Derbices\" (a people on the borders of India) kill and eat old people. \n\nHerodotus in \"\"The Histories\"\" (450s to the 420s BC) claimed, that after eleven days' voyage up the Borysthenes (Dnieper River in Europe) a desolated land extended for a long way, and later the country of the Man-eaters (other than Scythians) was located, and beyond it again a desolated area extended where no men lived.\nReports of cannibalism were recorded during the First Crusade, as Crusaders were alleged to have fed on the bodies of their dead opponents following the Siege of Ma'arrat al-Numan. Amin Maalouf also alleges further cannibalism incidents on the march to Jerusalem, and to the efforts made to delete mention of these from western history. During Europe's Great Famine of 1315–1317 there were many reports of cannibalism among the starving populations. In North Africa, as in Europe, there are references to cannibalism as a last resort in times of famine.\n\nThe Moroccan Muslim explorer Ibn Batutta reported that one African king advised him that nearby people were cannibals (although this may have been a prank played on Ibn Batutta by the king to fluster his guest). However Batutta reported that Arabs and Christians were safe, as their flesh was \"unripe\" and would cause the eater to fall ill.\n\nFor a brief time in Europe, an unusual form of cannibalism occurred when thousands of Egyptian mummies preserved in bitumen were ground up and sold as medicine. The practice developed into a wide-scale business which flourished until the late 16th century. This \"fad\" ended because the mummies were revealed actually to be recently killed slaves. Two centuries ago, mummies were still believed to have medicinal properties against bleeding, and were sold as pharmaceuticals in powdered form (see human mummy confection and mummia).\n\nIn China during the Tang dynasty, cannibalism was supposedly resorted to by rebel forces early in the period (who were said to raid neighboring areas for victims to eat), as well as both soldiers and civilians besieged during the rebellion of An Lushan. Eating an enemy's heart and liver was also claimed to be a feature of both official punishments and private vengeance. References to cannibalizing the enemy have also been seen in poetry written in the Song dynasty (for example, in \"Man Jiang Hong\"), although the cannibalizing is perhaps poetic symbolism, expressing hatred towards the enemy.\n\nCharges of cannibalism were levied against the Qizilbash of the Safavid Ismail.\nThere is universal agreement that some Mesoamerican people practiced human sacrifice, but there is a lack of scholarly consensus as to whether cannibalism in pre-Columbian America was widespread. At one extreme, anthropologist Marvin Harris, author of \"Cannibals and Kings\", has suggested that the flesh of the victims was a part of an aristocratic diet as a reward, since the Aztec diet was lacking in proteins. While most historians of the pre-Columbian era believe that there was ritual cannibalism related to human sacrifices, they do not support Harris's thesis that human flesh was ever a significant portion of the Aztec diet. Others have hypothesized that cannibalism was part of a blood revenge in war.\n\nEuropean explorers and colonizers brought home many stories of cannibalism practiced by the native peoples they encountered, but there is now archeological and written evidence for English settlers' cannibalism in 1609 in the Jamestown Colony under famine conditions.\n\nIn Spain's overseas expansion to the \"New World\", the practice of cannibalism in was observed by Christopher Columbus in the Caribbean islands, and the Caribs were greatly feared because of their practice of it. Queen Isabel of Castile had forbade the Spaniards to enslave the indigenous, but if they were guilty of cannibalism then they could be justifiably enslaved. Cannibalism became a pretext for attacks on indigenous groups and justification for the Spanish conquest. In Yucatán, shipwrecked Spaniard Jerónimo de Aguilar, who later became a translator for Hernán Cortés, witnessed fellow Spaniards sacrificed and eaten, but escaped from captivity where he was being fattened for sacrifice himself. In the Florentine Codex (1576) compiled by Franciscan Bernardino de Sahagún from information provided by indigenous informants has abundant evidence of Mexica (Aztec) cannibalism, including a specific Nahuatl word for the practice, \"tlacatlacualli\" (\"people-food\"). Franciscan friar Diego de Landa reported on Yucatán instances, and there have been similar reports by Purchas from Popayán, Colombia.\n\nIn early Brazil, there is extensive reportage of cannibalism among the Tupinamba. It is recorded about the natives of the captaincy of Sergipe in Brazil: \"They eat human flesh when they can get it, and if a woman miscarries devour the abortive immediately. If she goes her time out, she herself cuts the navel-string with a shell, which she boils along with the secondine, and eats them both.\" In modern Brazil, a black comedy film, \"How Tasty Was My Little Frenchman\", mostly in the Tupi language, portrays a Frenchman captured by the indigenous and his demise.\n\nThree are also reports from the Marquesas Islands of Polynesia, where human flesh was called \"\"long pig\"\". According to Hans Egede, when the Inuit killed a woman accused of witchcraft, they ate a portion of her heart.\n\nThe 1913 \"Handbook of Indians of Canada\" (reprinting 1907 material from the Bureau of American Ethnology), claims that North American natives practicing cannibalism included \"... the Montagnais, and some of the tribes of Maine; the Algonkin, Armouchiquois, Iroquois, and Micmac; farther west the Assiniboine, Cree, Foxes, Chippewa, Miami, Ottawa, Kickapoo, Illinois, Sioux, and Winnebago; in the South the people who built the mounds in Florida, and the Tonkawa, Attacapa, Karankawa, Caddo, and Comanche (?); in the Northwest and West, portions of the continent, the Thlingchadinneh and other Athapascan tribes, the Tlingit, Heiltsuk, Kwakiutl, Tsimshian, Nootka, Siksika, some of the Californian tribes, and the Ute. There is also a tradition of the practice among the Hopi, and mentions of the custom among other tribes of New Mexico and Arizona. The Mohawk, and the Attacapa, Tonkawa, and other Texas tribes were known to their neighbours as 'man-eaters.'\" The forms of cannibalism described included both resorting to human flesh during famines and ritual cannibalism, the latter usually consisting of eating a small portion of an enemy warrior.\n\nAs with most lurid tales of native cannibalism, these stories are treated with a great deal of scrutiny, as accusations of cannibalism were often used as justifications for the subjugation or destruction of \"savages\". However, there were several well-documented cultures that engaged in regular eating of the dead, such as New Zealand's Māori. In an 1809 incident known as the Boyd massacre, about 66 passengers and crew of the \"Boyd\" were killed and eaten by Māori on the Whangaroa peninsula, Northland. Cannibalism was already a regular practice in Māori wars. In another instance, on July 11, 1821 warriors from the Ngapuhi tribe killed 2,000 enemies and remained on the battlefield \"eating the vanquished until they were driven off by the smell of decaying bodies\". Māori warriors fighting the New Zealand government in Titokowaru's War in New Zealand's North Island in 1868–69 revived ancient rites of cannibalism as part of the radical Hauhau movement of the Pai Marire religion.\n\nOther islands in the Pacific were home to cultures that allowed cannibalism to some degree. In parts of Melanesia, cannibalism was still practiced in the early 20th century, for a variety of reasons—including retaliation, to insult an enemy people, or to absorb the dead person's qualities. One tribal chief, Ratu Udre Udre in Rakiraki, Fiji, is said to have consumed 872 people and to have made a pile of stones to record his achievement. Fiji was nicknamed the \"Cannibal Isles\" by European sailors, who avoided disembarking there. The dense population of Marquesas Islands, Polynesia, was concentrated in the narrow valleys, and consisted of warring tribes, who sometimes practiced cannibalism on their enemies. W. D. Rubinstein wrote:\n\nThis period of time was also rife with instances of explorers and seafarers resorting to cannibalism for survival. The survivors of the sinking of the French ship \"Méduse\" in 1816 resorted to cannibalism after four days adrift on a raft and their plight was made famous by Théodore Géricault's painting Raft of the Medusa. After the sinking of the \"Essex\" of Nantucket by a whale on 20 November 1820 (an important source event for Herman Melville's \"Moby-Dick\"), the survivors, in three small boats, resorted, by common consent, to cannibalism in order for some to survive. Sir John Franklin's lost polar expedition is another example of cannibalism out of desperation. On land, the Donner Party found itself stranded by snow in a high mountain pass in California without adequate supplies during the Mexican–American War, leading to several instances of cannibalism. Another notorious cannibal was mountain man Boone Helm, who was known as \"The Kentucky Cannibal\" for eating several of his fellow travelers, from 1850 until his eventual hanging in 1864.\n\nThe case of \"R v. Dudley and Stephens\" (1884) 14 QBD 273 (QB) is an English case which dealt with four crew members of an English yacht, the \"Mignonette\", who were cast away in a storm some from the Cape of Good Hope. After several days, one of the crew, a seventeen-year-old cabin boy, fell unconscious due to a combination of the famine and drinking seawater. The others (one possibly objecting) decided then to kill him and eat him. They were picked up four days later. Two of the three survivors were found guilty of murder. A significant outcome of this case was that necessity was determined to be no defence against a charge of murder.\n\nAmerican consul James W. Davidson described in his 1903 book, \"The Island of Formosa\", how the Chinese in Taiwan ate and traded in the flesh of Taiwanese aboriginals.\n\nRoger Casement, writing to a consular colleague in Lisbon on August 3, 1903 from Lake Mantumba in the Congo Free State, said: \"The people round here are all cannibals. You never saw such a weird looking lot in your life. There are also dwarfs (called Batwas) in the forest who are even worse cannibals than the taller human environment. They eat man flesh raw! It's a fact.\" Casement then added how assailants would \"bring down a dwarf on the way home, for the marital cooking pot ... The Dwarfs, as I say, dispense with cooking pots and eat and drink their human prey fresh cut on the battlefield while the blood is still warm and running. These are not fairy tales my dear Cowper but actual gruesome reality in the heart of this poor, benighted savage land.\"\n\nMany instances of cannibalism by necessity were recorded during World War II. For example, during the 872-day Siege of Leningrad, reports of cannibalism began to appear in the winter of 1941–1942, after all birds, rats and pets were eaten by survivors. Leningrad police even formed a special division to combat cannibalism.\n\nSome 2.8 million Soviet POWs died in Nazi custody in less than eight months during 1941–42. According to the USHMM, by the winter of 1941, \"starvation and disease resulted in mass death of unimaginable proportions\". This deliberate starvation led to many incidents of cannibalism.\n\nFollowing the Soviet victory at Stalingrad it was found that some German soldiers in the besieged city, cut off from supplies, resorted to cannibalism. Later, following the German surrender in January 1943, roughly 100,000 German soldiers were taken prisoner of war (POW). Almost all of them were sent to POW camps in Siberia or Central Asia where, due to being chronically underfed by their Soviet captors, many resorted to cannibalism. Fewer than 5,000 of the prisoners taken at Stalingrad survived captivity.\n\nThe Australian War Crimes Section of the Tokyo tribunal, led by prosecutor William Webb (the future Judge-in-Chief), collected numerous written reports and testimonies that documented Japanese soldiers' acts of cannibalism among their own troops, on enemy dead, as well as on Allied prisoners of war in many parts of the Greater East Asia Co-Prosperity Sphere. In September 1942, Japanese daily rations on New Guinea consisted of 800 grams of rice and tinned meat. However, by December, this had fallen to 50 grams. According to historian Yuki Tanaka, \"cannibalism was often a systematic activity conducted by whole squads and under the command of officers\".\n\nIn some cases, flesh was cut from living people. An Indian POW, Lance Naik Hatam Ali (later a citizen of Pakistan), testified that in New Guinea: \"the Japanese started selecting prisoners and every day one prisoner was taken out and killed and eaten by the soldiers. I personally saw this happen and about 100 prisoners were eaten at this place by the Japanese. The remainder of us were taken to another spot away where 10 prisoners died of sickness. At this place, the Japanese again started selecting prisoners to eat. Those selected were taken to a hut where their flesh was cut from their bodies while they were alive and they were thrown into a ditch where they later died.\"\n\nAnother well-documented case occurred in Chichi-jima in February 1945, when Japanese soldiers killed and consumed five American airmen. This case was investigated in 1947 in a war crimes trial, and of 30 Japanese soldiers prosecuted, five (Maj. Matoba, Gen. Tachibana, Adm. Mori, Capt. Yoshii, and Dr. Teraki) were found guilty and hanged. In his book \"\", James Bradley details several instances of cannibalism of World War II Allied prisoners by their Japanese captors. The author claims that this included not only ritual cannibalization of the livers of freshly killed prisoners, but also the cannibalization-for-sustenance of living prisoners over the course of several days, amputating limbs only as needed to keep the meat fresh.\n\nThe Korowai tribe of south-eastern Papua could be one of the last surviving tribes in the world engaging in cannibalism. A local cannibal cult killed and ate victims as late as 2012.\n\nDuring the 1892–1894 war between the Congo Free State and the Swahili-Arab city-states of Nyangwe and Kasongo in Eastern Congo, there were reports of widespread cannibalization of the bodies of defeated Arab combatants by the Batetela allies of Belgian commander Francis Dhanis. The Batetela, \"like most of their neighbors were inveterate cannibals.\" According to Dhanis' medical officer, Captain Hinde, their town of Ngandu had \"at least 2,000 polished human skulls\" as a \"solid white pavement in front\" of its gates, with human skulls crowning every post of the stockade.\n\nIn April 1892, 10,000 of the Batetela, under the command of Gongo Lutete, joined forces with Dhanis in a campaign against the Swahili-Arab leaders Sefu and Mohara. After one early skirmish in the campaign, Hinde \"noticed that the bodies of both the killed and wounded had vanished.\" When fighting broke out again, Hinde saw his Batetela allies drop human arms, legs and heads on the road. One young Belgian officer wrote home: \"Happily Gongo's men ate them up [in a few hours]. It's horrible but exceedingly useful and hygenic ... I should have been horrified at the idea in Europe! But it seems quite natural to me here. Don't show this letter to anyone indiscreet.\" After the massacre at Nyangwe, Lutete \"hid himself in his quarters, appalled by the sight of thousands of men smoking human hands and human chops on their camp fires, enough to feed his army for many days.\"\n\nIn the 1980s, Médecins Sans Frontières, the international medical charity, supplied photographic and other documentary evidence of ritualized cannibal feasts among the participants in Liberia's internecine strife to representatives of Amnesty International who were on a fact-finding mission to the neighboring state of Guinea. However, Amnesty International declined to publicize this material; the Secretary-General of the organization, Pierre Sane, said at the time in an internal communication that \"what they do with the bodies after human rights violations are committed is not part of our mandate or concern\". The existence of cannibalism on a wide scale in Liberia was subsequently verified.\n\nThe self-declared Emperor of the Central African Empire, Jean-Bédel Bokassa (Emperor Bokassa I), was tried on October 24, 1986, for several cases of cannibalism although he was never convicted. Between April 17, and April 19, 1979, a number of elementary school students were arrested after they had protested against wearing the expensive, government-required school uniforms. Around 100 were killed. Bokassa is said to have participated in the massacre, beating some of the children to death with his cane and allegedly ate some of his victims.\n\nFurther reports of cannibalism were reported against the Seleka Muslim minority during the ongoing Central African Republic conflict.\n\nCannibalism has been reported in several recent African conflicts, including the Second Congo War, and the civil wars in Liberia and Sierra Leone. A UN human rights expert reported in July 2007 that sexual atrocities against Congolese women go \"far beyond rape\" and include sexual slavery, forced incest, and cannibalism. This may be done in desperation, as during peacetime cannibalism is much less frequent; at other times, it is consciously directed at certain groups believed to be relatively helpless, such as Congo Pygmies, even considered subhuman by some other Congolese. It is also reported by some that witch doctors sometimes use the body parts of children in their medicine. In the 1970s the Ugandan dictator Idi Amin was reputed to practice cannibalism.\n\nIn Uganda, the Lord's Resistance Army has been accused of routinely engaging in ritual or magical cannibalism.\n\nReports of widespread cannibalism began to emerge from North Korea during the famine of the 1990s and subsequent ongoing starvation. Kim Jong-il was reported to have ordered a crackdown on cannibalism in 1996. Chinese travellers reported in 1998 that cannibalism had occurred. Three people in North Korea were reported to have been executed for selling or eating human flesh in 2006. Further reports of cannibalism emerged in early 2013, including reports of a man executed for killing his two children for food.\n\nThere are competing claims about how widespread cannibalism was in North Korea. While refugees reported that it was widespread wrote that it did not seem to be.\n\nCannibalism is documented to have occurred in China during the Great Leap Forward, when rural China was hit hard by drought and famine.\n\nDuring Mao Zedong's Cultural Revolution, local governments' documents revealed hundreds of incidents of cannibalism for ideological reasons. Public events for cannibalism were organised by local Communist Party officials, and people took part in them together in order to prove their revolutionary passion. The writer Zheng Yi documented incidents of cannibalism in Guangxi province in 1968 in his 1993 book, \"\".\n\nFlesh pills were used by Tibetan Buddhists. It was believed that mystical powers were bestowed upon people when they consumed Brahmin flesh.\n\nTullio Favali's brain was consumed by Filipino Ilaga member Norberto Manero.\n\nFurther instances include cannibalism as ritual practice, in times of drought, famine and other destitution, as well as those being criminal acts and war crimes throughout the 20th century, and also 21st century.\n\nIn West Africa, the Leopard Society was a cannibalistic secret society that existed until the mid-1900s. Centered in Sierra Leone, Liberia and Ivory Coast, the \"Leopard men\" would dress in leopard skins, and waylay travelers with sharp claw-like weapons in the form of leopards' claws and teeth. The victims' flesh would be cut from their bodies and distributed to members of the society.\n\nAs in some other Papuan societies, the Urapmin people engaged in cannibalism in war. Notably, the Urapmin also had a system of food taboos wherein dogs could not be eaten and they had to be kept from breathing on food, unlike humans who could be eaten and with whom food could be shared.\n\nThe Aghoris are Indian ascetics who believe that eating human flesh confers spiritual and physical benefits, such as prevention of aging. They claim to only eat those who have voluntarily willed their body to the sect upon their death, although an Indian TV crew witnessed one Aghori feasting on a corpse discovered floating in the Ganges, and a member of the Dom caste reports that Aghoris often take bodies from the cremation \"ghat\" (or funeral pyre).\nDuring the 1930s, multiple acts of cannibalism were reported from Ukraine and Russia's Volga, South Siberian and Kuban regions during the Soviet famine of 1932–1933.\n\nSurvival was a moral as well as a physical struggle. A woman doctor wrote to a friend in June 1933 that she had not yet become a cannibal, but was \"not sure that I shall not be one by the time my letter reaches you.\" The good people died first. Those who refused to steal or to prostitute themselves died. Those who gave food to others died. Those who refused to eat corpses died. Those who refused to kill their fellow man died. ... At least 2,505 people were sentenced for cannibalism in the years 1932 and 1933 in Ukraine, though the actual number of cases was certainly much higher.\n\nPrior to 1931, \"New York Times\" reporter William Buehler Seabrook, allegedly in the interests of research, obtained from a hospital intern at the Sorbonne a chunk of human meat from the body of a healthy human killed in an accident, then cooked and ate it. He reported, \"It was like good, fully developed veal, not young, but not yet beef. It was very definitely like that, and it was not like any other meat I had ever tasted. It was so nearly like good, fully developed veal that I think no person with a palate of ordinary, normal sensitiveness could distinguish it from veal. It was mild, good meat with no other sharply defined or highly characteristic taste such as for instance, goat, high game, and pork have. The steak was slightly tougher than prime veal, a little stringy, but not too tough or stringy to be agreeably edible. The roast, from which I cut and ate a central slice, was tender, and in color, texture, smell as well as taste, strengthened my certainty that of all the meats we habitually know, veal is the one meat to which this meat is accurately comparable.\"\n\nIn his book, \"The Gulag Archipelago\", Soviet writer Aleksandr Solzhenitsyn described cases of cannibalism in 20th-century USSR. Of the famine in Povolzhie (1921–1922) he wrote: \"That horrible famine was up to cannibalism, up to consuming children by their own parents — the famine, which Russia had never known even in Time of Troubles [in 1601–1603] ...\"\n\nHe said of the Siege of Leningrad (1941–1944): \"Those who consumed human flesh, or dealt with the human liver trading from dissecting rooms ... were accounted as the political criminals ...\" And of the building of Northern Railway Labor Camp (\"Sevzheldorlag\") Solzhenitsyn reports, \"An ordinary hard working political prisoner almost could not survive at that penal camp. In the camp Sevzheldorlag (chief: colonel Klyuchkin) in 1946–47 there were many cases of cannibalism: they cut human bodies, cooked and ate.\"\n\nDuring the dekulakization process in the USSR in the 1920s and 1930s, many deportees were forced to eat one another by genocidal Soviet authorities, e.g. on the Nazino island or during Holodomor.\n\nThe Soviet journalist Yevgenia Ginzburg was a former long-term political prisoner who spent time in the Soviet prisons, Gulag camps and settlements from 1938 to 1955. She described in her memoir, \"Harsh Route\" (or \"Steep Route\"), of a case which she was directly involved in during the late 1940s, after she had been moved to the prisoners' hospital.\n...The chief warder shows me the black smoked pot, filled with some food: 'I need your medical expertise regarding this meat.' I look into the pot, and hardly hold vomiting. The fibres of that meat are very small, and don't resemble me anything I have seen before. The skin on some pieces bristles with black hair (...) A former smith from Poltava, Kulesh worked together with Centurashvili. At this time, Centurashvili was only one month away from being discharged from the camp (...) And suddenly he surprisingly disappeared. The wardens looked around the hills, stated Kulesh's evidence, that last time Kulesh had seen his workmate near the fireplace, Kulesh went out to work and Centurashvili left to warm himself more; but when Kulesh returned to the fireplace, Centurashvili had vanished; who knows, maybe he got frozen somewhere in snow, he was a weak guy (...) The wardens searched for two more days, and then assumed that it was an escape case, though they wondered why, since his imprisonment period was almost over (...) The crime was there. Approaching the fireplace, Kulesh killed Centurashvili with an axe, burned his clothes, then dismembered him and hid the pieces in snow, in different places, putting specific marks on each burial place. ... Just yesterday, one body part was found under two crossed logs.\n\nWhen Uruguayan Air Force Flight 571 crashed into the Andes on October 13, 1972, the survivors resorted to eating the deceased during their 72 days in the mountains. Their story was later recounted in the books \"\" and \"\" as well as the film \"Alive\", by Frank Marshall, and the documentaries \"\" (1993) and \"\" (2008).\n\nCannibalism was reported by the journalist Neil Davis during the South East Asian wars of the 1960s and 1970s. Davis reported that Cambodian troops ritually ate portions of the slain enemy, typically the liver. However he, and many refugees, also report that cannibalism was practiced non-ritually when there was no food to be found. This usually occurred when towns and villages were under Khmer Rouge control, and food was strictly rationed, leading to widespread starvation. Any civilian caught participating in cannibalism would have been immediately executed.\n\nOn July 23, 1988, Rick Gibson ate the flesh of another person in public. Because England does not have a specific law against cannibalism, he legally ate a canapé of donated human tonsils in Walthamstow High Street, London. A year later, on April 15, 1989, he publicly ate a slice of human testicle in Lewisham High Street, London. When he tried to eat another slice of human testicle at the Pitt International Galleries in Vancouver on July 14, 1989, the Vancouver police confiscated the testicle hors d'œuvre. However, the charge of publicly exhibiting a disgusting object was dropped and he finally ate the piece of human testicle on the steps of the Vancouver court house on September 22, 1989.\n\n", "id": "5658", "title": "Cannibalism"}
{"url": "https://en.wikipedia.org/wiki?curid=5659", "text": "Chemical element\n\nA chemical element or element is a species of atoms having the same number of protons in their atomic nuclei (i.e. the same atomic number, or \"Z\"). There are 118 elements that have been identified, of which the first 94 occur naturally on Earth with the remaining 24 being synthetic elements. There are 80 elements that have at least one stable isotope and 38 that have exclusively radioactive isotopes, which decay over time into other elements. Iron is the most abundant element (by mass) making up Earth, while oxygen is the most common element in the Earth's crust.\n\nChemical elements constitute all of the ordinary matter of the universe. However astronomical observations suggest that ordinary observable matter makes up only about 15% of the matter in the universe: the remainder is dark matter; the composition of this is unknown, but it is not composed of chemical elements.\nThe two lightest elements, hydrogen and helium, were mostly formed in the Big Bang and are the most common elements in the universe. The next three elements (lithium, beryllium and boron) were formed mostly by cosmic ray spallation, and are thus rarer than those that follow. Formation of elements with from 6 to 26 protons occurred and continues to occur in main sequence stars via stellar nucleosynthesis. The high abundance of oxygen, silicon, and iron on Earth reflects their common production in such stars. Elements with greater than 26 protons are formed by supernova nucleosynthesis in supernovae, which, when they explode, blast these elements as supernova remnants far into space, where they may become incorporated into planets when they are formed.\n\nThe term \"element\" is used for atoms with a given number of protons (regardless of whether or not they are ionized or chemically bonded, e.g. hydrogen in water) as well as for a pure chemical substance consisting of a single element (e.g. hydrogen gas). For the second meaning, the terms \"elementary substance\" and \"simple substance\" have been suggested, but they have not gained much acceptance in English chemical literature, whereas in some other languages their equivalent is widely used (e.g. French ', Russian '). A single element can form multiple substances differing in their structure; they are called allotropes of the element.\n\nWhen different elements are chemically combined, with the atoms held together by chemical bonds, they form chemical compounds. Only a minority of elements are found uncombined as relatively pure minerals. Among the more common of such native elements are copper, silver, gold, carbon (as coal, graphite, or diamonds), and sulfur. All but a few of the most inert elements, such as noble gases and noble metals, are usually found on Earth in chemically combined form, as chemical compounds. While about 32 of the chemical elements occur on Earth in native uncombined forms, most of these occur as mixtures. For example, atmospheric air is primarily a mixture of nitrogen, oxygen, and argon, and native solid elements occur in alloys, such as that of iron and nickel.\n\nThe history of the discovery and use of the elements began with primitive human societies that found native elements like carbon, sulfur, copper and gold. Later civilizations extracted elemental copper, tin, lead and iron from their ores by smelting, using charcoal. Alchemists and chemists subsequently identified many more; almost all of the naturally occurring elements were known by 1900.\n\nThe properties of the chemical elements are summarized in the periodic table, which organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. Save for unstable radioactive elements with short half-lives, all of the elements are available industrially, most of them in low degrees of impurities.\n\nThe lightest chemical elements are hydrogen and helium, both created by Big Bang nucleosynthesis during the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms), along with tiny traces of the next two elements, lithium and beryllium. Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay.\n\nOf the 94 naturally occurring elements, those with atomic numbers 1 through 82 each have at least one stable isotope (except for technetium, element 43 and promethium, element 61, which have no stable isotopes). Isotopes considered stable are those for which no radioactive decay has yet been observed. Elements with atomic numbers 83 through 94 are unstable to the point that radioactive decay of all isotopes can be detected. Some of these elements, notably bismuth (atomic number 83), thorium (atomic number 90), and uranium (atomic number 92), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy metals before the formation of our Solar System. At over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element, and is almost always considered on par with the 80 stable elements. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with half-lives so short that they are not found in nature and must be synthesized.\n\nAs of 2010, there are 118 known elements (in this context, \"known\" means observed well enough, even from just a few decay products, to have been differentiated from other elements). Of these 118 elements, 94 occur naturally on Earth. Six of these occur in extreme trace quantities: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; and plutonium, number 94. These 94 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 94 elements have been detected directly on Earth as primordial nuclides present from the formation of the solar system, or as naturally occurring fission or transmutation products of uranium and thorium.\n\nThe remaining 24 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: these are all radioactive, with very short half-lives; if any atoms of these elements were present at the formation of Earth, they are extremely likely, to the point of certainty, to have already decayed, and if present in novae, have been in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, although trace amounts of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally occurring rare elements.\n\nList of the elements are available by name, atomic number, density, melting point, boiling point and by symbol, as well as ionization energies of the elements. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures).\n\nThe atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element.\n\nThe number of protons in the atomic nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic orbitals that determine the atom's various chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties (except in the case of hydrogen and deuterium). Thus, all carbon isotopes have nearly identical chemical properties because they all have six protons and six electrons, even though carbon atoms may, for example, have 6 or 8 neutrons. That is why the atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of a chemical element.\n\nThe symbol for atomic number is \"Z\".\n\nIsotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having \"different\" numbers of neutrons. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons in the nucleus, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, the three isotopes of carbon are known as carbon-12, carbon-13, and carbon-14, often abbreviated to C, C, and C. Carbon in everyday life and in chemistry is a mixture of C (about 98.9%), C (about 1.1%) and about 1 atom per trillion of C.\n\nMost (66 of 94) naturally occurring elements have more than one stable isotope. Except for the isotopes of hydrogen (which differ greatly from each other in relative mass—enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable.\n\nAll of the elements have some isotopes that are radioactive (radioisotopes), although not all of these radioisotopes occur naturally. The radioisotopes typically decay into other elements upon radiating an alpha or beta particle. If an element has isotopes that are not radioactive, these are termed \"stable\" isotopes. All of the known stable isotopes occur naturally (see primordial isotope). The many radioisotopes that are not found in nature have been characterized after being artificially made. Certain elements have no stable isotopes and are composed \"only\" of radioactive isotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic numbers greater than 82.\n\nOf the 80 elements with at least one stable isotope, 26 have only one single stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes that occur for a single element is 10 (for tin, element 50).\n\nThe mass number of an element, \"A\", is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a given element are distinguished by their mass numbers, which are conventionally written as a superscript on the left hand side of the atomic symbol (e.g. U). The mass number is always a whole number and has units of \"nucleons\". For example, magnesium-24 (24 is the mass number) is an atom with 24 nucleons (12 protons and 12 neutrons).\n\nWhereas the mass number simply counts the total number of neutrons and protons and is thus a natural (or whole) number, the atomic mass of a single atom is a real number giving the mass of a particular isotope (or \"nuclide\") of the element, expressed in atomic mass units (\"u\"). In general, the mass number of a given nuclide differs in value slightly from its atomic mass, since the mass of each proton and neutron is not exactly 1 \"u\"; since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number; and (finally) because of the nuclear binding energy. For example, the atomic mass of chlorine-35 to five significant digits is 34.969 u and that of chlorine-37 is 36.966 u. However, the atomic mass in u of each isotope is quite close to its simple mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is C, which by definition has a mass of exactly 12, because u is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state.\n\nThe relative atomic mass (historically and commonly also called \"atomic weight\") of an element is the \"average\" of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit (u). This number may be a fraction that is \"not\" close to a whole number. For example, the relative atomic mass of chlorine is 35.453 u, which differs greatly from a whole number as it is an average of about 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than 1% from a whole number, it is due to this averaging effect, as significant amounts of more than one isotope are naturally present in a sample of that element.\n\nChemists and nuclear scientists have different definitions of a \"pure element\". In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one stable isotope.\n\nFor example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since ordinary copper consists of two stable isotopes, 69% Cu and 31% Cu, with different numbers of neutrons. However, a pure gold ingot would be both chemically and isotopically pure, since ordinary gold consists only of one isotope, Au.\n\nAtoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple chemical structures (spatial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'.\n\nThe standard state, also known as reference state, of an element is defined as its thermodynamically most stable state at a pressure of 1 bar and a given temperature (typically at 298.15 K). In thermochemistry, an element is defined to have an enthalpy of formation of zero in its standard state. For example, the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes.\n\nSeveral kinds of descriptive categorizations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins.\n\nSeveral terms are commonly used to characterize the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals, which do not, and a small group, (the \"metalloids\"), having intermediate properties and often behaving as semiconductors.\n\nA more refined classification is often shown in colored presentations of the periodic table. This system restricts the terms \"metal\" and \"nonmetal\" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, transition metals, post-transition metals, metalloids, polyatomic nonmetals, diatomic nonmetals, and noble gases. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the polyatomic nonmetals, diatomic nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals.\n\nAnother commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at a selected standard temperature and pressure (STP). Most of the elements are solids at conventional temperatures and atmospheric pressure, while several are gases. Only bromine and mercury are liquids at 0 degrees Celsius (32 degrees Fahrenheit) and normal atmospheric pressure; caesium and gallium are solids at that temperature, but melt at 28.4 °C (83.2 °F) and 29.8 °C (85.6 °F), respectively.\n\nMelting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations.\n\nThe density at a selected standard temperature and pressure (STP) is frequently used in characterizing the elements. Density is often expressed in grams per cubic centimeter (g/cm). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements.\n\nWhen an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm, respectively.\n\nThe elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.\n\nChemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of man-made nuclear reactions.\n\nOf the 94 naturally occurring elements, 84 are considered primordial and either stable or weakly radioactive. The remaining 10 naturally occurring elements possess half lives too short for them to have been present at the beginning of the Solar System, and are therefore considered transient elements. (Plutonium is usually also considered a transient element because primordial plutonium has by now decayed to almost undetectable traces.) Of these 10 transient elements, 5 (polonium, radon, radium, actinium, and protactinium) are relatively common decay products of thorium, uranium, and plutonium. The remaining 6 transient elements (technetium, promethium, astatine, francium, neptunium, and plutonium) occur only rarely, as products of rare decay modes or nuclear reaction processes involving uranium or other heavy elements.\n\nElements with atomic numbers 1 through 40 are all stable, while those with atomic numbers 41 through 82 (except technetium and promethium) are metastable. The half-lives of these metastable \"theoretical radionuclides\" are so long (at least 100 million times longer than the estimated age of the universe) that their radioactive decay has yet to be detected by experiment. Elements with atomic numbers 83 through 94 are unstable to the point that their radioactive decay can be detected. Three of these elements, bismuth (element 83), thorium (element 90), and uranium (element 92) have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any naturally occurring element. The very heaviest 24 elements (those beyond plutonium, element 94) undergo radioactive decay with short half-lives and cannot be produced as daughters of longer-lived elements, and thus they do not occur in nature at all.\n\nThe properties of the chemical elements are often summarized using the periodic table, which powerfully and elegantly organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The current standard table contains 118 confirmed elements as of 10 April 2010.\n\nAlthough earlier precursors to this presentation exist, its invention is generally credited to the Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior.\n\nUse of the periodic table is now ubiquitous within the academic discipline of chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering.\n\nThe various chemical elements are formally identified by their unique atomic numbers, by their accepted names, and by their symbols.\n\nThe known elements have atomic numbers from 1 through 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as in a periodic table), sets of elements are sometimes specified by such notation as \"through\", \"beyond\", or \"from ... through\", as in \"through iron\", \"beyond uranium\", or \"from lanthanum through lutetium\". The terms \"light\" and \"heavy\" are sometimes also used informally to indicate relative atomic numbers (not densities), as in \"lighter than carbon\" or \"heavier than lead\", although technically the weight or mass of atoms of an element (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers.\n\nThe naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, although at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently-known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the names of elements either for convenience, linguistic niceties, or nationalism. For a few illustrative examples: German speakers use \"Wasserstoff\" (water substance) for \"hydrogen\", \"Sauerstoff\" (acid substance) for \"oxygen\" and \"Stickstoff\" (smothering substance) for \"nitrogen\", while English and some romance languages use \"sodium\" for \"natrium\" and \"potassium\" for \"kalium\", and the French, Italians, Greeks, Portuguese and Poles prefer \"azote/azot/azoto\" (from roots meaning \"no life\") for \"nitrogen\".\n\nFor purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognized are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting \"gold\" rather than \"aurum\" as the name for the 79th element (Au). IUPAC prefers the British spellings \"aluminium\" and \"caesium\" over the U.S. spellings \"aluminum\" and \"cesium\", and the U.S. \"sulfur\" over the British \"sulphur\". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names.\n\nAccording to IUPAC, chemical elements are not proper nouns in English; consequently, the full name of an element is not routinely capitalized in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names of chemical elements are also uncapitalized if written out, \"e.g.,\" carbon-12 or uranium-235. Chemical element \"symbols\" (such as Cf for californium and Es for einsteinium), are always capitalized (see below).\n\nIn the second half of the twentieth century, physics laboratories became able to produce nuclei of chemical elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that delayed the naming of elements with atomic number of 104 and higher for a considerable amount of time. (See element naming controversy).\n\nPrecursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, \"lutetium\" was named in reference to Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it \"cassiopeium\". Similarly, the British discoverer of \"niobium\" originally named it \"columbium,\" in reference to the New World. It was used extensively as such by American publications prior to the international standardization (in 1950).\n\nBefore chemistry became a science, alchemists had designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules.\n\nThe current system of chemical notation was invented by Berzelius. In this typographical system, chemical symbols are not mere abbreviations—though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets.\n\nThe first of these symbols were intended to be fully universal. Since Latin was the common language of science at that time, they were abbreviations based on the Latin names of metals. Cu comes from Cuprum, Fe comes from Ferrum, Ag from Argentum. The symbols were not followed by a period (full stop) as with abbreviations. Later chemical elements were also assigned unique chemical symbols, based on the name of the element, but not necessarily in English. For example, sodium has the chemical symbol 'Na' after the Latin \"natrium\". The same applies to \"W\" (wolfram) for tungsten, \"Fe\" (ferrum) for iron, \"Hg\" (hydrargyrum) for mercury, \"Sn\" (stannum) for tin, \"K\" (kalium) for potassium, \"Au\" (aurum) for gold, \"Ag\" (argentum) for silver, \"Pb\" (plumbum) for lead, \"Cu\" (cuprum) for copper, and \"Sb\" (stibium) for antimony.\n\nChemical symbols are understood internationally when element names might require translation. There have sometimes been differences in the past. For example, Germans in the past have used \"J\" (for the alternate name Jod) for iodine, but now use \"I\" and \"Iod\".\n\nThe first letter of a chemical symbol is always capitalized, as in the preceding examples, and the subsequent letters, if any, are always lower case (small letters). Thus, the symbols for californium and einsteinium are Cf and Es.\n\nThere are also symbols in chemical equations for groups of chemical elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, an \"X\" indicates a variable group (usually a halogen) in a class of compounds, while \"R\" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter \"Q\" is reserved for \"heat\" in a chemical reaction. \"Y\" is also often used as a general chemical symbol, although it is also the symbol of yttrium. \"Z\" is also frequently used as a general variable group. \"E\" is used in organic chemistry to denote an electron-withdrawing group or an electrophile; similarly \"Nu\" denotes a nucleophile. \"L\" is used to represent a general ligand in inorganic and organometallic chemistry. \"M\" is also often used in place of a general metal.\n\nAt least two additional, two-letter generic chemical symbols are also in informal usage, \"Ln\" for any lanthanide element and \"An\" for any actinide element. \"Rg\" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and the symbol \"Rg\" has now been assigned to the element roentgenium.\n\nIsotopes are distinguished by the atomic mass number (total protons and neutrons) for a particular isotope of an element, with this number combined with the pertinent element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example C and U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used.\n\nAs a special case, the three naturally occurring isotopes of the element hydrogen are often specified as H for H (protium), D for H (deuterium), and T for H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number for each atom. For example, the formula for heavy water may be written DO instead of HO.\n\nOnly about 4% of the total mass of the universe is made of atoms or ions, and thus represented by chemical elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of chemical elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even more mysterious dark energy).\n\nThe universe's 94 naturally occurring chemical elements are thought to have been produced by at least four cosmic processes. Most of the hydrogen and helium in the universe was produced primordially in the first few minutes of the Big Bang. Three recurrently occurring later processes are thought to have produced the remaining elements. Stellar nucleosynthesis, an ongoing process, produces all elements from carbon through iron in atomic number, but little lithium, beryllium, or boron. Elements heavier in atomic number than iron, as heavy as uranium and plutonium, are produced by explosive nucleosynthesis in supernovas and other cataclysmic cosmic events. Cosmic ray spallation (fragmentation) of carbon, nitrogen, and oxygen is important to the production of lithium, beryllium and boron.\n\nDuring the early phases of the Big Bang, nucleosynthesis of hydrogen nuclei resulted in the production of hydrogen-1 (protium, H) and helium-4 (He), as well as a smaller amount of deuterium (H) and very minuscule amounts (on the order of 10) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. It is generally agreed that no heavier elements than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of roughly 75% H, 25% He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means.\n\nOn Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of nuclear transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial nuclides. For example, trace (but detectable) amounts of carbon-14 (C) are continually produced in the atmosphere by cosmic rays impacting nitrogen atoms, and argon-40 (Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable radioactive elements such as radium and radon, which are transiently present in any sample of these metals or their ores or compounds. Three other radioactive elements, technetium, promethium, and neptunium, occur only incidentally in natural materials, produced as individual atoms by nuclear fission of the nuclei of various heavy elements or in other rare nuclear processes.\n\nHuman technology has produced various additional elements beyond these first 94, with those through atomic number 118 now known.\n\nThe following graph (note log scale) shows the abundance of elements in our Solar System. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientist expect that these galaxies evolved elements in similar abundance.\n\nThe abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number.\n\nThe abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the Solar system (as seen in the Sun and heavy planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon (as hydrocarbons), nitrogen and sulfur, as a result of solar heating in the early formation of the solar system. Oxygen, the most abundant Earth element by mass, is retained on Earth by combination with silicon. Aluminum at 8% by mass is more common in the Earth's crust than in the universe and solar system, but the composition of the far more bulky mantle, which has magnesium and iron in place of aluminum (which occurs there only at 2% of mass) more closely mirrors the elemental composition of the solar system, save for the noted loss of volatile elements to space, and loss of iron which has migrated to the Earth's core.\n\nThe composition of the human body, by contrast, more closely follows the composition of seawater—save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids, together with phosphorus in the nucleic acids and energy transfer molecule adenosine triphosphate (ATP) that occurs in the cells of all living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrate animals' red blood cells.\n\nThe concept of an \"element\" as an undivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions.\n\nAncient philosophy posited a set of classical elements to explain observed patterns in nature. These \"elements\" originally referred to \"earth\", \"water\", \"air\" and \"fire\" rather than the chemical elements of modern science.\n\nThe term 'elements' (\"stoicheia\") was first used by the Greek philosopher Plato in about 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).\n\nAristotle, c. 350 BCE, also used the term \"stoicheia\" and added a fifth element called aether, which formed the heavens. Aristotle defined an element as:\n\nIn 1661, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted by irreducible units of matter (atoms) and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. The first modern list of chemical elements was given in Antoine Lavoisier's 1789 \"Elements of Chemistry\", which contained thirty-three elements, including light and caloric. By 1818, Jöns Jakob Berzelius had determined atomic weights for forty-five of the forty-nine then-accepted elements. Dmitri Mendeleev had sixty-six elements in his periodic table of 1869.\n\nFrom Boyle until the early 20th century, an element was defined as a pure substance that could not be decomposed into any simpler substance. Put another way, a chemical element cannot be transformed into other chemical elements by chemical processes. Elements during this time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques.\n\nThe 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for an atom's atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons per atomic nucleus). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers), and also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10 seconds it takes the nucleus to form an electronic cloud.\n\nBy 1914, seventy-two elements were known, all naturally occurring. The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D.I. Mendeleev, the first to arrange the elements in a periodic manner. Most recently, the synthesis of element 118 was reported in October 2006, and the synthesis of element 117 was reported in April 2010.\n\nTen materials familiar to various prehistoric cultures are now known to be chemical elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognized as distinct substances prior to 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750.\n\nMost of the remaining naturally occurring chemical elements were identified and characterized by 1900, including:\n\nElements isolated or produced since 1900 include:\n\nThe first transuranium element (element with atomic number greater than 92) discovered was neptunium in 1940. Since 1999 claims for the discovery of new elements have been considered by the IUPAC/IUPAP Joint Working Party. As of January 2016, all 118 elements have been confirmed as discovered by IUPAC. The discovery of element 112 was acknowledged in 2009, and the name \"copernicium\" and the atomic symbol \"Cn\" were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesized to date is element 118, oganesson, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Tennessine, element 117 was the latest element claimed to be discovered, in 2009. On 28 November 2016, scientists at the IUPAC officially recognized the names for four of the newest chemical elements, with atomic numbers 113, 115, 117, and 118.\n\nThe following sortable table shows the 118 known chemical elements.\n\n\n", "id": "5659", "title": "Chemical element"}
{"url": "https://en.wikipedia.org/wiki?curid=5661", "text": "Centime\n\nCentime (from ) is French for \"cent\", and is used in English as the name of the fraction currency in several Francophone countries (including Switzerland, Algeria, Belgium, Morocco and France).\n\nIn France the usage of \"centime\" goes back to the introduction of the decimal monetary system under Napoleon. This system aimed at replacing non-decimal fractions of older coins. A five-centime coin was known as a \"sou\", i.e. a solidus or shilling.\n\nIn Francophone Canada of a Canadian dollar is officially known as a \"cent\" (pronounced /sɛnt/) in both English and French. However, in practice, the form of \"cenne\" (pronounced /sɛn/) has completely replaced the official \"cent\". Spoken and written use of the official form \"cent\" in Francophone Canada is exceptionally uncommon.\nIn the Canadian French vernacular \"sou\", \"sou noir\" (\"noir\" means \"black\" in French), \"cenne\", and \"cenne noire\" are all widely known, used, and accepted monikers when referring to either of a Canadian dollar or the 1¢ coin (colloquially known as a \"penny\" in North American English).\n\nIn the European community \"cent\" is the official name for one hundredth of a euro. However, in French-speaking countries the word \"centime \" is the preferred term. Indeed, the Superior Council of the French language of Belgium recommended in 2001 the use of \"centime\", since \"cent\" is also the French word for \"hundred\". An analogous decision was published in the \"Journal officiel\" in France (December 2, 1997).\n\nIn Morocco, dirhams are divided into 100 \"centime\"s and one may find prices in the country quoted in \"centime\"s rather than in dirhams. Sometimes \"centime\"s are known as francs or in former Spanish areas, pesetas.\n\nA centime is one-hundredth of the following basic monetary units:\n", "id": "5661", "title": "Centime"}
{"url": "https://en.wikipedia.org/wiki?curid=5662", "text": "Calendar year\n\nGenerally speaking, a calendar year begins on the New Year's Day of the given calendar system and ends on the day before the following New Year's Day, and thus consists of a whole number of days. A calendar year can also start on any other named day of the calendar, and end on the day before this named day in the following year. To reconcile the calendar year with the astronomical cycle (which has a fractional number of days) certain years contain extra days.\n\nThe Gregorian year, which is in use in most of the world, begins on January 1 and ends on December 31. It has a length of 365 days in an ordinary year, with 8,760 hours, 525,600 minutes, and 31,536,000 seconds; but 366 days in a leap year, with 8,784 hours, 527,040 minutes, and 31,622,400 seconds. With 97 leap years every 400 years, the year has an average length of 365.2425 days. Other formula-based calendars can have lengths which are further out of step with the solar cycle: for example, the Julian calendar has an average length of 365.25 days, and the Hebrew calendar has an average length of 365.2468 days.\n\nThe astronomer's mean tropical year which is averaged over equinoxes and solstices is currently 365.24219 days, slightly shorter than the average length of the year in most calendars, but the astronomer's value changes over time, so William Herschel's suggested correction to the Gregorian calendar may become unnecessary by the year 4000.\n\nThe calendar year can be divided into four quarters, often abbreviated as Q1, Q2, Q3, and Q4.\n\n\nWhen combined with a year, the quarter/year combination may be written, for example, as Q42017 or 4Q2017 or 2017Q4.\n\n", "id": "5662", "title": "Calendar year"}
{"url": "https://en.wikipedia.org/wiki?curid=5663", "text": "CFA franc\n\nThe CFA franc (in French: \"franc CFA\" , or colloquially \"franc\") is the name of two currencies used in Africa which are guaranteed by the French treasury. The two CFA franc currencies are the West African CFA franc and the Central African CFA franc. Although theoretically separate, the two CFA franc currencies are effectively interchangeable.\n\nBoth CFA francs have a fixed exchange rate to the euro: 100 CFA francs = 1 former French (nouveau) franc = 0.152449 euro; or 1 euro = 655.957 CFA francs exactly.\n\nAlthough Central African CFA francs and West African CFA francs have always been at parity and have therefore always had the same monetary value against other currencies, they are in principle separate currencies. They could theoretically have different values from any moment if one of the two CFA monetary authorities, or France, decided it. Therefore, West African CFA coins and banknotes are \"theoretically\" not accepted in countries using Central African CFA francs, and vice versa. However, in practice, the permanent parity of the two CFA franc currencies is widely assumed.\n\nCFA francs are used in fourteen countries: twelve formerly French-ruled nations in West and Central Africa, Guinea-Bissau (a former Portuguese colony), and Equatorial Guinea (a former Spanish colony). These fourteen countries have a combined population of 147.5 million people (as of 2013), and a combined GDP of US$166.6 billion (as of 2012). The ISO currency codes are XAF for the Central African CFA franc and XOF for the West African CFA franc.\n\nThe currency has been criticized for making economic planning for the developing countries of French West Africa all but impossible since the CFA's value is pegged to the euro (whose monetary policy is set by the European Central Bank). Others disagree and argue that the CFA \"helps stabilize the national currencies of Franc Zone member-countries and greatly facilitates the flow of exports and imports between France and the member-countries.\" The European Union's own assessment of the CFA's link to the euro, carried out in 2008, noted that \"benefits from economic integration within each of the two monetary unions of the CFA franc zone, and even more so between them, remained remarkably low\" but that \"the peg to the French franc and, since 1999, to the euro as exchange rate anchor is usually found to have had favourable effects in the region in terms of macroeconomic stability.\n\nBetween 1945 and 1958, CFA stood for ' (\"French colonies of Africa\"); then for ' (\"French Community of Africa\") between 1958 (establishment of the French Fifth Republic) and the independence of these African countries at the beginning of the 1960s. Since independence, CFA is taken to mean \"\" (African Financial Community), but in actual use, the term can have two meanings (see Institutions below).\n\nThe CFA franc was created on 26 December 1945, along with the CFP franc. The reason for their creation was the weakness of the French franc immediately after World War II. When France ratified the Bretton Woods Agreement in December 1945, the French franc was devalued in order to set a fixed exchange rate with the US dollar. New currencies were created in the French colonies to spare them the strong devaluation, thereby facilitating exports to France. French officials presented the decision as an act of generosity. René Pleven, the French minister of finance, was quoted as saying:\n\nThe CFA franc was created with a fixed exchange rate versus the French franc. This exchange rate was changed only twice: in 1948 and in 1994.\n\nExchange rate:\n\nThe 1960 and 1999 events were merely changes in the currency in use in France: the relative value of the CFA franc versus the French franc/euro changed only in 1948 and 1994.\n\nThe value of the CFA franc has been widely criticized as being too high, which many economists believe favours the urban elite of the African countries, who can buy imported manufactured goods cheaply at the expense of farmers who cannot easily export agricultural products. The devaluation of 1994 was an attempt to reduce these imbalances.\n\nOver time, the number of countries and territories using the CFA franc has changed as some countries began introducing their own separate currencies. A couple of nations in West Africa have also chosen to adopt the CFA franc since its introduction, despite the fact that they were never French colonies.\n\n\nIn 1998, in anticipation of Economic and Monetary Union of the European Union, the Council of the European Union addressed the monetary agreements France has with the CFA Zone and Comoros and ruled that:\n\nThere are two different currencies called the CFA franc: the West African CFA franc (ISO 4217 currency code XOF), and the Central Africa CFA franc (ISO 4217 currency code XAF). They are distinguished in French by the meaning of the abbreviation CFA. These two CFA francs have the same exchange rate with the euro (1 euro = 655.957 XOF = 655.957 XAF), and they are both guaranteed by the French treasury (), but the West African CFA franc cannot be used in Central African countries, and the Central Africa CFA franc cannot be used in West African countries.\n\nThe West African CFA franc (XOF) is known in French as the , where CFA stands for \"Communauté financière d'Afrique\" (\"Financial Community of Africa\") or (\"African Financial Community\"). It is issued by the BCEAO (, i.e., \"Central Bank of the West African States\"), located in Dakar, Senegal, for the eight countries of the UEMOA (, i.e., \"West African Economic and Monetary Union\"):\n\nThese eight countries have a combined population of 102.5 million people (as of 2013), and a combined GDP of US$78.4 billion (as of 2012).\n\nThe Central Africa CFA franc (XAF) is known in French as the , where CFA stands for (\"Financial Cooperation in Central Africa\"). It is issued by the BEAC (, i.e., \"Bank of the Central African States\"), located in Yaoundé, Cameroon, for the six countries of the CEMAC (, i.e., \"Economic and Monetary Community of Central Africa\"):\n\nThese six countries have a combined population of 45.0 million people (as of 2013), and a combined GDP of US$88.2 billion (as of 2012).\n\nIn 1975, Central African CFA banknotes were issued with an obverse unique to each participating country, and common reverse, in a fashion similar to euro coins.\n\nEquatorial Guinea, the only former Spanish colony in the zone, adopted the CFA in 1984.\n\n\nGeneral:\n\n\n", "id": "5663", "title": "CFA franc"}
{"url": "https://en.wikipedia.org/wiki?curid=5664", "text": "Consciousness\n\nConsciousness is the state or quality of awareness, or, of being aware of an external object or something within oneself. It has been defined variously in terms of sentience, awareness, subjectivity, the ability to experience or to feel, wakefulness, having a sense of selfhood or soul, the fact that there is something \"that it is like\" to \"have\" or \"be\" it, and the executive control system of the mind, or the state or quality of awareness, or, of being aware of an external object or something within oneself. In contemporary philosophy its definition is often hinted at via the logical possibility of its absence, the philosophical zombie, which is defined as a being whose behavior and function are identical to one's own yet there is \"no-one in there\" experiencing it.\n\nDespite the difficulty in definition, many philosophers believe that there is a broadly shared underlying intuition about what consciousness is. As Max Velmans and Susan Schneider wrote in \"The Blackwell Companion to Consciousness\": \"Anything that we are aware of at a given moment forms part of our consciousness, making conscious experience at once the most familiar and most mysterious aspect of our lives.\"\n\nWestern philosophers, since the time of Descartes and Locke, have struggled to comprehend the nature of consciousness and identify its essential properties. Issues of concern in the philosophy of consciousness include whether the concept is fundamentally coherent; whether consciousness can ever be explained mechanistically; whether non-human consciousness exists and if so how can it be recognized; how consciousness relates to language; whether consciousness can be understood in a way that does not require a dualistic distinction between mental and physical states or properties; and whether it may ever be possible for computing machines like computers or robots to be conscious, a topic studied in the field of artificial intelligence.\n\nThanks to developments in technology over the past few decades, consciousness has become a significant topic of interdisciplinary research in cognitive science, with significant contributions from fields such as psychology, neuropsychology and neuroscience. The primary focus is on understanding what it means biologically and psychologically for information to be present in consciousness—that is, on determining the neural and psychological correlates of consciousness. The majority of experimental studies assess consciousness in humans by asking subjects for a verbal report of their experiences (e.g., \"tell me if you notice anything when I do this\"). Issues of interest include phenomena such as subliminal perception, blindsight, denial of impairment, and altered states of consciousness produced by alcohol and other drugs, or spiritual or meditative techniques.\n\nIn medicine, consciousness is assessed by observing a patient's arousal and responsiveness, and can be seen as a continuum of states ranging from full alertness and comprehension, through disorientation, delirium, loss of meaningful communication, and finally loss of movement in response to painful stimuli. Issues of practical concern include how the presence of consciousness can be assessed in severely ill, comatose, or anesthetized people, and how to treat conditions in which consciousness is impaired or disrupted.\n\nThe origin of the modern concept of consciousness is often attributed to John Locke's \"Essay Concerning Human Understanding\", published in 1690. Locke defined consciousness as \"the perception of what passes in a man's own mind\". His essay influenced the 18th-century view of consciousness, and his definition appeared in Samuel Johnson's celebrated \"Dictionary\" (1755).\n\"Consciousness\" (French: \"conscience\") is also defined in the 1753 volume of Diderot and d'Alembert's Encyclopédie, as \"the opinion or internal feeling that we ourselves have from what we do.\" \n\nThe earliest English language uses of \"conscious\" and \"consciousness\" date back, however, to the 1500s. The English word \"conscious\" originally derived from the Latin \"conscius\" (\"con-\" \"together\" and \"scio\" \"to know\"), but the Latin word did not have the same meaning as our word—it meant \"knowing with\", in other words \"having joint or common knowledge with another\". There were, however, many occurrences in Latin writings of the phrase \"conscius sibi\", which translates literally as \"knowing with oneself\", or in other words \"sharing knowledge with oneself about something\". This phrase had the figurative meaning of \"knowing that one knows\", as the modern English word \"conscious\" does. In its earliest uses in the 1500s, the English word \"conscious\" retained the meaning of the Latin \"conscius\". For example, Thomas Hobbes in \"Leviathan\" wrote: \"Where two, or more men, know of one and the same fact, they are said to be Conscious of it one to another.\" The Latin phrase \"conscius sibi\", whose meaning was more closely related to the current concept of consciousness, was rendered in English as \"conscious to oneself\" or \"conscious unto oneself\". For example, Archbishop Ussher wrote in 1613 of \"being so conscious unto myself of my great weakness\". Locke's definition from 1690 illustrates that a gradual shift in meaning had taken place.\n\nA related word was \"\", which primarily means moral conscience. In the literal sense, \"conscientia\" means knowledge-with, that is, shared knowledge. The word first appears in Latin juridical texts by writers such as Cicero. Here, \"conscientia\" is the knowledge that a witness has of the deed of someone else. René Descartes (1596–1650) is generally taken to be the first philosopher to use \"conscientia\" in a way that does not fit this traditional meaning. Descartes used \"conscientia\" the way modern speakers would use \"conscience\". In \"Search after Truth\" (\"\", Amsterdam 1701) he says \"conscience or internal testimony\" (\"conscientiâ, vel interno testimonio\").\n\nThe dictionary meaning of the word \"consciousness\" extends through several centuries and associated cognate meanings which have ranged from formal definitions to somewhat more skeptical definitions. One formal definition indicating the range of these cognate meanings is given in \"Webster's Third New International Dictionary\" stating that \"consciousness\" is: \"(1) \"a.\" awareness or perception of an inward psychological or spiritual fact: intuitively perceived knowledge of something in one's inner self. \"b.\" inward awareness of an external object, state, or fact. \"c.\" concerned awareness: INTEREST, CONCERN -- often used with an attributive noun. (2): the state or activity that is characterized by sensation, emotion, volition, or thought: mind in the broadest possible sense: something in nature that is distinguished from the physical. (3): the totality in psychology of sensations, perceptions, ideas, attitudes and feelings of which an individual or a group is aware at any given time or within a particular time span -- compare STREAM OF CONSCIOUSNESS.\"\n\nThe philosophy of mind has given rise to many stances regarding consciousness. The \"Routledge Encyclopedia of Philosophy\" in 1998 defines consciousness as follows:\n\nIn a more skeptical definition of \"consciousness\", Stuart Sutherland has exemplified some of the difficulties in fully ascertaining all of its cognate meanings in his entry for the 1989 version of the \"Macmillan Dictionary of Psychology\":\n\nMost writers on the philosophy of consciousness have been concerned with defending a particular point of view, and have organized their material accordingly. For surveys, the most common approach is to follow a historical path by associating stances with the philosophers who are most strongly associated with them, for example Descartes, Locke, Kant, etc. An alternative is to organize philosophical stances according to basic issues.\n\nPhilosophers and non-philosophers differ in their intuitions about what consciousness is. While most people have a strong intuition for the existence of what they refer to as consciousness, skeptics argue that this intuition is false, either because the concept of consciousness is intrinsically incoherent, or because our intuitions about it are based in illusions. Gilbert Ryle, for example, argued that traditional understanding of consciousness depends on a Cartesian dualist outlook that improperly distinguishes between mind and body, or between mind and world. He proposed that we speak not of minds, bodies, and the world, but of individuals, or persons, acting in the world. Thus, by speaking of \"consciousness\" we end up misleading ourselves by thinking that there is any sort of thing as consciousness separated from behavioral and linguistic understandings. More generally, many philosophers and scientists have been unhappy about the difficulty of producing a definition that does not involve circularity or fuzziness.\n\nMany philosophers have argued that consciousness is a unitary concept that is understood intuitively by the majority of people in spite of the difficulty in defining it. Others, though, have argued that the level of disagreement about the meaning of the word indicates that it either means different things to different people (for instance, the objective versus subjective aspects of consciousness), or else is an umbrella term encompassing a variety of distinct meanings with no simple element in common.\n\nNed Block proposed a distinction between two types of consciousness that he called \"phenomenal\" (P-consciousness) and \"access\" (A-consciousness). P-consciousness, according to Block, is simply raw experience: it is moving, colored forms, sounds, sensations, emotions and feelings with our bodies' and responses at the center. These experiences, considered independently of any impact on behavior, are called \"qualia\". A-consciousness, on the other hand, is the phenomenon whereby information in our minds is accessible for verbal report, reasoning, and the control of behavior. So, when we perceive, information about what we perceive is access conscious; when we introspect, information about our thoughts is access conscious; when we remember, information about the past is access conscious, and so on. Although some philosophers, such as Daniel Dennett, have disputed the validity of this distinction, others have broadly accepted it. David Chalmers has argued that A-consciousness can in principle be understood in mechanistic terms, but that understanding P-consciousness is much more challenging: he calls this the \"hard problem of consciousness\".\n\nSome philosophers believe that Block's two types of consciousness are not the end of the story. William Lycan, for example, argued in his book \"Consciousness and Experience\" that at least eight clearly distinct types of consciousness can be identified (organism consciousness; control consciousness; consciousness \"of\"; state/event consciousness; reportability; introspective consciousness; subjective consciousness; self-consciousness)—and that even this list omits several more obscure forms.\n\nThere is also debate over whether or not a-consciousness and p-consciousness always co-exist or if they can exist separately. Although p-consciousness without a-consciousness is more widely accepted, there have been some hypothetical examples of A without P. Block for instance suggests the case of a “zombie” that is computationally identical to a person but without any subjectivity. However, he remains somewhat skeptical concluding \"I don’t know whether there are any actual cases of A-consciousness without P-consciousness, but I hope I have illustrated their conceptual possibility.\" \n\nWhile philosophers tend to focus on types of consciousness that occur 'in the mind', in other disciplines such as sociology the emphasis is on the practical meaning of consciousness. In this vein, it is possible to identify four forms of consciousness: \n\nMental processes (such as consciousness) and physical processes (such as brain events) seem to be correlated: but what is the basis of this connection and correlation between what seem to be two very different kinds of processes?\n\nThe first influential philosopher to discuss this question specifically was Descartes, and the answer he gave is known as Cartesian dualism. Descartes proposed that consciousness resides within an immaterial domain he called res cogitans (the realm of thought), in contrast to the domain of material things, which he called res extensa (the realm of extension). He suggested that the interaction between these two domains occurs inside the brain, perhaps in a small midline structure called the pineal gland.\n\nAlthough it is widely accepted that Descartes explained the problem cogently, few later philosophers have been happy with his solution, and his ideas about the pineal gland have especially been ridiculed. However, no alternative solution has gained general acceptance. Proposed solutions can be divided broadly into two categories: dualist solutions that maintain Descartes' rigid distinction between the realm of consciousness and the realm of matter but give different answers for how the two realms relate to each other; and monist solutions that maintain that there is really only one realm of being, of which consciousness and matter are both aspects. Each of these categories itself contains numerous variants. The two main types of dualism are substance dualism (which holds that the mind is formed of a distinct type of substance not governed by the laws of physics) and property dualism (which holds that the laws of physics are universally valid but cannot be used to explain the mind). The three main types of monism are physicalism (which holds that the mind consists of matter organized in a particular way), idealism (which holds that only thought or experience truly exists, and matter is merely an illusion), and neutral monism (which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them). There are also, however, a large number of idiosyncratic theories that cannot cleanly be assigned to any of these schools of thought.\n\nSince the dawn of Newtonian science with its vision of simple mechanical principles governing the entire universe, some philosophers have been tempted by the idea that consciousness could be explained in purely physical terms. The first influential writer to propose such an idea explicitly was Julien Offray de La Mettrie, in his book \"Man a Machine\" (\"L'homme machine\"). His arguments, however, were very abstract. The most influential modern physical theories of consciousness are based on psychology and neuroscience. Theories proposed by neuroscientists such as Gerald Edelman and Antonio Damasio, and by philosophers such as Daniel Dennett, seek to explain consciousness in terms of neural events occurring within the brain. Many other neuroscientists, such as Christof Koch, have explored the neural basis of consciousness without attempting to frame all-encompassing global theories. At the same time, computer scientists working in the field of artificial intelligence have pursued the goal of creating digital computer programs that can simulate or embody consciousness.\n\nA few theoretical physicists have argued that classical physics is intrinsically incapable of explaining the holistic aspects of consciousness, but that quantum theory may provide the missing ingredients. Several theorists have therefore proposed quantum mind (QM) theories of consciousness. Notable theories falling into this category include the holonomic brain theory of Karl Pribram and David Bohm, and the Orch-OR theory formulated by Stuart Hameroff and Roger Penrose. Some of these QM theories offer descriptions of phenomenal consciousness, as well as QM interpretations of access consciousness. None of the quantum mechanical theories has been confirmed by experiment. Recent publications by G. Guerreshi, J. Cia, S. Popescu, and H. Briegel could falsify proposals such as those of Hameroff, which rely on quantum entanglement in protein. At the present time many scientists and philosophers consider the arguments for an important role of quantum phenomena to be unconvincing.\n\nApart from the general question of the \"hard problem\" of consciousness, roughly speaking, the question of how mental experience arises from a physical basis, a more specialized question is how to square the subjective notion that we are in control of our decisions (at least in some small measure) with the customary view of causality that subsequent events are caused by prior events. The topic of free will is the philosophical and scientific examination of this conundrum.\n\nMany philosophers consider experience to be the essence of consciousness, and believe that experience can only fully be known from the inside, subjectively. But if consciousness is subjective and not visible from the outside, why do the vast majority of people believe that other people are conscious, but rocks and trees are not? This is called the problem of other minds. It is particularly acute for people who believe in the possibility of philosophical zombies, that is, people who think it is possible in principle to have an entity that is physically indistinguishable from a human being and behaves like a human being in every way but nevertheless lacks consciousness. Related issues have also been studied extensively by Greg Littmann of the University of Illinois. and Colin Allen a professor at Indiana University regarding the literature and research studying artificial intelligence in androids.\n\nThe most commonly given answer is that we attribute consciousness to other people because we see that they resemble us in appearance and behavior; we reason that if they look like us and act like us, they must be like us in other ways, including having experiences of the sort that we do. There are, however, a variety of problems with that explanation. For one thing, it seems to violate the principle of parsimony, by postulating an invisible entity that is not necessary to explain what we observe. Some philosophers, such as Daniel Dennett in an essay titled \"The Unimagined Preposterousness of Zombies\", argue that people who give this explanation do not really understand what they are saying. More broadly, philosophers who do not accept the possibility of zombies generally believe that consciousness is reflected in behavior (including verbal behavior), and that we attribute consciousness on the basis of behavior. A more straightforward way of saying this is that we attribute experiences to people because of what they can \"do\", including the fact that they can tell us about their experiences.\n\nThe topic of animal consciousness is beset by a number of difficulties. It poses the problem of other minds in an especially severe form, because non-human animals, lacking the ability to express human language, cannot tell us about their experiences. Also, it is difficult to reason objectively about the question, because a denial that an animal is conscious is often taken to imply that it does not feel, its life has no value, and that harming it is not morally wrong. Descartes, for example, has sometimes been blamed for mistreatment of animals due to the fact that he believed only humans have a non-physical mind. Most people have a strong intuition that some animals, such as cats and dogs, are conscious, while others, such as insects, are not; but the sources of this intuition are not obvious, and are often based on personal interactions with pets and other animals they have observed.\n\nPhilosophers who consider subjective experience the essence of consciousness also generally believe, as a correlate, that the existence and nature of animal consciousness can never rigorously be known. Thomas Nagel spelled out this point of view in an influential essay titled \"What Is it Like to Be a Bat?\". He said that an organism is conscious \"if and only if there is something that it is like to be that organism — something it is like \"for\" the organism\"; and he argued that no matter how much we know about an animal's brain and behavior, we can never really put ourselves into the mind of the animal and experience its world in the way it does itself. Other thinkers, such as Douglas Hofstadter, dismiss this argument as incoherent. Several psychologists and ethologists have argued for the existence of animal consciousness by describing a range of behaviors that appear to show animals holding beliefs about things they cannot directly perceive — Donald Griffin's 2001 book \"Animal Minds\" reviews a substantial portion of the evidence.\n\nOn July 7, 2012, eminent scientists from different branches of neuroscience gathered at the University of Cambridge to celebrate the Francis Crick Memorial Conference, which deals with consciousness in humans and pre-linguistic consciousness in nonhuman animals. After the conference, they signed in the presence of Stephen Hawking, the 'Cambridge Declaration on Consciousness', which summarizes the most important findings of the survey:\n\n\"We decided to reach a consensus and make a statement directed to the public that is not scientific. It's obvious to everyone in this room that animals have consciousness, but it is not obvious to the rest of the world. It is not obvious to the rest of the Western world or the Far East. It is not obvious to the society.\"\n\n\"Convergent evidence indicates that non-human animals [...], including all mammals and birds, and other creatures, [...] have the necessary neural substrates of consciousness and the capacity to exhibit intentional behaviors.\"\n\nThe idea of an artifact made conscious is an ancient theme of mythology, appearing for example in the Greek myth of Pygmalion, who carved a statue that was magically brought to life, and in medieval Jewish stories of the Golem, a magically animated homunculus built of clay. However, the possibility of actually constructing a conscious machine was probably first discussed by Ada Lovelace, in a set of notes written in 1842 about the Analytical Engine invented by Charles Babbage, a precursor (never built) to modern electronic computers. Lovelace was essentially dismissive of the idea that a machine such as the Analytical Engine could think in a humanlike way. She wrote:\n\nOne of the most influential contributions to this question was an essay written in 1950 by pioneering computer scientist Alan Turing, titled \"Computing Machinery and Intelligence\". Turing disavowed any interest in terminology, saying that even \"Can machines think?\" is too loaded with spurious connotations to be meaningful; but he proposed to replace all such questions with a specific operational test, which has become known as the Turing test. To pass the test, a computer must be able to imitate a human well enough to fool interrogators. In his essay Turing discussed a variety of possible objections, and presented a counterargument to each of them. The Turing test is commonly cited in discussions of artificial intelligence as a proposed criterion for machine consciousness; it has provoked a great deal of philosophical debate. For example, Daniel Dennett and Douglas Hofstadter argue that anything capable of passing the Turing test is necessarily conscious, while David Chalmers argues that a philosophical zombie could pass the test, yet fail to be conscious. A third group of scholars have argued that with technological growth once machines begin to display any substantial signs of human-like behavior then the dichotomy (of human consciousness compared to human-like consciousness) becomes passé and issues of machine autonomy begin to prevail even as observed in its nascent form within contemporary industry and technology. Jürgen Schmidhuber argues that consciousness is simply the result of compression. As an agent sees representation of itself recurring in the environment, the compression of this representation can be called consciousness.\n\nIn a lively exchange over what has come to be referred to as \"the Chinese room argument\", John Searle sought to refute the claim of proponents of what he calls \"strong artificial intelligence (AI)\" that a computer program can be conscious, though he does agree with advocates of \"weak AI\" that computer programs can be formatted to \"simulate\" conscious states. His own view is that consciousness has subjective, first-person causal powers by being essentially intentional due simply to the way human brains function biologically; conscious persons can perform computations, but consciousness is not inherently computational the way computer programs are. To make a Turing machine that speaks Chinese, Searle imagines a room with one monolingual English speaker (Searle himself, in fact), a book that designates a combination of Chinese symbols to be output paired with Chinese symbol input, and boxes filled with Chinese symbols. In this case, the English speaker is acting as a computer and the rulebook as a program. Searle argues that with such a machine, he would be able to process the inputs to outputs perfectly without having any understanding of Chinese, nor having any idea what the questions and answers could possibly mean. If the experiment were done in English, since Searle knows English, he would be able to take questions and give answers without any algorithms for English questions, and he would be effectively aware of what was being said and the purposes it might serve. Searle would pass the Turing test of answering the questions in both languages, but he is only conscious of what he is doing when he speaks English. Another way of putting the argument is to say that computer programs can pass the Turing test for processing the syntax of a language, but that the syntax cannot lead to semantic meaning in the way strong AI advocates hoped.\n\nIn the literature concerning artificial intelligence, Searle's essay has been second only to Turing's in the volume of debate it has generated. Searle himself was vague about what extra ingredients it would take to make a machine conscious: all he proposed was that what was needed was \"causal powers\" of the sort that the brain has and that computers lack. But other thinkers sympathetic to his basic argument have suggested that the necessary (though perhaps still not sufficient) extra conditions may include the ability to pass not just the verbal version of the Turing test, but the robotic version, which requires grounding the robot's words in the robot's sensorimotor capacity to categorize and interact with the things in the world that its words are about, Turing-indistinguishably from a real person. Turing-scale robotics is an empirical branch of research on embodied cognition and situated cognition.\n\nIn 2014, Victor Argonov has suggested a non-Turing test for machine consciousness based on machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produces judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures’ consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be\ncaused by lack of the machine’s intellect, not by absence of consciousness.\n\nFor many decades, consciousness as a research topic was avoided by the majority of mainstream scientists, because of a general feeling that a phenomenon defined in subjective terms could not properly be studied using objective experimental methods. In 1975 George Mandler published an influential psychological study which distinguished between slow, serial, and limited conscious processes and fast, parallel and extensive unconscious ones. Starting in the 1980s, an expanding community of neuroscientists and psychologists have associated themselves with a field called \"Consciousness Studies\", giving rise to a stream of experimental work published in books, journals such as \"Consciousness and Cognition\", \"Frontiers in Consciousness Research\", and the \"Journal of Consciousness Studies\", along with regular conferences organized by groups such as the Association for the Scientific Study of Consciousness.\n\nModern medical and psychological investigations into consciousness are based on psychological experiments (including, for example, the investigation of priming effects using subliminal stimuli), and on case studies of alterations in consciousness produced by trauma, illness, or drugs. Broadly viewed, scientific approaches are based on two core concepts. The first identifies the content of consciousness with the experiences that are reported by human subjects; the second makes use of the concept of consciousness that has been developed by neurologists and other medical professionals who deal with patients whose behavior is impaired. In either case, the ultimate goals are to develop techniques for assessing consciousness objectively in humans as well as other animals, and to understand the neural and psychological mechanisms that underlie it.\n\nExperimental research on consciousness presents special difficulties, due to the lack of a universally accepted operational definition. In the majority of experiments that are specifically about consciousness, the subjects are human, and the criterion used is verbal report: in other words, subjects are asked to describe their experiences, and their descriptions are treated as observations of the contents of consciousness. For example, subjects who stare continuously at a Necker cube usually report that they experience it \"flipping\" between two 3D configurations, even though the stimulus itself remains the same. The objective is to understand the relationship between the conscious awareness of stimuli (as indicated by verbal report) and the effects the stimuli have on brain activity and behavior. In several paradigms, such as the technique of response priming, the behavior of subjects is clearly influenced by stimuli for which they report no awareness, and suitable experimental manipulations can lead to \"increasing\" priming effects despite \"decreasing\" prime identification (double dissociation).\n\nVerbal report is widely considered to be the most reliable indicator of consciousness, but it raises a number of issues. For one thing, if verbal reports are treated as observations, akin to observations in other branches of science, then the possibility arises that they may contain errors—but it is difficult to make sense of the idea that subjects could be wrong about their own experiences, and even more difficult to see how such an error could be detected. Daniel Dennett has argued for an approach he calls heterophenomenology, which means treating verbal reports as stories that may or may not be true, but his ideas about how to do this have not been widely adopted. Another issue with verbal report as a criterion is that it restricts the field of study to humans who have language: this approach cannot be used to study consciousness in other species, pre-linguistic children, or people with types of brain damage that impair language. As a third issue, philosophers who dispute the validity of the Turing test may feel that it is possible, at least in principle, for verbal report to be dissociated from consciousness entirely: a philosophical zombie may give detailed verbal reports of awareness in the absence of any genuine awareness.\n\nAlthough verbal report is in practice the \"gold standard\" for ascribing consciousness, it is not the only possible criterion. In medicine, consciousness is assessed as a combination of verbal behavior, arousal, brain activity and purposeful movement. The last three of these can be used as indicators of consciousness when verbal behavior is absent. The scientific literature regarding the neural bases of arousal and purposeful movement is very extensive. Their reliability as indicators of consciousness is disputed, however, due to numerous studies showing that alert human subjects can be induced to behave purposefully in a variety of ways in spite of reporting a complete lack of awareness. Studies of the neuroscience of free will have also shown that the experiences that people report when they behave purposefully sometimes do not correspond to their actual behaviors or to the patterns of electrical activity recorded from their brains.\n\nAnother approach applies specifically to the study of self-awareness, that is, the ability to distinguish oneself from others. In the 1970s Gordon Gallup developed an operational test for self-awareness, known as the mirror test. The test examines whether animals are able to differentiate between seeing themselves in a mirror versus seeing other animals. The classic example involves placing a spot of coloring on the skin or fur near the individual's forehead and seeing if they attempt to remove it or at least touch the spot, thus indicating that they recognize that the individual they are seeing in the mirror is themselves. Humans (older than 18 months) and other great apes, bottlenose dolphins, killer whales, pigeons, European magpies and elephants have all been observed to pass this test.\n\nA major part of the scientific literature on consciousness consists of studies that examine the relationship between the experiences reported by subjects and the activity that simultaneously takes place in their brains—that is, studies of the neural correlates of consciousness. The hope is to find that activity in a particular part of the brain, or a particular pattern of global brain activity, which will be strongly predictive of conscious awareness. Several brain imaging techniques, such as EEG and fMRI, have been used for physical measures of brain activity in these studies.\n\nAnother idea that has drawn attention for several decades is that consciousness is associated with high-frequency (gamma band) oscillations in brain activity. This idea arose from proposals in the 1980s, by Christof von der Malsburg and Wolf Singer, that gamma oscillations could solve the so-called binding problem, by linking information represented in different parts of the brain into a unified experience. Rodolfo Llinás, for example, proposed that consciousness results from recurrent thalamo-cortical resonance where the specific thalamocortical systems (content) and the non-specific (centromedial thalamus) thalamocortical systems (context) interact in the gamma band frequency via synchronous oscillations.\n\nA number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a \"top-down\" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a \"bottom-up\" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry).\n\nModulation of neural responses may correlate with phenomenal experiences. In contrast to the raw electrical responses that do not correlate with consciousness, the modulation of these responses by other stimuli correlates surprisingly well with an important aspect of consciousness: namely with the phenomenal experience of stimulus intensity (brightness, contrast). In the research group of Danko Nikolić it has been shown that some of the changes in the subjectively perceived brightness correlated with the modulation of firing rates while others correlated with the modulation of neural synchrony. An fMRI investigation suggested that these findings were strictly limited to the primary visual areas. This indicates that, in the primary visual areas, changes in firing rates and synchrony can be considered as neural correlates of qualia—at least for some type of qualia.\n\nIn 2011, Graziano and Kastner proposed the \"attention schema\" theory of awareness. In that theory, specific cortical areas, notably in the superior temporal sulcus and the temporo-parietal junction, are used to build the construct of awareness and attribute it to other people. The same cortical machinery is also used to attribute awareness to oneself. Damage to these cortical regions can lead to deficits in consciousness such as hemispatial neglect. In the attention schema theory, the value of explaining the feature of awareness and attributing it to a person is to gain a useful predictive model of that person's attentional processing. Attention is a style of information processing in which a brain focuses its resources on a limited set of interrelated signals. Awareness, in this theory, is a useful, simplified schema that represents attentional states. To be aware of X is explained by constructing a model of one's attentional focus on X.\n\nIn the 2013, the \"perturbational complexity index\" (PCI) was proposed, a measure of the algorithmic complexity of the electrophysiological response of the cortex to transcranial magnetic stimulation. This measure was shown to be higher in individuals that are awake, in REM sleep or in a locked-in state than in those who are in deep sleep or in a vegetative state, making it potentially useful as a quantitative assessment of consciousness states.\n\nAssuming that not only humans but even some non-mammalian species are conscious, a number of evolutionary approaches to the problem of neural correlates of consciousness open up. For example, assuming that birds are conscious — a common assumption among neuroscientists and ethologists due to the extensive cognitive repertoire of birds — there are comparative neuroanatomical ways to validate some of the principal, currently competing, mammalian consciousness–brain theories. The rationale for such a comparative study is that the avian brain deviates structurally from the mammalian brain. So how similar are they? What homologues can be identified? The general conclusion from the study by Butler, et al., is that some of the major theories for the mammalian brain also appear to be valid for the avian brain. The structures assumed to be critical for consciousness in mammalian brains have homologous counterparts in avian brains. Thus the main portions of the theories of Crick and Koch, Edelman and Tononi, and Cotterill seem to be compatible with the assumption that birds are conscious. Edelman also differentiates between what he calls primary consciousness (which is a trait shared by humans and non-human animals) and higher-order consciousness as it appears in humans alone along with human language capacity. Certain aspects of the three theories, however, seem less easy to apply to the hypothesis of avian consciousness. For instance, the suggestion by Crick and Koch that layer 5 neurons of the mammalian brain have a special role, seems difficult to apply to the avian brain, since the avian homologues have a different morphology. Likewise, the theory of Eccles seems incompatible, since a structural homologue/analogue to the dendron has not been found in avian brains. The assumption of an avian consciousness also brings the reptilian brain into focus. The reason is the structural continuity between avian and reptilian brains, meaning that the phylogenetic origin of consciousness may be earlier than suggested by many leading neuroscientists.\n\nJoaquin Fuster of UCLA has advocated the position of the importance of the prefrontal cortex in humans, along with the areas of Wernicke and Broca, as being of particular importance to the development of human language capacities neuro-anatomically necessary for the emergence of higher-order consciousness in humans.\n\nOpinions are divided as to where in biological evolution consciousness emerged and about whether or not consciousness has any survival value. It has been argued that consciousness emerged (i) exclusively with the first humans, (ii) exclusively with the first mammals, (iii) independently in mammals and birds, or (iv) with the first reptiles. Other authors date the origins of consciousness to the first animals with nervous systems or early vertebrates in the Cambrian over 500 million years ago. Donald Griffin suggests in his book \"Animal Minds\" a gradual evolution of consciousness. Each of these scenarios raises the question of the possible survival value of consciousness.\n\nThomas Henry Huxley defends in an essay titled \"On the Hypothesis that Animals are Automata, and its History\" an epiphenomenalist theory of consciousness according to which consciousness is a causally inert effect of neural activity — “as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery”. To this William James objects in his essay \"Are We Automata?\" by stating an evolutionary argument for mind-brain interaction implying that if the preservation and development of consciousness in the biological evolution is a result of natural selection, it is plausible that consciousness has not only been influenced by neural processes, but has had a survival value itself; and it could only have had this if it had been efficacious. Karl Popper develops in the book \"The Self and Its Brain\" a similar evolutionary argument.\n\nRegarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the \"integration consensus\". Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see \"Neural correlates\" section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of E. Morsella.\n\nAs noted earlier, even among writers who consider consciousness to be a well-defined thing, there is widespread dispute about which animals other than humans can be said to possess it. Edelman has described this distinction as that of humans possessing higher-order consciousness while sharing the trait of primary consciousness with non-human animals (see previous paragraph). Thus, any examination of the evolution of consciousness is faced with great difficulties. Nevertheless, some writers have argued that consciousness can be viewed from the standpoint of evolutionary biology as an adaptation in the sense of a trait that increases fitness. In his article \"Evolution of consciousness\", John Eccles argued that special anatomical and physical properties of the mammalian cerebral cortex gave rise to consciousness (\"[a] psychon ... linked to [a] dendron through quantum physics\"). Bernard Baars proposed that once in place, this \"recursive\" circuitry may have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms. Peter Carruthers has put forth one such potential adaptive advantage gained by conscious creatures by suggesting that consciousness allows an individual to make distinctions between appearance and reality. This ability would enable a creature to recognize the likelihood that their perceptions are deceiving them (e.g. that water in the distance may be a mirage) and behave accordingly, and it could also facilitate the manipulation of others by recognizing how things appear to them for both cooperative and devious ends.\n\nOther philosophers, however, have suggested that consciousness would not be necessary for any functional advantage in evolutionary processes. No one has given a causal explanation, they argue, of why it would not be possible for a functionally equivalent non-conscious organism (i.e., a philosophical zombie) to achieve the very same survival advantages as a conscious organism. If evolutionary processes are blind to the difference between function \"F\" being performed by conscious organism \"O\" and non-conscious organism \"O*\", it is unclear what adaptive advantage consciousness could provide. As a result, an exaptive explanation of consciousness has gained favor with some theorists that posit consciousness did not evolve as an adaptation but was an exaptation arising as a consequence of other developments such as increases in brain size or cortical rearrangement. Consciousness in this sense has been compared to the blind spot in the retina where it is not an adaption of the retina, but instead just a by-product of the way the retinal axons were wired. Several scholars including Pinker, Chomsky, Edelman, and Luria have indicated the importance of the emergence of human language as an important regulative mechanism of learning and memory in the context of the development of higher-order consciousness (see \"Neural correlates\" section above).\n\nThere are some brain states in which consciousness seems to be absent, including dreamless sleep, coma, and death. There are also a variety of circumstances that can change the relationship between the mind and the world in less drastic ways, producing what are known as altered states of consciousness. Some altered states occur naturally; others can be produced by drugs or brain damage. Altered states can be accompanied by changes in thinking, disturbances in the sense of time, feelings of loss of control, changes in emotional expression, alternations in body image and changes in meaning or significance.\n\nThe two most widely accepted altered states are sleep and dreaming. Although dream sleep and non-dream sleep appear very similar to an outside observer, each is associated with a distinct pattern of brain activity, metabolic activity, and eye movement; each is also associated with a distinct pattern of experience and cognition. During ordinary non-dream sleep, people who are awakened report only vague and sketchy thoughts, and their experiences do not cohere into a continuous narrative. During dream sleep, in contrast, people who are awakened report rich and detailed experiences in which events form a continuous progression, which may however be interrupted by bizarre or fantastic intrusions. Thought processes during the dream state frequently show a high level of irrationality. Both dream and non-dream states are associated with severe disruption of memory: it usually disappears in seconds during the non-dream state, and in minutes after awakening from a dream unless actively refreshed.\n\nResearch conducted on the effects of partial epileptic seizures on consciousness found that patients who suffer from partial epileptic seizures experience altered states of consciousness. In partial epileptic seizures, consciousness is impaired or lost while some aspects of consciousness, often automated behaviors, remain intact. Studies found that when measuring the qualitative features during partial epileptic seizures, patients exhibited an increase in arousal and became absorbed in the experience of the seizure, followed by difficulty in focusing and shifting attention.\n\nA variety of psychoactive drugs and alcohol have notable effects on consciousness. These range from a simple dulling of awareness produced by sedatives, to increases in the intensity of sensory qualities produced by stimulants, cannabis, empathogens–entactogens such as MDMA (\"Ecstasy\"), or most notably by the class of drugs known as psychedelics. LSD, mescaline, psilocybin, Dimethyltryptamine, and others in this group can produce major distortions of perception, including hallucinations; some users even describe their drug-induced experiences as mystical or spiritual in quality. The brain mechanisms underlying these effects are not as well understood as those induced by use of alcohol, but there is substantial evidence that alterations in the brain system that uses the chemical neurotransmitter serotonin play an essential role.\n\nThere has been some research into physiological changes in yogis and people who practise various techniques of meditation. Some research with brain waves during meditation has reported differences between those corresponding to ordinary relaxation and those corresponding to meditation. It has been disputed, however, whether there is enough evidence to count these as physiologically distinct states of consciousness.\n\nThe most extensive study of the characteristics of altered states of consciousness was made by psychologist Charles Tart in the 1960s and 1970s. Tart analyzed a state of consciousness as made up of a number of component processes, including exteroception (sensing the external world); interoception (sensing the body); input-processing (seeing meaning); emotions; memory; time sense; sense of identity; evaluation and cognitive processing; motor output; and interaction with the environment. Each of these, in his view, could be altered in multiple ways by drugs or other manipulations. The components that Tart identified have not, however, been validated by empirical studies. Research in this area has not yet reached firm conclusions, but a recent questionnaire-based study identified eleven significant factors contributing to drug-induced states of consciousness: experience of unity; spiritual experience; blissful state; insightfulness; disembodiment; impaired control and cognition; anxiety; complex imagery; elementary imagery; audio-visual synesthesia; and changed meaning of percepts.\n\nPhenomenology is a method of inquiry that attempts to examine the structure of consciousness in its own right, putting aside problems regarding the relationship of consciousness to the physical world. This approach was first proposed by the philosopher Edmund Husserl, and later elaborated by other philosophers and scientists. Husserl's original concept gave rise to two distinct lines of inquiry, in philosophy and psychology. In philosophy, phenomenology has largely been devoted to fundamental metaphysical questions, such as the nature of intentionality (\"\"aboutness\"\"). In psychology, phenomenology largely has meant attempting to investigate consciousness using the method of introspection, which means looking into one's own mind and reporting what one observes. This method fell into disrepute in the early twentieth century because of grave doubts about its reliability, but has been rehabilitated to some degree, especially when used in combination with techniques for examining brain activity.\nIntrospectively, the world of conscious experience seems to have considerable structure. Immanuel Kant asserted that the world as we perceive it is organized according to a set of fundamental \"intuitions\", which include \"object\" (we perceive the world as a set of distinct things); \"shape\"; \"quality\" (color, warmth, etc.); \"space\" (distance, direction, and location); and \"time\". Some of these constructs, such as space and time, correspond to the way the world is structured by the laws of physics; for others the correspondence is not as clear. Understanding the physical basis of qualities, such as redness or pain, has been particularly challenging. David Chalmers has called this the \"hard problem of consciousness\". Some philosophers have argued that it is intrinsically unsolvable, because qualities (\"\"qualia\"\") are ineffable; that is, they are \"raw feels\", incapable of being analyzed into component processes. Most psychologists and neuroscientists reject these arguments. For example, research on ideasthesia shows that qualia are organised into a semantic-like network. Nevertheless, it is clear that the relationship between a physical entity such as light and a perceptual quality such as color is extraordinarily complex and indirect, as demonstrated by a variety of optical illusions such as neon color spreading.\n\nIn neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world—Gerald Edelman expressed this point vividly by titling one of his books about consciousness \"The Remembered Present\". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.\n\nDespite the large amount of information available, many important aspects of perception remain mysterious. A great deal is known about low-level signal processing in sensory systems, but the ways by which sensory systems interact with each other, with \"executive\" systems in the frontal cortex, and with the language system are very incompletely understood. At a deeper level, there are still basic conceptual issues that remain unresolved. Many scientists have found it difficult to reconcile the fact that information is distributed across multiple brain areas with the apparent unity of consciousness: this is one aspect of the so-called \"binding problem\". There are also some scientists who have expressed grave reservations about the idea that the brain forms representations of the outside world at all: influential members of this group include psychologist J. J. Gibson and roboticist Rodney Brooks, who both argued in favor of \"intelligence without representation\".\n\nThe medical approach to consciousness is practically oriented. It derives from a need to treat people whose brain function has been impaired as a result of disease, brain damage, toxins, or drugs. In medicine, conceptual distinctions are considered useful to the degree that they can help to guide treatments. Whereas the philosophical approach to consciousness focuses on its fundamental nature and its contents, the medical approach focuses on the \"amount\" of consciousness a person has: in medicine, consciousness is assessed as a \"level\" ranging from coma and brain death at the low end, to full alertness and purposeful responsiveness at the high end.\n\nConsciousness is of concern to patients and physicians, especially neurologists and anesthesiologists. Patients may suffer from disorders of consciousness, or may need to be anesthetized for a surgical procedure. Physicians may perform consciousness-related interventions such as instructing the patient to sleep, administering general anesthesia, or inducing medical coma. Also, bioethicists may be concerned with the ethical implications of consciousness in medical cases of patients such as the Karen Ann Quinlan case, while neuroscientists may study patients with impaired consciousness in hopes of gaining information about how the brain works.\n\nIn medicine, consciousness is examined using a set of procedures known as neuropsychological assessment. There are two commonly used methods for assessing the level of consciousness of a patient: a simple procedure that requires minimal training, and a more complex procedure that requires substantial expertise. The simple procedure begins by asking whether the patient is able to move and react to physical stimuli. If so, the next question is whether the patient can respond in a meaningful way to questions and commands. If so, the patient is asked for name, current location, and current day and time. A patient who can answer all of these questions is said to be \"alert and oriented times four\" (sometimes denoted \"A&Ox4\" on a medical chart), and is usually considered fully conscious.\n\nThe more complex procedure is known as a neurological examination, and is usually carried out by a neurologist in a hospital setting. A formal neurological examination runs through a precisely delineated series of tests, beginning with tests for basic sensorimotor reflexes, and culminating with tests for sophisticated use of language. The outcome may be summarized using the Glasgow Coma Scale, which yields a number in the range 3—15, with a score of 3 to 8 indicating coma, and 15 indicating full consciousness. The Glasgow Coma Scale has three subscales, measuring the \"best motor response\" (ranging from \"no motor response\" to \"obeys commands\"), the \"best eye response\" (ranging from \"no eye opening\" to \"eyes opening spontaneously\") and the \"best verbal response\" (ranging from \"no verbal response\" to \"fully oriented\"). There is also a simpler pediatric version of the scale, for children too young to be able to use language.\n\nIn 2013, an experimental procedure was developed to measure degrees of consciousness, the procedure involving stimulating the brain with a magnetic pulse, measuring resulting waves of electrical activity, and developing a consciousness score based on the complexity of the brain activity.\n\nMedical conditions that inhibit consciousness are considered disorders of consciousness. This category generally includes minimally conscious state and persistent vegetative state, but sometimes also includes the less severe locked-in syndrome and more severe chronic coma. Differential diagnosis of these disorders is an active area of biomedical research. Finally, brain death results in an irreversible disruption of consciousness. While other conditions may cause a moderate deterioration (e.g., dementia and delirium) or transient interruption (e.g., grand mal and petit mal seizures) of consciousness, they are not included in this category.\n\nOne of the most striking disorders of consciousness goes by the name anosognosia, a Greek-derived term meaning \"unawareness of disease\". This is a condition in which patients are disabled in some way, most commonly as a result of a stroke, but either misunderstand the nature of the problem or deny that there is anything wrong with them. The most frequently occurring form is seen in people who have experienced a stroke damaging the parietal lobe in the right hemisphere of the brain, giving rise to a syndrome known as hemispatial neglect, characterized by an inability to direct action or attention toward objects located to the left with respect to their bodies. Patients with hemispatial neglect are often paralyzed on the right side of the body, but sometimes deny being unable to move. When questioned about the obvious problem, the patient may avoid giving a direct answer, or may give an explanation that doesn't make sense. Patients with hemispatial neglect may also fail to recognize paralyzed parts of their bodies: one frequently mentioned case is of a man who repeatedly tried to throw his own paralyzed right leg out of the bed he was lying in, and when asked what he was doing, complained that somebody had put a dead leg into the bed with him. An even more striking type of anosognosia is Anton–Babinski syndrome, a rarely occurring condition in which patients become blind but claim to be able to see normally, and persist in this claim in spite of all evidence to the contrary.\n\nWilliam James is usually credited with popularizing the idea that human consciousness flows like a stream, in his \"Principles of Psychology\" of 1890. According to James, the \"stream of thought\" is governed by five characteristics: \"(1) Every thought tends to be part of a personal consciousness. (2) Within each personal consciousness thought is always changing. (3) Within each personal consciousness thought is sensibly continuous. (4) It always appears to deal with objects independent of itself. (5) It is interested in some parts of these objects to the exclusion of others\". A similar concept appears in Buddhist philosophy, expressed by the Sanskrit term \"Citta-saṃtāna\", which is usually translated as mindstream or \"mental continuum\". Buddhist teachings describe that consciousness manifests moment to moment as sense impressions and mental phenomena that are continuously changing. The teachings list six triggers that can result in the generation of different mental events. These triggers are input from the five senses (seeing, hearing, smelling, tasting or touch sensations), or a thought (relating to the past, present or the future) that happen to arise in the mind. The mental events generated as a result of these triggers are: feelings, perceptions and intentions/behaviour. The moment-by-moment manifestation of the mind-stream is said to happen in every person all the time. It even happens in a scientist who analyses various phenomena in the world, or analyses the material body including the organ brain. The manifestation of the mindstream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws. The purpose of the Buddhist practice of mindfulness is to understand the inherent nature of the consciousness and its characteristics.\n\nIn the west, the primary impact of the idea has been on literature rather than science: stream of consciousness as a narrative mode means writing in a way that attempts to portray the moment-to-moment thoughts and experiences of a character. This technique perhaps had its beginnings in the monologues of Shakespeare's plays, and reached its fullest development in the novels of James Joyce and Virginia Woolf, although it has also been used by many other noted writers.\n\nHere for example is a passage from Joyce's Ulysses about the thoughts of Molly Bloom:\n\nTo most philosophers, the word \"consciousness\" connotes the relationship between the mind and the world. To writers on spiritual or religious topics, it frequently connotes the relationship between the mind and God, or the relationship between the mind and deeper truths that are thought to be more fundamental than the physical world. Krishna consciousness, for example, is a term used to mean an intimate linkage between the mind of a worshipper and the god Krishna. The mystical psychiatrist Richard Maurice Bucke distinguished between three types of consciousness: \"Simple Consciousness\", awareness of the body, possessed by many animals; \"Self Consciousness\", awareness of being aware, possessed only by humans; and \"Cosmic Consciousness\", awareness of the life and order of the universe, possessed only by humans who are enlightened. Many more examples could be given. The most thorough account of the spiritual approach may be Ken Wilber's book \"The Spectrum of Consciousness\", a comparison of western and eastern ways of thinking about the mind. Wilber described consciousness as a spectrum with ordinary awareness at one end, and more profound types of awareness at higher levels.\n\n\n", "id": "5664", "title": "Consciousness"}
{"url": "https://en.wikipedia.org/wiki?curid=5665", "text": "Currency\n\nA currency (from , \"in circulation\", from ) in the most specific use of the word refers to money in any form when in actual use or circulation as a medium of exchange, especially circulating banknotes and coins. A more general definition is that a currency is a \"system of money\" (monetary units) in common use, especially in a nation. Under this definition, US dollars, British pounds, Australian dollars, and European euros are examples of currency. These various currencies are recognized stores of value and are traded between nations in foreign exchange markets, which determine the relative values of the different currencies. Currencies in this sense are defined by governments, and each type has limited boundaries of acceptance.\n\nOther definitions of the term \"currency\" are discussed in their respective synonymous articles banknote, coin, and money. The latter definition, pertaining to the currency systems of nations, is the topic of this article. Currencies can be classified into two monetary systems: fiat money and commodity money, depending on what guarantees the value (the economy at large vs. the government's physical metal reserves). Some currencies are legal tender in certain political jurisdictions, which means they cannot be refused as payment for debt. Others are simply traded for their economic value. Digital currency has arisen with the popularity of computers and the Internet.\n\nCurrency evolved from two basic innovations, both of which had occurred by 2000 BC. Originally money was a form of receipt, representing grain stored in temple granaries in Sumer in ancient Mesopotamia, then Ancient Egypt.\n\nIn this first stage of currency, metals were used as symbols to represent value stored in the form of commodities. This formed the basis of trade in the Fertile Crescent for over 1500 years. However, the collapse of the Near Eastern trading system pointed to a flaw: in an era where there was no place that was safe to store value, the value of a circulating medium could only be as sound as the forces that defended that store. Trade could only reach as far as the credibility of that military. By the late Bronze Age, however, a series of treaties had established safe passage for merchants around the Eastern Mediterranean, spreading from Minoan Crete and Mycenae in the northwest to Elam and Bahrain in the southeast. It is not known what was used as a currency for these exchanges, but it is thought that ox-hide shaped ingots of copper, produced in Cyprus, may have functioned as a currency.\n\nIt is thought that the increase in piracy and raiding associated with the Bronze Age collapse, possibly produced by the Peoples of the Sea, brought the trading system of oxhide ingots to an end. It was only with the recovery of Phoenician trade in the 10th and 9th centuries BC that saw a return to prosperity, and the appearance of real coinage, possibly first in Anatolia with Croesus of Lydia and subsequently with the Greeks and Persians. In Africa, many forms of value store have been used, including beads, ingots, ivory, various forms of weapons, livestock, the manilla currency, and ochre and other earth oxides. The manilla rings of West Africa were one of the currencies used from the 15th century onwards to sell slaves. African currency is still notable for its variety, and in many places various forms of barter still apply.\n\nThese factors led to the metal itself being the store of value: first silver, then both silver and gold, and at one point also bronze. Now we have copper coins and other non-precious metals as coins. Metals were mined, weighed, and stamped into coins. This was to assure the individual taking the coin that he was getting a certain known weight of precious metal. Coins could be counterfeited, but they also created a new unit of account, which helped lead to banking. Archimedes' principle provided the next link: coins could now be easily tested for their fine weight of metal, and thus the value of a coin could be determined, even if it had been shaved, debased or otherwise tampered with (see Numismatics).\n\nMost major economies using coinage had several tiers of coins, using a mix of copper, silver and gold. Gold coins were used for large purchases, payment of the military and backing of state activities; they were more often used as measures of account than physical coins. Silver coins were used for midsized transactions, and as a unit of account for taxes, dues, contracts and fealty, while coins of copper, silver, or some mixture thereof (see debasement), were used for everyday transactions. This system had been used in ancient India since the time of the Mahajanapadas. The exact ratio in value of the three metals varied greatly in different eras and places; for example, the opening of silver mines in the Harz mountains of central Europe made silver relatively less valuable, as did the flood of New World silver after the Spanish conquests. However, the rarity of gold consistently made it more valuable than silver, and likewise silver was consistently worth more than copper.\n\nIn premodern China, the need for credit and for a medium of exchange that was less physically cumbersome than large numbers of copper coins led to the introduction of paper money, i.e. banknotes. Their introduction was a gradual process which lasted from the late Tang dynasty (618–907) into the Song dynasty (960–1279). It began as a means for merchants to exchange heavy coinage for receipts of deposit issued as promissory notes by wholesalers' shops. These notes were valid for temporary use in a small regional territory. In the 10th century, the Song dynasty government began to circulate these notes amongst the traders in its monopolized salt industry. The Song government granted several shops the right to issue banknotes, and in the early 12th century the government finally took over these shops to produce state-issued currency. Yet the banknotes issued were still only locally and temporarily valid: it was not until the mid 13th century that a standard and uniform government issue of paper money became an acceptable nationwide currency. The already widespread methods of woodblock printing and then Pi Sheng's movable type printing by the 11th century were the impetus for the mass production of paper money in premodern China.\nAt around the same time in the medieval Islamic world, a vigorous monetary economy was created during the 7th–12th centuries on the basis of the expanding levels of circulation of a stable high-value currency (the dinar). Innovations introduced by Muslim economists, traders and merchants include the earliest uses of credit, cheques, promissory notes, savings accounts, transactional accounts, loaning, trusts, exchange rates, the transfer of credit and debt, and banking institutions for loans and deposits.\n\nIn Europe, paper money was first introduced on a regular basis in Sweden in 1661 (although Washington Irving records an earlier emergency use of it, by the Spanish in a siege during the Conquest of Granada). As Sweden was rich in copper, its low value necessitated extraordinarily big coins, often weighing several kilograms. \n\nThe advantages of paper currency were numerous: it reduced the need to transport gold and silver, which was risky; it facilitated loans of gold or silver at interest, since the underlying specie (gold or silver) never left the possession of the lender until someone else redeemed the note; and it allowed a division of currency into credit and specie backed forms. It enabled the sale of stock in joint-stock companies, and the redemption of those shares in paper.\n\nBut there were also disadvantages. First, since a note has no intrinsic value, there was nothing to stop issuing authorities from printing more notes than they had specie to back them with. Second, because it increased the money supply, it increased inflationary pressures, a fact observed by David Hume in the 18th century. Thus paper money would often lead to an inflationary bubble, which could collapse if people began demanding hard money, causing the demand for paper notes to fall to zero. The printing of paper money was also associated with wars, and financing of wars, and therefore regarded as part of maintaining a standing army. For these reasons, paper currency was held in suspicion and hostility in Europe and America. It was also addictive, since the speculative profits of trade and capital creation were quite large. Major nations established mints to print money and mint coins, and branches of their treasury to collect taxes and hold gold and silver stock.\n\nAt that time, both silver and gold were considered legal tender, and accepted by governments for taxes. However, the instability in the ratio between the two grew over the course of the 19th century, with the increases both in supply of these metals, particularly silver, and in trade. The parallel use of both metals is called bimetallism, and the attempt to create a bimetallic standard where both gold and silver backed currency remained in circulation occupied the efforts of inflationists. Governments at this point could use currency as an instrument of policy, printing paper currency such as the United States Greenback, to pay for military expenditures. They could also set the terms at which they would redeem notes for specie, by limiting the amount of purchase, or the minimum amount that could be redeemed.\n\nBy 1900, most of the industrializing nations were on some form of gold standard, with paper notes and silver coins constituting the circulating medium. Private banks and governments across the world followed Gresham's Law: keeping the gold and silver they received, but paying out in notes. This did not happen all around the world at the same time, but occurred sporadically, generally in times of war or financial crisis, beginning in the early part of the 20th century and continuing across the world until the late 20th century, when the regime of floating fiat currencies came into force. One of the last countries to break away from the gold standard was the United States in 1971, an action known as the Nixon shock. No country has an enforceable gold standard or silver standard currency system.\n\nA banknote (more commonly known as a bill in the United States and Canada) is a type of currency, and commonly used as legal tender in many jurisdictions. With coins, banknotes make up the cash form of all money. Banknotes are mostly paper, but Australia's Commonwealth Scientific and Industrial Research Organisation developed the world's first polymer currency in the 1980s that went into circulation on the nation's bicentenary in 1988. Now used in some 22 countries (over 40 if counting commemorative issues), polymer currency dramatically improves the life span of banknotes and prevents counterfeiting.\n\nCurrency use is based on the concept of lex monetae; that a sovereign state decides which currency it shall use. Currently, the International Organization for Standardization has introduced a three-letter system of codes (ISO 4217) to define currency (as opposed to simple names or currency signs), in order to remove the confusion that there are dozens of currencies called the dollar and many called the franc. Even the pound is used in nearly a dozen different countries; most of these are tied to the Pound Sterling, while the remainder have varying values. In general, the three-letter code uses the ISO 3166-1 country code for the first two letters and the first letter of the name of the currency (D for dollar, for instance) as the third letter. United States currency, for instance is globally referred to as USD.\n\nThe International Monetary Fund uses a variant system when referring to national currencies.\n\nDistinct from centrally controlled government-issued currencies, private decentralized trust networks support alternative currencies such as Bitcoin, Litecoin, Peercoin or Dogecoin, as well as branded currencies, for example 'obligation' based stores of value, such as quasi-regulated BarterCard, Loyalty Points (Credit Cards, Airlines) or Game-Credits (MMO games) that are based on reputation of commercial products, or highly regulated 'asset backed' 'alternative currencies' such as mobile-money schemes like MPESA (called E-Money Issuance).\n\nCurrency may be Internet-based and digital, for instance, Bitcoin and not tied to any specific country, or the IMF's SDR that is based on a basket of currencies (and assets held).\n\nIn most cases, a central bank has a monopoly right to issue of coins and banknotes (fiat money) for its own area of circulation (a country or group of countries); it regulates the production of currency by banks (credit) through monetary policy.\n\nAn exchange rate is the price at which two currencies can be exchanged against each other. This is used for trade between the two currency zones. Exchange rates can be classified as either floating or fixed. In the former, day-to-day movements in exchange rates are determined by the market; in the latter, governments intervene in the market to buy or sell their currency to balance supply and demand at a fixed exchange rate.\n\nIn cases where a country has control of its own currency, that control is exercised either by a central bank or by a Ministry of Finance. The institution that has control of monetary policy is referred to as the monetary authority. Monetary authorities have varying degrees of autonomy from the governments that create them. In the United States, the Federal Reserve System operates without direct oversight by the legislative or executive branches. A monetary authority is created and supported by its sponsoring government, so independence can be reduced by the legislative or executive authority that creates it.\n\nSeveral countries can use the same name for their own separate currencies (for example, \"dollar\" in Australia, Canada and the United States). By contrast, several countries can also use the same currency (for example, the euro or the CFA franc), or one country can declare the currency of another country to be legal tender. For example, Panama and El Salvador have declared U.S. currency to be legal tender, and from 1791 to 1857, Spanish silver coins were legal tender in the United States. At various times countries have either re-stamped foreign coins, or used currency board issuing one note of currency for each note of a foreign government held, as Ecuador currently does.\n\nEach currency typically has a main currency unit (the dollar, for example, or the euro) and a fractional unit, often defined as of the main unit: 100 cents = 1 dollar, 100 centimes = 1 franc, 100 pence = 1 pound, although units of or occasionally also occur. Some currencies do not have any smaller units at all, such as the Icelandic króna.\n\nMauritania and Madagascar are the only remaining countries that do not use the decimal system; instead, the Mauritanian ouguiya is in theory divided into 5 khoums, while the Malagasy ariary is theoretically divided into 5 iraimbilanja. In these countries, words like \"dollar\" or \"pound\" \"were simply names for given weights of gold.\" Due to inflation khoums and iraimbilanja have in practice fallen into disuse. (See non-decimal currencies for other historic currencies with non-decimal divisions.)\n\nConvertibility of a currency determines the ability of an individual, corporate or government to convert its local currency to another currency or vice versa with or without central bank/government intervention. Based on the above restrictions or free and readily conversion features, currencies are classified as:\n\nIn economics, a local currency is a currency not backed by a national government, and intended to trade only in a small area. Advocates such as Jane Jacobs argue that this enables an economically depressed region to pull itself up, by giving the people living there a medium of exchange that they can use to exchange services and locally produced goods (in a broader sense, this is the original purpose of all money). Opponents of this concept argue that local currency creates a barrier which can interfere with economies of scale and comparative advantage, and that in some cases they can serve as a means of tax evasion.\n\nLocal currencies can also come into being when there is economic turmoil involving the national currency. An example of this is the Argentinian economic crisis of 2002 in which IOUs issued by local governments quickly took on some of the characteristics of local currencies.\n\nOne of the best examples of a local currency is the original LETS currency, founded on Vancouver Island in the early 1980s. In 1982, the Canadian Central Bank’s lending rates ran up to 14% which drove chartered bank lending rates as high as 19%. The resulting currency and credit scarcity left island residents with few options other than to create a local currency.\n\nThe following table are estimates for 15 most frequently used currencies in World Payments from 2012 to 2015 by SWIFT. \n\n\nRelated concepts\n\nAccounting units\n\nLists\n", "id": "5665", "title": "Currency"}
{"url": "https://en.wikipedia.org/wiki?curid=5666", "text": "Central bank\n\nA central bank, reserve bank, or monetary authority is an institution that manages a state's currency, money supply, and interest rates. Central banks also usually oversee the commercial banking system of their respective countries. In contrast to a commercial bank, a central bank possesses a monopoly on increasing the monetary base in the state, and usually also prints the national currency, which usually serves as the state's legal tender.\n\nThe primary function of a central bank is to control the nation's money supply (monetary policy), through active duties such as managing interest rates, setting the reserve requirement, and acting as a lender of last resort to the banking sector during times of bank insolvency or financial crisis. Central banks usually also have supervisory powers, intended to prevent bank runs and to reduce the risk that commercial banks and other financial institutions engage in reckless or fraudulent behavior. Central banks in most developed nations are institutionally designed to be independent from political interference. Still, limited control by the executive and legislative bodies usually exists.\n\nPrior to the 17th century most money was commodity money, typically gold or silver. However, promises to pay were widely circulated and accepted as value at least five hundred years earlier in both Europe and Asia. The Song dynasty was the first to issue generally circulating paper currency, while the Yuan Dynasty was the first to use notes as the predominant circulating medium. In 1455, in an effort to control inflation, the succeeding Ming Dynasty ended the use of paper money and closed much of Chinese trade. The medieval European Knights Templar ran an early prototype of a central banking system, as their promises to pay were widely respected, and many regard their activities as having laid the basis for the modern banking system.\n\nThe Bank of Amsterdam (Amsterdamsche Wisselbank), established in the Dutch Republic in 1609, is often considered to be the forerunner to modern central banks. The Wisselbank's innovations helped lay the foundations for the birth and development of the central banking system that now plays a vital role in the world's economy. Along with a number of subsidiary local banks, it performed many functions of a central banking system. It occupied a central position in the financial world of its day, providing an effective, efficient and trusted system for national and international payments, and introduced the first ever international reserve currency, the bank guilder. Lucien Gillard (2004) calls it the \"European guilder\" (\"le florin européen\"), and Adam Smith devotes many pages to explaining how the bank guilder works (Smith 1776: 446-455). The model of the Wisselbank as a state bank was adapted throughout Europe, including the Bank of Sweden (1668) and the Bank of England (1694).\n\nEstablished by Dutch-Latvian Johan Palmstruch in 1668, Sveriges Riksbank (Bank of Sweden) is often considered by many as the world's oldest central bank.\n\nThe establishment of the Bank of England, the model on which most modern central banks have been based, was devised by Charles Montagu, 1st Earl of Halifax, in 1694, following a proposal by the banker William Paterson three years earlier, which had not been acted upon. In the Kingdom of England in the 1690s, public funds were in short supply, and the credit of William III's government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8 percent) needed to finance the ongoing Nine Years' War with France. In order to induce subscription to the loan, Montagu proposed that the subscribers were to be incorporated as \"The Governor and Company of the Bank of England\" with long-term banking privileges including the issue of notes. The lenders would give the government cash (bullion) and also issue notes against the government bonds, which could be lent again. A Royal Charter was granted on 27 July through the passage of the Tonnage Act 1694. The bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue banknotes. The £1.2M was raised in 12 days; half of this was used to rebuild the Navy.\nAlthough this establishment of the Bank of England marks the origin of central banking, it did not have the functions of a modern central bank, namely, to regulate the value of the national currency, to finance the government, to be the sole authorised distributor of banknotes, and to function as a 'lender of last resort' to banks suffering a liquidity crisis. These modern central banking functions evolved slowly through the 18th and 19th centuries.\n\nAlthough the Bank was originally a private institution, by the end of the 18th century it was increasingly being regarded as a public authority with civic responsibility toward the upkeep of a healthy financial system. The currency crisis of 1797, caused by panicked depositors withdrawing from the Bank led to the government suspending convertibility of notes into specie payment. The bank was soon accused by the bullionists of causing the exchange rate to fall from over issuing banknotes, a charge which the Bank denied. Nevertheless, it was clear that the Bank was being treated as an organ of the state.\n\nHenry Thornton, a merchant banker and monetary theorist has been described as the father of the modern central bank. An opponent of the real bills doctrine, he was a defender of the bullionist position and a significant figure in monetary theory. Thornton's process of monetary expansion anticipated the theories of Knut Wicksell regarding the \"cumulative process which restates the Quantity Theory in a theoretically coherent form\". As a response to the 1797 currency crisis, Thornton wrote in 1802 \"An Enquiry into the Nature and Effects of the Paper Credit of Great Britain\", in which he argued that the increase in paper credit did not cause the crisis. The book also gives a detailed account of the British monetary system as well as a detailed examination of the ways in which the Bank of England should act to counteract fluctuations in the value of the pound.\nUntil the mid-nineteenth century, commercial banks were able to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. Many consider the origins of the central bank to lie with the passage of the Bank Charter Act of 1844. Under this law, authorisation to issue new banknotes was restricted to the Bank of England. At the same time, the Bank of England was restricted to issue new banknotes only if they were 100% backed by gold or up to £14 million in government debt. The Act served to restrict the supply of new notes reaching circulation, and gave the Bank of England an effective monopoly on the printing of new notes.\n\nThe Bank accepted the role of 'lender of last resort' in the 1870s after criticism of its lacklustre response to the Overend-Gurney crisis. The journalist Walter Bagehot wrote on the subject in \"\", in which he advocated for the Bank to officially become a lender of last resort during a credit crunch, sometimes referred to as \"Bagehot's dictum\". Paul Tucker phrased the dictum in 2009 as follows:\n\nCentral banks were established in many European countries during the 19th century. The War of the Second Coalition led to the creation of the Banque de France in 1800, in an effort to improve the public financing of the war.\n\nAlthough central banks today are generally associated with fiat money, the 19th and early 20th centuries central banks in most of Europe and Japan developed under the international gold standard, elsewhere free banking or currency boards were more usual at this time. Problems with collapses of banks during downturns, however, led to wider support for central banks in those nations which did not as yet possess them, most notably in Australia.\n\nOn 23 December 1913 the U.S. Congress created the US Federal Reserve through the passing of The Federal Reserve Act in the Senate and its signing by President Woodrow Wilson on the same day. Australia established its first central bank in 1920, Peru in 1922, Colombia in 1923, Mexico and Chile in 1925 and Canada and New Zealand in the aftermath of the Great Depression in 1934. By 1935, the only significant independent nation that did not possess a central bank was Brazil, which subsequently developed a precursor thereto in 1945 and the present Central Bank of Brazil twenty years later. After gaining independence, African and Asian countries also established central banks or monetary unions.\nThe People's Bank of China evolved its role as a central bank starting in about 1979 with the introduction of market reforms, which accelerated in 1989 when the country adopted a generally capitalist approach to its export economy. Evolving further partly in response to the European Central Bank, the People's Bank of China had by 2000 become a modern central bank. The most recent bank model was introduced together with the euro, and involves coordination of the European national banks, which continue to manage their respective economies separately in all respects other than currency exchange and base interest rates.\n\nThere is no standard terminology for the name of a central bank, but many countries use the \"Bank of Country\" form — for example: Bank of England (which, despite its name, is in fact the central bank of the United Kingdom as a whole. The name's lack of representation of the entire United Kingdom ('Bank of Britain', for example) can be owed to the fact that its establishment occurred when the Kingdoms of England, Scotland and Ireland were separate entities (at least in name), and therefore pre-dates the merger of the Kingdoms of England and Scotland, the Kingdom of Ireland's absorption into the Union and the formation of the present day United Kingdom), Bank of Canada, Bank of Mexico.\n\nThe word \"Reserve\" is also often included, such as the Reserve Bank of India, Reserve Bank of Australia, Reserve Bank of New Zealand, the South African Reserve Bank, and Federal Reserve System. Other central banks are known as monetary authorities such as the Saudi Arabian Monetary Authority, Hong Kong Monetary Authority, Monetary Authority of Singapore, Maldives Monetary Authority and Cayman Islands Monetary Authority. There is an instance where native language was used to name the central bank: in the Philippines the Filipino name Bangko Sentral ng Pilipinas is used even in English.\n\nSome are styled \"national\" banks, such as the Swiss National Bank and National Bank of Ukraine, although the term national bank is also used for private commercial banks in some countries such as National Bank of Pakistan. In other cases, central banks may incorporate the word \"Central\" (for example, European Central Bank, Central Bank of Ireland, Central Bank of Brazil). In some countries, particularly in formerly Communist ones, the term national bank may be used to indicate both the monetary authority and the leading banking entity, such as the Soviet Union's Gosbank (state bank). In rare cases, central banks are styled \"state\" banks such as the State Bank of Pakistan and State Bank of Vietnam.\n\nMany countries have state-owned banks or other quasi-government entities that have entirely separate functions, such as financing imports and exports. In other countries, the term national bank may be used to indicate that the central bank's goals are broader than monetary stability, such as full employment, industrial development, or other goals. Some state-owned commercial banks have names suggestive of central banks, even if they are not: examples are the Bank of India and Central Bank of India.\n\nThe chief executive of a central bank is usually known as the Governor, President or Chair.\n\nAfter the financial crisis of 2007-2008 central banks led change, but as of 2015 their ability to boost economic growth has stalled. Central banks debate whether they should experiment with new measures like negative interest rates or direct financing of government, \"lean even more on politicians to do more\". Andy Haldane from the Bank of England said \"central bankers may need to accept that their good old days – of adjusting interest rates to boost employment or contain inflation – may be gone for good\". The European Central Bank and The Bank of Japan whose economies are in or close to deflation, continue quantitative easing buying securities to encourage more lending.\n\nFunctions of a central bank may include:\n\n\nCentral banks implement a country's chosen monetary policy.\n\nAt the most basic level, monetary policy involves establishing what form of currency the country may have, whether a fiat currency, gold-backed currency (disallowed for countries in the International Monetary Fund), currency board or a currency union. When a country has its own national currency, this involves the issue of some form of standardized currency, which is essentially a form of promissory note: a promise to exchange the note for \"money\" under certain circumstances. Historically, this was often a promise to exchange the money for precious metals in some fixed amount. Now, when many currencies are fiat money, the \"promise to pay\" consists of the promise to accept that currency to pay for taxes.\n\nA central bank may use another country's currency either directly in a currency union, or indirectly on a currency board. In the latter case, exemplified by the Bulgarian National Bank, Hong Kong and Latvia, the local currency is backed at a fixed rate by the central bank's holdings of a foreign currency.\nSimilar to commercial banks, central banks hold assets (government bonds, foreign exchange, gold, and other financial assets) and incur liabilities (currency outstanding). Central banks create money by issuing interest-free currency notes and selling them to the public (government) in exchange for interest-bearing assets such as government bonds. When a central bank wishes to purchase more bonds than their respective national governments make available, they may purchase private bonds or assets denominated in foreign currencies.\n\nThe European Central Bank remits its interest income to the central banks of the member countries of the European Union. The US Federal Reserve remits all its profits to the U.S. Treasury. This income, derived from the power to issue currency, is referred to as seigniorage, and usually belongs to the national government. The state-sanctioned power to create currency is called the Right of Issuance. Throughout history there have been disagreements over this power, since whoever controls the creation of currency controls the seigniorage income.\nThe expression \"monetary policy\" may also refer more narrowly to the interest-rate targets and other active measures undertaken by the monetary authority.\n\nFrictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. Unemployment beyond frictional unemployment is classified as unintended unemployment.\n\nFor example, structural unemployment is a form of unemployment resulting from a mismatch between demand in the labour market and the skills and locations of the workers seeking employment. Macroeconomic policy generally aims to reduce unintended unemployment.\n\nKeynes labeled any jobs that would be created by a rise in wage-goods (i.e., a decrease in real-wages) as involuntary unemployment:\n\nInflation is defined either as the devaluation of a currency or equivalently the rise of prices relative to a currency.\n\nSince inflation lowers real wages, Keynesians view inflation as the solution to involuntary unemployment. However, \"unanticipated\" inflation leads to lender losses as the real interest rate will be lower than expected. Thus, Keynesian monetary policy aims for a steady rate of inflation. A publication from the Austrian School, \"The Case Against the Fed\", argues that the efforts of the central banks to control inflation have been counterproductive.\n\nEconomic growth can be enhanced by investment in capital, such as more or better machinery. A low interest rate implies that firms can borrow money to invest in their capital stock and pay less interest for it. Lowering the interest is therefore considered to encourage economic growth and is often used to alleviate times of low economic growth. On the other hand, raising the interest rate is often used in times of high economic growth as a contra-cyclical device to keep the economy from overheating and avoid market bubbles.\n\nFurther goals of monetary policy are stability of interest rates, of the financial market, and of the foreign exchange market.\nGoals frequently cannot be separated from each other and often conflict. Costs must therefore be carefully weighed before policy implementation.\n\nThe main monetary policy instruments available to central banks are open market operation, bank reserve requirement, interest rate policy, re-lending and re-discount (including using the term repurchase market), and credit policy (often coordinated with trade policy). While capital adequacy is important, it is defined and regulated by the Bank for International Settlements, and central banks in practice generally do not apply stricter rules.\n\nTo enable open market operations, a central bank must hold foreign exchange reserves (usually in the form of government bonds) and official gold reserves. It will often have some influence over any official or mandated exchange rates: Some exchange rates are managed, some are market based (free float) and many are somewhere in between (\"managed float\" or \"dirty float\").\n\nBy far the most visible and obvious power of many modern central banks is to influence market interest rates; contrary to popular belief, they rarely \"set\" rates to a fixed number. Although the mechanism differs from country to country, most use a similar mechanism based on a central bank's ability to create as much fiat money as required.\n\nThe mechanism to move the market towards a 'target rate' (whichever specific rate is used) is generally to lend money or borrow money in theoretically unlimited quantities, until the targeted market rate is sufficiently close to the target. Central banks may do so by lending money to and borrowing money from (taking deposits from) a limited number of qualified banks, or by purchasing and selling bonds. As an example of how this functions, the Bank of Canada sets a target overnight rate, and a band of plus or minus 0.25%. Qualified banks borrow from each other within this band, but never above or below, because the central bank will always lend to them at the top of the band, and take deposits at the bottom of the band; in principle, the capacity to borrow and lend at the extremes of the band are unlimited. Other central banks use similar mechanisms.\n\nThe target rates are generally short-term rates. The actual rate that borrowers and lenders receive on the market will depend on (perceived) credit risk, maturity and other factors. For example, a central bank might set a target rate for overnight lending of 4.5%, but rates for (equivalent risk) five-year bonds might be 5%, 4.75%, or, in cases of inverted yield curves, even below the short-term rate. Many central banks have one primary \"headline\" rate that is quoted as the \"central bank rate\". In practice, they will have other tools and rates that are used, but only one that is rigorously targeted and enforced.\n\n\"The rate at which the central bank lends money can indeed be chosen at will by the central bank; this is the rate that makes the financial headlines.\" – Henry C.K. Liu. Liu explains further that \"the U.S. central-bank lending rate is known as the Fed funds rate. The Fed sets a target for the Fed funds rate, which its Open Market Committee tries to match by lending or borrowing in the money market ... a fiat money system set by command of the central bank. The Fed is the head of the central-bank because the U.S. dollar is the key reserve currency for international trade. The global money market is a USA dollar market. All other currencies markets revolve around the U.S. dollar market.\" Accordingly, the U.S. situation is not typical of central banks in general.\n\nTypically a central bank controls certain types of short-term interest rates. These influence the stock- and bond markets as well as mortgage and other interest rates. The European Central Bank for example announces its interest rate at the meeting of its Governing Council; in the case of the U.S. Federal Reserve, the Federal Reserve Board of Governors. Both the Federal Reserve and the ECB are composed of one or more central bodies that are responsible for the main decisions about interest rates and the size and type of open market operations, and several branches to execute its policies. In the case of the Federal Reserve, they are the local Federal Reserve Banks; for the ECB they are the national central banks.\n\nA typical central bank has several interest rates or monetary policy tools it can set to influence markets.\n\n\nThese rates directly affect the rates in the money market, the market for short term loans.\n\nThrough open market operations, a central bank influences the money supply in an economy. Each time it buys securities (such as a government bond or treasury bill), it in effect creates money. The central bank exchanges money for the security, increasing the money supply while lowering the supply of the specific security. Conversely, selling of securities by the central bank reduces the money supply.\n\nOpen market operations usually take the form of:\n\n\nAll of these interventions can also influence the foreign exchange market and thus the exchange rate. For example, the People's Bank of China and the Bank of Japan have on occasion bought several hundred billions of U.S. Treasuries, presumably in order to stop the decline of the U.S. dollar versus the renminbi and the yen.\n\nAll banks are required to hold a certain percentage of their assets as capital, a rate which may be established by the central bank or the banking supervisor. For international banks, including the 55 member central banks of the Bank for International Settlements, the threshold is 8% (see the Basel Capital Accords) of risk-adjusted assets, whereby certain assets (such as government bonds) are considered to have lower risk and are either partially or fully excluded from total assets for the purposes of calculating capital adequacy. Partly due to concerns about asset inflation and repurchase agreements, capital requirements may be considered more effective than reserve requirements in preventing indefinite lending: when at the threshold, a bank cannot extend another loan without acquiring further capital on its balance sheet.\n\nHistorically, bank reserves have formed only a small fraction of deposits, a system called fractional reserve banking. Banks would hold only a small percentage of their assets in the form of cash reserves as insurance against bank runs. Over time this process has been regulated and insured by central banks. Such legal reserve requirements were introduced in the 19th century as an attempt to reduce the risk of banks overextending themselves and suffering from bank runs, as this could lead to knock-on effects on other overextended banks. \"See also money multiplier.\"\n\nAs the early 20th century gold standard was undermined by inflation and the late 20th century fiat dollar hegemony evolved, and as banks proliferated and engaged in more complex transactions and were able to profit from dealings globally on a moment's notice, these practices became mandatory, if only to ensure that there was some limit on the ballooning of money supply. Such limits have become harder to enforce. The People's Bank of China retains (and uses) more powers over reserves because the yuan that it manages is a non-convertible currency.\n\nLoan activity by banks plays a fundamental role in determining the money supply. The central-bank money after aggregate settlement – \"final money\" – can take only one of two forms:\nThe currency component of the money supply is far smaller than the deposit component. Currency, bank reserves and institutional loan agreements together make up the monetary base, called M1, M2 and M3. The Federal Reserve Bank stopped publishing M3 and counting it as part of the money supply in 2006.\n\nTo influence the money supply, some central banks may require that some or all foreign exchange receipts (generally from exports) be exchanged for the local currency. The rate that is used to purchase local currency may be market-based or arbitrarily set by the bank. This tool is generally used in countries with non-convertible currencies or partially convertible currencies. The recipient of the local currency may be allowed to freely dispose of the funds, required to hold the funds with the central bank for some period of time, or allowed to use the funds subject to certain restrictions. In other cases, the ability to hold or use the foreign exchange may be otherwise limited.\n\nIn this method, money supply is increased by the central bank when it purchases the foreign currency by issuing (selling) the local currency. The central bank may subsequently reduce the money supply by various means, including selling bonds or foreign exchange interventions.\n\nIn some countries, central banks may have other tools that work indirectly to limit lending practices and otherwise restrict or regulate capital markets. For example, a central bank may regulate margin lending, whereby individuals or companies may borrow against pledged securities. The margin requirement establishes a minimum ratio of the value of the securities to the amount borrowed.\n\nCentral banks often have requirements for the quality of assets that may be held by financial institutions; these requirements may act as a limit on the amount of risk and leverage created by the financial system. These requirements may be direct, such as requiring certain assets to bear certain minimum credit ratings, or indirect, by the central bank lending to counterparties only when security of a certain quality is pledged as collateral.\n\nAlthough the perception by the public may be that the \"central bank\" controls some or all interest rates and currency rates, economic theory (and substantial empirical evidence) shows that it is impossible to do both at once in an open economy. Robert Mundell's \"impossible trinity\" is the most famous formulation of these limited powers, and postulates that it is impossible to target monetary policy (broadly, interest rates), the exchange rate (through a fixed rate) and maintain free capital movement. Since most Western economies are now considered \"open\" with free capital movement, this essentially means that central banks may target interest rates or exchange rates with credibility, but not both at once.\n\nIn the most famous case of policy failure, Black Wednesday, George Soros arbitraged the pound sterling's relationship to the ECU and (after making $2 billion himself and forcing the UK to spend over $8bn defending the pound) forced it to abandon its policy. Since then he has been a harsh critic of clumsy bank policies and argued that no one should be able to do what he did.\n\nThe most complex relationships are those between the yuan and the US dollar, and between the euro and its neighbors. US dollars were ubiquitous in Cuba's economy after its legalization in 1991, but were officially removed from circulation in 2004 and replaced by the convertible peso.\n\nIn some countries a central bank, through its subsidiaries, controls and monitors the banking sector. In other countries banking supervision is carried out by a government department such as the UK Treasury, or by an independent government agency, for example, UK's Financial Conduct Authority. It examines the banks' balance sheets and behaviour and policies toward consumers. Apart from refinancing, it also provides banks with services such as transfer of funds, bank notes and coins or foreign currency. Thus it is often described as the \"bank of banks\".\n\nMany countries will monitor and control the banking sector through several different agencies and for different purposes. the Bank regulation in the United States for example is highly fragmented with 3 federal agencies, the Federal Deposit Insurance Corporation, the Federal Reserve Board, or Office of the Comptroller of the Currency and numerous others on the state and the private level. There is usually significant cooperation between the agencies. For example, money center banks, deposit-taking institutions, and other types of financial institutions may be subject to different (and occasionally overlapping) regulation. Some types of banking regulation may be delegated to other levels of government, such as state or provincial governments.\n\nAny cartel of banks is particularly closely watched and controlled. Most countries control bank mergers and are wary of concentration in this industry due to the danger of groupthink and runaway lending bubbles based on a single point of failure, the credit culture of the few large banks.\n\nGovernments generally have some degree of influence over even \"independent\" central banks; the aim of independence is primarily to prevent short-term interference. For example, the Board of Governors of the U.S. Federal Reserve are nominated by the President of the U.S. and confirmed by the Senate, publishes verbatim transcripts, and balance sheets are audited by the Government Accountability Office.\n\nThe German Federal Bank was the first central bank to be given full independence, leading this form of central bank to be referred to as the Bundesbank model, as opposed, for instance, to the New Zealand model, which has a goal (i.e. inflation target) set by the government.\n\nCentral bank independence is usually guaranteed by legislation and the institutional framework governing the bank's relationship with elected officials, particularly the minister of finance. Central bank legislation will enshrine specific procedures for selecting and appointing the head of the central bank. Often the minister of finance will appoint the governor in consultation with the central bank's board and its incumbent governor. In addition, the legislation will specify banks governor's term of appointment. The most independent central banks enjoy a fixed non-renewable term for the governor in order to eliminate pressure on the governor to please the government in the hope of being re-appointed for a second term. Generally, independent central banks enjoy both goal and instrument independence.\n\nIn the 2000s there has been a trend towards increasing the independence of central banks as a way of improving long-term economic performance. While a large volume of economic research has been done to define the relationship between central bank independence and economic performance, the results are ambiguous.\n\nAdvocates of central bank independence argue that a central bank which is too susceptible to political direction or pressure may encourage economic cycles (\"boom and bust\"), as politicians may be tempted to boost economic activity in advance of an election, to the detriment of the long-term health of the economy and the country. In this context, independence is usually defined as the central bank's operational and management independence from the government.\n\nThe literature on central bank independence has defined a number of types of independence.\n\n\n\n\n\nIt is argued that an independent central bank can run a more credible monetary policy, making market expectations more responsive to signals from the central bank. Both the Bank of England (1997) and the European Central Bank have been made independent and follow a set of published inflation targets so that markets know what to expect. Even the People's Bank of China has been accorded great latitude, though in China the official role of the bank remains that of a national bank rather than a central bank, underlined by the official refusal to \"unpeg\" the yuan or to revalue it \"under pressure\". The People's Bank of China's independence can thus be read more as independence from the USA, which rules the financial markets, rather than from the Communist Party of China which rules the country. The fact that the Communist Party is not elected also relieves the pressure to please people, increasing its independence.\n\nInternational organizations such as the World Bank, the Bank for International Settlements (BIS) and the International Monetary Fund (IMF) strongly support central bank independence. \n\nAs of 2016, 75% of the world’s central-bank assets are controlled by four centers in China, the United States, Japan and the eurozone. The central banks of Brazil, Switzerland, Saudi Arabia, the U.K., India and Russia, each account for an average of 2.5 percent. The remaining 107 central banks hold less than 13 percent. According to data compiled by Bloomberg News, the top 10 largest central banks owned $21.4 trillion in assets, a 10 percent increase from 2015.\n\n\n\n", "id": "5666", "title": "Central bank"}
{"url": "https://en.wikipedia.org/wiki?curid=5667", "text": "Chlorine\n\nChlorine is a chemical element with symbol Cl and atomic number 17. The second-lightest of the halogens, it appears between fluorine and bromine in the periodic table and its properties are mostly intermediate between them. Chlorine is a yellow-green gas at room temperature. It is an extremely reactive element and a strong oxidising agent: among the elements, it has the highest electron affinity and the third-highest electronegativity, behind only oxygen and fluorine.\n\nThe most common compound of chlorine, sodium chloride (common salt), has been known since ancient times. Around 1630, chlorine gas was first synthesised in a chemical reaction, but not recognised as a fundamentally important substance. Carl Wilhelm Scheele wrote a description of chlorine gas in 1774, supposing it to be an oxide of a new element. In 1809, chemists suggested that the gas might be a pure element, and this was confirmed by Sir Humphry Davy in 1810, who named it from based on its colour.\n\nBecause of its great reactivity, all chlorine in the Earth's crust is in the form of ionic chloride compounds, which includes table salt. It is the second-most abundant halogen (after fluorine) and twenty-first most abundant chemical element in Earth's crust. These crustal deposits are nevertheless dwarfed by the huge reserves of chloride in seawater.\n\nElemental chlorine is commercially produced from brine by electrolysis. The high oxidising potential of elemental chlorine led to the development of commercial bleaches and disinfectants, and a reagent for many processes in the chemical industry. Chlorine is used in the manufacture of a wide range of consumer products, about two-thirds of them organic chemicals such as polyvinyl chloride, and many intermediates for the production of plastics and other end products which do not contain the element. As a common disinfectant, elemental chlorine and chlorine-generating compounds are used more directly in swimming pools to keep them clean and sanitary. Elemental chlorine at high concentrations is extremely dangerous and poisonous for all living organisms, and was used in World War I as the first gaseous chemical warfare agent.\n\nIn the form of chloride ions, chlorine is necessary to all known species of life. Other types of chlorine compounds are rare in living organisms, and artificially produced chlorinated organics range from inert to toxic. In the upper atmosphere, chlorine-containing organic molecules such as chlorofluorocarbons have been implicated in ozone depletion. Small quantities of elemental chlorine are generated by oxidation of chloride to hypochlorite in neutrophils as part of the immune response against bacteria.\n\nThe most common compound of chlorine, sodium chloride, has been known since ancient times; archaeologists have found evidence that rock salt was used as early as 3000 BC and brine as early as 6000 BC. Its importance in food was very well known in classical antiquity and was sometimes used as payment for services for Roman generals and military tribunes. Elemental chlorine was probably first isolated around 1200 with the discovery of \"aqua regia\" and its ability to dissolve gold, since chlorine gas is one of the products of this reaction: it was however not recognised as a new substance. Around 1630, chlorine was recognized as a gas by the Flemish chemist and physician Jan Baptist van Helmont.\n\nThe element was first studied in detail in 1774 by Swedish chemist Carl Wilhelm Scheele, and he is credited with the discovery. He called it \"\"dephlogisticated muriatic acid air\"\" since it is a gas (then called \"airs\") and it came from hydrochloric acid (then known as \"muriatic acid\"). He failed to establish chlorine as an element, mistakenly thinking that it was the oxide obtained from the hydrochloric acid (see phlogiston theory). He named the new element within this oxide as \"muriaticum\". Regardless of what he thought, Scheele did isolate chlorine by reacting MnO (as the mineral pyrolusite) with HCl:\n\nScheele observed several of the properties of chlorine: the bleaching effect on litmus, the deadly effect on insects, the yellow-green color, and the smell similar to aqua regia.\n\nCommon chemical theory at that time held that an acid is a compound that contains oxygen (remnants of this survive in the German and Dutch names of oxygen: \"sauerstoff\" or \"zuurstof\", both translating into English as \"acid substance\"), so a number of chemists, including Claude Berthollet, suggested that Scheele's \"dephlogisticated muriatic acid air\" must be a combination of oxygen and the yet undiscovered element, \"muriaticum\".\n\nIn 1809, Joseph Louis Gay-Lussac and Louis-Jacques Thénard tried to decompose \"dephlogisticated muriatic acid air\" by reacting it with charcoal to release the free element \"muriaticum\" (and carbon dioxide). They did not succeed and published a report in which they considered the possibility that \"dephlogisticated muriatic acid air\" is an element, but were not convinced.\n\nIn 1810, Sir Humphry Davy tried the same experiment again, and concluded that the substance was an element, and not a compound. He announced his results to the Royal Society on 15 November that year. The next year, he named this new element \"chlorine\", from the Greek word χλωρος (\"chlōros\"), meaning green-yellow. The name \"halogen\", meaning \"salt producer\", was originally used for chlorine in 1811 by Johann Salomo Christoph Schweigger. This term was later used as a generic term to describe all the elements in the chlorine family (fluorine, bromine, iodine), after a suggestion by Jöns Jakob Berzelius in 1842. In 1823, Michael Faraday liquefied chlorine for the first time, and demonstrated that what was then known as \"solid chlorine\" had a structure of chlorine hydrate (Cl·HO).\n\nChlorine gas was first used by French chemist Claude Berthollet to bleach textiles in 1785. Modern bleaches resulted from further work by Berthollet, who first produced sodium hypochlorite in 1789 in his laboratory in the town of Javel (now part of Paris, France), by passing chlorine gas through a solution of sodium carbonate. The resulting liquid, known as \"\"Eau de Javel\"\" (\"Javel water\"), was a weak solution of sodium hypochlorite. This process was not very efficient, and alternative production methods were sought. Scottish chemist and industrialist Charles Tennant first produced a solution of calcium hypochlorite (\"chlorinated lime\"), then solid calcium hypochlorite (bleaching powder). These compounds produced low levels of elemental chlorine and could be more efficiently transported than sodium hypochlorite, which remained as dilute solutions because when purified to eliminate water, it became a dangerously powerful and unstable oxidizer. Near the end of the nineteenth century, E. S. Smith patented a method of sodium hypochlorite production involving electrolysis of brine to produce sodium hydroxide and chlorine gas, which then mixed to form sodium hypochlorite. This is known as the chloralkali process, first introduced on an industrial scale in 1892, and now the source of most elemental chlorine and sodium hydroxide. In 1884 Chemischen Fabrik Griesheim of Germany developed another chloralkali process which entered commercial production in 1888.\n\nElemental chlorine solutions dissolved in chemically basic water (sodium and calcium hypochlorite) were first used as anti-putrefaction agents and disinfectants in the 1820s, in France, long before the establishment of the germ theory of disease. This practice was pioneered by Antoine-Germain Labarraque, who adapted Berthollet's \"Javel water\" bleach and other chlorine preparations (for a more complete history, see below). Elemental chlorine has since served a continuous function in topical antisepsis (wound irrigation solutions and the like) and public sanitation, particularly in swimming and drinking water.\n\nChlorine gas was first used as a weapon on April 22, 1915, at Ypres by the German Army. The effect on the allies was devastating because the existing gas masks were difficult to deploy and had not been broadly distributed.\n\nChlorine is the second halogen, being a nonmetal in group 17 of the periodic table. Its properties are thus similar to fluorine, bromine, and iodine, and are largely intermediate between those of the first two. Chlorine has the electron configuration [Ne]3s3p, with the seven electrons in the third and outermost shell acting as its valence electrons. Like all halogens, it is thus one electron short of a full octet, and is hence a strong oxidising agent, reacting with many elements in order to complete its outer shell. Corresponding to periodic trends, it is intermediate in electronegativity between fluorine and bromine (F: 3.98, Cl: 3.16, Br: 2.96, I: 2.66), and is less reactive than fluorine and more reactive than bromine. It is also a weaker oxidising agent than fluorine, but a stronger one than bromine. Conversely, the chloride ion is a weaker reducing agent than bromide, but a stronger one than fluoride. It is intermediate in atomic radius between fluorine and bromine, and this leads to many of its atomic properties similarly continuing the trend from iodine to bromine upward, such as first ionisation energy, electron affinity, enthalpy of dissociation of the X molecule (X = Cl, Br, I), ionic radius, and X–X bond length. (Fluorine is anomalous due to its small size.)\n\nAll four stable halogens experience intermolecular van der Waals forces of attraction, and their strength increases together with the number of electrons among all homonuclear diatomic halogen molecules. Thus, the melting and boiling points of chlorine are intermediate between those of fluorine and bromine: chlorine melts at −101.0 °C and boils at −34.0 °C. As a result of the increasing molecular weight of the halogens down the group, the density and heats of fusion and vaporisation of chlorine are again intermediate between those of bromine and fluorine, although all their heats of vaporisation are fairly low (leading to high volatility) thanks to their diatomic molecular structure. The halogens darken in colour as the group is descended: thus, while fluorine is a pale yellow gas, chlorine is distinctly yellow-green. This trend occurs because the wavelengths of visible light absorbed by the halogens increase down the group. Specifically, the colour of a halogen, such as chlorine, results from the electron transition between the highest occupied antibonding \"π\" molecular orbital and the lowest vacant antibonding \"σ\" molecular orbital. The colour fades at low temperatures, so that solid chlorine at −195 °C is almost colourless.\n\nLike solid bromine and iodine, solid chlorine crystallises in the orthorhombic crystal system, in a layered lattice of Cl molecules. The Cl–Cl distance is 198 pm (close to the gaseous Cl–Cl distance of 199 pm) and the Cl···Cl distance between molecules is 332 pm within a layer and 382 pm between layers (compare the van der Waals radius of chlorine, 180 pm). This structure means that chlorine is a very poor conductor of electricity, and indeed its conductivity is so low as to be practically unmeasurable.\n\nChlorine has two stable isotopes, Cl and Cl. These are its only two natural isotopes occurring in quantity, with Cl making up 76% of natural chlorine and Cl making up the remaining 24%. Both are synthesised in stars in the oxygen-burning and silicon-burning processes. Both have nuclear spin 3/2+ and thus may be used for nuclear magnetic resonance, although the spin magnitude being greater than 1/2 results in non-spherical nuclear charge distribution and thus resonance broadening as a result of a nonzero nuclear quadrupole moment and resultant quadrupolar relaxation. The other chlorine isotopes are all radioactive, with half-lives too short to occur in nature primordially. Of these, the most commonly used in the laboratory are Cl (t = 3.0×10 y) and Cl (t = 37.2 min), which may be produced from the neutron activation of natural chlorine.\n\nThe most stable chlorine radioisotope is Cl. The primary decay mode of isotopes lighter than Cl is electron capture to isotopes of sulfur; that of isotopes heavier than Cl is beta decay to isotopes of argon; and Cl may decay by either mode to stable S or Ar. Cl occurs in trace quantities in nature as a cosmogenic nuclide in a ratio of about (7–10) × 10 to 1 with stable chlorine isotopes: it is produced in the atmosphere by spallation of Ar by interactions with cosmic ray protons. In the top meter of the lithosphere, Cl is generated primarily by thermal neutron activation of Cl and spallation of K and Ca. In the subsurface environment, muon capture by Ca becomes more important as a way to generate Cl.\n\nChlorine is intermediate in reactivity between fluorine and bromine, and is one of the most reactive elements. Chlorine is a weaker oxidising agent than fluorine but a stronger one than bromine or iodine. This can be seen from the standard electrode potentials of the X/X couples (F, +2.866 V; Cl, +1.395 V; Br, +1.087 V; I, +0.615 V; At, approximately +0.3 V). However, this trend is not shown in the bond energies because fluorine is singular due to its small size, low polarisability, and lack of low-lying d-orbitals available for bonding (which chlorine has). As another difference, chlorine has a significant chemistry in positive oxidation states while fluorine does not. Chlorination often leads to higher oxidation states than bromination or iodination but lower oxidation states to fluorination. Chlorine tends to react with compounds including M–M, M–H, or M–C bonds to form M–Cl bonds.\n\nGiven that E°(O/HO) = +1.229 V, which is less than +1.395 V, it would be expected that chlorine should be able to oxidise water to oxygen and hydrochloric acid. However, the kinetics of this reaction are unfavorable, and there is also a bubble overpotential effect to consider, so that electrolysis of aqueous chloride solutions evolves chlorine gas and not oxygen gas.\n\nThe simplest chlorine compound is hydrogen chloride, HCl, a major chemical in industry as well as in the laboratory, both as a gas and dissolved in water as hydrochloric acid. It is often produced by burning hydrogen gas in chlorine gas, or as a byproduct of chlorinating hydrocarbons. Another approach is to treat sodium chloride with concentrated sulfuric acid to produce hydrochloric acid, also known as the \"salt-cake\" process:\nIn the laboratory, hydrogen chloride gas may be made by drying the acid with concentrated sulfuric acid. Deuterium chloride, DCl, may be produced by reacting benzoyl chloride with heavy water (DO).\n\nAt room temperature, hydrogen chloride is a colourless gas, like all the hydrogen halides apart from hydrogen fluoride, since hydrogen cannot form strong hydrogen bonds to the larger electronegative chlorine atom; however, weak hydrogen bonding is present in solid crystalline hydrogen chloride at low temperatures, similar to the hydrogen fluoride structure, before disorder begins to prevail as the temperature is raised. Hydrochloric acid is a strong acid (p\"K\" = −7) because the hydrogen bonds to bromine are too weak to inhibit dissociation. The HCl/HO system has many hydrates HCl·\"n\"HO for \"n\" = 1, 2, 3, 4, and 6. Beyond a 1:1 mixture of HCl and HO, the system separates completely into two separate liquid phases. Hydrochloric acid forms an azeotrope with boiling point 108.58 °C at 20.22 g HCl per 100 g solution; thus hydrochloric acid cannot be concentrated beyond this point by distillation.\n\nUnlike hydrogen fluoride, anhydrous liquid hydrogen chloride is difficult to work with as a solvent, because its boiling point is low, it has a small liquid range, its dielectric constant is low and it does not dissociate appreciably into HCl and ions – the latter, in any case, are much less stable than the bifluoride ions () due to the very weak hydrogen bonding between hydrogen and chlorine, though its salts with very large and weakly polarising cations such as Cs and (R = Me, Et, Bu) may still be isolated. Anhydrous hydrogen chloride is a poor solvent, only able to dissolve small molecular compounds such as nitrosyl chloride and phenol, or salts with very low lattice energies such as tetraalkylammonium halides. It readily protonates electrophiles containing lone-pairs or π bonds. Solvolysis, ligand replacement reactions, and oxidations are well-characterised in hydrogen chloride solution:\n\nNearly all elements in the periodic table form binary chlorides. The exceptions are decidedly in the minority and stem in each case from one of three causes: extreme inertness and reluctance to participate in chemical reactions (the noble gases, with the exception of xenon in XeCl and XeCl); extreme nuclear instability hampering chemical investigation before decay and transmutation (many of the heaviest elements beyond bismuth); and having an electronegativity higher than chlorine's (oxygen and fluorine) so that the resultant binary compounds are formally not chlorides but rather oxides or fluorides of chlorine.\n\nChlorination of metals with Cl usually leads to a higher oxidation state than bromination with Br when multiple oxidation states are available, such as in MoCl and MoBr. Chlorides can be made by reaction of an element or its oxide, hydroxide, or carbonate with hydrochloric acid, and then dehydrated by mildly high temperatures combined with either low pressure or anhydrous hydrogen chloride gas. These methods work best when the chloride product is stable to hydrolysis; otherwise, the possibilities include high-temperature oxidative chlorination of the element with chlorine or hydrogen chloride, high-temperature chlorination of a metal oxide or other halide by chlorine, a volatile metal chloride, carbon tetrachloride, or an organic chloride. For instance, zirconium dioxide reacts with chlorine at standard conditions to produce zirconium tetrachloride, and uranium trioxide reacts with hexachloropropene when heated under reflux to give uranium tetrachloride. The second example also involves a reduction in oxidation state, which can also be achieved by reducing a higher chloride using hydrogen or a metal as a reducing agent. This may also be achieved by thermal decomposition or disproportionation as follows:\n\nMost of the chlorides of the pre-transition metals (groups 1, 2, and 3, along with the lanthanides and actinides in the +2 and +3 oxidation states) are mostly ionic, while nonmetals tend to form covalent molecular chlorides, as do metals in high oxidation states from +3 and above. Silver chloride is very insoluble in water and is thus often used as a qualitative test for chlorine.\n\nAlthough dichlorine is a strong oxidising agent with a high first ionisation energy, it may be oxidised under extreme conditions to form the cation. This is very unstable and has only been characterised by its electronic band spectrum when produced in a low-pressure discharge tube. The yellow cation is more stable and may be produced as follows:\nThis reaction is conducted in the oxidising solvent arsenic pentafluoride. The trichloride anion, , has also been characterised; it is analogous to triiodide.\n\nThe three fluorides of chlorine form a subset of the interhalogen compounds, all of which are diamagnetic. Some cationic and anionic derivatives are known, such as , , , and ClF. Some pseudohalides of chlorine are also known, such as cyanogen chloride (ClCN, linear), chlorine cyanate (ClNCO), chlorine thiocyanate (ClSCN, unlike its oxygen counterpart), and chlorine azide (ClN).\n\nChlorine monofluoride (ClF) is extremely thermally stable, and is sold commercially in 500-gram steel lecture bottles. It is a colourless gas that melts at −155.6 °C and boils at −100.1 °C. It may be produced by the direction of its elements at 225 °C, though it must then be separated and purified from chlorine trifluoride and its reactants. Its properties are mostly intermediate between those of chlorine and fluorine. It will react with many metals and nonmetals from room temperature and above, fluorinating them and liberating chlorine. It will also act as a chlorofluorinating agent, adding chlorine and fluorine across a multiple bond or by oxidation: for example, it will attack carbon monoxide to form carbonyl chlorofluoride, COFCl. It will react analogously with hexafluoroacetone, (CF)CO, with a potassium fluoride catalyst to produce heptafluoroisopropyl hypochlorite, (CF)CFOCl; with nitriles RCN to produce RCFNCl; and with the sulfur oxides SO and SO to produce ClOSOF and ClSOF respectively. It will also react exothermically violent with compounds containing –OH and –NH groups, such as water:\n\nChlorine trifluoride (ClF) is a volatile colourless molecular liquid which melts at −76.3 °C and boils at 11.8 °C. It may be formed by directly fluorinating gaseous chlorine or chlorine monofluoride at 200–300 °C. It is one of the most reactive known chemical compounds, reacting with many substances which in ordinary circumstances would be considered chemically inert, such as asbestos, concrete, and sand. It explodes on contact with water and most organic substances. The list of elements it sets on fire is diverse, containing hydrogen, potassium, phosphorus, arsenic, antimony, sulfur, selenium, tellurium, bromine, iodine, and powdered molybdenum, tungsten, rhodium, iridium, and iron. An impermeable fluoride layer is formed by sodium, magnesium, aluminium, zinc, tin, and silver, which may be removed by heating. When heated, even such noble metals as palladium, platinum, and gold are attacked and even the noble gases xenon and radon do not escape fluorination. Nickel containers are usually used due to that metal's great resistance to attack by chlorine trifluoride, stemming from the formation of an unreactive nickel fluoride layer. Its reaction with hydrazine to form hydrogen fluoride, nitrogen, and chlorine gases was used in experimental rocket motors, but has problems largely stemming from its extreme hypergolicity resulting in ignition without any measurable delay. For these reasons, it was used in bomb attacks during the Second World War by the Nazis. Today, it is mostly used in nuclear fuel processing, to oxidise uranium to uranium hexafluoride for its enriching and to separate it from plutonium. It can act as a fluoride ion donor or acceptor (Lewis base or acid), although it does not dissociate appreciably into and ions.\n\nChlorine pentafluoride (ClF) is made on a large scale by direct fluorination of chlorine with excess fluorine gas at 350 °C and 250 atm, and on a small scale by reacting metal chlorides with fluorine gas at 100–300 °C. It melts at −103 °C and boils at −13.1 °C. It is a very strong fluorinating agent, although it is still not as effective as chlorine trifluoride. Only a few specific stoichiometric reactions have been characterised. Arsenic pentafluoride and antimony pentafluoride form ionic adducts of the form [ClF][MF] (M = As, Sb) and water reacts vigorously as follows:\n\nThe product, chloryl fluoride, is one of the five known chlorine oxide fluorides. These range from the thermally unstable FClO to the chemically unreactive perchloryl fluoride (FClO), the other three being FClO, FClO, and FClO. All five behave similarly to the chlorine fluorides, both structurally and chemically, and may act as Lewis acids or bases by gaining or losing fluoride ions respectively or as very strong oxidising and fluorinating agents.\n\nThe chlorine oxides are well-studied in spite of their instability (all of them are endothermic compounds). They are important because they are produced when chlorofluorocarbons undergo photolysis in the upper atmosphere and cause the destruction of the ozone layer. None of them can be made from directly reacting the elements.\n\nDichlorine monoxide (ClO) is a brownish-yellow gas (red-brown when solid or liquid) which may be obtained by reacting chlorine gas with yellow mercury(II) oxide. It is very soluble in water, in which it is in equilibrium with hypochlorous acid (HOCl), which it is the anhydride of. It is thus an effective bleach and is mostly used to make hypochlorites. It explodes on heating or sparking or in the presence of ammonia gas.\n\nChlorine dioxide (ClO) was the first chlorine oxide to be discovered in 1811 by Humphry Davy. It is a yellow paramagnetic gas (deep-red as a solid or liquid), as expected from its having an odd number of electrons: it is stable towards dimerisation due to the delocalisation of the unpaired electron. It explodes above −40 °C as a liquid and under pressure as a gas and therefore must be made at low concentrations for wood-pulp bleaching and water treatment. It is usually prepared by reducing a chlorate as follows:\nIts production is thus intimately linked to the redox reactions of the chlorine oxoacids. It is a strong oxidising agent, reacting with sulfur, phosphorus, phosphorus halides, and potassium borohydride. It dissolves exothermically in water to form dark-green solutions that very slowly decompose in the dark. Crystalline clathrate hydrates ClO·\"n\"HO (\"n\" ≈ 6–10) separate out at low temperatures. However, in the presence of light, these solutions rapidly photodecompose to form a mixture of chloric and hydrochloric acids. Photolysis of individual ClO molecules result in the radicals ClO and ClOO, while at room temperature mostly chlorine, oxygen, and some ClO and ClO are produced. ClO is also produced when photolysing the solid at −78 °C: it is a dark brown solid that explodes below 0 °C. The ClO radical leads to the depletion of atmospheric ozone and is thus environmentally important as follows:\n\nChlorine perchlorate (ClOClO) is a pale yellow liquid that is less stable than ClO and decomposes at room temperature to form chlorine, oxygen, and dichlorine hexoxide (ClO). Chlorine perchlorate may also be considered a chlorine derivative of perchloric acid (HOClO), similar to the thermally unstable chlorine derivatives of other oxoacids: examples include chlorine nitrate (ClONO, vigorously reactive and explosive), and chlorine fluorosulfate (ClOSOF, more stable but still moisture-sensitive and highly reactive). Dichlorine hexoxide is a dark-red liquid that freezes to form a solid which turns yellow at −180 °C: it is usually made by reaction of chlorine dioxide with oxygen. Despite attempts to rationalise it as the dimer of ClO, it reacts more as though it were chloryl perchlorate, [ClO][ClO], which has been confirmed to be the correct structure of the solid. It hydrolyses in water to give a mixture of chloric and perchloric acids: the analogous reaction with anhydrous hydrogen fluoride does not proceed to completion.\n\nDichlorine heptoxide (ClO) is the anhydride of perchloric acid (HClO) and can readily be obtained from it by dehydrating it with phosphoric acid at −10 °C and then distilling the product at −35 °C and 1 mmHg. It is a shock-sensitive, colourless oily liquid. It is the least reactive of the chlorine oxides, being the only one to not set organic materials on fire at room temperature. It may be dissolved in water to regenerate perchloric acid or in aqueous alkalis to regenerate perchlorates. However, it thermally decomposes explosively by breaking one of the central Cl–O bonds, producing the radicals ClO and ClO which immediately decompose to the elements through intermediate oxides.\n\nChlorine forms four oxoacids: hypochlorous acid (HOCl), chlorous acid (HOClO), chloric acid (HOClO), and perchloric acid (HOClO). As can be seen from the redox potentials given in the table to the right, chlorine is much more stable towards disproportionation in acidic solutions than in alkaline solutions:\nThe hypochlorite ions also disproportionate further to produce chloride and chlorate (3 ClO 2 Cl + ) but this reaction is quite slow at temperatures below 70 °C. The chlorate ions may themselves disproportionate to form chloride and perchlorate (4 Cl + 3 ) but this is still very slow even at 100 °C. The rates of reaction for the chlorine oxyanions increases as the oxidation state of chlorine decreases. The strengths of the chlorine oxyacids increase very quickly as the oxidation state of chlorine increases due to the increasing delocalisation of charge over more and more oxygen atoms in their conjugate bases.\n\nMost of the chlorine oxoacids may be produced by exploiting these disproportionation reactions. Hypochlorous acid (HOCl) is highly reactive and quite unstable; its salts are mostly used for their bleaching and sterilising abilities. They are very strong oxidising agents, transferring an oxygen atom to most inorganic species. Chlorous acid (HOClO) is even more unstable and cannot be isolated or concentrated without decomposition: it is known from the decomposition of aqueous chlorine dioxide. However, sodium chlorite is a stable salt and is useful for bleaching and stripping textiles, as an oxidising agent, and as a source of chlorine dioxide. Chloric acid (HOClO) is a strong acid that is quite stable in cold water up to 30% concentration, but on warming gives chlorine and chlorine dioxide. Evaporation under reduced pressure allows it to be concentrated further to about 40%, but then it decomposes to perchloric acid, chlorine, oxygen, water, and chlorine dioxide. Its most important salt is sodium chlorate, mostly used to make chlorine dioxide to bleach paper pulp. The decomposition of chlorate to chloride and oxygen is a common way to produce oxygen in the laboratory on a small scale. Chloride and chlorate may comproportionate to form chlorine as follows:\n\nPerchlorates and perchloric acid (HOClO) are the most stable oxo-compounds of chlorine, in keeping with the fact that chlorine compounds are most stable when the chlorine atom is in its lowest (−1) or highest (+7) possible oxidation states. Perchloric acid and aqueous perchlorates are vigorous and sometimes violent oxidising agents when heated, in stark contrast to their mostly inactive nature at room temperature due to the high activation energies for these reactions for kinetic reasons. Perchlorates are made by electrolytically oxidising sodium chlorate, and perchloric acid is made by reacting anhydrous sodium perchlorate or barium perchlorate with concentrated hydrochloric acid, filtering away the chloride precipitated and distilling the filtrate to concentrate it. Anhydrous perchloric acid is a colourless mobile liquid that is sensitive to shock that explodes on contact with most organic compounds, sets hydrogen iodide and thionyl chloride on fire and even oxidises silver and gold. Although it is a weak ligand, weaker than water, a few compounds involving coordinated are known.\n\nLike the other carbon–halogen bonds, the C–Cl bond is a common functional group that forms part of core organic chemistry. Formally, compounds with this functional group may be considered organic derivatives of the chloride anion. Due to the difference of electronegativity between chlorine (3.16) and carbon (2.55), the carbon in a C–Cl bond is electron-deficient and thus electrophilic. Chlorination modifies the physical properties of hydrocarbons in several ways: chlorocarbons are typically denser than water due to the higher atomic weight of chlorine versus hydrogen, and aliphatic organochlorides are alkylating agents because chloride is a leaving group.\n\nAlkanes and aryl alkanes may be chlorinated under free radical conditions, with UV light. However, the extent of chlorination is difficult to control: the reaction is not regioselective and often results in a mixture of various isomers with different degrees of chlorination, though this may be permissible if the products are easily separated. Aryl chlorides may be prepared by the Friedel-Crafts halogenation, using chlorine and a Lewis acid catalyst. The haloform reaction, using chlorine and sodium hydroxide, is also able to generate alkyl halides from methyl ketones, and related compounds. Chlorine adds to the multiple bonds on alkenes and alkynes as well, giving di- or tetra-chloro compounds. However, due to the expense and reactivity of chlorine, organochlorine compounds are more commonly produced by using hydrogen chloride, or with chlorinating agents such as phosphorus pentachloride (PCl) or thionyl chloride (SOCl). The last is very convenient in the laboratory because all side products are gaseous and do not have to be distilled out.\n\nMany organochlorine compounds have been isolated from natural sources ranging from bacteria to humans. Chlorinated organic compounds are found in nearly every class of biomolecules including alkaloids, terpenes, amino acids, flavonoids, steroids, and fatty acids. Organochlorides, including dioxins, are produced in the high temperature environment of forest fires, and dioxins have been found in the preserved ashes of lightning-ignited fires that predate synthetic dioxins. In addition, a variety of simple chlorinated hydrocarbons including dichloromethane, chloroform, and carbon tetrachloride have been isolated from marine algae. A majority of the chloromethane in the environment is produced naturally by biological decomposition, forest fires, and volcanoes.\n\nSome types of organochlorides, though not all, have significant toxicity to plants or animals, including humans. Dioxins, produced when organic matter is burned in the presence of chlorine, and some insecticides, such as DDT, are persistent organic pollutants which pose dangers when they are released into the environment. For example, DDT, which was widely used to control insects in the mid 20th century, also accumulates in food chains, and causes reproductive problems (e.g., eggshell thinning) in certain bird species. Due to the ready homolytic fission of the C–Cl bond to create chlorine radicals in the upper atmosphere, chlorofluorocarbons have been phased out due to the harm they do to the ozone layer.\n\nChlorine is too reactive to occur as the free element in nature but is very abundant in the form of its chloride salts. It is the twentieth most abundant element in Earth's crust and makes up 126 parts per million of it, through the large deposits of chloride minerals, especially sodium chloride, that have been evaporated from water bodies. All of these pale in comparison to the reserves of chloride ions in seawater: smaller amounts at higher concentrations occur in some inland seas and underground brine wells, such as the Great Salt Lake in Utah and the Dead Sea in Israel.\n\nSmall batches of chlorine gas are prepared in the laboratory by combining hydrochloric acid and manganese dioxide, but the need rarely arises due to its ready availability. In industry, elemental chlorine is usually produced by the electrolysis of sodium chloride dissolved in water. This method, the chloralkali process industrialized in 1892, now provides most industrial chlorine gas. Along with chlorine, the method yields hydrogen gas and sodium hydroxide, which is the most valuable product. The process proceeds according to the following chemical equation:\n\nThe electrolysis of chloride solutions all proceed according to the following equations:\n\nIn diaphragm cell electrolysis, an asbestos (or polymer-fiber) diaphragm separates a cathode and an anode, preventing the chlorine forming at the anode from re-mixing with the sodium hydroxide and the hydrogen formed at the cathode. The salt solution (brine) is continuously fed to the anode compartment and flows through the diaphragm to the cathode compartment, where the caustic alkali is produced and the brine is partially depleted. Diaphragm methods produce dilute and slightly impure alkali, but they are not burdened with the problem of mercury disposal and they are more energy efficient.\n\nMembrane cell electrolysis employs permeable membrane as an ion exchanger. Saturated sodium (or potassium) chloride solution is passed through the anode compartment, leaving at a lower concentration. This method also produces very pure sodium (or potassium) hydroxide but has the disadvantage of requiring very pure brine at high concentrations.\n\nIn the Deacon process, hydrogen chloride recovered from the production of organochlorine compounds is recovered as chlorine. The process relies on oxidation using oxygen:\n\nThe reaction requires a catalyst. As introduced by Deacon, early catalysts were based on copper. Commercial processes, such as the Mitsui MT-Chlorine Process, have switched to chromium and ruthenium-based catalysts. The chlorine produced is available in cylinders from sizes ranging from 450 g to 70 kg, as well as drums (865 kg), tank wagons (15 tonnes on roads; 27–90 tonnes by rail), and barges (600–1200 tonnes).\n\nSodium chloride is by a huge margin the most common chlorine compound, and it is the main source of chlorine and hydrochloric acid for the enormous chlorine-chemicals industry today. About 15000 chlorine-containing compounds are commercially traded, including such diverse compounds as chlorinated methanes and ethanes, vinyl chloride and its polymer polyvinyl chloride (PVC), aluminium trichloride for catalysis, the chlorides of magnesium, titanium, zirconium, and hafnium which are the precursors for producing the pure elements, and so on.\n\nQuantitatively, of all elemental chlorine produced, about 63% is used in the manufacture of organic compounds, and 18% in the manufacture of inorganic chlorine compounds. About 15,000 chlorine compounds are used commercially. The remaining 19% of chlorine produced is used for bleaches and disinfection products. The most significant of organic compounds in terms of production volume are 1,2-dichloroethane and vinyl chloride, intermediates in the production of PVC. Other particularly important organochlorines are methyl chloride, methylene chloride, chloroform, vinylidene chloride, trichloroethylene, perchloroethylene, allyl chloride, epichlorohydrin, chlorobenzene, dichlorobenzenes, and trichlorobenzenes. The major inorganic compounds include HCl, ClO, HOCl, NaClO, chlorinated isocyanurates, AlCl, SiCl, SnCl, PCl, PCl, POCl, AsCl, SbCl, SbCl, BiCl, SCl, SCl, SOCI, ClF, ICl, ICl, TiCl, TiCl, MoCl, FeCl, ZnCl, and so on.\n\nIn France (as elsewhere), animal intestines were processed to make musical instrument strings, Goldbeater's skin and other products. This was done in \"gut factories\" (\"boyauderies\"), and it was an odiferous and unhealthy process. In or about 1820, the offered a prize for the discovery of a method, chemical or mechanical, for separating the peritoneal membrane of animal intestines without putrefaction. The prize was won by Antoine-Germain Labarraque, a 44-year-old French chemist and pharmacist who had discovered that Berthollet's chlorinated bleaching solutions (\"\"Eau de Javel\"\") not only destroyed the smell of putrefaction of animal tissue decomposition, but also actually retarded the decomposition.\n\nLabarraque's research resulted in the use of chlorides and hypochlorites of lime (calcium hypochlorite) and of sodium (sodium hypochlorite) in the \"boyauderies.\" The same chemicals were found to be useful in the routine disinfection and deodorization of latrines, sewers, markets, abattoirs, anatomical theatres, and morgues. They were successful in hospitals, lazarets, prisons, infirmaries (both on land and at sea), magnaneries, stables, cattle-sheds, etc.; and they were beneficial during exhumations, embalming, outbreaks of epidemic disease, fever, and blackleg in cattle.\n\nLabarraque's chlorinated lime and soda solutions have been advocated since 1828 to prevent infection (called \"contagious infection\", presumed to be transmitted by \"miasmas\"), and to treat putrefaction of existing wounds, including septic wounds. In his 1828 work, Labarraque recommended that doctors breathe chlorine, wash their hands in chlorinated lime, and even sprinkle chlorinated lime about the patients' beds in cases of \"contagious infection\". In 1828, the contagion of infections was well known, even though the agency of the microbe was not discovered until more than half a century later.\n\nDuring the Paris cholera outbreak of 1832, large quantities of so-called \"chloride of lime\" were used to disinfect the capital. This was not simply modern calcium chloride, but chlorine gas dissolved in lime-water (dilute calcium hydroxide) to form calcium hypochlorite (chlorinated lime). Labarraque's discovery helped to remove the terrible stench of decay from hospitals and dissecting rooms, and by doing so, effectively deodorised the Latin Quarter of Paris. These \"putrid miasmas\" were thought by many to cause the spread of \"contagion\" and \"infection\" – both words used before the germ theory of infection. Chloride of lime was used for destroying odors and \"putrid matter\". One source claims chloride of lime was used by Dr. John Snow to disinfect water from the cholera-contaminated well that was feeding the Broad Street pump in 1854 London, though three other reputable sources that describe that famous cholera epidemic do not mention the incident. One reference makes it clear that chloride of lime was used to disinfect the offal and filth in the streets surrounding the Broad Street pump—a common practice in mid-nineteenth century England.\n\nPerhaps the most famous application of Labarraque's chlorine and chemical base solutions was in 1847, when Ignaz Semmelweis used chlorine-water (chlorine dissolved in pure water, which was cheaper chlorinated lime solutions) to disinfect the hands of Austrian doctors, which Semmelweis noticed still carried the stench of decomposition from the dissection rooms to the patient examination rooms. Long before the germ theory of disease, Semmelweis theorized that \"cadaveric particles\" were transmitting decay from fresh medical cadavers to living patients, and he used the well-known \"Labarraque's solutions\" as the only known method to remove the smell of decay and tissue decomposition (which he found that soap did not). The solutions proved to be far more effective antiseptics than soap (Semmelweis was also aware of their greater efficacy, but not the reason), and this resulted in Semmelweis's celebrated success in stopping the transmission of childbed fever (\"puerperal fever\") in the maternity wards of Vienna General Hospital in Austria in 1847.\n\nMuch later, during World War I in 1916, a standardized and diluted modification of Labarraque's solution containing hypochlorite (0.5%) and boric acid as an acidic stabilizer, was developed by Henry Drysdale Dakin (who gave full credit to Labarraque's prior work in this area). Called Dakin's solution, the method of wound irrigation with chlorinated solutions allowed antiseptic treatment of a wide variety of open wounds, long before the modern antibiotic era. A modified version of this solution continues to be employed in wound irrigation in modern times, where it remains effective against bacteria that are resistant to multiple antibiotics (see Century Pharmaceuticals).\n\nBy 1918, the US Department of Treasury called for all drinking water to be disinfected with chlorine. Chlorine is presently an important chemical for water purification (such as in water treatment plants), in disinfectants, and in bleach. As a disinfectant in water, chlorine is more than three times as effective against \"Escherichia coli\" as bromine, and more than six times as effective as iodine.\n\nChlorine is usually used (in the form of hypochlorous acid) to kill bacteria and other microbes in drinking water supplies and public swimming pools. In most private swimming pools, chlorine itself is not used, but rather sodium hypochlorite, formed from chlorine and sodium hydroxide, or solid tablets of chlorinated isocyanurates. The drawback of using chlorine in swimming pools is that the chlorine reacts with the proteins in human hair and skin (see Hypochlorous acid), and becomes chemically bonded. Even small water supplies are now routinely chlorinated.\n\nIt is often impractical to store and use poisonous chlorine gas for water treatment, so alternative methods of adding chlorine are used. These include hypochlorite solutions, which gradually release chlorine into the water, and compounds like sodium dichloro-s-triazinetrione (dihydrate or anhydrous), sometimes referred to as \"dichlor\", and trichloro-s-triazinetrione, sometimes referred to as \"trichlor\". These compounds are stable while solid and may be used in powdered, granular, or tablet form. When added in small amounts to pool water or industrial water systems, the chlorine atoms hydrolyze from the rest of the molecule forming hypochlorous acid (HOCl), which acts as a general biocide, killing germs, micro-organisms, algae, and so on.\n\nChlorine gas, also known as bertholite, was first used as a weapon in World War I by Germany on April 22, 1915 in the Second Battle of Ypres. As described by the soldiers, it had the distinctive smell of a mixture of pepper and pineapple. It also tasted metallic and stung the back of the throat and chest. Chlorine reacts with water in the mucosa of the lungs to form hydrochloric acid, destructive to living tissue and potentially lethal. Human respiratory systems can be protected from chlorine gas by gas masks with activated charcoal or other filters, which makes chlorine gas much less lethal than other chemical weapons. It was pioneered by a German scientist later to be a Nobel laureate, Fritz Haber of the Kaiser Wilhelm Institute in Berlin, in collaboration with the German chemical conglomerate IG Farben, which developed methods for discharging chlorine gas against an entrenched enemy. After its first use, both sides in the conflict used chlorine as a chemical weapon, but it was soon replaced by the more deadly phosgene and mustard gas.\n\nChlorine gas was also used during the Iraq War in Anbar Province in 2007, with insurgents packing truck bombs with mortar shells and chlorine tanks. The attacks killed two people from the explosives and sickened more than 350. Most of the deaths were caused by the force of the explosions rather than the effects of chlorine since the toxic gas is readily dispersed and diluted in the atmosphere by the blast. In some bombings, over a hundred civilians were hospitalized due to breathing difficulties. The Iraqi authorities tightened security for elemental chlorine, which is essential for providing safe drinking water to the population.\n\nOn 24 October 2014, it was reported that the Islamic State of Iraq and the Levant had used chlorine gas in the town of Duluiyah, Iraq. Laboratory analysis of clothing and soil samples confirmed the use of chlorine gas against Kurdish Peshmerga Forces in a vehicle-borne improvised explosive device attack on 23 January 2015 at the Highway 47 Kiske Junction near Mosul.\n\nThe chloride anion is an essential nutrient for metabolism. Chlorine is needed for the production of hydrochloric acid in the stomach and in cellular pump functions. The main dietary source is table salt, or sodium chloride. Overly low or high concentrations of chloride in the blood are examples of electrolyte disturbances. Hypochloremia (having too little chloride) rarely occurs in the absence of other abnormalities. Its sometimes associated with hypoventilation. It can be associated with chronic respiratory acidosis. Hyperchloremia (having too much chloride) usually does not produce symptoms. When symptoms do occur, they tend to resemble those of hypernatremia (having too much sodium). Reduction in blood chloride leads to cerebral dehydration; symptoms are most often caused by rapid rehydration which results in cerebral edema. Hyperchloremia can affect oxygen transport.\n\nChlorine is a toxic gas that attacks the respiratory system, eyes, and skin. Because it is denser than air, it tends to accumulate at the bottom of poorly ventilated spaces. Chlorine gas is a strong oxidizer, which may react with flammable materials.\n\nChlorine is detectable with measuring devices in concentrations as low as 0.2 parts per million (ppm), and by smell at 3 ppm. Coughing and vomiting may occur at 30 ppm and lung damage at 60 ppm. About 1000 ppm can be fatal after a few deep breaths of the gas. The IDLH (immediately dangerous to life and health) concentration is 10 ppm. Breathing lower concentrations can aggravate the respiratory system and exposure to the gas can irritate the eyes. The toxicity of chlorine comes from its oxidizing power. When chlorine is inhaled at concentrations greater than 30 ppm, it reacts with water and cellular fluid, producing hydrochloric acid (HCl) and hypochlorous acid (HClO).\n\nWhen used at specified levels for water disinfection, the reaction of chlorine with water is not a major concern for human health. Other materials present in the water may generate disinfection by-products that are associated with negative effects on human health.\n\nIn the United States, the Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for elemental chlorine at 1 ppm, or 3 mg/m. The National Institute for Occupational Safety and Health has designated a recommended exposure limit of 0.5 ppm over 15 minutes.\n\nIn the home, accidents occur when hypochlorite bleach solutions come into contact with certain acidic drain-cleaners to produce chlorine gas. Hypochlorite bleach (a popular laundry additive) combined with ammonia (another popular laundry additive) produces chloramines, another toxic group of chemicals.\n\nChlorine is widely used for purifying water, especially potable water supplies and water used in swimming pools. Several catastrophic collapses of swimming pool ceilings have occurred from chlorine-induced stress corrosion cracking of stainless steel suspension rods. Some polymers are also sensitive to attack, including acetal resin and polybutene. Both materials were used in hot and cold water domestic plumbing, and stress corrosion cracking caused widespread failures in the USA in the 1980s and 1990s. The picture on the right shows a fractured acetal joint in a water supply system. The cracks started at injection molding defects in the joint and slowly grew until the part failed. The fracture surface shows iron and calcium salts that were deposited in the leaking joint from the water supply before failure.\n\nThe element iron can combine with chlorine at high temperatures in a strong exothermic reaction, creating a \"chlorine-iron fire\". Chlorine-iron fires are a risk in chemical process plants, where much of the pipework that carries chlorine gas is made of steel.\n\n\n", "id": "5667", "title": "Chlorine"}
{"url": "https://en.wikipedia.org/wiki?curid=5668", "text": "Calcium\n\nCalcium is a chemical element with symbol Ca and atomic number 20. Calcium is a soft grayish-yellow alkaline earth metal, fifth-most-abundant element by mass in the Earth's crust. The ion Ca is also the fifth-most-abundant dissolved ion in seawater by both molarity and mass, after sodium, chloride, magnesium, and sulfate. Free calcium metal is too reactive to occur in nature. Calcium is produced in supernova nucleosynthesis.\n\nCalcium is an essential trace element in living organisms. It is the most abundant metal by mass in many animals, and it is an important constituent of bone, teeth, and shells. In cell biology, the movement of the calcium ion into and out of the cytoplasm functions as a signal for many cellular processes. Calcium carbonate and calcium citrate are often taken as dietary supplements. Calcium is on the World Health Organization's List of Essential Medicines. \nCalcium has a wide variety of applications, almost all of which are associated with calcium compounds and salts.\n\nIn solution, the calcium ion varies remarkably to the human taste, being reported as mildly salty, sour, \"mineral-like\", or even \"soothing.\" It is apparent that many animals can taste, or develop a taste, for calcium, and use this sense to detect the mineral in salt licks or other sources. In human nutrition, soluble calcium salts may be added to tart juices without much effect to the average palate.\n\nCalcium is an important component of a healthy diet and a mineral necessary for life. The National Osteoporosis Foundation states, \"Calcium plays an important role in building stronger, denser bones early in life and keeping bones strong and healthy later in life.\" Approximately 99 percent of the calcium in the human body is in the bones and teeth. Intracellular calcium overload can cause oxidative stress and apoptosis in some cells, sometimes leading to several diseases. In the electrical conduction system of the heart, calcium replaces sodium as the mineral that depolarizes the cell, proliferating the action potential. In cardiac muscle, sodium influx commences an action potential, but during potassium efflux, the cardiac myocyte experiences calcium influx, prolonging the action potential and creating a plateau phase of dynamic equilibrium. Long-term calcium deficiency can lead to rickets and poor blood clotting; in menopausal women, deficiency can lead to osteoporosis, a condition in which the bone deteriorates and fractures more readily. While a lifelong deficit of calcium can affect bone and tooth formation, over-retention can cause hypercalcemia (elevated levels of calcium in the blood), impaired kidney function, and decreased absorption of other minerals. Vitamin D is needed to absorb calcium. \n\nThe release of calcium ions from the sarcoplasmic reticulum into the cytoplasm is an essential intracellular signal, important in many cellular functions and processes, including muscle contraction, neuronal transmission as in an excitatory synapse, cellular motility (including the movement of flagella and cilia), fertilisation, cell growth or proliferation, learning, memory (as with synaptic plasticity), and secretion of saliva.\nCalcium signalling can be studied by loading a cell's cytoplasm with a calcium-sensitive fluorescent dye such as Fura-2. Many of these dyes were developed by Roger Y. Tsien.\n\nCalcium supplements are used to prevent and to treat calcium deficiencies. However, it has been found that the taking of calcium supplements by people with a history of stroke or with white matter lesions greatly increased their chances of developing dementia. The Office of Dietary Supplements (National Institutes of Health) recommends that no more than 600 mg of supplement should be taken at a time because the percent of calcium absorbed decreases as the amount of calcium in the supplement increases. It is therefore recommended to spread doses throughout the day. Recommended daily calcium intake for adults ranges from 1000 to 1300 mg. Calcium supplements may have side effects such as bloating and constipation in some people. It is suggested that taking the supplements with food may aid in nullifying these side effects.\n\nFor U.S. dietary supplement and food labeling purposes the amount in a serving is expressed in milligrams and as a percent of Daily Value (%DV). The weight is for the calcium part of the compound - for example, calcium carbonate - in the supplement. For calcium labeling purposes 100% of the Daily Value was 1000 mg, but as of May 2016 it has been revised to 1300 mg. A table of the pre-change adult Daily Values is provided at Reference Daily Intake. Food and supplement companies have until July 2018 to comply with the labeling change.\n\nCompared with other metals, the calcium ion and most calcium compounds have low toxicity. This is not surprising, given the very high natural abundance of calcium compounds in the environment and in organisms. As for oral consumption safety, the Food and Nutrition Board of the U.S. Institute of Medicine sets Tolerable Upper Intake Levels (known as ULs) for vitamins and minerals when evidence is sufficient. In the case of calcium the UL is set at 2500 mg/day for adults ages 19 to 50 and 2000 mg/day for ages 51 and up. This is not toxicity per se. From the FNB \"Excessively high levels of calcium in the blood known as hypercalcemia can cause renal insufficiency, vascular and soft tissue calcification, hypercalciuria (high levels of calcium in the urine) and kidney stones.\" The European Food Safety Authority reviewed the same safety question and set its UL at 2500 mg/day. \n\nCalcium poses few serious environmental problems.\n\nCalcium metal is hazardous because of its sometimes-violent reactions with water and acids. Calcium metal is found in some drain cleaners, where it functions to generate heat and calcium hydroxide that saponifies the fats and liquefies the proteins (e.g., hair) that block drains. When swallowed, calcium metal has the same effect on the mouth, esophagus, and stomach, and can be fatal.\n\nCalcium can be extracted by electrolysis from a fused salt like calcium chloride. Calcium is relatively soft for a metal; although harder than lead, it can be cut with a knife with difficulty. Calcium is chemically reactive; when exposed to the air, it rapidly forms a gray-white coating of calcium oxide and calcium nitride. In bulk form (typically as chips or \"turnings\"), the metal is somewhat difficult to ignite, more difficult even than magnesium chips; but, when lit, the metal burns in air with a brilliant high-intensity orange-red light. Calcium metal reacts with water, producing hydrogen gas at a moderate rate without generating much heat, making it useful for generating hydrogen. In powdered form, however, the reaction with water is extremely rapid, as the increased surface area of the powder accelerates the reaction. Part of the reason for the slowness of the calcium–water reaction is a partial passivation (chemically protective coating) of insoluble white calcium hydroxide; in acidic solutions, where this compound is more soluble, calcium reacts vigorously.\n\nWith a density of 1.54 g/cm, calcium is the lightest of the alkaline earth metals; magnesium (specific gravity 1.74) and beryllium (1.84) are denser though lighter in atomic mass. From strontium onward, the alkali earth metals become denser with increasing atomic mass. Calcium has two allotropes.\n\nCalcium metal has a higher electrical resistivity than copper or aluminium, yet weight-for-weight, due to its much lower density, it is a better conductor than either. Its use as such in terrestrial applications is usually limited by its high reactivity with air; however, it has potential for use as wiring in off-world applications.\n\nCalcium is the fifth-most-abundant element by mass in the human body, where it is a cellular ionic messenger with many functions. Calcium also serves as a structural element in bone. It is the relatively high atomic number of calcium that causes bone to be radio-opaque.\n\nVisible spectra of many stars, including the Sun, exhibit strong emission lines of singly ionized calcium. Prominent among these are the H-line at 3968.5 Å and the K line at 3933.7 Å of singly ionized calcium, or Ca II. In the Sun or other stars with low temperatures, the prominence of the H and K lines in the visible spectra can be an indication of strong magnetic activity in the chromosphere.\n\nCalcium chemistry is almost exclusively that of Ca salts. Ca is a \"hard cation\", that is, it characteristically favors oxide ligands. Hence the abundance of carbonates, nitrates, phosphates, and sulfates in the mineral kingdom. Many of these species crystallize with water. Because it is generally nontoxic and abundant, calcium is found in many foods and useful materials. Most calcium salts are colorless. As with magnesium salts and other alkaline earth metal salts, the halides are soluble in water.\n\nCombined with phosphate, calcium forms hydroxylapatite (Ca(PO)OH), the mineral portion of animal bones, teeth, and some corals. Large-scale chemical processes are involved in the conversion of calcium phosphate minerals into fertilizer.\n\nCalcium is the main problematic ion in hard water: it forms insoluble deposits of calcium carbonate that are problematic in plumbing. It also reacts with soap to form soap scum. Calcium carbonate occurs naturally as limestone and chalk. When water percolates through limestone or other calcium-containing rocks, it partially dissolves the rock. The slow re-precipitation of minerals derived from dissolved calcium leads to formation of stalactites and stalagmites.\n\nWhen heated above 825 °C, calcium carbonate converts calcium oxide (CaO), also known as quicklime:\nWhen added to water, quicklime vigorously reacts (hence its name) to form calcium hydroxide:\nAlso known as slaked lime, Ca(OH) is an inexpensive base material used throughout the chemical industry. When mixed with sand, it hardens into a mortar and is turned into plaster by carbon dioxide uptake. Mixed with other compounds, lime forms an important part of Portland cement.\n\nCombined with sulfate, calcium forms the mineral gypsum. When heated to about 300 °F (150 °C), it undergoes partial dehydration:\nThe resulting powder, when mixed with water, forms a stiff but workable paste that hardens to give Plaster of Paris.\n\nOrganocalcium compounds, those containing Ca-C bonds are known, but generally of specialized interest in the research laboratory. One major exception is calcium carbide, which arises from heating calcium compounds with coal or other carbon-rich reducing agents. \nIt was historically important precursor to acetylene.\n\nOther important calcium compounds are calcium nitrate used in fertilizers, calcium chloride used as for deicing roads, calcium cyanamide, and calcium hypochlorite, used for bleaching.\nFocusing on chemical structure, Ca is a relatively large ion that tends to adopt a high coordination number. In CaF, the mineral fluorite, each Ca ion is surrounded by eight F ligands.\n\nCalcium has five stable isotopes (Ca, Ca, Ca, Ca and Ca), plus one more (Ca) that has such a long half-life, it can be considered stable for many purposes. The 20% range in relative mass among naturally occurring calcium isotopes is greater than for any element other than hydrogen and helium. Calcium also has a cosmogenic isotope, radioactive Ca, which has a half-life of 103,000 years. Unlike cosmogenic isotopes produced in the atmosphere, Ca is produced by neutron activation of Ca, primarily in the top metre of the soil column, where the cosmogenic neutron flux is sufficiently strong. Ca has received much attention in stellar studies because it decays to K, a critical indicator of solar-system anomalies.\n\nNinety-seven percent of naturally occurring calcium is in the form of Ca, which is a daughter product of K and Ar decay. While K–Ar dating is used extensively in the geological sciences, the prevalence of Ca in nature has impeded its use in dating. Techniques using mass spectrometry and a double spike isotope dilution are used for K-Ca age dating.\n\nCa has a nucleus of 20 protons and 20 neutrons and is the heaviest stable isotope of any element that has equal numbers of protons and neutrons. In supernova explosions, calcium is formed from the reaction of carbon with various numbers of alpha particles (helium nuclei), until the most common calcium isotope (containing 10 helium nuclei) has been synthesized.\n\nAs with the isotopes of other elements, a variety of processes fractionate, or alter the relative abundance of, calcium isotopes. The best studied of these processes is the mass-dependent fractionation of calcium isotopes that accompanies the precipitation of calcium minerals, such as calcite, aragonite and apatite, from solution. Isotopically light calcium is preferentially incorporated into minerals, leaving the solution from which the mineral precipitated enriched in isotopically heavy calcium. At room temperature the magnitude of this fractionation is roughly 0.25‰ (0.025%) per atomic mass unit (AMU). Mass-dependent differences in calcium isotope composition conventionally are expressed by the ratio of two isotopes (usually Ca/Ca) in a sample compared to the same ratio in a standard reference material. Ca/Ca varies by about 1% among common earth materials.\n\nCalcium isotope fractionation during mineral formation has led to several applications of calcium isotopes. In particular, the 1997 observation by Skulan and DePaolo that calcium minerals are isotopically lighter than the solutions from which the minerals precipitate is the basis of analogous applications in medicine and in paleooceanography. In animals with skeletons mineralized with calcium, the calcium isotopic composition of soft tissues reflects the relative rate of formation and dissolution of skeletal mineral. In humans, changes in the calcium isotopic composition of urine have been shown to be related to changes in bone mineral balance. When the rate of bone formation exceeds the rate of bone resorption, the ration Ca/Ca in soft tissue rises. Soft tissue Ca/Ca falls when bone resorption exceeds bone formation. Because of this relationship, calcium isotopic measurements of urine or blood may be useful in the early detection of metabolic bone diseases like osteoporosis.\n\nA similar system exists in the ocean, where Ca/Ca in seawater tends to rise when the rate of removal of Ca from seawater by mineral precipitation exceeds the input of new calcium into the ocean, and fall when calcium input exceeds mineral precipitation. It follows that rising Ca/Ca corresponds to falling seawater Ca concentration, and falling Ca/Ca corresponds to rising seawater Ca concentration. In 1997 Skulan and DePaolo presented the first evidence of change in seawater Ca/Ca over geologic time, along with a theoretical explanation of these changes. More recent papers have confirmed this observation, demonstrating that seawater Ca concentration is not constant, and that the ocean probably never is in “steady state” with respect to its calcium input and output. This has important climatological implications, as the marine calcium cycle is closely tied to the carbon cycle (see below).\n\nCalcium is not naturally found in its elemental state. Calcium occurs most commonly in sedimentary rocks in the minerals calcite, dolomite, and gypsum. It also occurs in igneous and metamorphic rocks chiefly in the silicate minerals: plagioclases, amphiboles, pyroxenes, and garnets.\n\nDairy products, such as milk and cheese, are a well-known sources of calcium.\n\nVegetable sources for calcium include:\nNumerous vegetables, notably spinach, chard, and rhubarb have a high calcium content, but they may also contain varying amounts of oxalic acid that binds calcium and reduces its absorption. The same problem may affect the absorption of calcium from amaranth, collard greens, and chicory greens. This process may also be related to the generation of calcium oxalate.\n\nAn overlooked source of calcium is eggshell, which can be ground into a powder and mixed into food or a glass of water.\n\nThe calcium content of most foods can be found in the USDA National Nutrient Database.\n\n\nCalcium provides a link between tectonics, climate, and the carbon cycle. In the simplest terms, uplift of mountains exposes calcium-bearing rocks to chemical weathering and releases Ca into surface water. This Ca eventually is transported to the ocean where it reacts with dissolved CO to form limestone. Some of this limestone settles to the sea floor where it is incorporated into new rocks. Dissolved CO, along with carbonate and bicarbonate ions, are termed \"dissolved inorganic carbon\" (DIC).\n\nThe actual reaction is more complicated and involves the bicarbonate ion (HCO) that forms when CO reacts with water at seawater pH:\n\nNote that at seawater pH, most of the CO is immediately converted back into . The reaction results in a net transport of one molecule of CO from the ocean/atmosphere into the lithosphere.\n\nThe result is that each Ca ion released by chemical weathering ultimately removes one CO molecule from the surficial system (atmosphere, ocean, soils and living organisms), storing it in carbonate rocks where it is likely to stay for hundreds of millions of years. The weathering of calcium from rocks thus scrubs CO from the ocean and atmosphere, exerting a strong long-term effect on climate. Analogous cycles involving magnesium, and to a much smaller extent strontium and barium, have the same effect.\n\nAs the weathering of limestone (CaCO) liberates equimolar amounts of Ca and CO, it has no net effect on the CO content of the atmosphere and ocean. The weathering of silicate rocks like granite, on the other hand, is a net CO sink because it produces abundant Ca but very little CO.\n\nLime as building material was used since prehistoric times going as far back as 7000 to 14000 BC. Statues made from lime plaster have been dated into the 7 millennia BC. The first dated lime kiln dates back to 2500 BC and was found in Khafajah mesopotamia. Calcium (from Latin , genitive \"calcis\", meaning \"lime\") was known as early as the first century when the Ancient Romans prepared lime as calcium oxide. Literature dating back to 975 AD notes that plaster of paris (calcium sulfate), is useful for setting broken bones. It was not isolated until 1808 in England when Sir Humphry Davy electrolyzed a mixture of lime and mercuric oxide. Calcium metal was not available in large scale until the beginning of the 20th century.\n\n\n", "id": "5668", "title": "Calcium"}
{"url": "https://en.wikipedia.org/wiki?curid=5669", "text": "Chromium\n\nChromium is a chemical element with symbol Cr and atomic number 24. It is the first element in Group 6. It is a steely-grey, lustrous, hard and brittle metal which takes a high polish, resists tarnishing, and has a high melting point. The name of the element is derived from the Greek word χρῶμα, \"chrōma\", meaning color, because many of the compounds are intensely colored.\n\nFerrochromium alloy is commercially produced from chromite by silicothermic or aluminothermic reactions and chromium metal by roasting and leaching processes followed by reduction with carbon and then aluminium. Chromium metal is of high value for its high corrosion resistance and hardness. A major development was the discovery that steel could be made highly resistant to corrosion and discoloration by adding metallic chromium to form stainless steel. Stainless steel and chrome plating (electroplating with chromium) together comprise 85% of the commercial use.\n\nTrivalent chromium (Cr(III)) ion is an essential nutrient in trace amounts in humans for insulin, sugar and lipid metabolism, although the issue is debated. \n\nWhile chromium metal and Cr(III) ions are not considered toxic, hexavalent chromium (Cr(VI)) is toxic and carcinogenic. Abandoned chromium production sites often require environmental cleanup.\n\nChromium is remarkable for its magnetic properties: it is the only elemental solid which shows antiferromagnetic ordering at room temperature (and below). Above 38 °C, it changes to paramagnetic.\n\nChromium metal left standing in air is passivated by oxidation, forming a thin, protective, surface layer. This layer is a spinel structure only a few molecules thick. It is very dense, and prevents the diffusion of oxygen into the underlying metal. This is different from the oxide that forms on iron and carbon steel, through which elemental oxygen continues to migrate, reaching the underlying material to cause incessant rusting. Passivation can be enhanced by short contact with oxidizing acids like nitric acid. Passivated chromium is stable against acids. Passivation can be removed with a strong reducing agent that destroys the protective oxide layer on the metal. Chromium metal treated in this way readily dissolves in weak acids.\n\nChromium, unlike such metals as iron and nickel, does not suffer from hydrogen embrittlement. However, it does suffer from nitrogen embrittlement, reacting with nitrogen from air and forming brittle nitrides at the high temperatures necessary to work the metal parts.\n\nChromium is the 22nd most abundant element in Earth's crust with an average concentration of 100 ppm. Chromium compounds are found in the environment from the erosion of chromium-containing rocks, and can be redistributed by volcanic eruptions. Typical background concentrations of chromium in environmental media are: atmosphere <10 ng m; soil <500 mg kg; vegetation <0.5 mg kg; freshwater <10 ug L; seawater <1 ug L; sediment <80 mg kg.\n\nChromium is mined as chromite (FeCrO) ore. About two-fifths of the chromite ores and concentrates in the world are produced in South Africa, while Kazakhstan, India, Russia, and Turkey are also substantial producers. Untapped chromite deposits are plentiful, but geographically concentrated in Kazakhstan and southern Africa.\n\nAlthough rare, deposits of native chromium exist. The Udachnaya Pipe in Russia produces samples of the native metal. This mine is a kimberlite pipe, rich in diamonds, and the reducing environment helped produce both elemental chromium and diamond.\n\nThe relation between Cr(III) and Cr(VI) strongly depends on pH and oxidative properties of the location. In most cases, Cr(III) is the dominating species, but in some areas, the ground water can contain up to 39 µg/liter of total chromium of which 30 µg/liter is Cr(VI).\n\nNaturally occurring chromium is composed of three stable isotopes; Cr, Cr and Cr, with Cr being the most abundant (83.789% natural abundance). 19 radioisotopes have been characterized, with the most stable being Cr with a half-life of (more than) 1.8 years, and Cr with a half-life of 27.7 days. All of the remaining radioactive isotopes have half-lives that are less than 24 hours and the majority less than 1 minute. This element also has 2 meta states.\n\nCr is the radiogenic decay product of Mn (half-life = 3.74 million years), and chromium isotopes are typically collocated (and compounded) with manganese isotopes. This circumstance is useful in isotope geology. Mangenese-chromium isotope ratios reinforce the evidence from Al and Pd concerning the early history of the solar system. Variations in Cr/Cr and Mn/Cr ratios from several meteorites indicate an initial Mn/Mn ratio that suggests Mn-Cr isotopic composition must result from in-situ decay of Mn in differentiated planetary bodies. Hence Cr provides additional evidence for nucleosynthetic processes immediately before coalescence of the solar system.\n\nThe isotopes of chromium range in atomic mass from 43 u (Cr) to 67 u (Cr). The primary decay mode before the most abundant stable isotope, Cr, is electron capture and the primary mode after is beta decay. Cr has been posited as a proxy for atmospheric oxygen concentration.\n\nChromium is a member of Group 6, of the transition metals. Chromium(0) has an electronic configuration of [Ar]3d4s, owing to the lower energy of the high spin configuration. Chromium exhibits a wide range of oxidation states, with +3 the most stable; the +3 and +6 states are the most common in chromium compounds, while +1, +4 and +5 are rare.\n\nThe following is the Pourbaix diagram for chromium in pure water, perchloric acid or sodium hydroxide:\n\nA large number of chromium(III) compounds are known. Chromium(III) can be obtained by dissolving elemental chromium in acids like hydrochloric acid or sulfuric acid. The ion has a similar radius (63 pm) to (radius 50 pm), and they can replace each other in some compounds, such as in chrome alum and alum. When a trace amount of replaces in corundum (aluminium oxide, AlO), pink sapphire or red-colored ruby is formed, depending on the amount of chromium.\n\nChromium(III) ions tend to form octahedral complexes. The color of these complexes is determined by the ligands attached to the Cr center. Commercially available chromium(III) chloride hydrate is the dark green complex [CrCl(HO)]Cl. Closely related compounds have different colors: pale green [CrCl(HO)]Cl and violet [Cr(HO)]Cl. If water-free green chromium(III) chloride is dissolved in water, the green solution turns violet after some time as the chloride in the inner coordination sphere is replaced by water. This kind of reaction is also observed with solutions of chrome alum and other water-soluble chromium(III) salts.\n\nChromium(III) hydroxide (Cr(OH)) is amphoteric, dissolving in acidic solutions to form [Cr(HO)], and in basic solutions to form . It is dehydrated by heating to form the green chromium(III) oxide (CrO), a stable oxide with a crystal structure identical to that of corundum.\n\nChromium(VI) compounds are powerful oxidants at low or neutral pH. Most important are chromate anion () and dichromate (CrO) anions, which exist in equilibrium:\nChromium(VI) halides are known also and include the hexafluoride CrF and chromyl chloride ().\n\nSodium chromate is produced industrially by the oxidative roasting of chromite ore with calcium or sodium carbonate. The dominant species is therefore, by the law of mass action, determined by the pH of the solution. The change in equilibrium is visible by a change from yellow (chromate) to orange (dichromate), such as when an acid is added to a neutral solution of potassium chromate. At yet lower pH values, further condensation to more complex oxyanions of chromium is possible.\n\nBoth the chromate and dichromate anions are strong oxidizing reagents at low pH:\n\nThey are, however, only moderately oxidizing at high pH:\n\nChromium(VI) compounds in solution can be detected by adding an acidic hydrogen peroxide solution. The unstable dark blue chromium(VI) peroxide (CrO) is formed, which can be stabilized as an ether adduct .\n\nChromic acid has the hypothetical formula . It is a vaguely described chemical, despite many well-defined chromates and dichromates being known. The dark red chromium(VI) oxide , the acid anhydride of chromic acid, is sold industrially as \"chromic acid\". It can be produced by mixing sulfuric acid with dichromate, and is a strong oxidizing agent.\n\nThe oxidation state +5 is only realized in few compounds but are intermediates in many reactions involving oxidations by chromate. The only binary compound is the volatile chromium(V) fluoride (CrF). This red solid has a melting point of 30 °C and a boiling point of 117 °C. It can be prepared by treating chromium metal with fluorine at 400 °C and 200 bar pressure. The peroxochromate(V) is another example of the +5 oxidation state. Potassium peroxochromate (K[Cr(O)]) is made by reacting potassium chromate with hydrogen peroxide at low temperatures. This red brown compound is stable at room temperature but decomposes spontaneously at 150–170 °C.\n\nCompounds of chromium(IV) (in the +4 oxidation state) are slightly more common than those of chromium(V). The tetrahalides, CrF, CrCl, and CrBr, can be produced by treating the trihalides () with the corresponding halogen at elevated temperatures. Such compounds are susceptible to disproportionation reactions and are not stable in water.\n\nMany chromium(II) compounds are known, including the water-stable chromium(II) chloride, , which can be made by reducing chromium(III) chloride with zinc. The resulting bright blue solution is only stable at neutral pH. Many chromous carboxylates are known, most famously the red chromous acetate (Cr(OCCH)) that features a quadruple bond.\n\nMost Cr(I) compounds are obtained by oxidation of electron-rich, octahedral Cr(0) complexes. Other Cr(I) complexes contain cyclopentadienyl ligands. As verified by X-ray diffraction, a Cr-Cr quintuple bond (length 183.51(4)  pm) has also been described. Extremely bulky monodentate ligands stabilize this compound by shielding the quintuple bond from further reactions.\n\nMany chromium(0) compounds are known. Most are derivatives of chromium hexacarbonyl or bis(benzene)chromium.\n\nChromium was discovered as an element after it came to the attention of the Western world in the red crystalline mineral crocoite (lead(II) chromate), discovered in 1761 and initially used as a pigment. Nearly all chromium is commercially extracted from the single commercially viable ore chromite, which is iron chromium oxide (FeCrO). Chromite is now the principal source of chromium for pigments.\n\nWeapons found in burial pits dating from the late 3rd century B.C. Qin Dynasty of the Terracotta Army near Xi'an, China have been analyzed by archaeologists. Although buried more than 2,000 years ago, the ancient bronze tips of crossbow bolts and swords found at the site showed unexpectedly little corrosion, possibly because the bronze was deliberately coated with a thin layer of chromium oxide. However, this oxide layer was not chromium metal or chrome plating as we know it.\n\nChromium minerals as pigments came to the attention of the west in the 18th century. On 26 July 1761, Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains which he named \"Siberian red lead\". Though misidentified as a lead compound with selenium and iron components, the mineral was in fact crocoite (\"lead chromate\") with a formula of PbCrO.\n\nIn 1770, Peter Simon Pallas visited the same site as Lehmann and found a red lead mineral that had useful properties as a pigment in paints. The use of Siberian red lead as a paint pigment then developed rapidly. A bright yellow pigment made from crocoite also became popular.\n\nIn 1797, Louis Nicolas Vauquelin received samples of crocoite ore. He produced chromium trioxide (CrO) by mixing crocoite with hydrochloric acid. In 1798, Vauquelin discovered that he could isolate metallic chromium by heating the oxide in a charcoal oven, for which he is credited as the discoverer of the element. Vauquelin was also able to detect traces of chromium in precious gemstones, such as ruby or emerald.\n\nDuring the 1800s, chromium was primarily used as a component of paints and in tanning salts. At first, crocoite from Russia was the main source, but in 1827, a larger chromite deposit was discovered near Baltimore, United States. This made the United States the largest producer of chromium products till 1848 when large deposits of chromite were found near Bursa, Turkey.\n\nChromium is also known for its luster when polished. It is used as a protective and decorative coating on car parts, plumbing fixtures, furniture parts and many other items, usually applied by electroplating. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.\n\nApproximately 28.8 million metric tons (Mt) of marketable chromite ore was produced in 2013, and converted into 7.5 Mt of ferrochromium. According to John F. Papp, writing for the USGS, \"Ferrochromium is the leading end use of chromite ore, [and] stainless steel is the leading end use of ferrochromium.\"\n\nThe largest producers of chromium ore in 2013 have been South Africa (48%), Kazakhstan (13%), Turkey (11%), India (10%) with several other countries producing the rest of about 18% of the world production.\n\nThe two main products of chromium ore refining are ferrochromium and metallic chromium. For those products the ore smelter process differs considerably. For the production of ferrochromium, the chromite ore (FeCrO) is reduced in large scale in electric arc furnace or in smaller smelters with either aluminium or silicon in an aluminothermic reaction.\n\nFor the production of pure chromium, the iron must be separated from the chromium in a two step roasting and leaching process. The chromite ore is heated with a mixture of calcium carbonate and sodium carbonate in the presence of air. The chromium is oxidized to the hexavalent form, while the iron forms the stable FeO. The subsequent leaching at higher elevated temperatures dissolves the chromates and leaves the insoluble iron oxide. The chromate is converted by sulfuric acid into the dichromate.\n\nThe dichromate is converted to the chromium(III) oxide by reduction with carbon and then reduced in an aluminothermic reaction to chromium.\n\nMetal alloys now account for 85% of the use of chromium. The remainder is used in the chemical, refractory, and foundry industries.\n\nThe strengthening effect of forming stable metal carbides at the grain boundaries and the strong increase in corrosion resistance made chromium an important alloying material for steel. The high-speed tool steels contain between 3 and 5% chromium. Stainless steel, the main corrosion-proof metal alloy, is formed when chromium is added to iron in sufficient concentrations, usually above 11%. For its formation, ferrochromium is added to the molten iron. Also nickel-based alloys increase in strength due to the formation of discrete, stable metal carbide particles at the grain boundaries. For example, Inconel 718 contains 18.6% chromium. Because of the excellent high-temperature properties of these nickel superalloys, they are used in jet engines and gas turbines in lieu of common structural materials.\n\nThe relative high hardness and corrosion resistance of unalloyed chromium makes it a good surface coating, being still the most \"popular\" metal coating with unparalleled combined durability. A thin layer of chromium is deposited on pretreated metallic surfaces by electroplating techniques. There are two deposition methods: Thin, below 1 µm thickness, layers are deposited by chrome plating, and are used for decorative surfaces. If wear-resistant surfaces are needed then thicker chromium layers are deposited. Both methods normally use acidic chromate or dichromate solutions. To prevent the energy-consuming change in oxidation state, the use of chromium(III) sulfate is under development, but for most applications, the established process is used.\n\nIn the chromate conversion coating process, the strong oxidative properties of chromates are used to deposit a protective oxide layer on metals like aluminium, zinc and cadmium. This passivation and the self-healing properties by the chromate stored in the chromate conversion coating, which is able to migrate to local defects, are the benefits of this coating method. Because of environmental and health regulations on chromates, alternative coating methods are under development.\n\nChromic acid anodizing (or Type I anodizing) of aluminium is another electrochemical process, which does not lead to the deposition of chromium, but uses chromic acid as electrolyte in the solution. During anodization, an oxide layer is formed on the aluminium. The use of chromic acid, instead of the normally used sulfuric acid, leads to a slight difference of these oxide layers.\nThe high toxicity of Cr(VI) compounds, used in the established chromium electroplating process, and the strengthening of safety and environmental regulations demand a search for substitutes for chromium or at least a change to less toxic chromium(III) compounds.\n\nThe mineral crocoite (lead chromate PbCrO) was used as a yellow pigment shortly after its discovery. After a synthesis method became available starting from the more abundant chromite, chrome yellow was, together with cadmium yellow, one of the most used yellow pigments. The pigment does not photodegrade, but it tends to darken due to the formation of chromium(III) oxide. It has a strong color, and was used for school buses in the US and for Postal Service (for example Deutsche Post) in Europe. The use of chrome yellow declined due to environmental and safety concerns and was replaced by organic pigments or alternatives free from lead and chromium. Other pigments based on chromium are, for example, the bright red pigment chrome red, which is a basic lead chromate (PbCrO·Pb(OH)). A very important chromate pigment, which was used widely in metal primer formulations, was zinc chromate, now replaced by zinc phosphate. A wash primer was formulated to replace the dangerous practice of pretreating aluminium aircraft bodies with a phosphoric acid solution. This used zinc tetroxychromate dispersed in a solution of polyvinyl butyral. An 8% solution of phosphoric acid in solvent was added just before application. It was found that an easily oxidized alcohol was an essential ingredient. A thin layer of about 10–15 µm was applied, which turned from yellow to dark green when it was cured. There is still a question as to the correct mechanism. Chrome green is a mixture of Prussian blue and chrome yellow, while the chrome oxide green is chromium(III) oxide.\n\nChromium oxides are also used as a green color in glassmaking and as a glaze in ceramics. Green chromium oxide is extremely light-fast and as such is used in cladding coatings. It is also the main ingredient in infrared reflecting paints, used by the armed forces, to paint vehicles, to give them the same IR reflectance as green leaves.\n\nNatural rubies are corundum (aluminum oxide) crystals that are colored red (the rarest type) due to chromium (III) ions (other colors of corundum gems are termed sapphires). A red-colored artificial ruby may also be achieved by doping chromium(III) into artificial corundum crystals, thus making chromium a requirement for making synthetic rubies. Such a synthetic ruby crystal was the basis for the first laser, produced in 1960, which relied on stimulated emission of light from the chromium atoms in such a crystal.\n\nBecause of their toxicity, chromium(VI) salts are used for the preservation of wood. For example, chromated copper arsenate (CCA) is used in timber treatment to protect wood from decay fungi, wood-attacking insects, including termites, and marine borers. The formulations contain chromium based on the oxide CrO between 35.3% and 65.5%. In the United States, 65,300 metric tons of CCA solution were used in 1996.\n\nChromium(III) salts, especially chrome alum and chromium(III) sulfate, are used in the tanning of leather. The chromium(III) stabilizes the leather by cross linking the collagen fibers. Chromium tanned leather can contain between 4 and 5% of chromium, which is tightly bound to the proteins. Although the form of chromium used for tanning is not the toxic hexavalent variety, there remains interest in management of chromium in the tanning industry such as recovery and reuse, direct/indirect recycling, use of less chromium or \"chrome-less\" tanning are practiced to better manage chromium in tanning.\n\nThe high heat resistivity and high melting point makes chromite and chromium(III) oxide a material for high temperature refractory applications, like blast furnaces, cement kilns, molds for the firing of bricks and as foundry sands for the casting of metals. In these applications, the refractory materials are made from mixtures of chromite and magnesite. The use is declining because of the environmental regulations due to the possibility of the formation of chromium(VI). \n\nSeveral chromium compounds are used as catalysts for processing hydrocarbons. For example, the Phillips catalyst, prepared from chromium oxides, is used for the production of about half the world's polyethylene. Fe-Cr mixed oxides are employed as high-temperature catalysts for the water gas shift reaction. Copper chromite is a useful hydrogenation catalyst.\n\n\nIn the form trivalent chromium, Cr(III), or Cr, chromium was identified as an essential nutrient in the late 1950s and later accepted as a trace element for its roles in the action of insulin, a hormone critical to the metabolism and storage of carbohydrate, fat and protein. The precise mechanism of its actions in the body, however, have not been fully defined, leaving in question whether chromium is essential for healthy people.\n\nTrivalent chromium occurs in trace amounts in foods, wine and water. In contrast, hexavalent chromium (Cr(VI) or Cr) is highly toxic and mutagenic when inhaled. Cr(VI) has not been established as a carcinogen when in solution, although it may cause allergic contact dermatitis (ACD).\n\nChromium deficiency, involving a lack of Cr(III) in the body, or perhaps some complex of it, such as glucose tolerance factor is controversial. Some studies suggest that the biologically active form of chromium (III) in an oligopeptide called low-molecular-weight chromium-binding substance (LMWCr), which might play a role in the insulin signaling pathway.\n\nAlthough the mechanism in biological roles for chromium is unclear, dietary supplements for chromium include chromium(III) picolinate, chromium(III) polynicotinate, and related materials. The benefit of supplements has not been proven.\n\nIn the United States, the dietary guidelines for daily chromium intake were lowered in 2001 from 50–200 µg for an adult to 35 µg (adult male) and to 25 µg (adult female). In 2014, the European Food Safety Authority published a report stating that the intake of chromium(III) has no beneficial effect on healthy people, thus the Panel removed chromium from the list of nutrients and essential elements.\n\nChromium content of common foods is generally low (1-13 micrograms per serving). Chromium content of food varies widely due to differences in soil mineral content, growing season, plant cultivar, and contamination during processing. In addition, large amounts of chromium (and nickel) leach into food cooked in stainless steel.\n\nThe Food and Nutrition Board of the U.S. Institute of Medicine updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) in 2001. For chromium there was not sufficient information to set EARs and RDAs, so needs are described as estimates for Adequate Intakes (AIs). The current AIs for chromium for women ages 14 and up is 25 μg/day up to age 50 and 20 μg/day for older. AI for pregnancy is 30 μg/day. RDA for lactation is 45 μg/day. For men ages 14 and up 35 μg/day up to age 50 and 30 μg/day for older. For infants to children ages 1–13 years the AI increases with age from 0.2 to 25 μg/day. As for safety, the Food and Nutrition Board also sets Tolerable Upper Intake Levels (known as ULs) for vitamins and minerals when evidence is sufficient. In the case of chromium there is not yet enough information and hence no UL. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes. The European Food Safety Authority reviewed the same safety question and did not set a UL. The World Health Organization has tentatively set UL at 250 μg/day. In the United States many dietary supplement companies offer chromium products at 200 to 800 μg/day. Multi-vitamin/mineral products tend to have chromium at 120 μg per tablet because until recently that was the 100% Daily Value (see below). \n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For chromium labeling purposes 100% of the Daily Value was 120 μg, but as of May 2016 it has been revised to 35 μg. Food and supplement companies have until July 28, 2018 to comply with the change. A table of the pre-change adult Daily Values is provided at Reference Daily Intake.\n\nWater-insoluble chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known for a long time. Because of the specific transport mechanisms, only limited amounts of chromium(III) enter the cells. Several \"in vitro\" studies indicated that high concentrations of chromium(III) in the cell can lead to DNA damage. Acute oral toxicity ranges between 1.5 and 3.3 mg/kg. A 2008 review suggested that moderate uptake of chromium(III) through dietary supplements poses no genetic-toxic risk. In the US, the Occupational Safety and Health Administration (OSHA) has designated a permissible exposure limit (PEL) in the workplace as a time-weighted average (TWA) of 1 mg/m. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5 mg/m, time-weighted average. The IDLH (immediately dangerous to life and health) value is 250 mg/m.\n\nThe acute oral toxicity for chromium(VI) ranges between 50 and 150 µg/kg. In the body, chromium(VI) is reduced by several mechanisms to chromium(III) already in the blood before it enters the cells. The chromium(III) is excreted from the body, whereas the chromate ion is transferred into the cell by a transport mechanism, by which also sulfate and phosphate ions enter the cell. The acute toxicity of chromium(VI) is due to its strong oxidational properties. After it reaches the blood stream, it damages the kidneys, the liver and blood cells through oxidation reactions. Hemolysis, renal, and liver failure result. Aggressive dialysis can be therapeutic.\n\nThe carcinogenity of chromate dust has been known for a long time, and in 1890 the first publication described the elevated cancer risk of workers in a chromate dye company. Three mechanisms have been proposed to describe the genotoxicity of chromium(VI). The first mechanism includes highly reactive hydroxyl radicals and other reactive radicals which are by products of the reduction of chromium(VI) to chromium(III). The second process includes the direct binding of chromium(V), produced by reduction in the cell, and chromium(IV) compounds to the DNA. The last mechanism attributed the genotoxicity to the binding to the DNA of the end product of the chromium(III) reduction.\n\nChromium salts (chromates) are also the cause of allergic reactions in some people. Chromates are often used to manufacture, amongst other things, leather products, paints, cement, mortar and anti-corrosives. Contact with products containing chromates can lead to allergic contact dermatitis and irritant dermatitis, resulting in ulceration of the skin, sometimes referred to as \"chrome ulcers\". This condition is often found in workers that have been exposed to strong chromate solutions in electroplating, tanning and chrome-producing manufacturers.\n\nBecause chromium compounds were used in dyes, paints, and leather tanning compounds, these compounds are often found in soil and groundwater at abandoned industrial sites, now needing environmental cleanup and remediation. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.\n\nIn 2010, the Environmental Working Group studied the drinking water in 35 American cities in the first nationwide study. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limit. Concentrations of Cr(VI) in US municipal drinking water supplies reported by EWG are within likely, natural background levels for the areas tested and not necessarily indicative of industrial pollution, as asserted by EWG. This factor was not taken into consideration in the EWG report.\n\n", "id": "5669", "title": "Chromium"}
{"url": "https://en.wikipedia.org/wiki?curid=5671", "text": "Cymbal\n\nA cymbal is a common percussion instrument. Often used in pairs, cymbals consist of thin, normally round plates of various alloys. The majority of cymbals are of indefinite pitch, although small disc-shaped cymbals based on ancient designs sound a definite note (see: crotales). Cymbals are used in many ensembles ranging from the orchestra, percussion ensembles, jazz bands, heavy metal bands, and marching groups. Drum kits usually incorporate at least a crash, ride or crash/ride, and a pair of hi-hat cymbals. A player of cymbals is known as a cymbalist.\n\nThe word cymbal is derived from the Latin \"cymbalum\", which is the latinisation of the Greek word \"kymbalon\", \"cymbal\", which in turn derives from \"kymbē\", \"cup, bowl\".\n\nIn orchestral scores, cymbals may be indicated by the French \"cymbales\"; German \"Becken\", \"Schellbecken\", \"Tellern\", or \"Tschinellen\"; Italian \"piatti\" or \"cinelli\"; and Spanish \"platillos\". Many of these derive from the word for plates.\n\nCymbals have existed since ancient times. Representations of cymbals may be found in reliefs and paintings from Armenian Highlands (VII century BC), Larsa, Babylon, Assyria, ancient Egypt, ancient Greece, and ancient Rome. References to cymbals also appear throughout the Bible, through many Psalms and songs of praise to God. Cymbals may have been introduced to China from Central Asia in the 3rd or 4th century AD.\n\nCymbals were employed by Turkish janissaries in the 14th century or earlier. By the 17th century, such cymbals were used in European music, and more commonly played in military bands and orchestras by the mid 18th century. Since the 19th century, some composers have called for larger roles for cymbals in musical works, and a variety of cymbal shapes, techniques, and hardware have been developed in response.\n\nThe anatomy of the cymbal plays a large part in the sound it creates. A hole is drilled in the center of the cymbal, which is used to either mount the cymbal on a stand or for tying straps through (for hand playing). The bell, dome, or cup is the raised section immediately surrounding the hole. The bell produces a higher \"pinging\" pitch than the rest of the cymbal. The bow is the rest of the surface surrounding the bell. The bow is sometimes described in two areas: the ride and crash area. The ride area is the thicker section closer to the bell while the crash area is the thinner tapering section near the edge. The edge or rim is the immediate circumference of the cymbal.\n\nCymbals are measured by their diameter either in inches or centimeters. The size of the cymbal affects its sound, larger cymbals usually being louder and having longer sustain. The weight describes how thick the cymbal is. Cymbal weights are important to the sound they produce and how they play. Heavier cymbals have a louder volume, more cut, and better stick articulation (when using drum sticks). Thin cymbals have a fuller sound, lower pitch, and faster response.\n\nThe profile of the cymbal is the vertical distance of the bow from the bottom of the bell to the cymbal edge (higher profile cymbals are more bowl shaped). The profile affects the pitch of the cymbal: higher profile cymbals have higher pitch.\n\nCymbals offer a composer nearly endless amounts of color and effect. Their unique timbre allows them to project even against a full orchestra and through the heaviest of orchestrations and enhance articulation and nearly any dynamic. Cymbals have been utilized historically to suggest frenzy, fury or bacchanalian revels, as seen in the Venus music in Wagner's \"Tannhäuser\", Grieg's \"Peer Gynt suite\", and Osmin's aria \"O wie will ich triumphieren\" from Mozart's \"Die Entführung aus dem Serail\".\n\nOrchestral crash cymbals are traditionally used in pairs, each one having a strap set in the bell of the cymbal by which they are held. Such a pair is always known as crash cymbals or plates.\n\nThe sound can be obtained by rubbing their edges together in a sliding movement for a \"sizzle\", striking them against each other in what is called a \"crash\", tapping the edge of one against the body of the other in what is called a \"tap-crash\", scraping the edge of one from the inside of the bell to the edge for a \"scrape\" or \"zischen,\" or shutting the cymbals together and choking the sound in what is called a \"hi-hat chick\" or crush. A skilled player can obtain an enormous dynamic range from such a pair of cymbals. For example, in Beethoven's ninth symphony, the percussionist is employed to first play cymbals at pianissimo, adding a touch of colour rather than loud crash.\n\nCrash cymbals are usually damped by pressing them against the player's body. A composer may write \"laissez vibrer\", \"Let vibrate\" (usually abbreviated l.v.), \"secco\" (dry), or equivalent indications on the score; more usually, the player must judge exactly when to damp the cymbals based on the written duration of clash and the context in which it occurs.\n\nCrash cymbals have traditionally been accompanied by the bass drum playing an identical part. This combination, played loudly, is an effective way to accentuate a note since the two instruments together contribute to both very low and very high frequency ranges and provide a satisfying \"crash-bang-wallop\". In older music the composer sometimes provided just one part for this pair of instruments, writing \"senza piatti\" or \"piatti soli\" () if only one of the instruments is needed. This came from the common practice of only having one percussionist play both instruments, using one cymbal mounted to the shell of the bass drum itself. The player would crash the cymbals with his left hand and use a mallet to strike the bass drum in his right. This method is often employed today in pit orchestras and is called for specifically by composers who desire a certain effect. Stravinsky calls for this in his ballet Petrushka and Mahler calls for this in his Titan Symphony.\n\nThe modern convention is for the instruments to have independent parts. However, in kit drumming, a cymbal crash is still most often accompanied by a simultaneous kick to the bass drum, which provides both a musical effect and a support to the crash stroke.\n\nCrash cymbals evolved into the low-sock and from this to the modern hi-hat. Even in a modern drum kit, they remain paired with the bass drum as the two instruments which are played with the player's feet. However, hi-hat cymbals tend to be heavy with little taper, more similar to a ride cymbal than to a clash cymbal as found in a drum kit, and perform a ride rather than a crash function.\n\nAnother use of cymbals is the suspended cymbal. This instrument takes its name from the traditional method of suspending the cymbal by means of a leather strap or rope, thus allowing the cymbal to vibrate as freely as possible for maximum musical effect. Early jazz drumming pioneers borrowed this style of cymbal mounting during the early 1900s and later drummers further developed this instrument into the mounted horizontal or nearly horizontally mounted \"crash\" cymbals of a modern drum kit, However, most modern drum kits do not employ a leather strap suspension system. Many modern drum kits use a mount with felt or otherwise dampening fabric to act as a barrier to hold the cymbals between metal clamps: thus forming the modern day ride cymbal.\n\nSuspended cymbals can be played with yarn, sponge or cord wrapped mallets. The first known instance of using a sponge-headed mallet on a cymbal is the final chord of Hector Berlioz' Symphonie Fantastique. Composers sometimes specifically request other types of mallets like felt mallets or timpani beaters for different attack and sustain qualities.\n\nSuspended cymbals can produce bright and slicing tones when forcefully struck, and give an eerie transparent \"windy\" sound when played quietly. A tremolo, or roll (played with two mallets alternately striking on opposing sides of the cymbal) can build in volume from almost inaudible to an overwhelming climax in a satisfyingly smooth manner (as in Humperdink's Mother Goose Suite).\n\nThe edge of a suspended cymbal may be hit with shoulder of a drum stick to obtain a sound somewhat akin to that of a pair of clash cymbals. Other methods of playing include scraping a coin or a triangle beater rapidly across the ridges on the top of the cymbal, giving a \"zing\" sound (as some players do in the fourth movement of Dvořák's Symphony No. 9). Other effects that can be used include drawing a cello or bass bow across the edge of the cymbal for a sound not unlike squealing car brakes.\n\nAncient cymbals or tuned cymbals are much more rarely called for. Their timbre is entirely different, more like that of small hand-bells or of the notes of the keyed harmonica. They are not struck full against each other, but by one of their edges, and the note given in by them is higher in proportion as they are thicker and smaller. Berlioz's \"Romeo and Juliet\" calls for two pairs of cymbals, modelled on some old Pompeian instruments no larger than the hand (some are no larger than a crown piece), and tuned to F and B flat. The modern instruments descended from this line are the crotales.\n\nCymbal types include:\n\n", "id": "5671", "title": "Cymbal"}
{"url": "https://en.wikipedia.org/wiki?curid=5672", "text": "Cadmium\n\nCadmium is a chemical element with symbol Cd and atomic number 48. This soft, bluish-white metal is chemically similar to the two other stable metals in group 12, zinc and mercury. Like zinc, it demonstrates oxidation state +2 in most of its compounds, and like mercury, it has a lower melting point than other transition metals. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled \"d\" or \"f\" electron shells in the elemental or common oxidation states. The average concentration of cadmium in Earth's crust is between 0.1 and 0.5 parts per million (ppm). It was discovered in 1817 simultaneously by Stromeyer and Hermann, both in Germany, as an impurity in zinc carbonate.\n\nCadmium occurs as a minor component in most zinc ores and is a byproduct of zinc production. Cadmium was used for a long time as a corrosion-resistant plating on steel, and cadmium compounds are used as red, orange and yellow pigments, to colour glass, and to stabilize plastic. Cadmium use is generally decreasing because it is toxic (it is specifically listed in the European Restriction of Hazardous Substances) and nickel-cadmium batteries have been replaced with nickel-metal hydride and lithium-ion batteries. One of its few new uses is cadmium telluride solar panels.\n\nAlthough cadmium has no known biological function in higher organisms, a cadmium-dependent carbonic anhydrase has been found in marine diatoms.\n\nCadmium is a soft, malleable, ductile, bluish-white divalent metal. It is similar in many respects to zinc but forms complex compounds. Unlike most other metals, cadmium is resistant to corrosion and is used as a protective plate on other metals. As a bulk metal, cadmium is insoluble in water and is not flammable; however, in its powdered form it may burn and release toxic fumes.\n\nAlthough cadmium usually has an oxidation state of +2, it also exists in the +1 state. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. Cadmium burns in air to form brown amorphous cadmium oxide (CdO); the crystalline form of this compound is a dark red which changes color when heated, similar to zinc oxide. Hydrochloric acid, sulfuric acid, and nitric acid dissolve cadmium by forming cadmium chloride (CdCl), cadmium sulfate (CdSO), or cadmium nitrate (Cd(NO)). The oxidation state +1 can be produced by dissolving cadmium in a mixture of cadmium chloride and aluminium chloride, forming the Cd cation, which is similar to the Hg cation in mercury(I) chloride.\nThe structures of many cadmium complexes with nucleobases, amino acids, and vitamins have been determined.\n\nNaturally occurring cadmium is composed of 8 isotopes. Two of them are radioactive, and three are expected to decay but have not done so under laboratory conditions. The two natural radioactive isotopes are Cd (beta decay, half-life is 7.7 × 10 years) and Cd (two-neutrino double beta decay, half-life is 2.9 × 10 years). The other three are Cd, Cd (both double electron capture), and Cd (double beta decay); only lower limits on these half-lives have been determined. At least three isotopes – Cd, Cd, and Cd – are stable. Among the isotopes that do not occur naturally, the most long-lived are Cd with a half-life of 462.6 days, and Cd with a half-life of 53.46 hours. All of the remaining radioactive isotopes have half-lives of less than 2.5 hours, and the majority have half-lives of less than 5 minutes. Cadmium has 8 known meta states, with the most stable being Cd (t = 14.1 years), Cd (t = 44.6 days), and Cd (t = 3.36 hours).\n\nThe known isotopes of cadmium range in atomic mass from 94.950 u (Cd) to 131.946 u (Cd). For isotopes lighter than 112 u, the primary decay mode is electron capture and the dominant decay product is element 47 (silver). Heavier isotopes decay mostly through beta emission producing element 49 (indium).\n\nOne isotope of cadmium, Cd, absorbs neutrons with high selectivity: With very high probability, neutrons with energy below the \"cadmium cut-off\" will be absorbed; those higher than the \"cut-off will be transmitted\". The cadmium cut-off is about 0.5 eV, and neutrons below that level are deemed slow neutrons, distinct from intermediate and fast neutrons.\n\nCadmium is created via the long s-process in low-medium mass stars with masses of 0.6 to 10 solar masses, taking thousands of years. In that process, a silver atom captures a neutron and then undergoes beta decay.\n\nCadmium (Latin \"cadmia\", Greek \"καδμεία\" meaning \"calamine\", a cadmium-bearing mixture of minerals that was named after the Greek mythological character Κάδμος, Cadmus, the founder of Thebes) was discovered simultaneously in 1817 by Friedrich Stromeyer and Karl Samuel Leberecht Hermann, both in Germany, as an impurity in zinc carbonate. Stromeyer found the new element as an impurity in zinc carbonate (calamine), and, for 100 years, Germany remained the only important producer of the metal. The metal was named after the Latin word for calamine, because it was found in this zinc ore. Stromeyer noted that some impure samples of calamine changed color when heated but pure calamine did not. He was persistent in studying these results and eventually isolated cadmium metal by roasting and reducing the sulfide. The potential for cadmium yellow as pigment was recognized in the 1840s, but the lack of cadmium limited this application.\n\nEven though cadmium and its compounds are toxic in certain forms and concentrations, the British Pharmaceutical Codex from 1907 states that cadmium iodide was used as a medication to treat \"enlarged joints, scrofulous glands, and chilblains\".\n\nIn 1907, the International Astronomical Union defined the international ångström in terms of a red cadmium spectral line (1 wavelength = 6438.46963 Å). This was adopted by the 7th General Conference on Weights and Measures in 1927. In 1960, the definitions of both the metre and ångström were changed to use krypton.\n\nAfter the industrial scale production of cadmium started in the 1930s and 1940s, the major application of cadmium was the coating of iron and steel to prevent corrosion; in 1944, 62% and in 1956, 59% of the cadmium in the United States was used for plating. In 1956, 24% of the cadmium in the United States was used for a second application in red, orange and yellow pigments from sulfides and selenides of cadmium. \n\nThe stabilizing effect of cadmium chemicals like the carboxylates cadmium laurate and cadmium stearate on PVC led to an increased use of those compounds in the 1970s and 1980s. The demand for cadmium in pigments, coatings, stabilizers, and alloys declined as a result of environmental and health regulations in the 1980s and 1990s; in 2006, only 7% of to total cadmium consumption was used for plating, and only 10% was used for pigments.\nAt the same time, these decreases in consumption were compensated by a growing demand for cadmium for nickel-cadmium batteries, which accounted for 81% of the cadmium consumption in the United States in 2006.\n\nCadmium makes up about 0.1 mg kg (ppm) of Earth's crust. Compared with the more abundant 65 ppm zinc, cadmium is rare. No significant deposits of cadmium-containing ores are known. Greenockite (CdS), the only cadmium mineral of importance, is nearly always associated with sphalerite (ZnS). This association is caused by geochemical similarity between zinc and cadmium, with no geological process likely to separate them. Thus, cadmium is produced mainly as a byproduct from mining, smelting, and refining sulfidic ores of zinc, and, to a lesser degree, lead and copper. Small amounts of cadmium, about 10% of consumption, are produced from secondary sources, mainly from dust generated by recycling iron and steel scrap. Production in the United States began in 1907, but not until after World War I did cadmium come into wide use. \n\nMetallic cadmium can be found in the Vilyuy River basin in Siberia.\n\nRocks mined for phosphate fertilizers contain varying amounts of cadmium, resulting in a cadmium concentration of as much as 300 mg/kg in the fertilizers and a high cadmium content in agricultural soils. Coal can contain significant amounts of cadmium, which ends up mostly in flue dust.\n\nThe British Geological Survey reports that in 2001, China was the top producer of cadmium with almost one-sixth of the world's production, closely followed by South Korea and Japan.\n\nCadmium is a common impurity in zinc ores, and it is most often isolated during the production of zinc. Some zinc ores concentrates from sulfidic zinc ores contain up to 1.4% of cadmium. In the 1970s, the output of cadmium was 6.5 pounds per ton of zinc. Zinc sulfide ores are roasted in the presence of oxygen, converting the zinc sulfide to the oxide. Zinc metal is produced either by smelting the oxide with carbon or by electrolysis in sulfuric acid. Cadmium is isolated from the zinc metal by vacuum distillation if the zinc is smelted, or cadmium sulfate is precipitated from the electrolysis solution.\n\nCadmium is a common component of electric batteries, pigments, coatings, and electroplating.\n\nIn 2009, 86% of cadmium was used in batteries, predominantly in rechargeable nickel-cadmium batteries. Nickel-cadmium cells have a nominal cell potential of 1.2 V. The cell consists of a positive nickel hydroxide electrode and a negative cadmium electrode plate separated by an alkaline electrolyte (potassium hydroxide). The European Union put a limit on cadmium in electronics in 2004 of 0.01%, with some exceptions, and reduced the limit on cadmium content to 0.002%.\n\nCadmium electroplating, consuming 6% of the global production, is used in the aircraft industry to reduce corrosion of steel components. This coating is passivated by chromate salts. A limitation of cadmium plating is hydrogen embrittlement of high-strength steels from the electroplating process. Therefore, steel parts heat-treated to tensile strength above 1300 MPa (200 ksi) should be coated by an alternative method (such as special low-embrittlement cadmium electroplating processes or physical vapor deposition). \n\nTitanium embrittlement from cadmium-plated tool residues resulted in banishment of those tools (and the implementation of routine tool testing to detect cadmium contamination) in the A-12/SR-71, U-2, and subsequent aircraft programs that use titanium.\n\nCadmium is used in the control rods of nuclear reactors, acting as a very effective \"neutron poison\" to control neutron flux in nuclear fission. When cadmium rods are inserted in the core of a nuclear reactor, cadmium absorbs neutrons preventing them from creating additional fission events, thus controlling the amount of reactivity. The pressurized water reactor designed by Westinghouse Electric Company uses an alloy consisting of 80% silver, 15% indium, and 5% cadmium.\n\nCadmium oxide was used in black and white television phosphors and in the blue and green phosphors of color television cathode ray tubes. Cadmium sulfide (CdS) is used as a photoconductive surface coating for photocopier drums.\nVarious cadmium salts are used in paint pigments, with CdS as a yellow pigment being the most common. Cadmium selenide is a red pigment, commonly called \"cadmium red\". To painters who work with the pigment, cadmium provides the most brilliant and durable yellows, oranges, and reds — so much so that during production, these colors are significantly toned down before they are ground with oils and binders or blended into watercolors, gouaches, acrylics, and other paint and pigment formulations. Because these pigments are potentially toxic, users should use a barrier cream on the hands to prevent absorption through the skin even though the amount of cadmium absorbed into the body through the skin is reported to be less than 1%.\n\nIn PVC, cadmium was used as heat, light, and weathering stabilizers. Currently, cadmium stabilizers have been completely replaced with barium-zinc, calcium-zinc and organo-tin stabilizers. Cadmium is used in many kinds of solder and bearing alloys, because a low coefficient of friction and fatigue resistance. It is also found in some of the lowest-melting alloys, such as Wood's metal.\n\nHelium–cadmium lasers are a common source of blue-ultraviolet laser light. They operate at either 325 or 422 nm in fluorescence microscopes and various laboratory experiments. Cadmium selenide quantum dots emit bright luminescence under UV excitation (He-Cd laser, for example). The color of this luminescence can be green, yellow or red depending on the particle size. Colloidal solutions of those particles are used for imaging of biological tissues and solutions with a fluorescence microscope.\n\nCadmium is a component of some compound semiconductors, such as cadmium sulfide, cadmium selenide, and cadmium telluride, used for light detection and solar cells. HgCdTe is sensitive to infrared light and can be used as an infrared detector, motion detector, or switch in remote control devices.\n\nIn molecular biology, cadmium is used to block voltage-dependent calcium channels from fluxing calcium ions, as well as in hypoxia research to stimulate proteasome-dependent degradation of Hif-1α.\n\nCadmium-selective sensors based on the fluorophore BODIPY have been developed for imaging and sensing of cadmium in cells.\n\nCadmium has no known function in higher organisms, but a cadmium-dependent carbonic anhydrase has been found in some marine diatoms. The diatoms live in environments with very low zinc concentrations and cadmium performs the function normally carried out by zinc in other anhydrases. This was discovered with X-ray absorption fluorescence spectroscopy (XAFS).\n\nThe highest concentration of cadmium is absorbed in the kidneys of humans, and up to about 30 mg of cadmium is commonly inhaled throughout human childhood and adolescence. \n\nCadmium can be used to block calcium channels in chicken neurons.\nAnalytical methods for the determination of cadmium in biological samples have been reviewed.\n\nThe biogeochemistry of cadmium and its release to the environment has been the subject of review, as has the speciation of cadmium in the environment.\n\nThe bioinorganic aspects of cadmium toxicity have been reviewed.\n\nThe most dangerous form of occupational exposure to cadmium is inhalation of fine dust and fumes, or ingestion of highly soluble cadmium compounds. Inhalation of cadmium fumes can result initially in metal fume fever but may progress to chemical pneumonitis, pulmonary edema, and death.\n\nCadmium is also an environmental hazard. Human exposure is primarily from fossil fuel combustion, phosphate fertilizers, natural sources, iron and steel production, cement production and related activities, nonferrous metals production, and municipal solid waste incineration. Bread, root crops, and vegetables also contribute to the cadmium in modern populations. \n\nThere have been a few instances of general population poisoning as the result of long-term exposure to cadmium in contaminated food and water, and research into an estrogen mimicry that may induce breast cancer is ongoing. In the decades leading up to World War II, mining operations contaminated the Jinzū River in Japan with cadmium and traces of other toxic metals. As a consequence, cadmium accumulated in the rice crops along the riverbanks downstream of the mines. Some members of the local agricultural communities consumed the contaminated rice and developed itai-itai disease and renal abnormalities, including proteinuria and glucosuria.\nThe victims of this poisoning were almost exclusively post-menopausal women with low iron and other mineral body stores. Similar general population cadmium exposures in other parts of the world have not resulted in the same health problems because the populations maintained sufficient iron and other mineral levels. Thus, although cadmium is a major factor in the itai-itai disease in Japan, most researchers have concluded that it was one of several factors. Cadmium is one of six substances banned by the European Union's Restriction on Hazardous Substances (RoHS) directive, which regulates hazardous substances in electrical and electronic equipment but allows for certain exemptions and exclusions from the scope of the law.\nThe International Agency for Research on Cancer has classified cadmium and cadmium compounds as carcinogenic to humans. Although occupational exposure to cadmium is linked to lung and prostate cancer, there is still a substantial controversy about the carcinogenicity of cadmium in low environmental exposure. Recent data from epidemiological studies suggest that intake of cadmium through diet associates to higher risk of endometrial, breast and prostate cancer as well as to osteoporosis in humans. A recent study has demonstrated that endometrial tissue is characterized by higher levels of cadmium in current and former smoking females.\n\nCadmium exposure is a risk factor associated with a large number of illnesses including kidney disease, early atherosclerosis, hypertension, and cardiovascular diseases. Although studies show a significant correlation between cadmium exposure and occurrence of disease in human populations, a necessary molecular mechanism has not been identified. One hypothesis holds that cadmium is an endocrine disruptor and some experimental studies have shown that it can interact with different hormonal signaling pathways. For example, cadmium can bind to the estrogen receptor alpha, and affect signal transduction along the estrogen and MAPK signaling pathways at low doses.\n\nTobacco smoking is the most important single source of cadmium exposure in the general population. An estimated 10% of the cadmium content of a cigarette is inhaled through smoking. Absorption of cadmium through the lungs is more effective than through the gut, and as much as 50% of the cadmium inhaled in cigarette smoke may be absorbed.\nOn average, cadmium concentrations in the blood of smokers is 4 times 5 times greater and in the kidney, 2–3 times greater than non-smokers. Despite the high cadmium content in cigarette smoke, there seems to be little exposure to cadmium from passive smoking.\n\nIn a non-smoking population, food is the greatest source of exposure. High quantities of cadmium can be found in crustaceans, mollusks, offal, and algae products. However, grains, vegetables, and starchy roots and tubers are consumed in much greater quantity in the US, and are the source of the greatest dietary exposure. Most plants bio-accumulate metal toxins like Cd, and when composted to form organic fertilizers yield a product which can often contain high amounts (e.g., over 0.5 mg) of metal toxins for every kilo of fertilizer. Fertilizers made from animal dung (e.g., cow dung) or urban waste can contain similar amounts of Cd. The Cd added to the soil from fertilizers (rock phosphates or organic fertilizers) become bio-available and toxic only if the soil pH is low (i.e., acidic soils). Zinc is chemically similar to cadmium and some evidence indicates the presence of Zn ions reduces cadmium toxicity. \n\nZinc, Cu, Ca, and Fe ions, and selenium with vitamin C are used to treat Cd intoxication, though it is not easily reversed.\n\nBecause of the adverse effects of cadmium on the environment and human health, the supply and use of cadmium is restricted in Europe under the REACH Regulation.\n\nThe EFSA Panel on Contaminants in the Food Chain specifies that 2.5 μg/kg body weight is a tolerable weekly intake for humans. The Joint FAO/WHO Expert Committee on Food Additives has declared 7 μg/kg bw to be the provisional tolerable weekly intake level.\n\nThe US Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit (PEL) for cadmium at a time-weighted average (TWA) of 0.005 ppm. The National Institute for Occupational Safety and Health (NIOSH) has not set a recommended exposure limit (REL) and has designated cadmium as a known human carcinogen. The IDLH (immediately dangerous to life and health) level for cadmium is 9 mg/m.\nIn May 2006, a sale of the seats from Arsenal F.C.'s old stadium, Highbury in London, England was cancelled when the seats were discovered to contain trace amounts of cadmium. Reports of high levels of cadmium use in children's jewelry in 2010 led to a US Consumer Product Safety Commission investigation. The U.S. CPSC issued specific recall notices for cadmium content in jewelry sold by Claire's and Wal-Mart stores. \n\nIn June 2010, McDonald's voluntarily recalled more than 12 million promotional \"Shrek Forever After 3D\" Collectable Drinking Glasses because of the cadmium levels in paint pigments on the glassware. The glasses were manufactured by Arc International, of Millville, NJ, USA.\n\n\n\n", "id": "5672", "title": "Cadmium"}
{"url": "https://en.wikipedia.org/wiki?curid=5675", "text": "Curium\n\nCurium is a transuranic radioactive chemical element with symbol Cm and atomic number 96. This element of the actinide series was named after Marie and Pierre Curie – both were known for their research on radioactivity. Curium was first intentionally produced and identified in July 1944 by the group of Glenn T. Seaborg at the University of California, Berkeley. The discovery was kept secret and only released to the public in November 1945. Most curium is produced by bombarding uranium or plutonium with neutrons in nuclear reactors – one tonne of spent nuclear fuel contains about 20 grams of curium.\n\nCurium is a hard, dense, silvery metal with a relatively high melting point and boiling point for an actinide. Whereas it is paramagnetic at ambient conditions, it becomes antiferromagnetic upon cooling, and other magnetic transitions are also observed for many curium compounds. In compounds, curium usually exhibits valence +3 and sometimes +4, and the +3 valence is predominant in solutions. Curium readily oxidizes, and its oxides are a dominant form of this element. It forms strongly fluorescent complexes with various organic compounds, but there is no evidence of its incorporation into bacteria and archaea. When introduced into the human body, curium accumulates in the bones, lungs and liver, where it promotes cancer.\n\nAll known isotopes of curium are radioactive and have a small critical mass for a sustained nuclear chain reaction. They predominantly emit α-particles, and the heat released in this process can serve as a heat source in radioisotope thermoelectric generators, but this application is hindered by the scarcity, high cost, and radioactivity of curium isotopes. Curium is used in production of heavier actinides and of the Pu radionuclide for power sources in artificial pacemakers. It served as the α-source in the alpha particle X-ray spectrometers installed on several space probes, including the Sojourner, Spirit, Opportunity and Curiosity Mars rovers and the Philae lander on comet 67P/Churyumov-Gerasimenko, to analyze the composition and structure of the surface.\n\nAlthough curium had likely been produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Ralph A. James, and Albert Ghiorso. In their experiments, they used a cyclotron.\n\nCurium was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) at the University of Chicago. It was the third transuranium element to be discovered even though it is the fourth in the series – the lighter element americium was unknown at the time.\n\nThe sample was prepared as follows: first plutonium nitrate solution was coated on a platinum foil of about 0.5 cm area, the solution was evaporated and the residue was converted into plutonium(IV) oxide (PuO) by annealing. Following cyclotron irradiation of the oxide, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid, and further separation was carried out by ion exchange to yield a certain isotope of curium. The separation of curium and americium was so painstaking that the Berkeley group initially called those elements \"pandemonium\" (from Greek for \"all demons\" or \"hell\") and \"delirium\" (from Latin for \"madness\").\n\nThe curium-242 isotope was produced in July–August 1944 by bombarding Pu with α-particles to produce curium with the release of a neutron:\n\nCurium-242 was unambiguously identified by the characteristic energy of the α-particles emitted during the decay:\nThe half-life of this alpha decay was first measured as 150 days and then corrected to 162.8 days.\n\nAnother isotope Cm was produced in a similar reaction in March 1945:\nThe half-life of the Cm α-decay was correctly determined as 26.7 days.\n\nThe discovery of curium, as well as americium, in 1944 was closely related to the Manhattan Project, the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children, the Quiz Kids, five days before the official presentation at an American Chemical Society meeting on November 11, 1945, when one of the listeners asked whether any new transuranium element beside plutonium and neptunium had been discovered during the war. The discovery of curium (Cm and Cm), their production and compounds were later patented listing only Seaborg as the inventor.\nThe new element was named after Marie Skłodowska-Curie and her husband Pierre Curie who are noted for discovering radium and for their work in radioactivity. It followed the example of gadolinium, a lanthanide element above curium in the periodic table, which was named after the explorer of the rare earth elements Johan Gadolin:\n\nThe first curium samples were barely visible, and were identified by their radioactivity. Louis Werner and Isadore Perlman created the first substantial sample of 30 µg curium-242 hydroxide at the University of California in 1947 by bombarding americium-241 with neutrons. Macroscopic amounts of curium(III) fluoride were obtained in 1950 by W. W. T. Crane, J. C. Wallmann and B. B. Cunningham. Its magnetic susceptibility was very close to that of GdF providing the first experimental evidence for the +3 valence of curium in its compounds. Curium metal was produced only in 1951 by reduction of CmF with barium.\n\nA synthetic, radioactive element, curium is a hard dense metal with silvery-white appearance and physical and chemical properties resembling those of gadolinium. Its melting point of 1340 °C is significantly higher than that of the previous transuranic elements neptunium (637 °C), plutonium (639 °C) and americium (1173 °C). In comparison, gadolinium melts at 1312 °C. The boiling point of curium is 3110 °C. With a density of 13.52 g/cm, curium is significantly lighter than neptunium (20.45 g/cm) and plutonium (19.8 g/cm), but is heavier than most other metals. Between two crystalline forms of curium, the α-Cm is more stable at ambient conditions. It has a hexagonal symmetry, space group P6/mmc, lattice parameters \"a\" = 365 pm and \"c\" = 1182 pm, and four formula units per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with α-lanthanum. At pressures above 23 GPa, at room temperature, α-Cm transforms into β-Cm, which has a face-centered cubic symmetry, space group Fmm and the lattice constant \"a\" = 493 pm. Upon further compression to 43 GPa, curium transforms to an orthorhombic γ-Cm structure similar to that of α-uranium, with no further transitions observed up to 52 GPa. These three curium phases are also referred to as Cm I, II and III.\n\nCurium has peculiar magnetic properties. Whereas its neighbor element americium shows no deviation from Curie-Weiss paramagnetism in the entire temperature range, α-Cm transforms to an antiferromagnetic state upon cooling to 65–52 K, and β-Cm exhibits a ferrimagnetic transition at about 205 K. Meanwhile, curium pnictides show ferromagnetic transitions upon cooling: CmN and CmAs at 109 K, CmP at 73 K and CmSb at 162 K. Similarly, the lanthanide analogue of curium, gadolinium, as well as its pnictides also show magnetic transitions upon cooling, but the transition character is somewhat different: Gd and GdN become ferromagnetic, and GdP, GdAs and GdSb show antiferromagnetic ordering.\n\nIn accordance with magnetic data, electrical resistivity of curium increases with temperature – about twice between 4 and 60 K – and then remains nearly constant up to room temperature. There is a significant increase in resistivity over time (about 10 µΩ·cm/h) due to self-damage of the crystal lattice by alpha radiation. This makes uncertain the absolute resistivity value for curium (about 125 µΩ·cm). The resistivity of curium is similar to that of gadolinium and of the actinides plutonium and neptunium, but is significantly higher than that of americium, uranium, polonium and thorium.\n\nUnder ultraviolet illumination, curium(III) ions exhibit strong and stable yellow-orange fluorescence with a maximum in the range about 590–640 nm depending on their environment. The fluorescence originates from the transitions from the first excited state D and the ground state S. Analysis of this fluorescence allows monitoring interactions between Cm(III) ions in organic and inorganic complexes.\n\nCurium ions in solution almost exclusively assume the oxidation state of +3, which is the most stable oxidation state for curium. The +4 oxidation state is observed mainly in a few solid phases, such as CmO and CmF. Aqueous curium(IV) is only known in the presence of strong oxidizers such as potassium persulfate, and is easily reduced to curium(III) by radiolysis and even by water. The chemical behavior of curium is different from the actinides thorium and uranium, and is similar to that of americium and many lanthanides. In aqueous solution, the Cm ion is colorless to pale green, and Cm ion is pale yellow. The optical absorption of Cm ions contains three sharp peaks at 375.4, 381.2 and 396.5 nanometers and their strength can be directly converted into the concentration of the ions. The +6 oxidation state has only been reported once in solution in 1978, as the curyl ion (): this was prepared from the beta decay of americium-242 in the americium(V) ion . Failure to obtain Cm(VI) from oxidation of Cm(III) and Cm(IV) may be due to the high Cm/Cm ionization potential and the instability of Cm(V).\n\nCurium ions are hard Lewis acids and thus form most stable complexes with hard bases. The bonding is mostly ionic, with a small covalent component. Curium in its complexes commonly exhibits a 9-fold coordination environment, within a tricapped trigonal prismatic geometry.\n\nAbout 20 radioisotopes and 7 nuclear isomers between Cm and Cm are known for curium, and no stable isotopes. The longest half-lives have been reported for Cm (15.6 million years) and Cm (348,000 years). Other long-lived isotopes are Cm (half-life 8500 years), Cm (8,300 years) and Cm (4,760 years). Curium-250 is unusual in that it predominantly (about 86%) decays via spontaneous fission. The most commonly used curium isotopes are Cm and Cm with the half-lives of 162.8 days and 18.1 years, respectively.\nAll isotopes between Cm and Cm, as well as Cm, undergo a self-sustaining nuclear chain reaction and thus in principle can act as a nuclear fuel in a reactor. As in most transuranic elements, the nuclear fission cross section is especially high for the odd-mass curium isotopes Cm, Cm and Cm. These can be used in thermal-neutron reactors, whereas a mixture of curium isotopes is only suitable for fast breeder reactors since the even-mass isotopes are not fissile in a thermal reactor and accumulate as burn-up increases. The mixed-oxide (MOX) fuel, which is to be used in power reactors, should contain little or no curium because the neutron activation of Cm will create californium. This is strong neutron emitter, and would pollute the back end of the fuel cycle and increase the dose to reactor personnel. Hence, if the minor actinides are to be used as fuel in a thermal neutron reactor, the curium should be excluded from the fuel or placed in special fuel rods where it is the only actinide present.\n\nThe table to the right lists the critical masses for curium isotopes for a sphere, without a moderator and reflector. With a metal reflector (30 cm of steel), the critical masses of the odd isotopes are about 3–4 kg. When using water (thickness ~20–30 cm) as the reflector, the critical mass can be as small as 59 gram for Cm, 155 gram for Cm and 1550 gram for Cm. There is a significant uncertainty in these critical mass values. Whereas it is usually of the order 20%, the values for Cm and Cm were listed as large as 371 kg and 70.1 kg, respectively, by some research groups.\n\nCurrently, curium is not used as a nuclear fuel owing to its low availability and high price. Cm and Cm have a very small critical mass and therefore could be used in portable nuclear weapons, but none have been reported thus far. Curium-243 is not suitable for this purpose because of its short half-life and strong α emission which would result in excessive heat. Curium-247 would be highly suitable, having a half-life 647 times that of plutonium-239.\n\nThe longest-lived isotope of curium, Cm, has a half-life of 15.6 million years. Therefore, any primordial curium, that is curium present on the Earth during its formation, should have decayed by now, although some of it would be detectable as an extinct radionuclide as an excess of its nearly stable daughter U. Curium is produced artificially, in small quantities for research purposes. Furthermore, it occurs in spent nuclear fuel. Curium is present in nature in certain areas used for the atmospheric nuclear weapons tests, which were conducted between 1945 and 1980. So the analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), beside einsteinium, fermium, plutonium and americium also revealed isotopes of berkelium, californium and curium, in particular Cm, Cm and smaller quantities of Cm, Cm and Cm. For reasons of military secrecy, this result was published only in 1956.\n\nAtmospheric curium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 4,000 times higher concentration of curium at the sandy soil particles than in water present in the soil pores. An even higher ratio of about 18,000 was measured in loam soils.\n\nThe transuranic elements from americium to fermium, including curium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\n\nCurium is produced in small quantities in nuclear reactors, and by now only kilograms of it have been accumulated for the Cm and Cm and grams or even milligrams for heavier isotopes. This explains the high price of curium, which has been quoted at 160–185 USD per milligram, with a more recent estimate at 2,000 USD/g for Cm and 170 USD/g for Cm. In nuclear reactors, curium is formed from U in a series of nuclear reactions. In the first chain, U captures a neutron and converts into U, which via β decay transforms into Np and Pu.\n\nFurther neutron capture followed by β-decay produces the Am isotope of americium which further converts into Cm:\nFor research purposes, curium is obtained by irradiating not uranium but plutonium, which is available in large amounts from spent nuclear fuel. Much higher neutron flux is used for the irradiation that results in a different reaction chain and formation of Cm:\nCurium-244 decays into Pu by emission of alpha particle, but it also absorbs neutrons resulting in a small amount of heavier curium isotopes. Among those, Cm and Cm are popular in scientific research because of their long half-lives. However, the production rate of Cm in thermal neutron reactors is relatively low because of it is prone to undergo fission induced by thermal neutrons. Synthesis of Cm via neutron absorption is also rather unlikely because of the short half-life of the intermediate product Cm (64 min), which converts by β decay to the berkelium isotope Bk.\n\nThe above cascade of (n,γ) reactions produces a mixture of different curium isotopes. Their post-synthesis separation is cumbersome, and therefore a selective synthesis is desired. Curium-248 is favored for research purposes because of its long half-life. The most efficient preparation method of this isotope is via α-decay of the californium isotope Cf, which is available in relatively large quantities due to its long half-life (2.65 years). About 35–50 mg of Cm is being produced by this method every year. The associated reaction produces Cm with isotopic purity of 97%.\n\nAnother interesting for research isotope Cm can be obtained from the α-decay of Cf, and the latter isotope is produced in minute quantities from the β-decay of the berkelium isotope Bk.\n\nMost synthesis routines yield a mixture of different actinide isotopes as oxides, from which a certain isotope of curium needs to be separated. An example procedure could be to dissolve spent reactor fuel (e.g. MOX fuel) in nitric acid, and remove the bulk of the uranium and plutonium using a PUREX (Plutonium – URanium EXtraction) type extraction with tributyl phosphate in a hydrocarbon. The lanthanides and the remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction to give, after stripping, a mixture of trivalent actinides and lanthanides. A curium compound is then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. \"Bis\"-triazinyl bipyridine complex has been recently proposed as such reagent which is highly selective to curium. Separation of curium from a very similar americium can also be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone at elevated temperature. Both americium and curium are present in solutions mostly in the +3 valence state; whereas americium oxidizes to soluble Am(IV) complexes, curium remains unchanged and can thus be isolated by repeated centrifugation.\n\nMetallic curium is obtained by reduction of its compounds. Initially, curium(III) fluoride was used for this purpose. The reaction was conducted in the environment free from water and oxygen, in the apparatus made of tantalum and tungsten, using elemental barium or lithium as reducing agents.\n\nAnother possibility is the reduction of curium(IV) oxide using a magnesium-zinc alloy in a melt of magnesium chloride and magnesium fluoride.\n\nCurium readily reacts with oxygen forming mostly CmO and CmO oxides, but the divalent oxide CmO is also known. Black CmO can be obtained by burning curium oxalate (Cm(CO)), nitrate (Cm(NO)) or hydroxide in pure oxygen. Upon heating to 600–650 °C in vacuum (about 0.01 Pa), it transforms into the whitish CmO:\n\nAlternatively, CmO can be obtained by reducing CmO with molecular hydrogen:\n\nFurthermore, a number of ternary oxides of the type M(II)CmO are known, where M stands for a divalent metal, such as barium.\n\nThermal oxidation of trace quantities of curium hydride (CmH) has been reported to produce a volatile form of CmO and the volatile trioxide CmO, one of the two known examples of the very rare +6 state for curium. Another observed species was reported to behave similarly to plutonium tetroxide and was tentatively characterized as CmO, with curium in the extremely rare +8 state; however, new experiments seem to indicate that CmO does not exist.\n\nThe colorless curium(III) fluoride (CmF) can be produced by introducing fluoride ions into curium(III)-containing solutions. The brown tetravalent curium(IV) fluoride (CmF) on the other hand is only obtained by reacting curium(III) fluoride with molecular fluorine:\n\nA series of ternary fluorides are known of the form ACmF, where A stands for alkali metal.\n\nThe colorless curium(III) chloride (CmCl) is produced in the reaction of curium(III) hydroxide (Cm(OH)) with anhydrous hydrogen chloride gas. It can further be converted into other halides, such as curium(III) bromide (colorless to light green) and curium(III) iodide (colorless), by reacting it with the ammonia salt of the corresponding halide at elevated temperature of about 400–450 °C:\n\nAn alternative procedure is heating curium oxide to about 600 °C with the corresponding acid (such as hydrobromic for curium bromide). Vapor phase hydrolysis of curium(III) chloride results in curium oxychloride:\n\nSulfides, selenides and tellurides of curium have been obtained by treating curium with gaseous sulfur, selenium or tellurium in vacuum at elevated temperature. The pnictides of curium of the type CmX are known for the elements nitrogen, phosphorus, arsenic and antimony. They can be prepared by reacting either curium(III) hydride (CmH) or metallic curium with these elements at elevated temperatures.\n\nOrganometallic complexes analogous to uranocene are known also for other actinides, such as thorium, protactinium, neptunium, plutonium and americium. Molecular orbital theory predicts a stable \"curocene\" complex (η-CH)Cm, but it has not been reported experimentally yet.\n\nFormation of the complexes of the type Cm(n-CH-BTP), where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-CH-BTP and Cm ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with curium and therefore are useful in its selective separation from lanthanides and another actinides. Dissolved Cm ions bind with many organic compounds, such as hydroxamic acid, urea, fluorescein and adenosine triphosphate. Many of these compounds are related to biological activity of various microorganisms. The resulting complexes exhibit strong yellow-orange emission under UV light excitation, which is convenient not only for their detection, but also for studying the interactions between the Cm ion and the ligands via changes in the half-life (of the order ~0.1 ms) and spectrum of the fluorescence.\n\nCurium has no biological significance. There are a few reports on biosorption of Cm by bacteria and archaea, however no evidence for incorporation of curium into them.\n\nCurium is one of the most radioactive isolable elements. Its two most common isotopes Cm and Cm are strong alpha emitters (energy 6 MeV); they have relatively short half-lives of 162.8 days and 18.1 years, and produce as much as 120 W/g and 3 W/g of thermal energy, respectively. Therefore, curium can be used in its common oxide form in radioisotope thermoelectric generators like those in spacecraft. This application has been studied for the Cm isotope, while Cm was abandoned due to its prohibitive price of around 2000 USD/g. Curium-243 with a ~30 year half-life and good energy yield of ~1.6 W/g could make for a suitable fuel, but it produces significant amounts of harmful gamma and beta radiation from radioactive decay products. Though as an α-emitter, Cm requires a much thinner radiation protection shielding, it has a high spontaneous fission rate, and thus the neutron and gamma radiation rate are relatively strong. As compared to a competing thermoelectric generator isotope such as Pu, Cm emits a 500 time greater fluence of neutrons, and its higher gamma emission requires a shield that is 20 times thicker — about 2 inches of lead for a 1 kW source, as compared to 0.1 in for Pu. Therefore, this application of curium is currently considered impractical.\n\nA more promising application of Cm is to produce Pu, a more suitable radioisotope for thermoelectric generators such as in cardiac pacemakers. The alternative routes to Pu use the (n,γ) reaction of Np, or the deuteron bombardment of uranium, which both always produce Pu as an undesired by-product — since the latter decays to U with strong gamma emission. Curium is also a common starting material for the production of higher transuranic elements and transactinides. Thus, bombardment of Cm with oxygen (O) or magnesium (Mg) yielded certain isotopes of seaborgium (Sg) and hassium (Hs and Hs). Californium was discovered when a microgram-sized target of curium-242 was irradiated with 35 MeV alpha particles using the cyclotron at Berkeley:\nOnly about 5,000 atoms of californium were produced in this experiment.\n\nThe most practical application of Cm — though rather limited in total volume — is as α-particle source in the alpha particle X-ray spectrometers (APXS). These instruments were installed on the Sojourner, Mars, Mars 96, Mars Exploration Rovers and Philae comet lander, as well as the Mars Science Laboratory to analyze the composition and structure of the rocks on the surface of planet Mars. APXS was also used in the Surveyor 5–7 moon probes but with a Cm source.\n\nAn elaborated APXS setup is equipped with a sensor head containing six curium sources having the total radioactive decay rate of several tens of millicuries (roughly a gigabecquerel). The sources are collimated on the sample, and the energy spectra of the alpha particles and protons scattered from the sample are analyzed (the proton analysis is implemented only in some spectrometers). These spectra contain quantitative information on all major elements in the samples except for hydrogen, helium and lithium.\n\nOwing to its high radioactivity, curium and its compounds must be handled in appropriate laboratories under special arrangements. Whereas curium itself mostly emits α-particles which are absorbed by thin layers of common materials, some of its decay products emit significant fractions of beta and gamma radiation, which require a more elaborate protection. If consumed, curium is excreted within a few days and only 0.05% is absorbed in the blood. From there, about 45% goes to the liver, 45% to the bones, and the remaining 10% is excreted. In the bone, curium accumulates on the inside of the interfaces to the bone marrow and does not significantly redistribute with time; its radiation destroys bone marrow and thus stops red blood cell creation. The biological half-life of curium is about 20 years in the liver and 50 years in the bones. Curium is absorbed in the body much more strongly via inhalation, and the allowed total dose of Cm in soluble form is 0.3 μC. Intravenous injection of Cm and Cm containing solutions to rats increased the incidence of bone tumor, and inhalation promoted pulmonary and liver cancer.\n\nCurium isotopes are inevitably present in spent nuclear fuel with a concentration of about 20 g/tonne. Among them, the Cm–Cm isotopes have decay times of thousands of years and need to be removed to neutralize the fuel for disposal. The associated procedure involves several steps, where curium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure, nuclear transmutation, while well documented for other elements, is still being developed for curium.\n\n\n", "id": "5675", "title": "Curium"}
{"url": "https://en.wikipedia.org/wiki?curid=5676", "text": "Californium\n\nCalifornium is a radioactive metallic chemical element with symbol Cf and atomic number 98. The element was first made in 1950 at the University of California Radiation Laboratory in Berkeley, by bombarding curium with alpha particles (helium-4 ions). It is an actinide element, the sixth transuranium element to be synthesized, and has the second-highest atomic mass of all the elements that have been produced in amounts large enough to see with the unaided eye (after einsteinium). The element was named after the university and the state of California.\n\nTwo crystalline forms exist for californium under normal pressure: one above and one below . A third form exists at high pressure. Californium slowly tarnishes in air at room temperature. Compounds of californium are dominated by a chemical form of the element, designated californium(III), that can participate in three chemical bonds. The most stable of californium's twenty known isotopes is californium-251, which has a half-life of 898 years. This short half-life means the element is not found in significant quantities in the Earth's crust. Californium-252, with a half-life of about 2.64 years, is the most common isotope used and is produced at the Oak Ridge National Laboratory in the United States and the Research Institute of Atomic Reactors in Russia.\n\nCalifornium is one of the few transuranium elements that have practical applications. Most of these applications exploit the property of certain isotopes of californium to emit neutrons. For example, californium can be used to help start up nuclear reactors, and it is employed as a source of neutrons when studying materials with neutron diffraction and neutron spectroscopy. Californium can also be used in nuclear synthesis of higher mass elements; oganesson (element 118) was synthesized by bombarding californium-249 atoms with calcium-48 ions. Users of californium must take into account radiological concerns and the element's ability to disrupt the formation of red blood cells by bioaccumulating in skeletal tissue.\n\nCalifornium is a silvery white actinide metal with a melting point of and an estimated boiling point of . The pure metal is malleable and is easily cut with a razor blade. Californium metal starts to vaporize above when exposed to a vacuum. Below 51 K (−220 °C) californium metal is either ferromagnetic or ferrimagnetic (it acts like a magnet), between 48 and 66 K it is antiferromagnetic (an intermediate state), and above it is paramagnetic (external magnetic fields can make it magnetic). It forms alloys with lanthanide metals but little is known about them.\n\nThe element has two crystalline forms under 1 standard atmosphere of pressure: A double-hexagonal close-packed form dubbed alpha (α) and a face-centered cubic form designated beta (β). The α form exists below 600–800 °C with a density of 15.10 g/cm and the β form exists above 600–800 °C with a density of 8.74 g/cm. At 48 GPa of pressure the β form changes into an orthorhombic crystal system due to delocalization of the atom's 5f electrons, which frees them to bond.\n\nThe bulk modulus of a material is a measure of its resistance to uniform pressure. Californium's bulk modulus is 50 ± 5 GPa, which is similar to trivalent lanthanide metals but smaller than more familiar metals, such as aluminium (70 GPa).\n\nCalifornium exhibits oxidation states of 4, 3, or 2. It typically forms eight or nine bonds to surrounding atoms or ions. Its chemical properties are predicted to be similar to other primarily 3+ valence actinide elements and the element dysprosium, which is the lanthanide above californium in the periodic table. The element slowly tarnishes in air at room temperature, with the rate increasing when moisture is added. Californium reacts when heated with hydrogen, nitrogen, or a chalcogen (oxygen family element); reactions with dry hydrogen and aqueous mineral acids are rapid. \n\nCalifornium is only water-soluble as the californium(III) cation. Attempts to reduce or oxidize the +3 ion in solution have failed. The element forms a water-soluble chloride, nitrate, perchlorate, and sulfate and is precipitated as a fluoride, oxalate, or hydroxide. Californium is the heaviest actinide to exhibit covalent properties, as is observed in the californium borate.\n\nTwenty radioisotopes of californium have been characterized, the most stable being californium-251 with a half-life of 898 years, californium-249 with a half-life of 351 years, californium-250 with a half-life of 13.08 years, and californium-252 with a half-life of 2.645 years. All the remaining isotopes have half-lives shorter than a year, and the majority of these have half-lives shorter than 20 minutes. The isotopes of californium range in mass number from 237 to 256.\n\nCalifornium-249 is formed from the beta decay of berkelium-249, and most other californium isotopes are made by subjecting berkelium to intense neutron radiation in a nuclear reactor. Although californium-251 has the longest half-life, its production yield is only 10% due to its tendency to collect neutrons (high neutron capture) and its tendency to interact with other particles (high neutron cross-section).\n\nCalifornium-252 is a very strong neutron emitter, which makes it extremely radioactive and harmful. Californium-252 undergoes alpha decay 96.9% of the time to form curium-248 while the remaining 3.1% of decays are spontaneous fission. One microgram (µg) of californium-252 emits 2.3 million neutrons per second, an average of 3.7 neutrons per spontaneous fission. Most of the other isotopes of californium decay to isotopes of curium (atomic number 96) via alpha decay.\n\nCalifornium was first synthesized at the University of California Radiation Laboratory in Berkeley, by the physics researchers Stanley G. Thompson, Kenneth Street, Jr., Albert Ghiorso, and Glenn T. Seaborg on or about February 9, 1950. It was the sixth transuranium element to be discovered; the team announced its discovery on March 17, 1950.\n\nTo produce californium, a microgram-sized target of curium-242 () was bombarded with 35 MeV-alpha particles () in the cyclotron at Berkeley, which produced californium-245 () plus one free neutron ().\nOnly about 5,000 atoms of californium were produced in this experiment, and these atoms had a half-life of 44 minutes.\n\nThe discoverers named the new element after the university and the state. This was a break from the convention used for elements 95 to 97, which drew inspiration from how the elements directly above them in the periodic table were named. However, the element directly above element 98 in the periodic table, dysprosium, has a name that simply means \"hard to get at\" so the researchers decided to set aside the informal naming convention. They added that \"the best we can do is to point out [that] ... searchers a century ago found it difficult to get to California.\"\n\nWeighable quantities of californium were first produced by the irradiation of plutonium targets at the Materials Testing Reactor at the National Reactor Testing Station in eastern Idaho; and these findings were reported in 1954. The high spontaneous fission rate of californium-252 was observed in these samples. The first experiment with californium in concentrated form occurred in 1958. The isotopes californium-249 to californium-252 were isolated that same year from a sample of plutonium-239 that had been irradiated with neutrons in a nuclear reactor for five years. Two years later, in 1960, Burris Cunningham and James Wallman of the Lawrence Radiation Laboratory of the University of California created the first californium compounds—californium trichloride, californium oxychloride, and californium oxide—by treating californium with steam and hydrochloric acid.\n\nThe High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, started producing small batches of californium in the 1960s. By 1995, the HFIR nominally produced of californium annually. Plutonium supplied by the United Kingdom to the United States under the 1958 US-UK Mutual Defence Agreement was used for californium production.\n\nThe Atomic Energy Commission sold californium-252 to industrial and academic customers in the early 1970s for $10 per microgram and an average of of californium-252 were shipped each year from 1970 to 1990. Californium metal was first prepared in 1974 by Haire and Baybarz who reduced californium(III) oxide with lanthanum metal to obtain microgram amounts of sub-micrometer thick films.\n\nTraces of californium can be found near facilities that use the element in mineral prospecting and in medical treatments. The element is fairly insoluble in water, but it adheres well to ordinary soil; and concentrations of it in the soil can be 500 times higher than in the water surrounding the soil particles.\n\nFallout from atmospheric nuclear testing prior to 1980 contributed a small amount of californium to the environment. Californium isotopes with mass numbers 249, 252, 253, and 254 have been observed in the radioactive dust collected from the air after a nuclear explosion. Californium is not a major radionuclide at United States Department of Energy legacy sites since it was not produced in large quantities.\n\nCalifornium was once believed to be produced in supernovas, as their decay matches the 60-day half-life of Cf. However, subsequent studies failed to demonstrate any californium spectra, and supernova light curves are now thought to follow the decay of nickel-56.\n\nThe transuranic elements from americium to fermium, including californium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\n\nCalifornium is produced in nuclear reactors and particle accelerators. Californium-250 is made by bombarding berkelium-249 () with neutrons, forming berkelium-250 () via neutron capture (n,γ) which, in turn, quickly beta decays (β) to californium-250 () in the following reaction:\nBombardment of californium-250 with neutrons produces californium-251 and californium-252.\n\nProlonged irradiation of americium, curium, and plutonium with neutrons produces milligram amounts of californium-252 and microgram amounts of californium-249. As of 2006, curium isotopes 244 to 248 are irradiated by neutrons in special reactors to produce primarily californium-252 with lesser amounts of isotopes 249 to 255.\n\nMicrogram quantities of californium-252 are available for commercial use through the U.S. Nuclear Regulatory Commission. Only two sites produce californium-252 – the Oak Ridge National Laboratory in the United States, and the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. As of 2003, the two sites produce 0.25 grams and 0.025 grams of californium-252 per year, respectively.\n\nThree californium isotopes with significant half-lives are produced, requiring a total of 15 neutron captures by uranium-238 without nuclear fission or alpha decay occurring during the process. Californium-253 is at the end of a production chain that starts with uranium-238, includes several isotopes of plutonium, americium, curium, berkelium, and the californium isotopes 249 to 253 (see diagram).\n\nCalifornium-252 has a number of specialized applications as a strong neutron emitter, and each microgram of fresh californium produces 139 million neutrons per minute. This property makes californium useful as a neutron startup source for some nuclear reactors and as a portable (non-reactor based) neutron source for neutron activation analysis to detect trace amounts of elements in samples. Neutrons from californium are employed as a treatment of certain cervical and brain cancers where other radiation therapy is ineffective. It has been used in educational applications since 1969 when the Georgia Institute of Technology received a loan of 119 µg of californium-252 from the Savannah River Plant. It is also used with online elemental coal analyzers and bulk material analyzers in the coal and cement industries.\n\nNeutron penetration into materials makes californium useful in detection instruments such as fuel rod scanners; neutron radiography of aircraft and weapons components to detect corrosion, bad welds, cracks and trapped moisture; and in portable metal detectors. Neutron moisture gauges use californium-252 to find water and petroleum layers in oil wells, as a portable neutron source for gold and silver prospecting for on-the-spot analysis, and to detect ground water movement. The major uses of californium-252 in 1982 were, in order of use, reactor start-up (48.3%), fuel rod scanning (25.3%), and activation analysis (19.4%). By 1994 most californium-252 was used in neutron radiography (77.4%), with fuel rod scanning (12.1%) and reactor start-up (6.9%) as important but distant secondary uses.\n\nCalifornium-251 has a very small calculated critical mass of about , high lethality, and a relatively short period of toxic environmental irradiation. The low critical mass of californium led to some exaggerated claims about possible uses for the element.\n\nIn October 2006, researchers announced that three atoms of oganesson (element 118) had been identified at the Joint Institute for Nuclear Research in Dubna, Russia, as the product of bombardment of californium-249 with calcium-48, making it the heaviest element ever synthesized. The target for this experiment contained about 10 mg of californium-249 deposited on a titanium foil of 32 cm area. Californium has also been used to produce other transuranium elements; for example, element 103 (later named lawrencium) was first synthesized in 1961 by bombarding californium with boron nuclei.\n\nCalifornium that bioaccumulates in skeletal tissue releases radiation that disrupts the body's ability to form red blood cells. The element plays no natural biological role in any organism due to its intense radioactivity and low concentration in the environment.\n\nCalifornium can enter the body from ingesting contaminated food or drinks or by breathing air with suspended particles of the element. Once in the body, only 0.05% of the californium will reach the bloodstream. About 65% of that californium will be deposited in the skeleton, 25% in the liver, and the rest in other organs, or excreted, mainly in urine. Half of the californium deposited in the skeleton and liver are gone in 50 and 20 years, respectively. Californium in the skeleton adheres to bone surfaces before slowly migrating throughout the bone.\n\nThe element is most dangerous if taken into the body. In addition, californium-249 and californium-251 can cause tissue damage externally, through gamma ray emission. Ionizing radiation emitted by californium on bone and in the liver can cause cancer.\n\n\n", "id": "5676", "title": "Californium"}
{"url": "https://en.wikipedia.org/wiki?curid=5679", "text": "Christian Social Union in Bavaria\n\nThe Christian Social Union in Bavaria () is a Christian democratic and conservative political party in Germany. The CSU operates only in Bavaria, while its larger counterpart, the Christian Democratic Union (CDU), operates in the other fifteen states of Germany. The CSU has 56 seats in the Bundestag making it the smallest of the five parties represented.\n\nThe CSU was founded in some ways as a continuation of the Weimar-era Catholic Bavarian People's Party (BVP). At the federal level, the CSU forms a common 'CDU/CSU' faction in the Bundestag with the CDU, which is frequently referred to as the Union Faction (\"die Unionsfraktion\"). Until the 2013 federal election, the CDU/CSU formed federal government in coalition with the Free Democratic Party (FDP).\n\nIn the state of Bavaria, the CSU has governed alone with an absolute majority since 1966 until 2008-2013, when it was the leading party of a coalition government with the FDP. The CSU differs from their partners, the CDU, by being somewhat more conservative in social matters while the CSU is economically a bit more pro-interventionist.\nThe CSU is a member of the European People's Party (EPP) and the International Democrat Union. The CSU currently has three ministers in the cabinet of Germany of the federal government in Berlin, while party leader Horst Seehofer serves as Minister-President of Bavaria, a position that CSU representatives have held from 1946 to 1954 and again since 1957.\n\nFranz Josef Strauß (1915–1988) had left behind the strongest legacy as a leader of the party, having led the party from 1961 until his death in 1988. His political career in the federal cabinet was unique in that he had served four ministerial posts in the years between 1953 and 1969. From 1978 until his death in 1988, Strauß served as the Minister-president of Bavaria. Strauß was the first leader of the CSU to be a candidate for the German chancellery, in 1980. In the 1980 federal election Strauß ran against the incumbent Helmut Schmidt of the Social Democratic Party of Germany (SPD), but lost thereafter, as the SPD and the Free Democratic Party (FDP) managed to secure an absolute majority together, forming a Social-liberal coalition.\n\nThe CSU has led the Bavarian state government since it came into existence in 1946, save from 1950 to 1953 when the Bavaria Party formed a state government in coalition with the state branches of the SPD and FDP. Before the 2008 elections in Bavaria, the CSU perennially achieved absolute majorities at the state level by itself. This level of dominance is unique among Germany's 16 states. Edmund Stoiber took over the CSU leadership in 1999. He ran for Chancellor of Germany in 2002, but his preferred CDU/CSU–FDP coalition lost against the SPD candidate Gerhard Schröder's SPD-Green alliance.\n\nIn the 2003 Bavarian state election, the CSU won 60.7% of the vote and 124 of 180 seats in the state parliament. This was the first time any party had won a 2/3 majority in a German state parliament. \"The Economist\" later suggested that this exceptional result was due to a backlash against Schröder's government in Berlin. The CSU's popularity declined in subsequent years. Stoiber stepped down from the posts of Minister-President and CSU chairman in September 2007. A year later, the CSU lost its majority in the 2008 Bavarian state election, with its vote share dropping from 60.7% to 43.4%. The CSU remained in power by forming a coalition with the Free Democratic Party. In the 2009 general election, the CSU received only 42.5% of the vote in Bavaria in the 2009 election, which constitutes its weakest showing in the party's history.\n\nThe CSU made gains in the 2013 Bavarian state election and the 2013 federal election, which were held a week apart in September 2013. The CSU regained their majority in the Bavarian Landtag and remained in government in Berlin. They have three ministers in Angela Merkel's current cabinet: Christian Schmidt (Minister of Food and Agriculture), Alexander Dobrindt (Minister of Transport and Digital Infrastructure) and Gerd Müller (Minister for Economic Cooperation and Development).\n\nThe CSU is the sister party of the Christian Democratic Union (CDU). Together, they are called 'The Union'. The CSU operates only within Bavaria, and the CDU operates in all other states, but not Bavaria. While virtually independent, at the federal level, the parties form a common CDU/CSU faction. No Chancellor has ever come from the CSU, although Strauß and Edmund Stoiber were CDU/CSU candidates for Chancellor in the 1980 federal election and the 2002 federal election, respectively, which were both won by the Social Democratic Party of Germany (SPD). Below the federal level, the parties are entirely independent.\n\nSince its formation, the CSU has been more conservative than the CDU. The CSU and the state of Bavaria decided not to sign the \"Grundgesetz\" of the Federal Republic of Germany, as they could not agree with the division of Germany into two states, after World War 2. Although Bavaria has a separate police and justice system (distinctive and non-federal), the CSU has actively participated in all political affairs of the German Parliament, the German Government, the German Bundesrat, the parliamentary elections of the German President, the European Parliament, and meetings with Gorbachev in Russia.\n\nThe CSU has contributed eleven of the twelve Ministers-President of Bavaria since 1945, with only Wilhelm Hoegner (1945–46, 1954–57) of the SPD also holding the office.\n\nSee: List of Bavarian Christian Social Union politicians\n\n\n", "id": "5679", "title": "Christian Social Union in Bavaria"}
{"url": "https://en.wikipedia.org/wiki?curid=5681", "text": "Corporate title\n\nCorporate titles or business titles are given to company and organization officials to show what duties and responsibilities they have in the organization. Such titles are used in publicly and privately held for-profit corporations. In addition, many non-profit organizations, educational institutions, partnerships, and sole proprietorships also confer corporate titles.\n\nThere are considerable variations in the composition and responsibilities of corporate titles.\n\nWithin the corporate office or corporate center of a company, some companies have a Chairman and Chief Executive Officer (CEO) as the top-ranking executive, while the number two is the President and Chief Operating Officer (COO); other companies have a President and CEO but no official deputy. Typically, senior managers are \"higher\" than vice presidents, although many times a senior officer may also hold a vice president title, such as Executive Vice President and Chief Financial Officer (CFO). The board of directors is technically not part of management itself, although its chairman may be considered part of the corporate office if he or she is an executive chairman.\n\nA corporation often consists of different businesses, whose senior executives report directly to the CEO or COO. If organized as a division then the top manager is often known as an Executive Vice President (EVP) (for example, Bobby Abraham, who is Global Head of Finance Shared Services at Vodafone Group Plc or Todd Bradley, who used to head the Personal Systems Group in Hewlett-Packard). If that business is a subsidiary which has considerably more independence, then the title might be chairman and CEO.\n\nIn many countries, particularly in Europe and Asia, there is a separate executive board for day-to-day business and supervisory board (elected by shareholders) for control purposes. In these countries, the CEO presides over the executive board and the chairman presides over the supervisory board, and these two roles will always be held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board. This seemingly allows for clear lines of authority. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service.\n\nIn the United States and other countries that follow a single-board corporate structure, the board of directors (elected by the shareholders) is often equivalent to the European/Asian supervisory board, while the functions of the executive board may be vested either in the board of directors or in a separate committee, which may be called an operating committee (J.P. Morgan Chase), management committee (Goldman Sachs), executive committee (Lehman Brothers), or executive council (Hewlett-Packard), or executive board (HeiG) composed of the division/subsidiary heads and senior officers that report directly to the CEO.\n\nState laws in the United States traditionally required certain positions to be created within every corporation, such as president, secretary and treasurer. Today, the approach under the Model Business Corporation Act, which is employed in many states, is to grant companies discretion in determining which titles to have, with the only mandated organ being the board of directors.\n\nSome states that do not employ the MBCA continue to require that certain offices be established. Under the law of Delaware, where most large US corporations are established, stock certificates must be signed by two officers with titles specified by law (e.g. a president and secretary or a president and treasurer). Every corporation incorporated in California must have a chairman of the board or a president (or both), as well as a secretary and a chief financial officer.\n\nLLC-structured companies are generally run directly by their members, but the members can agree to appoint officers such as a CEO, or to appoint \"managers\" to operate the company.\n\nAmerican companies are generally led by a chief executive officer (CEO). In some companies, the CEO also has the title of president. In other companies, the president is a different person, and the primary duties of the two positions are defined in the company's bylaws (or the laws of the governing legal jurisdiction). Many companies also have a chief financial officer (CFO), chief operating officer (COO) and other senior positions as necessary such as chief information officer, chief sales officer, etc. that report to the president and CEO as \"senior vice presidents\" of the company. The next level, which are not executive positions, is middle management and may be called vice president, director or manager, depending on the size and required managerial depth of the company.\n\nIn British English, the title of managing director is generally synonymous with that of chief executive officer. Managing directors do not have any particular authority under the Companies Act in the UK, but do have implied authority based on the general understanding of what their position entails, as well as any authority expressly delegated by the board of directors.\n\nIn Japan, corporate titles are roughly standardized across companies and organizations; although there is variation from company to company, corporate titles within a company are always consistent, and the large companies in Japan generally follow the same outline. These titles are the formal titles that are used on business cards. Korean corporate titles are similar to those of Japan, as the South Korean corporate structure had been influenced by the Japanese model.\n\nLegally, Japanese and Korean companies are only required to have a board of directors with at least one representative director. In Japanese, a company director is called a \"torishimariyaku\" (取締役) and the representative director is called a \"daihyo torishimariyaku\" (代表取締役). The equivalent Korean titles are \"isa\" (이사, 理事) and \"daepyo-isa\" (대표이사, 代表理事). These titles are often combined with lower titles, e.g. \"senmu torishimariyaku\" or \"jomu torishimariyaku\" for Japanese executives who are also board members. Most Japanese companies also have statutory auditors, who operate alongside the board of directors in a supervisory role.\n\nThe typical structure of executive titles in large companies includes the following:\n\nThe top management group, comprising \"jomu\"/\"sangmu\" and above, is often referred to collectively as \"senior management\" (幹部 or 重役; \"kambu\" or \"juyaku\" in Japanese; \"ganbu\" or \"jungyŏk\" in Korean).\n\nSome Japanese and Korean companies have also adopted American-style titles, but these are not yet widespread and their usage varies. For example, although there is a Korean translation for chief operating officer (\"최고운영책임자, choego unyŏng chaegimja\"), not many companies have yet adopted it with an exception of a few multi-national companies such as Samsung and CJ, while the chief financial officer title is often used alongside other titles such as \"bu-sajang\" (SEVP) or \"Jŏnmu\" (EVP).\n\nSince the late 1990s, many Japanese companies have introduced the title of \"shikko yakuin\" (執行役員) or \"officer,\" seeking to emulate the separation of directors and officers found in American companies. In 2002, the statutory title of \"shikko yaku\" (執行役) was introduced for use in companies that introduced a three-committee structure in their board of directors. The titles are frequently given to \"bucho\" and higher-level personnel. Although the two titles are very similar in intent and usage, there are several legal distinctions: \"shikko yaku\" make their own decisions in the course of performing work delegated to them by the board of directors, and are considered managers of the company rather than employees, with a legal status similar to that of directors. \"Shikko yakuin\" are considered employees of the company that follow the decisions of the board of directors, although in some cases directors may have the \"shikko yakuin\" title as well.\n\nThe highest-level executives in senior management usually have titles beginning with \"chief\" forming what is often called the \"C-Suite\". The traditional three such officers are chief executive officer (CEO), chief operations officer (COO), and chief financial officer (CFO). Depending on the management structure, titles may exist instead of or are blended/overlapped with other traditional executive titles, such as \"president\", various designations of \"vice presidents\" (e.g. VP of marketing), and \"general managers\" or \"directors\" of various divisions (such as director of marketing); the latter may or may not imply membership of the \"board of directors\".\n\nCertain other prominent positions have emerged, some of which are sector-specific. For example, CEO and chief risk officer (CRO) positions are often found in many types of financial services companies. Technology companies of all sorts now tend to have a chief technology officer (CTO) to manage technology development. A chief information officer (CIO) oversees IT (information technology) matters, either in companies that specialize in IT or in any kind of company that relies on it for supporting infrastructure.\n\nMany companies now also have a chief marketing officer (CMO), particularly mature companies in competitive sectors, where brand management is a high priority. A chief value officer (CVO) is introduced in companies where business processes and organizational entities are focused on the creation and maximization of value. A chief administrative officer may be found in many large complex organizations that have various departments or divisions. Additionally, many companies now call their top diversity leadership position the chief diversity officer (CDO). However, this and many other nontraditional and/or lower-ranking titles (see below) are not universally recognized as corporate officers, and they tend to be specific to particular organizational cultures or the preferences of employees.\n\n\n\n\n", "id": "5681", "title": "Corporate title"}
{"url": "https://en.wikipedia.org/wiki?curid=5683", "text": "Computer expo\n\nA computer expo or computer show is a trade fair or exposition for computers and electronics. Expos usually include company or organization booths where products and technologies are demonstrated; talks and lectures; and general mixing of people with common interests.\n\n", "id": "5683", "title": "Computer expo"}
{"url": "https://en.wikipedia.org/wiki?curid=5685", "text": "Cambridge, Massachusetts\n\nCambridge is a city in Middlesex County, Massachusetts, and is a part of the Boston metropolitan area.\n\nSituated directly north of the city of Boston, across the Charles River, it was named in honor of the University of Cambridge in the United Kingdom, an important center of the Puritan theology embraced by the town's founders. Harvard University and the Massachusetts Institute of Technology (MIT), two of the world's most prestigious universities, are located in Cambridge, as was Radcliffe College, one of the leading colleges for women in the United States until it merged with Harvard.\n\nAccording to the 2010 Census, the city's population was 105,162. , it was the fifth most populous city in the state, behind Boston, Worcester, Springfield and Lowell. Cambridge was one of the two seats of Middlesex County prior to the abolition of county government in 1997; Lowell was the other.\n\nKendall Square in Cambridge has been called \"the most innovative square mile on the planet\", in reference to the high concentration of entrepreneurial start-ups and quality of innovation which have emerged in the vicinity of the square since 2010.\n\nThe site for what would become Cambridge was chosen in December 1630, because it was located safely upriver from Boston Harbor, which made it easily defensible from attacks by enemy ships. Thomas Dudley, his daughter Anne Bradstreet, and her husband Simon, were among the first settlers of the town. The first houses were built in the spring of 1631. The settlement was initially referred to as \"the newe towne\". Official Massachusetts records show the name capitalized as Newe Towne by 1632 and a single word Newtowne by 1638. Located at the first convenient Charles River crossing west of Boston, Newe Towne was one of a number of towns (including Boston, Dorchester, Watertown, and Weymouth), founded by the 700 original Puritan colonists of the Massachusetts Bay Colony under governor John Winthrop. Its first preacher was Thomas Hooker, who led many of its original inhabitants west to found the Connecticut Colony; before leaving, however, they sold their plots to more recent immigrants from England. The original village site is in the heart of today's Harvard Square. The marketplace where farmers brought in crops from surrounding towns to sell survives today as the small park at the corner of John F. Kennedy and Winthrop Streets, then at the edge of a salt marsh and since filled. The town included a much larger area than the present city, with various outlying parts becoming independent towns over the years: Cambridge Village (later Newtown and now Newton) in 1688, Cambridge Farms (now Lexington) in 1712 or 1713, and Little or South Cambridge (now Brighton) and Menotomy or West Cambridge (now Arlington) in 1807. In the late 19th century, various schemes for annexing Cambridge itself to the city of Boston were pursued and rejected.\n\nIn 1636, the Newe College (later renamed Harvard College after benefactor John Harvard) was founded by the colony to train ministers. Newe Towne was chosen for the site of the college by the Great and General Court (the Massachusetts legislature) primarily—according to Cotton Mather—to be near the popular and highly respected Puritan preacher Thomas Shepard. In May 1638 the name of the settlement was changed to Cambridge in honor of the university in Cambridge, England. Hooker and Shepard, Newtowne's ministers, and the college's first president, major benefactor, and first schoolmaster were all Cambridge alumni, as was the colony's governor John Winthrop. In 1629, Winthrop had led the signing of the founding document of the city of Boston, which was known as the Cambridge Agreement, after the university. It was Governor Thomas Dudley who, in 1650, signed the charter creating the corporation which still governs Harvard College.\n\nCambridge grew slowly as an agricultural village eight miles (13 km) by road from Boston, the capital of the colony. By the American Revolution, most residents lived near the Common and Harvard College, with farms and estates comprising most of the town. Most of the inhabitants were descendants of the original Puritan colonists, but there was also a small elite of Anglican \"worthies\" who were not involved in village life, who made their livings from estates, investments, and trade, and lived in mansions along \"the Road to Watertown\" (today's Brattle Street, still known as Tory Row). Coming up from Virginia, George Washington took command of the volunteer American soldiers camped on Cambridge Common on July 3, 1775, now reckoned the birthplace of the U.S. Army. Most of the Tory estates were confiscated after the Revolution. On January 24, 1776, Henry Knox arrived with artillery captured from Fort Ticonderoga, which enabled Washington to drive the British army out of Boston.\nBetween 1790 and 1840, Cambridge began to grow rapidly, with the construction of the West Boston Bridge in 1792, that connected Cambridge directly to Boston, making it no longer necessary to travel eight miles (13 km) through the Boston Neck, Roxbury, and Brookline to cross the Charles River. A second bridge, the Canal Bridge, opened in 1809 alongside the new Middlesex Canal. The new bridges and roads made what were formerly estates and marshland into prime industrial and residential districts.\n\nIn the mid-19th century, Cambridge was the center of a literary revolution when it gave the country a new identity through poetry and literature. Cambridge was home to some of the famous Fireside Poets—so called because their poems would often be read aloud by families in front of their evening fires. The Fireside Poets—Henry Wadsworth Longfellow, James Russell Lowell, and Oliver Wendell Holmes—were highly popular and influential in their day.\n\nSoon after, turnpikes were built: the Cambridge and Concord Turnpike (today's Broadway and Concord Ave.), the Middlesex Turnpike (Hampshire St. and Massachusetts Ave. northwest of Porter Square), and what are today's Cambridge, Main, and Harvard Streets were roads to connect various areas of Cambridge to the bridges. In addition, the town was connected to the Boston & Maine Railroad, leading to the development of Porter Square as well as the creation of neighboring town Somerville from the formerly rural parts of Charlestown.\nCambridge was incorporated as a city in 1846. This was despite noticeable tensions between East Cambridge, Cambridgeport, and Old Cambridge that stemmed from differences in each area's culture, sources of income, and the national origins of the residents. The city's commercial center began to shift from Harvard Square to Central Square, which became the downtown of the city around this time. Between 1850 and 1900, Cambridge took on much of its present character—streetcar suburban development along the turnpikes, with working-class and industrial neighborhoods focused on East Cambridge, comfortable middle-class housing being built on old estates in Cambridgeport and Mid-Cambridge, and upper-class enclaves near Harvard University and on the minor hills of the city. The coming of the railroad to North Cambridge and Northwest Cambridge then led to three major changes in the city: the development of massive brickyards and brickworks between Massachusetts Ave., Concord Ave. and Alewife Brook; the ice-cutting industry launched by Frederic Tudor on Fresh Pond; and the carving up of the last estates into residential subdivisions to provide housing to the thousands of immigrants that arrived to work in the new industries.\n\nFor many decades, the city's largest employer was the New England Glass Company, founded in 1818. By the middle of the 19th century it was the largest and most modern glassworks in the world. In 1888, all production was moved, by Edward Drummond Libbey, to Toledo, Ohio, where it continues today under the name Owens Illinois. Flint glassware with heavy lead content, produced by that company, is prized by antique glass collectors today. There is none on public display in Cambridge, but there is a large collection in the Toledo Museum of Art. There are also a few pieces in the Museum of Fine Arts, Boston and in the Sandwich Glass Museum on Cape Cod.\n\nBy 1920, Cambridge was one of the main industrial cities of New England, with nearly 120,000 residents. Among the largest businesses located in Cambridge during the period of industrialization was the firm of Carter's Ink Company, whose neon sign long adorned the Charles River and which was for many years the largest manufacturer of ink in the world. Next door was the Atheneum Press. Confectionery and snack manufacturers in the Cambridgeport-Area 4-Kendall corridor included the Kennedy Biscuit Factory (later part of Nabisco and originator of the Fig Newton), Necco, Squirrel Brands), George Close Company (1861–1930s), Daggett Chocolate (1892–1960s, recipes bought by Necco), Fox Cross Company (1920–1980, originator of the Charleston Chew, and now part of Tootsie Roll Industries), Kendall Confectionery Company, and James O. Welch (1927–1963, originator of Junior Mints, Sugar Daddies, Sugar Mamas and Sugar Babies, now part of Tootsie Roll Industries). In the 2010s, only the Cambridge Brands subsidiary of Tootsie Roll Industries remains in town, still manufacturing Junior Mints in the old Welch factory on Main Street. The Blake and Knowles Steam Pump Company (1886) and the Kendall Boiler and Tank Company (1880, now in Chelmsford, Massachusetts) and the New England Glass Company (1818–1878) were among the industrial manufacturers in what are now the Kendall Square and East Cambridge neighborhoods.\n\nAs industry in New England began to decline during the Great Depression and after World War II, Cambridge lost much of its industrial base. It also began the transition to being an intellectual, rather than an industrial, center. Harvard University had always been important in the city (both as a landowner and as an institution), but it began to play a more dominant role in the city's life and culture. When Radcliffe College was established in 1879 the town became a mecca for some of the nation's most academically talented female students. Also, the move of the Massachusetts Institute of Technology from Boston in 1916 ensured Cambridge's status as an intellectual center of the United States.\n\nAfter the 1950s, the city's population began to decline slowly, as families tended to be replaced by single people and young couples. The 1980s brought a wave of high-technology startups, creating software such as Visicalc and Lotus 1-2-3, and advanced computers, but many of these companies fell into decline with the fall of the minicomputer and DOS-based systems. The city continues to be home to many startups. Kendall Square continued to be a major software hub through the dot-com boom and today hosts offices of major technology companies including Google, Microsoft, Amazon.com, and Akamai (headquarters).\n\nIn 1976, Harvard's plans to start experiments with recombinant DNA led to a three-month moratorium and a citizen review panel. In the end, Cambridge decided to allow such experiments but passed safety regulations in 1977. This led to regulatory certainty and acceptance when Biogen opened a lab in 1982, in contrast to hostility which caused the Genetic Institute (a Harvard spinoff) to abandon Somerville and Boston for Cambridge. The biotech and pharmaceutical industries have since thrived in Cambridge, which now includes headquarters for Biogen and Genzyme; and laboratories for Novartis, Teva, Takeda, Alnylam, Ironwood, Catabasis, Moderna Therapeutics, Editas Medicine; and support companies such as Cytel; and many smaller companies.\n\nBy the end of the 20th century, Cambridge had one of the most expensive housing markets in the Northeastern United States. While maintaining much diversity in class, race, and age, it became harder and harder for those who grew up in the city to be able to afford to stay. The end of rent control in 1994 prompted many Cambridge renters to move to housing that was more affordable, in Somerville and other communities.\n\nUntil recently, Cambridge's mix of amenities and proximity to Boston has kept housing prices relatively stable despite the bursting of the United States housing bubble. Cambridge has been a sanctuary city since 1985 and reaffirmed its status as such in 2006.\n\nAccording to the United States Census Bureau, Cambridge has a total area of , of which is land and (9.82%) is water.\n\nCambridge is located in eastern Massachusetts, bordered by:\n\nThe border between Cambridge and the neighboring city of Somerville passes through densely populated neighborhoods which are connected by the MBTA Red Line. Some of the main squares, Inman, Porter, and to a lesser extent, Harvard and Lechmere, are very close to the city line, as are Somerville's Union and Davis Squares.\n\nCambridge has been called the \"City of Squares\" by some, as most of its commercial districts are major street intersections known as squares. Each of the squares acts as a neighborhood center. These include:\n\nThe residential neighborhoods in Cambridge border, but are not defined by the squares. These neighborhoods include:\n\n\nAs of the census of 2010, there were 105,162 people, 44,032 households, and 17,420 families residing in the city. The population density was 16,354.9 people per square mile (6,314.6/km²). There were 47,291 housing units at an average density of 7,354.7 per square mile (2,840.3/km²). The racial makeup of the city was 66.60% White, 11.70% Black or African American, 0.20% Native American, 15.10% Asian (3.7% Chinese, 1.4% Asian Indian, 1.2% Korean, 1.0% Japanese), 0.01% Pacific Islander, 2.10% from other races, and 4.30% from two or more races. 7.60% of the population were Hispanic or Latino of any race (1.6% Puerto Rican, 1.4% Mexican, 0.6% Dominican, 0.5% Colombian, 0.5% Salvadoran, 0.4% Spaniard). Non-Hispanic Whites were 62.1% of the population in 2010, down from 89.7% in 1970. An individual resident of Cambridge is known as a Cantabrigian.\n\nIn 2010, there were 44,032 households out of which 16.9% had children under the age of 18 living with them, 28.9% were married couples living together, 8.4% had a female householder with no husband present, and 60.4% were non-families. 40.7% of all households were made up of individuals and 9.6% had someone living alone who was 65 years of age or older. The average household size was 2.00 and the average family size was 2.76.\n\nIn the city, the population was spread out with 13.3% of the population under the age of 18, 21.2% from 18 to 24, 38.6% from 25 to 44, 17.8% from 45 to 64, and 9.2% who were 65 years of age or older. The median age was 30.5 years. For every 100 females, there were 96.1 males. For every 100 females age 18 and over, there were 94.7 males.\n\nThe median income for a household in the city was $47,979, and the median income for a family was $59,423 (these figures had risen to $58,457 and $79,533 respectively ). Males had a median income of $43,825 versus $38,489 for females. The per capita income for the city was $31,156. About 8.7% of families and 12.9% of the population were below the poverty line, including 15.1% of those under age 18 and 12.9% of those age 65 or over.\n\nCambridge has been ranked as one of the most liberal cities in America. Locals living in and near the city jokingly refer to it as \"The People's Republic of Cambridge.\" For 2016, the residential property tax rate in Cambridge was $6.99 per $1,000. Cambridge enjoys the highest possible bond credit rating, AAA, with all three Wall Street rating agencies.\n\nIn 2000, 11.0% of city residents were of Irish ancestry; 7.2% were of English, 6.9% Italian, 5.5% West Indian and 5.3% German ancestry. 69.4% spoke only English at home, while 6.9% spoke Spanish, 3.2% Chinese or Mandarin, 3.0% Portuguese, 2.9% French Creole, 2.3% French, 1.5% Korean, and 1.0% Italian.\n\nData is from the 2009–2013 American Community Survey 5-Year Estimates.\n\nManufacturing was an important part of the economy in the late 19th and early 20th century, but educational institutions are the city's biggest employers today. Harvard and MIT together employ about 20,000. As a cradle of technological innovation, Cambridge was home to technology firms Analog Devices, Akamai, Bolt, Beranek, and Newman (BBN Technologies) (now part of Raytheon), General Radio (later GenRad), Lotus Development Corporation (now part of IBM), Polaroid, Symbolics, and Thinking Machines.\n\nIn 1996, Polaroid, Arthur D. Little, and Lotus were top employers with over 1,000 employees in Cambridge, but faded out a few years later. Health care and biotechnology firms such as Genzyme, Biogen Idec, Millennium Pharmaceuticals, Sanofi, Pfizer and Novartis have significant presences in the city. Though headquartered in Switzerland, Novartis continues to expand its operations in Cambridge. Other major biotech and pharmaceutical firms expanding their presence in Cambridge include GlaxoSmithKline, AstraZeneca, Shire, and Pfizer. Most Biotech firms in Cambridge are located around Kendall Square and East Cambridge, which decades ago were the city's center of manufacturing. A number of biotechnology companies are also located in University Park at MIT, a new development in another former manufacturing area.\n\nNone of the high technology firms that once dominated the economy was among the 25 largest employers in 2005, but by 2008 high tech companies Akamai and ITA Software had grown to be among the largest 25 employers. Google, IBM Research, Microsoft Research, and Philips Research maintain offices in Cambridge. In late January 2012—less than a year after acquiring Billerica-based analytic database management company, Vertica—Hewlett-Packard announced it would also be opening its first offices in Cambridge. Around this same time, e-commerce giants Staples and Amazon.com said they would be opening research and innovation centers in Kendall Square. LabCentral also provides a shared laboratory facility for approximately 25 emerging biotech companies.\n\nThe proximity of Cambridge's universities has also made the city a center for nonprofit groups and think tanks, including the National Bureau of Economic Research, the Smithsonian Astrophysical Observatory, the Lincoln Institute of Land Policy, Cultural Survival, and One Laptop per Child.\n\nIn September 2011, an initiative by the City of Cambridge called the \"Entrepreneur Walk of Fame\" was launched. It seeks to highlight individuals who have made contributions to innovation in the global business community.\n\n, the ten largest employers in the city are:\n\n\nCambridge has a large and varied collection of permanent public art, both on city property (managed by the Cambridge Arts Council), and on the campuses of Harvard and MIT. Temporary public artworks are displayed as part of the annual Cambridge River Festival on the banks of the Charles River, during winter celebrations in Harvard and Central Squares, and at university campus sites. Experimental forms of public artistic and cultural expression include the Central Square World's Fair, the Somerville-based annual Honk! Festival, and If This House Could Talk, a neighborhood art and history event. An active tradition of street musicians and other performers in Harvard Square entertains an audience of tourists and local residents during the warmer months of the year. The performances are coordinated through a public process that has been developed collaboratively by the performers, city administrators, private organizations and business groups. The Cambridge public library contains four Works Progress Administration murals, completed in 1935, by Elizabeth Tracy Montminy: \"Religion\", \"Fine Arts\", \"History of Books and Paper\", and \"The Development of the Printing Press\".\n\nDespite intensive urbanization during the late 19th century and 20th century, Cambridge has several historic buildings, including some dating to the 17th century. The city also contains an abundance of innovative contemporary architecture, largely built by Harvard and MIT.\n\nNotable historic buildings in the city include:\n\nContemporary architecture:\n\nThe city has an active music scene, from classical performances to the latest popular bands. Beyond performances at the colleges and universities, there are many venues in Cambridge including: The Middle East, Club Passim, The Plough and Stars, and the Nameless Coffeehouse.\n\nConsisting largely of densely built residential space, Cambridge lacks significant tracts of public parkland. This is partly compensated for, however, by the presence of easily accessible open space on the university campuses, including Harvard Yard, the Radcliffe Yard, and MIT's Great Lawn, as well as the considerable open space of Mount Auburn Cemetery. At the western edge of Cambridge, the cemetery is well known as the first garden cemetery, for its distinguished inhabitants, for its superb landscaping (the oldest planned landscape in the country), and as a first-rate arboretum. Although known as a Cambridge landmark, much of the cemetery lies within the bounds of Watertown. It is also a significant Important Bird Area (IBA) in the Greater Boston area.\n\nPublic parkland includes the esplanade along the Charles River, which mirrors its Boston counterpart; Cambridge Common, a busy and historic public park immediately adjacent to the Harvard campus; and the Alewife Brook Reservation and Fresh Pond in the western part of the city.\n\nCambridge is split between Massachusetts's 5th and 7th U.S. congressional districts. The 5th district seat is held by Democrat Katherine Clark, who replaced now-Senator Ed Markey in a 2013 special election; the 7th is represented by Democrat Mike Capuano, elected in 1998. The state's senior member of the United States Senate is Democrat Elizabeth Warren, elected in 2012, who lives in Cambridge. The Governor of Massachusetts is Republican Charlie Baker, elected in 2014.\n\nOn the state level, Cambridge is represented in six districts in the Massachusetts House of Representatives: the 24th Middlesex (which includes parts of Belmont and Arlington), the 25th and 26th Middlesex (the latter which includes a portion of Somerville), the 29th Middlesex (which includes a small part of Watertown), and the Eighth and Ninth Suffolk (both including parts of the City of Boston). The city is represented in the Massachusetts Senate as a part of the \"First Suffolk and Middlesex\" district (this contains parts of Boston, Revere and Winthrop each in Suffolk County); the \"Middlesex, Suffolk and Essex\" district, which includes Everett and Somerville, with Boston, Chelsea, and Revere of Suffolk, and Saugus in Essex; and the \"Second Suffolk and Middlesex\" district, containing parts of the City of Boston in Suffolk County, and Cambridge, Belmont and Watertown in Middlesex County.\n\nCambridge has a city government led by a mayor and nine-member city council. There is also a six-member school committee which functions alongside the Superintendent of public schools. The councilors and school committee members are elected every two years using the single transferable vote (STV) system.\n\nThe mayor is elected by the city councilors from amongst themselves, and serves as the chair of city council meetings. The mayor also sits on the school committee. However, the mayor is not the chief executive of the city. Rather, the city manager, who is appointed by the city council, serves in that capacity.\n\nUnder the city's Plan E form of government, the city council does not have the power to appoint or remove city officials who are under direction of the city manager. The city council and its individual members are also forbidden from giving orders to any subordinate of the city manager.\n\nLouis DePasquale is the City Manager. On November 14, 2016, he succeeded Lisa C. Peterson, the Acting City Manager and first woman City Manager in Cambridge. Peterson became Acting City Manager with the retirement of Richard C. Rossi on Sept. 30, 2016 after he announced that he would opt out of his contract renewal. Rossi succeeded Robert W. Healy, who retired in June 2013 after serving 32 years in the position. In recent history, the media has highlighted the salary of the city manager as being one of the highest for a civic employee in Massachusetts.\n\nThe city council consists of:\n\n\"* = current mayor\"<br>\n\"** = former mayor\"\n\nCambridge was a county seat of Middlesex County, along with Lowell, prior to the abolition of county government. Though the county government was abolished in 1997, the county still exists as a geographical and political region. The employees of Middlesex County courts, jails, registries, and other county agencies now work directly for the state. At present, the county's registrars of Deeds and Probate remain in Cambridge; however, the Superior Court and District Attorney have had their base of operations transferred to Woburn. Third District court has shifted operations to Medford, and the Sheriff's office for the county is still awaiting a near-term relocation.\n\nCambridge is perhaps best known as an academic and intellectual center, owing to its colleges and universities, which include:\n\nAt least 129 of the world's total 780 Nobel Prize winners have been, at some point in their careers, affiliated with universities in Cambridge.\n\nThe American Academy of Arts and Sciences is also based in Cambridge.\n\n\nThe 5 upper schools which are physically located in some of the same buildings as the elementary schools offer grades 6–8. They are:\n\nThere are three district public high school programs serving Cambridge students, the principal one being the Cambridge Rindge and Latin School (CRLS).\n\nOutside of the main public schools are other public charter schools including: Benjamin Banneker Charter School, which serves students in grades K–6, Community Charter School of Cambridge, which is located in Kendall Square and serves students in grades 7–12, and Prospect Hill Academy, a charter school whose upper school is in Central Square, though it is not a part of the Cambridge Public School District.\n\nThere are also many private schools in the city including:\n\n\nCambridge is served by a weekly newspaper, the \"Cambridge Chronicle\", which is also the oldest surviving weekly paper in the United States.\n\nCambridge is home to the following commercially licensed and student-run radio stations:\n\nCambridge Community Television (CCTV) has served the Cambridge community since its inception in 1988. CCTV operates Cambridge's public access television facility and programs three television channels, 8, 9, and 96 on the Cambridge cable system (Comcast). The city has invited tenders from other cable providers; however, presently Comcast remains the only fixed television and broadband utility for Cambridge. Services from American satellite TV providers, however, are available. In October 2014, Cambridge City Manager Richard Rossi appointed a citizen Broadband Task Force to \"examine options to increase competition, reduce pricing, and improve speed, reliability and customer service for both residents and businesses.\"\n\nSeveral major roads lead to Cambridge, including Route 2, Route 16 and the McGrath Highway (Route 28). The Massachusetts Turnpike does not pass through Cambridge, but provides access by an exit in nearby Allston. Both U.S. Route 1 and Interstate 93 also provide additional access on the eastern end of Cambridge at Leverett Circle in Boston. Route 2A runs the length of the city, chiefly along Massachusetts Avenue. The Charles River forms the southern border of Cambridge and is crossed by 11 bridges connecting Cambridge to Boston, including the Longfellow Bridge and the Harvard Bridge, eight of which are open to motorized road traffic.\n\nCambridge has an irregular street network because many of the roads date from the colonial era. Contrary to popular belief, the road system did not evolve from longstanding cow-paths. Roads connected various village settlements with each other and nearby towns, and were shaped by geographic features, most notably streams, hills, and swampy areas. Today, the major \"squares\" are typically connected by long, mostly straight roads, such as Massachusetts Avenue between Harvard Square and Central Square, or Hampshire Street between Kendall Square and Inman Square.\n\nCambridge is well served by the MBTA, including the Porter Square Station on the regional Commuter Rail; the Lechmere Station on the Green Line; and the Red Line at Alewife, Porter Square, Harvard Square, Central Square, and Kendall Square/MIT Stations. Alewife Station, the terminus of the Red Line, has a large multi-story parking garage (at a rate of $7 per day ). The Harvard Bus Tunnel, under Harvard Square, reduces traffic congestion on the surface, and connects to the Red Line underground. This tunnel was originally opened for streetcars in 1912, and served trackless trolleys (trolleybuses) and buses as the routes were converted; four lines of the MBTA trolleybus system continue to use it. The tunnel was partially reconfigured when the Red Line was extended to Alewife in the early 1980s.\n\nBesides the state-owned transit agency, the city is also served by the Charles River Transportation Management Agency (CRTMA) shuttles which are supported by some of the largest companies operating in city, in addition to the municipal government itself.\n\nCambridge has several bike paths, including one along the Charles River, and the Linear Park connecting the Minuteman Bikeway at Alewife with the Somerville Community Path. Bike parking is common and there are bike lanes on many streets, although concerns have been expressed regarding the suitability of many of the lanes. On several central MIT streets, bike lanes transfer onto the sidewalk. Cambridge bans cycling on certain sections of sidewalk where pedestrian traffic is heavy.\n\nWhile \"Bicycling Magazine\" in 2006 rated Boston as one of the worst cities in the nation for bicycling, it has given Cambridge honorable mention as one of the best and was called by the magazine \"Boston's Great Hope\". Boston has since then followed the example of Cambridge, and made considerable efforts to improve bicycling safety and convenience.\n\nCambridge has an official bicycle committee. The LivableStreets Alliance, headquartered in Cambridge, is an advocacy group for bicyclists, pedestrians, and walkable neighborhoods.\n\nWalking is a popular activity in Cambridge. In 2000, of US communities with more than 100,000 residents, Cambridge had the highest percentage of commuters who walked to work. Cambridge's major historic squares have changed into modern walking neighborhoods, including traffic calming features based on the needs of pedestrians rather than of motorists.\n\nThe Boston intercity bus and train stations at South Station, Boston, and Logan International Airport in East Boston, are accessible by subway. The Fitchburg Line rail service from Porter Square connects to some western suburbs. Since October 2010, there has also been intercity bus service between Alewife Station (Cambridge) and New York City.\n\nIn addition to the Cambridge Police Department, the city is patrolled by the Fifth (Brighton) Barracks of Troop H of the Massachusetts State Police. Due, however, to close proximity, the city also practices functional cooperation with the Fourth (Boston) Barracks of Troop H, as well. The campuses of Harvard and MIT are patrolled by the Harvard University Police Department and MIT Police Department, respectively.\n\nThe city of Cambridge is protected by the Cambridge Fire Department. Established in 1832, the CFD operates eight engine companies, four ladder companies, one rescue company, and two paramedic squad companies from eight fire stations located throughout the city. The Chief is Gerald R. Reardon.\n\nCambridge is unusual among cities inside Route 128 in having a non-MWRA water supply. City water is obtained from Hobbs Brook (in Lincoln and Waltham) and Stony Brook (Waltham and Weston). The city owns over of land in other towns that includes these reservoirs and portions of their watershed. Water from these reservoirs flows by gravity through an aqueduct to Fresh Pond in Cambridge. It is then treated in an adjacent plant and pumped uphill to an elevation of above sea level at the Payson Park Reservoir (Belmont); From there, the water is redistributed downhill via gravity to individual users in the city. A new water treatment plant opened in 2001. The city used MWRA water during the old plant's demolition and the new plant's construction. In October 2016, the City of Cambridge announced that, due to drought conditions, they would begin buying water from the MWRA. On January 3, 2017, Cambridge announced that \"As a result of continued rainfall each month since October 2016, we have been able to significantly reduce the need to use MWRA water. We have not purchased any MWRA water since December 12, 2016 and if 'average' rainfall continues this could continue for several months.\"\n\nFurther educational services are provided at the Cambridge Public Library. The large modern main building was built in 2009, and connects to the restored 1888 Richardson Romanesque building. It was founded as the private Cambridge Athenaeum in 1849 and was acquired by the city in 1858, and became the Dana Library. The 1888 building was a donation of Frederick H. Rindge.\n\nCambridge has six official sister cities with active relationships:\n\nCambridge is in the process of developing a relationship with Les Cayes, Haiti.\n\nCambridge has ten additional official sister cities which are not active:\n\n\n\n", "id": "5685", "title": "Cambridge, Massachusetts"}
{"url": "https://en.wikipedia.org/wiki?curid=5686", "text": "Cambridge (disambiguation)\n\nCambridge is a city and the county town of Cambridgeshire, United Kingdom, famous for being the location of the University of Cambridge.\n\nCambridge may also refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "5686", "title": "Cambridge (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=5688", "text": "Colin Dexter\n\nNorman Colin Dexter, OBE (29 September 1930 – 21 March 2017) was an English crime writer known for his \"Inspector Morse\" series of novels, which were written between 1975 and 1999 and adapted as an ITV television series, \"Inspector Morse\", from 1987 to 2000. His characters have spawned a sequel series, \"Lewis\", and a prequel series, \"Endeavour\".\n\nDexter was born in Stamford, Lincolnshire, to Alfred and Dorothy Dexter. He had a brother, John, a fellow classicist, who taught Classics at The King's School, Peterborough, and a sister, Avril. Alfred ran a small garage and taxi company from premises in Scotgate, Stamford. Colin was educated at St. John's Infants School, Bluecoat Junior School, from which he gained a scholarship to Stamford School, a boys' public school, where one of his contemporaries was the England international cricket captain and England international rugby player M. J. K. Smith.\n\nAfter leaving school, Dexter completed his national service with the Royal Corps of Signals and then read Classics at Christ's College, Cambridge, graduating in 1953 and receiving a master's degree in 1958.\n\nIn 1954, Dexter began his teaching career in the East Midlands, becoming assistant Classics master at Wyggeston School, Leicester. There he helped the Christian Union school society. However, in 2000 he stated that he shared the same views on politics and religion as Inspector Morse, who was portrayed in the final Morse novel, \"The Remorseful Day\", as an atheist.\n\nA post at Loughborough Grammar School followed in 1957 before he took up the position of senior Classics teacher at Corby Grammar School, Northamptonshire, in 1959. In 1956 he married Dorothy Cooper, and they had a daughter, Sally, and a son, Jeremy. In 1966, he was forced by the onset of deafness to retire from teaching and took up the post of senior assistant secretary at the University of Oxford Delegacy of Local Examinations (UODLE) in Oxford, a job he held until his retirement in 1988.\n\nIn November 2008, Dexter featured prominently in the BBC programme \"How to Solve a Cryptic Crossword\" as part of the \"Time Shift\" series, in which he recounted some of the crossword clues solved by Morse.\n\nThe initial books written by Dexter were general studies text books. He began writing mysteries in 1972 during a family holiday: \"We were in a little guest house halfway between Caernarfon and Pwllheli. It was a Saturday and it was raining—it's not unknown for it to rain in North Wales. The children were moaning... I was sitting at the kitchen table with nothing else to do, and I wrote the first few paragraphs of a potential detective novel.\" \"Last Bus to Woodstock\" was published in 1975 and introduced the character of Inspector Morse, the irascible detective whose penchants for cryptic crosswords, English literature, cask ale, and Wagner reflect Dexter's own enthusiasms. Dexter's plots used false leads and other red herrings.\n\nThe success of the 33 two-hour episodes of the ITV television series \"Inspector Morse\", produced between 1987 and 2000, brought further attention to Dexter's writings. In the manner of Alfred Hitchcock, he also made a cameo appearance in almost all episodes. From 2006 to 2016, Morse's assistant Robbie Lewis featured in a 33-episode ITV series titled \"Lewis\" (\"Inspector Lewis\" in the United States). A prequel series, \"Endeavour\", featuring a young Morse and starring Shaun Evans and Roger Allam, began airing on the ITV network in 2012. Dexter was a consultant. As with \"Morse\", Dexter occasionally made cameo appearances in \"Lewis\" and \"Endeavour\".\n\nDexter received several Crime Writers' Association awards: two Silver Daggers for \"Service of All the Dead\" in 1979 and \"The Dead of Jericho\" in 1981; two Gold Daggers for \"The Wench is Dead\" in 1989 and \"The Way Through the Woods\" in 1992; and a Cartier Diamond Dagger for lifetime achievement in 1997. In 1996, Dexter received a Macavity Award for his short story \"Evans Tries an O-Level\". In 1980, he was elected a member of the by-invitation-only Detection Club. In 2005 Dexter became a Fellow by Special Election of St Cross College, Oxford.\n\nIn 2000 Dexter was appointed an Officer of the Order of the British Empire for services to literature. In 2001 he was awarded the Freedom of the City of Oxford. In September 2011, the University of Lincoln awarded Dexter an honorary Doctor of Letters degree.\n\nOn 21 March 2017 Dexter's publisher, Macmillan, said in a statement \"With immense sadness, Macmillan announces the death of Colin Dexter who died peacefully at his home in Oxford this morning.\"\n\n\n\n\n\n\n", "id": "5688", "title": "Colin Dexter"}
{"url": "https://en.wikipedia.org/wiki?curid=5689", "text": "College\n\nCollege (Latin: \"collegium\") is an educational institution or a constituent part of one. A college may be a degree-awarding tertiary educational institution, a part of a collegiate or federal university, or an institution offering vocational education.\n\nIn the United States \"college\" refers to a constituent part of a university, but generally \"college\" and \"university\" are used interchangeably, whereas in the United Kingdom, Oceania, South Asia and Southern Africa, \"college\" may refer to a secondary or high school, a college of further education, a training institution that awards trade qualifications, a higher education provider that does not have university status (often without its own degree-awarding powers), or a constituent part of a university (See this comparison of British and American English educational terminology for further information).\n\nIn ancient Rome a \"collegium\" was a club or society, a group of people living together under a common set of rules (\"con-\" = \"together\" + \"leg-\" = \"law\" or \"lego\" = \"I choose\" or \"I read\").\n\nAside from the modern educational context - nowadays the most common use of \"college\" - there are various other meanings also derived from the original Latin term, such as Electoral college.\n\nWithin higher education, the term can be used to refer to:\n\nA sixth form college or college of further education is an educational institution in England, Wales, Northern Ireland, Belize, The Caribbean, Malta, Norway, Brunei, or Southern Africa, among others, where students aged 16 to 19 typically study for advanced school-level qualifications, such as A-levels, BTEC, HND or its equivalent and the International Baccalaureate Diploma, or school-level qualifications such as GCSEs. In Singapore and India, this is known as a junior college. The municipal government of the city of Paris uses the phrase \"sixth form college\" as the English name for a lycée.\n\nIn some national education systems, secondary schools may be called \"colleges\" or have \"college\" as part of their title.\n\nIn Australia the term \"college\" is applied to any private or independent (non-government) primary and, especially, secondary school as distinct from a state school. Melbourne Grammar School, Cranbrook School, Sydney and The King's School, Parramatta are considered colleges.\n\nThere has also been a recent trend to rename or create government secondary schools as \"colleges\". In the state of Victoria, some state high schools are referred to as \"secondary colleges\". Interestingly, the pre-eminent government secondary school for boys in Melbourne is still named Melbourne High School. In Western Australia, South Australia and the Northern Territory, \"college\" is used in the name of all state high schools built since the late 1990s, and also some older ones. In New South Wales, some high schools, especially multi-campus schools resulting from mergers, are known as \"secondary colleges\". In Queensland some newer schools which accept primary and high school students are styled \"state college\", but state schools offering only secondary education are called \"State High School\". In Tasmania and the Australian Capital Territory, \"college\" refers to the final two years of high school (years 11 and 12), and the institutions which provide this. In this context, \"college\" is a system independent of the other years of high school. Here, the expression is a shorter version of \"matriculation college\".\n\nIn a number of Canadian cities, many government-run secondary schools are called \"collegiates\" or \"collegiate institutes\" (C.I.), a complicated form of the word \"college\" which avoids the usual \"post-secondary\" connotation. This is because these secondary schools have traditionally focused on academic, rather than vocational, subjects and ability levels (for example, collegiates offered Latin while vocational schools offered technical courses). Some private secondary schools (such as Upper Canada College, Vancouver College) choose to use the word \"college\" in their names nevertheless. Some secondary schools elsewhere in the country, particularly ones within the separate school system, may also use the word \"college\" or \"collegiate\" in their names.\n\nIn New Zealand the word \"college\" normally refers to a secondary school for ages 13 to 17 and \"college\" appears as part of the name especially of private or integrated schools. \"Colleges\" most frequently appear in the North Island, whereas \"high schools\" are more common in the South Island.\n\nIn South Africa, some secondary schools, especially private schools on the English public school model, have \"college\" in their title. Thus no less than six of South Africa's Elite Seven high schools call themselves \"college\" and fit this description. A typical example of this category would be St John's College.\n\nPrivate schools that specialize in improving children's marks through intensive focus on examination needs are informally called \"cram-colleges\".\n\nIn Sri Lanka the word \"college\" (known as \"Vidyalaya\" in \"Sinhala\") normally refers to a secondary school, which usually signifies above the 5th standard. During the British colonial period a limited number of exclusive secondary schools were established based on English public school model (Royal College Colombo, S. Thomas' College, Mount Lavinia, Trinity College, Kandy) these along with several Catholic schools (St. Joseph's College, Colombo, St Anthony's College) traditionally carry their name as colleges. Following the start of free education in 1931 large group of central colleges were established to educate the rural masses. Since Sri Lanka gained Independence in 1948, many schools that have been established have been named as \"college\".\n\nAs well as an educational institution, the term can also refer, following its etymology, to any formal group of colleagues set up under statute or regulation; often under a Royal Charter. Examples are an electoral college, the College of Arms, a college of canons, and the College of Cardinals. Other collegiate bodies include professional associations, particularly in medicine and allied professions. In the UK these include the Royal College of Nursing and the Royal College of Physicians. Examples in the United States include the American College of Physicians, the American College of Surgeons, and the American College of Dentists. An example in Australia is the Royal Australian College of General Practitioners.\n\nIn Australia a college may be an institution of tertiary education that is smaller than a university, run independently or as part of a university. Following a reform in the 1980s many of the formerly independent colleges now belong to a larger universities.\n\nReferring to parts of a university, there are \"residential colleges\" which provide residence for students, both undergraduate and postgraduate, called university colleges. These colleges often provide additional tutorial assistance, and some host theological study. Many colleges have strong traditions and rituals, so are a combination of dormitory style accommodation and fraternity or sorority culture.\n\nMost technical and further education institutions (TAFEs), which offer certificate and diploma vocational courses, are styled \"TAFE colleges\" or \"Colleges of TAFE\".\n\nSome senior high schools also refer to themselves as colleges.\n\nIn Canada, the term \"college\" usually refers to a trades school, applied arts/science/technology/business/health school or community college. These are post-secondary institutions granting certificates, diplomas, associate's degree, and in some cases bachelor's degrees. In Quebec, the term is seldom used; the French acronym for public colleges, CEGEP (\"Collège d'enseignement général et professionnel\", \"college of general and professional education\"), is colloquially all collegiate level institutions specific to the Quebec education system, a step that is required to continue onto university (unless one applies as a \"mature\" student, meaning 21 years of age or over, and out of the educational system for at least 2 years), or to learn a trade. In Ontario and Alberta, there are also institutions which are designated university colleges, as they only grant undergraduate degrees. This is to differentiate between universities, which have both undergraduate and graduate programs and those that do not. In contrast to usage in the United States, there is a strong distinction between \"college\" and \"university\" in Canada. In conversation, one specifically would say either \"They are going to university\" (i.e., studying for a three- or four-year degree at a university) or \"They are going to college\" (suggesting technical/career training or university transfer courses).\n\nThe Royal Military College of Canada, a full-fledged degree-granting university, does not follow the naming convention used by the rest of the country, nor does its sister school Royal Military College Saint-Jean or the now closed Royal Roads Military College.\n\nThe term \"college\" also applies to distinct entities within a university (usually referred to as \"federated colleges\" or \"affiliated colleges\"), similar to the residential colleges in the United Kingdom. These colleges act independently, but in affiliation or federation with the university that actually grants the degrees. For example, Trinity College was once an independent institution, but later became federated with the University of Toronto, and is now one of its residential colleges (though it remains a degree-granting institution through its Faculty of Divinity). In the case of Memorial University of Newfoundland, located in St. John's, the Corner Brook campus is called Sir Wilfred Grenfell College. Occasionally, \"college\" refers to a subject specific faculty within a university that, while distinct, are neither \"federated\" nor \"affiliated\"—College of Education, College of Medicine, College of Dentistry, College of Biological Science among others.\n\nThere are also universities referred to as art colleges, empowered to grant academic degrees of BFA, Bdes, MFA, Mdes and sometimes collaborative PhD degrees. Some of them have \"university\" in their name (NSCAD University, OCAD University and Emily Carr University of Art and Design)and others do not. In some Canadian provinces, the word \"college\" may also be seen in the proper name of a high school, especially one with a history as a private school, but these institutions would not actually be considered colleges in the more general sense of the term.\n\nOnline and distance education (E-learning) use \"college\" in the name in the British sense, for example : Canada Capstone College.\n\nOne use of the term \"college\" in the American sense is by the Canadian Football League (CFL), which calls its annual entry draft the Canadian College Draft. The draft is restricted to players who qualify under CFL rules as \"non-imports\"—essentially, players who were raised in Canada (see the main CFL article for a more detailed definition). Because a player's designation as \"non-import\" is not affected by where he plays post-secondary football, the category includes former players at U.S. college football programs (\"universities\" in the Canadian sense) as well as CIS football programs at Canadian universities.\n\nIn Chile, the term \"college\" is usually used in the name of some bilingual schools, like Santiago College, Saint George's College etc.\n\nInternational Association of \"Tourists and Travelers\" College. International association \"tourists and travelers\" is a non-commercial, non political and non industrial organization, which is created to develop tourism in Georgia.\n\n\"Kollegio\" (in Greek Κολλέγιο) refers to the Centers of Post-Lyceum Education (in Greek Κέντρο Μεταλυκειακής Εκπαίδευσης, abbreviated as KEME), which are principally private and belong to the Greek post-secondary education system. Some of them have links to EU or US higher education institutions or accreditation organizations, such as the NEASC. \"Kollegio\" (or \"Kollegia\" in plural) may also refer to private non-tertiary schools, such as the Athens College.\n\nIn Hong Kong, the term 'college' is used by tertiary institutions as either part of their names or to refer to a constituent part of the university, such as the colleges in the collegiate The Chinese University of Hong Kong; or to a residence hall of a university, such as St. John's College, University of Hong Kong. Many older secondary schools have the term 'college' as part of their names.\n\nThe modern system of education was heavily influenced by the British starting in 1835.\n\nIn India, the term \"college\" is commonly reserved for institutions that offer degrees at year 12 (\"\"Junior College\"\", similar to American \"high schools\"), and those that offer the bachelor's degree; some colleges, however, offer programmes up to PhD level. Generally, colleges are located in different parts of a state and all of them are affiliated to a regional university. The colleges offer programmes leading to degrees of that university. Colleges may be either Autonomous or non-autonomous. Autonomous Colleges are empowered to establish their own syllabus, and conduct and assess their own examinations; in non-autonomous colleges, examinations are conducted by the university, at the same time for all colleges under its affiliation. There are several hundred universities and each university has affiliated colleges, often a large number.\n\nThe first liberal arts and sciences college in India was C. M. S. College Kottayam, Kerala, established in 1817, and the Presidency College, Kolkata, also 1817, initially known as Hindu College. The first college for the study of Christian theology and ecumenical enquiry was Serampore College (1818). The first Missionary institution to impart Western style education in India was the Scottish Church College, Calcutta (1830). The first commerce and economics college in India was Sydenham College, Mumbai (1913).\n\nIn Ireland the term \"college\" is normally used to describe an institution of tertiary education. University students often say they attend \"college\" rather than \"university\". Until 1989, no university provided teaching or research directly; they were formally offered by a constituent college of the university.\n\nThere are number of secondary education institutions that traditionally used the word \"college\" in their names: these are either older, private schools (such as Belvedere College, Gonzaga College and St. Michael's College) or what were formerly a particular kind of secondary school. These secondary schools, formerly known as \"technical colleges,\" were renamed \"community colleges,\" but remain secondary schools.\n\nThe country's only ancient university is the University of Dublin. Created during the reign of Elizabeth I, it is modelled on the collegiate universities of Cambridge and Oxford. However, only one constituent college was ever founded, hence the curious position of Trinity College, Dublin today; although both are usually considered one and the same, the University and College are completely distinct corporate entities with separate and parallel governing structures.\n\nAmong more modern foundations, the National University of Ireland, founded in 1908, consisted of constituent colleges and recognised colleges until 1997. The former are now referred to as constituent universities – institutions that are essentially universities in their own right. The National University can trace its existence back to 1850 and the creation of the Queen's University of Ireland and the creation of the Catholic University of Ireland in 1854. From 1880, the degree awarding roles of these two universities was taken over by the Royal University of Ireland, which remained until the creation of the National University in 1908 and Queen's University Belfast.\n\nThe state's two new universities Dublin City University and University of Limerick were initially National Institute for Higher Education institutions. These institutions offered university level academic degrees and research from the start of their existence and were awarded university status in 1989 in recognition of this.\n\nThird level technical education in the state has been carried out in the Institutes of Technology, which were established from the 1970s as Regional Technical Colleges. These institutions have \"delegated authority\" which entitles them to give degrees and diplomas from the Higher Education and Training Awards Council in their own name.\n\nA number of Private Colleges exist such as DBS, providing undergraduate and postgraduate courses validated by HETAC and in some cases by other Universities.\n\nOther types of college include Colleges of Education, such as the Church of Ireland College of Education. These are specialist institutions, often linked to a university, which provide both undergraduate and postgraduate academic degrees for people who want to train as teachers.\n\nA number of state funded further education colleges exist - which offer vocational education and training in a range of areas from business studies, I.C.T to sports injury therapy. These courses are usually 1, 2 or less often 3 three years in duration and are validated by FETAC at levels 5 or 6 or for the BTEC Higher National Diploma award - validated by Edexcel which is a level 6/7 qualification. There are numerous private colleges (particularly in Dublin and Limerick) which offer both further and higher education qualifications. These degrees and diplomas are often certified by foreign universities/international awarding bodies and are aligned to the National Framework of Qualifications at level 6, 7 and 8.\n\nIn Israel, any non university higher-learning facility is called a college. Institutions accredited by the Council for Higher Education in Israel (CHE) to confer a bachelor's degree are called \"Academic Colleges.\" These colleges (at least 4 for 2012) may also offer master's degrees and act as Research facilities. There are also over twenty teacher training colleges or seminaries, most of which may award only a Bachelor of Education (B.Ed.) degree.\n\n\nFollowing the Portuguese usage, the term \"college\" (\"colégio\") in Macau has traditionally been used in the names for private (and non-governmental) pre-university educational institutions, which correspond to form one to form six level tiers. Such schools are usually run by the Roman Catholic church or missionaries in Macau. Examples include Chan Sui Ki Perpetual Help College, Yuet Wah College, and Sacred Heart Canossian College.\n\nThe constituent colleges of the former University of New Zealand (such as Canterbury University College) have become independent universities. Some halls of residence associated with New Zealand universities retain the name of \"college\", particularly at the University of Otago (which although brought under the umbrella of the University of New Zealand, already possessed university status and degree awarding powers). The institutions formerly known as \"Teacher-training colleges\" now style themselves \"College of education\".\n\nSome universities, such as the University of Canterbury, have divided their University into constituent administrative \"Colleges\" – the College of Arts containing departments that teach Arts, Humanities and Social Sciences, College of Science containing Science departments, and so on. This is largely modelled on the Cambridge model, discussed above.\n\nLike the United Kingdom some professional bodies in New Zealand style themselves as \"colleges\", for example, the Royal Australasian College of Surgeons, the Royal Australasian College of Physicians.\n\nSecondary school is often referred to as college and the term is used interchangeably with high school. This is reflected in the names of many secondary schools such as Rangitoto College, New Zealand's largest secondary.\n\nIn the Philippines, colleges usually refer to institutions of learning that grant degrees but whose scholastic fields are not as diverse as that of a university (University of Santo Tomas, University of the Philippines, Ateneo de Manila University, De La Salle University and Far Eastern University), such as the San Beda College which specializes in law and the Mapúa Institute of Technology which specializes in engineering, or to component units within universities that do not grant degrees but rather facilitate the instruction of a particular field, such as a College of Science and College of Engineering, among many other colleges of the University of the Philippines.\n\nA state college may not have the word \"college\" on its name, but may have several component colleges, or departments. Thus, the Eulogio Amang Rodriguez Institute of Science and Technology is a state college by classification.\n\nUsually, the term \"college\" is also thought of as a hierarchical demarcation between the term \"university\", and quite a number of colleges seek to be recognized as universities as a sign of improvement in academic standards (Colegio de San Juan de Letran, San Beda College), and increase in the diversity of the offered degree programs (called \"courses\"). For private colleges, this may be done through a survey and evaluation by the Commission on Higher Education and accrediting organizations, as was the case of Urios College which is now the Fr. Saturnino Urios University. For state colleges, it is usually done by a legislation by the Congress or Senate. In common usage, \"going to college\" simply means attending school for an undergraduate degree, whether it's from an institution recognized as a college or a university.\n\nWhen it comes to referring to the level of education, \"college\" is the term more used to be synonymous to tertiary or higher education. A student who is or has studied her undergraduate degree at either an institution with \"college\" or \"university\" in its name is considered to be going to or have gone to \"college\".\n\nPresently in Portugal, the term \"colégio\" (college) is normally used as a generic reference to a private (non-government) school that provides from basic to secondary education. Many of the private schools include the term \"colégio\" in their name. Some special public schools - usually of the boarding school type - also include the term in their name, with a notable example being the \"Colégio Militar\" (Military College). The term \"colégio interno\" (literally \"internal college\") is used specifically as a generic reference to a boarding school.\n\nUntil the 19th century, a \"colégio\" was usually a secondary or pre-university school, of public or religious nature, where the students usually lived together. A model for these colleges was the Royal College of Arts and Humanities, founded in Coimbra by King John III of Portugal in 1542.\n\nThe term \"college\" in Singapore is generally only used for pre-university educational institutions called \"Junior Colleges\", which provide the final two years of secondary education (equivalent to sixth form in British terms or grades 11–12 in the American system). Since 1 January 2005, the term also refers to the three campuses of the Institute of Technical Education with the introduction of the \"collegiate system\", in which the three institutions are called ITE College East, ITE College Central, and ITE College West respectively.\n\nThe term \"university\" is used to describe higher-education institutions offering locally conferred degrees. Institutions offering diplomas are called \"polytechnics\", while other institutions are often referred to as \"institutes\" and so forth.\n\nAlthough the term \"college\" is hardly used in any context at any university in South Africa, some non-university tertiary institutions call themselves colleges. These include teacher training colleges, business colleges and wildlife management colleges. See: List of universities in South Africa#Private colleges and universities; List of post secondary institutions in South Africa.\n\nThere are several professional and vocational institutions that offer post-secondary education without granting degrees that are referred to as \"colleges\". This includes the Sri Lanka Law College, the many Technical Colleges and Teaching Colleges.\n\nFurther education (FE) colleges and sixth form colleges are institutions providing further education to students over 16. Some of these also provide higher education courses (see below). In the context of secondary education, 'college' is used in the names of some private schools, e.g. Eton College and Winchester College.\n\nIn higher education, a college is normally a provider that does not hold university status, although it can also refer to a constituent part of a collegiate or federal university or a grouping of academic faculties or departments within a university. Traditionally the distinction between colleges and universities was that colleges did not award degrees while universities did, but this is no longer the case with NCG having gained taught degree awarding powers (the same as some universities) on behalf of its colleges, and many of the colleges of the University of London holding full degree awarding powers and being effectively universities. Most colleges, however, do not hold their own degree awarding powers and continue to offer higher education courses that are validated by universities or other institutions that can award degrees.\n\nIn England, as of August 2016, over 60% of the higher education providers directly funded by HEFCE (208/340) are sixth-form or further education colleges, often termed colleges of further and higher education, along with 17 colleges of the University of London, one university college, 100 universities, and 14 other providers (six of which use 'college' in their name). Overall, this means over two thirds of state-supported higher education providers in England are colleges of one form or another. Many private providers are also called colleges, e.g. the New College of the Humanities and St Patrick's College, London.\n\nColleges within universities vary immensely in their responsibilities. The large constituent colleges of the University of London are effectively universities in their own right; colleges in some universities, including those of the University of the Arts London and smaller colleges of the University of London, run their own degree courses but do not award degrees; those at the University of Roehampton provide accommodation and pastoral care as well as delivering the teaching on university courses; those at Oxford and Cambridge deliver some teaching on university courses as well as providing accommodation and pastoral care; and those in Durham, Kent, Lancaster and York provide accommodation and pastoral care but do not normally participate in formal teaching. The legal status of these colleges also varies widely, with University of London colleges being independent corporations and recognised bodies, Oxbridge colleges, colleges of the University of the Highlands and Islands (UHI) and some Durham colleges being independent corporations and listed bodies, most Durham colleges being owned by the university but still listed bodies, and those of other collegiate universities not having formal recognition. When applying for undergraduate courses through UCAS, University of London colleges are treated as independent providers, colleges of Oxford, Cambridge, Durham and UHI are treated as locations within the universities that can be selected by specifying a 'campus code' in addition to selecting the university, and colleges of other universities are not recognised.\n\nThe UHI and the University of Wales Trinity Saint David (UWTSD) both include further education colleges. However, while the UHI colleges integrate FE and HE provision, UWTSD maintains a separation between the university campuses (Lampeter, Carmarthen and Swansea) and the two colleges (Coleg Sir Gâr and Coleg Ceredigion), which although part of the same group are treated as separate institutions rather than colleges within the university.\n\nA university college is an independent institution with the power to award taught degrees, but which has not been granted university status. University College is a protected title that can only be used with permission, although note that University College London, University College, Oxford and University College, Durham are colleges within their respective universities and not university colleges (in the case of UCL holding full degree awarding powers that set it above a university college), while University College Birmingham is a university in its own right and also not a university college.\n\nIn the United States, there are over 7,021 colleges and universities. A \"college\" in the US formally denotes a constituent part of a university, but in popular usage, the word \"college\" is the generic term for any post-secondary undergraduate education. Americans \"go to college\" after high school, regardless of whether the specific institution is formally a college or a university. Some students choose to dual-enroll, by taking college classes while still in high school. The word and its derivatives are the standard terms used to describe the institutions and experiences associated with American post-secondary undergraduate education.Students must pay for college before taking classes. Some borrow the money via loans, and some students fund their educations with cash, scholarships, or grants, or some combination of any two or more of those payment methods. In 2011, the state or federal government subsidized $8,000 to $100,000 for each undergraduate degree. For state-owned schools (called \"public\" universities), the subsidy was given to the college, with the student benefiting from lower tuition. The state subsidized on average 50% of public university tuition.\n\nColleges vary in terms of size, degree, and length of stay. Two-year colleges, also known as junior or community colleges, usually offer an associate's degree, and four-year colleges usually offer a bachelor's degree. Often, these are entirely undergraduate institutions, although some have graduate school programs.\n\nFour-year institutions in the U.S. that emphasize a liberal arts curriculum are known as liberal arts colleges. Until the 20th century, liberal arts, law, medicine, theology, and divinity were about the only form of higher education available in the United States. These schools have traditionally emphasized instruction at the undergraduate level, although advanced research may still occur at these institutions.While there is no national standard in the United States, the term \"university\" primarily designates institutions that provide undergraduate and graduate education. A university typically has as its core and its largest internal division an undergraduate college teaching a liberal arts curriculum, also culminating in a bachelor's degree. What often distinguishes a university is having, in addition, one or more graduate schools engaged in both teaching graduate classes and in research. Often these would be called a School of Law or School of Medicine, (but may also be called a college of law, or a faculty of law). An exception is Vincennes University, Indiana, which is styled and chartered as a \"university\" even though almost all of its academic programs lead only to two-year associate degrees. Some institutions, such as Dartmouth College and The College of William & Mary, have retained the term \"college\" in their names for historical reasons. In one unique case, Boston College and Boston University, both located in Boston, Massachusetts, are completely separate institutions.Usage of the terms varies among the states. In 1996 for example, Georgia changed all of its four-year institutions previously designated as colleges to universities, and all of its vocational technology schools to technical colleges.\n\nThe terms \"university\" and \"college\" do not exhaust all possible titles for an American institution of higher education. Other options include \"Polytechnic\" (Rensselaer Polytechnic Institute), \"Institute of Technology\" (Massachusetts Institute of Technology), \"academy\" (United States Military Academy), \"union\" (Cooper Union), \"conservatory\" (New England Conservatory), and \"school\" (Juilliard School). In colloquial use, they are still referred to as \"college\" when referring to their undergraduate studies.\n\nThe term \"college\" is also, as in the United Kingdom, used for a constituent semi-autonomous part of a larger university but generally organized on academic rather than residential lines. For example, at many institutions, the undergraduate portion of the university can be briefly referred to as the college (such as The College of the University of Chicago, Harvard College at Harvard, or Columbia College at Columbia) while at others, such as the University of California, Berkeley, each of the faculties may be called a \"college\" (the \"college of engineering\", the \"college of nursing\", and so forth). There exist other variants for historical reasons; for example, Duke University, which was called Trinity College until the 1920s, still calls its main undergraduate subdivision Trinity College of Arts and Sciences. \n\nSome American universities, such as Princeton, Rice, and Yale have established residential colleges (sometimes, as at Harvard, the first to establish such a system in the 1930s, known as houses) along the lines of Oxford or Cambridge. Unlike the Oxbridge colleges, but similarly to Durham, these residential colleges are not autonomous legal entities nor are they typically much involved in education itself, being primarily concerned with room, board, and social life. At the University of Michigan, University of California, San Diego and the University of California, Santa Cruz, however, each of the residential colleges does teach its own core writing courses and has its own distinctive set of graduation requirements.\n\nMany U.S. universities have placed increased emphasis on their residential colleges in recent years. This is exemplified by the creation of new colleges at Ivy League schools such as Yale University and Princeton University, and efforts to strengthen the contribution of the residential colleges to student education, including through a 2016 taskforce at Princeton on residential colleges.\n\nThe founders of the first institutions of higher education in the United States were graduates of the University of Oxford and the University of Cambridge. The small institutions they founded would not have seemed to them like universities – they were tiny and did not offer the higher degrees in medicine and theology. Furthermore, they were not composed of several small colleges. Instead, the new institutions felt like the Oxford and Cambridge colleges they were used to – small communities, housing and feeding their students, with instruction from residential tutors (as in the United Kingdom, described above). When the first students graduated, these \"colleges\" assumed the right to confer degrees upon them, usually with authority—for example, The College of William & Mary has a Royal Charter from the British monarchy allowing it to confer degrees while Dartmouth College has a charter permitting it to award degrees \"as are usually granted in either of the universities, or any other college in our realm of Great Britain.\"\n\nThe leaders of Harvard College (which granted America's first degrees in 1642) might have thought of their college as the first of many residential colleges that would grow up into a New Cambridge university. However, over time, few new colleges were founded there, and Harvard grew and added higher faculties. Eventually, it changed its title to university, but the term \"college\" had stuck and \"colleges\" have arisen across the United States.\n\nIn U.S. usage, the word \"college\" embodies not only a particular type of school, but has historically been used to refer to the general concept of higher education when it is not necessary to specify a school, as in \"going to college\" or \"college savings accounts\" offered by banks.\n\nIn a survey of more than 2,000 college students in 33 states and 156 different campuses, the U.S. Public Interest Research Group found the average student spends as much as $1,200 each year on textbooks and supplies alone. By comparison, the group says that's the equivalent of 39 percent of tuition and fees at a community college, and 14 percent of tuition and fees at a four-year public university.\n\nIn addition to private colleges and universities, the U.S. also has a system of government funded, public universities. Many were founded under the Morrill Land-Grant Colleges Act of 1862. A movement had arisen to bring a form of more practical higher education to the masses, as \"...many politicians and educators wanted to make it possible for all young Americans to receive some sort of advanced education.\" The Morrill Act \"...made it possible for the new western states to establish colleges for the citizens.\" Its goal was to make higher education more easily accessible to the citizenry of the country, specifically to improve agricultural systems by providing training and scholarship in the production and sales of agricultural products, and to provide formal education in \"...agriculture, home economics, mechanical arts, and other professions that seemed practical at the time.\"\n\nThe act was eventually extended to allow all states that had remained with the Union during the American Civil War, and eventually all states, to establish such institutions. Most of the colleges established under the Morrill Act have since become full universities, and some are among the elite of the world.\n\nSelection of a four-year college as compared to a two-year junior college, even by marginal students such as those with a C+ grade average in high school and SAT scores in the mid 800s, increases the probability of graduation and confers substantial economic and social benefits.\n\nThe term college is mainly used by private or independent secondary schools with Advanced Level (Upper 6th formers) and also Polytechnic Colleges which confer diplomas only. A student can complete secondary education (International General Certificate of Secondary Education, IGCSE) at 16 years and proceed straight to a poly-technical college or they can proceed to Advanced level (16 to 19 years) and obtain a General Certificate of Education (GCE) certificate which enables them to enrol at a University provided they have good grades alternatively with lower grades the GCE certificate holders will have an added advantage over their GCSE counterparts if they choose to enrol at a Poly-technical College. Some schools in Zimbabwe choose to offer the International Baccalaureate studies as an alternative to the IGCSE and GCE.\n\n", "id": "5689", "title": "College"}
{"url": "https://en.wikipedia.org/wiki?curid=5690", "text": "Chalmers University of Technology\n\nChalmers University of Technology (, often shortened to Chalmers) is a Swedish university located in Gothenburg that focuses on research and education in technology, natural science, architecture, maritime and other management areas.\n\nThe University was founded in 1829 following a donation by William Chalmers, a director of the Swedish East India Company. He donated part of his fortune for the establishment of an \"industrial school\". Chalmers was run as a private institution until 1937, when the institute became a state-owned university. In 1994, the school was incorporated as an\naktiebolag under the control of the Swedish Government, the faculty and the Student Union. Chalmers is one of only three universities in Sweden which are named after a person, the other two being Karolinska Institutet and Linnaeus University.\n\nOn 1 January 2005, the old schools were replaced by new departments:\n\nAs of 1 January 2016, the former departments of Applied Physics and Fundamental Physics have been joined to form the Department of Physics.\n\nIn addition to these, Chalmers is home to six national competence centres in key fields like Mathematical Modelling, Environmental Science and Vehicle Safety (SAFER).\n\nApproximately 40% of Sweden's graduate engineers and architects are educated at Chalmers. Each year, around 250 post graduate degrees are awarded as well as 850 graduate degrees. About 1,000 post-graduate students attend programmes at the university and many students are taking Master of Science engineering programmes and the Master of Architecture programme. From 2007, all Master's programmes are taught in English for both national and international students. This was a result of the adaptation to the Bologna process that started in 2004 at Chalmers (as the first technical university in Sweden).\nCurrently, about 10% of all students at Chalmers come from countries outside Sweden to enroll in a Master's or PhD program.\n\nAround 2,700 students also attend Bachelor of Science engineering programmes, merchant marine and other undergraduate courses at Campus Lindholmen. Chalmers also shares some students with Gothenburg University in the joint IT University project. The IT University focuses exclusively on information technology and offers Bachelor and Master programmes with degrees issued from either Chalmers or Gothenburg University, depending on the programme.\n\nChalmers confers honorary doctoral degrees to people outside the university who have shown great merit in their research or in society.\n\nChalmers is an aktiebolag with 100 shares à 1,000 SEK, all of which are owned by the Chalmers University of Technology Foundation, a private foundation, which appoints the university board and the president. The foundation has its members appointed by the Swedish government (4 to 8 seats), the departments appoints one member, the student union appoints one member and the president automatically gains one chair. Each department is led by a department head, usually a member of the faculty of that department. The faculty senate represents members of the faculty when decisions are taken.\n\nIn 1937, the school moved from the city center to the new Gibraltar Campus, named after the mansion which owned the grounds, where it is now located. The Lindholmen College Campus was created in the early 1990s and is located on the island of Hisingen. Campus Johanneberg and Campus Lindholmen, as they are now called, are connected by bus lines, but there have been numerous complaints that the campuses are too isolated from each other.\n\nTraditions include the graduation ceremony and the Cortège procession, an annual public event.\n\n\nChalmers has partnerships with major industries mostly in the Gothenburg region such as Ericsson, Volvo, and SKF.\nThe University has general exchange agreements with many European and U.S. universities and maintains a special exchange program agreement with National Chiao Tung University (NCTU) in Taiwan where the exchange students from the two universities maintains offices for, among other things, helping local students with applying and preparing for an exchange year as well as acting as representatives. It contributes also to the Top Industrial Managers for Europe (TIME) network.\n\nA close collaboration between the Department of Computer Science and Engineering at Chalmers and ICVR at ETH Zurich is being established. As of 2014, Chalmers University of Technology is a member of the IDEA League network.\n\nIn the 2011 International Professional Ranking of Higher Education Institutions, which is established on the basis of the number of alumni holding a post of Chief Executive Officer (CEO) or equivalent in one of the Fortune Global 500 companies, Chalmers University of Technology ranked 38th in the world, ranking 1st in Sweden and 15th in Europe.\n\nIn the latest QS World University Rankings (2016), the university was ranked 139th in the world (overall). In the latest Times Higher Education World University Rankings 2016/2017, Chalmers ranked 251-300 of all global universities. In the latest Academic Ranking of World Universities (2016), the university was ranked between places 201-300 of all universities in the world.\n\n\nAlthough the official Swedish title for the head is \"rektor\", the university now uses \"President\" as the English translation.\n\n\n", "id": "5690", "title": "Chalmers University of Technology"}
{"url": "https://en.wikipedia.org/wiki?curid=5691", "text": "Codex\n\nA codex (from the Latin \"caudex\" for \"trunk of a tree\" or \"block of wood\", \"book\"; plural \"codices\") is a book constructed of a number of sheets of paper, vellum, papyrus, or similar materials, with hand-written contents. The book is usually bound by stacking the pages and fixing one edge, and using a cover thicker than the sheets. Some codices are continuously folded like a concertina. The alternative to paged codex format for a long document is the continuous scroll. Examples of folded codices include the Maya codices. Sometimes people use the term for a book-style format, including modern printed books but excluding folded books.\n\nThe Romans developed the form from wooden writing tablets. The codex's gradual replacement of the scroll—the dominant book form in the ancient world—has been called the most important advance in book making before the invention of printing. The codex transformed the shape of the book itself, and offered a form that lasted for centuries. The spread of the codex is often associated with the rise of Christianity, which adopted the format for use with the Bible early on. First described by the 1st-century AD Roman poet Martial, who praised its convenient use, the codex achieved numerical parity with the scroll around AD 300, and had completely replaced it throughout the now Christianised Greco-Roman world by the 6th century.\n\nThe codex provides considerable advantages over other book formats:\n\nThe change from rolls to codices roughly coincides with the transition from papyrus to parchment as the preferred writing material, but the two developments are unconnected. In fact, any combination of codices and scrolls with papyrus and parchment is technically feasible and common in the historical record.\n\nThe codex began to replace the scroll almost as soon as it was invented. In Egypt, by the fifth century, the codex outnumbered the scroll by ten to one based on surviving examples. By the sixth century, the scroll had almost vanished as a medium for literature.\n\nTechnically, even modern paperbacks are codices, but publishers and scholars reserve the term for manuscript (hand-written) books produced from Late antiquity until the Middle Ages. The scholarly study of these manuscripts from the point of view of the bookbinding craft is called codicology. The study of ancient documents in general is called paleography.\n\nThe Romans used precursors made of reusable wax-covered tablets of wood for taking notes and other informal writings. Two ancient polyptychs, a \"pentatych\" and \"octotych\", excavated at Herculaneum used a unique connecting system that presages later sewing on of thongs or cords. Julius Caesar may have been the first Roman to reduce scrolls to bound pages in the form of a note-book, possibly even as a papyrus codex. At the turn of the 1st century AD, a kind of folded parchment notebook called \"pugillares membranei\" in Latin became commonly used for writing in the Roman Empire. Theodore Cressy Skeat theorized that this form of notebook was invented in Rome and then spread rapidly to the Near East. \n\nCodices are described in certain works by the Classical Latin poet, Martial. He wrote a series of five couplets meant to accompany gifts of literature that Romans exchanged during the festival of Saturnalia. Three of these books are specifically described by Martial as being in the form of a codex; the poet praises the compendiousness of the form (as opposed to the scroll), as well the convenience with which such a book can be read on a journey. In another poem by Martial, the poet advertises a new edition of his works, specifically noting that it is printed as a codex, taking less space than a scroll and more comfortable to hold in one hand. According to Theodore Cressy Skeat, this might be the first recorded known case of an entire edition of a literary work (not just a single copy) being published in codex form, though it was likely an isolated case and was not a common practice until a much later time.\n\nIn his discussion of one of the earliest parchment codices to survive from Oxyrhynchus in Egypt, Eric Turner seems to challenge Skeat’s notion when stating, “…its mere existence is evidence that this book form had a prehistory”, and that “early experiments with this book form may well have taken place outside of Egypt.” Early codices of parchment or papyrus appear to have been widely used as personal notebooks, for instance in recording copies of letters sent (Cicero \"Fam.\" 9.26.1). The parchment notebook pages were commonly washed or scraped for re-use (called a palimpsest) and consequently, writings in a codex were often considered informal and impermanent.\n\nAs early as the early 2nd century, there is evidence that a codex—usually of papyrus—was the preferred format among Christians. In the library of the Villa of the Papyri, Herculaneum (buried in AD 79), all the texts (of Greek literature) are scrolls (see Herculaneum papyri). However, in the Nag Hammadi library, hidden about AD 390, all texts (Gnostic Christian) are codices. Despite this comparison, a fragment of a non-Christian parchment codex of Demosthenes' \"De Falsa Legatione\" from Oxyrhynchus in Egypt demonstrates that the surviving evidence is insufficient to conclude whether Christians played a major or central role in the development of early codices—or if they simply adopted the format to distinguish themselves from Jews.\n\nThe earliest surviving fragments from codices come from Egypt, and are variously dated (always tentatively) towards the end of the 1st century or in the first half of the 2nd. This group includes the Rylands Library Papyrus P52, containing part of St John's Gospel, and perhaps dating from between 125 and 160.\n\nIn Western culture, the codex gradually replaced the scroll. Between the 4th century, when the codex gained wide acceptance, and the Carolingian Renaissance in the 8th century, many works that were not converted from scroll to codex were lost. The codex improved on the scroll in several ways. It could be opened flat at any page for easier reading, pages could be written on both front and back (recto and verso), and the protection of durable covers made it more compact and easier to transport.\n\nThe ancients stored codices with spines facing inward, and not always vertically. The spine could be used for the incipit, before the concept of a proper title developed in medieval times. Though most early codices were made of papyrus, papyrus was fragile and supplies from Egypt, the only place where papyrus grew and was made into paper, became scanty. The more durable parchment and vellum gained favor, despite the cost.\n\nThe codices of pre-Columbian Mesoamerica had the same form as the European codex, but were instead made with long folded strips of either fig bark (amatl) or plant fibers, often with a layer of whitewash applied before writing. New World codices were written as late as the 16th century (see Maya codices and Aztec codices). Those written before the Spanish conquests seem all to have been single long sheets folded concertina-style, sometimes written on both sides of the local amatl paper.\n\nIn East Asia, the scroll remained standard for far longer than in the Mediterranean world. There were intermediate stages, such as scrolls folded concertina-style and pasted together at the back and books that were printed only on one side of the paper.\n\nJudaism still retains the Torah scroll, at least for ceremonial use.\n\nAmong the experiments of earlier centuries, scrolls were sometimes unrolled horizontally, as a succession of columns. (The Dead Sea Scrolls are a famous example of this format.) This made it possible to fold the scroll as an accordion. The next step was then to cut the folios, sew and glue them at their centers, making it easier to use the papyrus or vellum recto-verso as with a modern book. Traditional bookbinders would call one of these assembled, trimmed and bound folios a \"codex\" to differentiate it from the \"case,\" which we now know as \"hard cover\". Binding the codex was clearly a different procedure from binding the case.\n\nThe first stage in creating a codex is to prepare the animal skin. The skin is washed with water and lime, but not together, and it has to soak in the lime for a couple of days. The hair is removed and the skin is dried by attaching it to a frame called a herse. The parchment maker attaches the skin at points around the circumference. The skin attaches to the herse by cords. To prevent tearing, the maker wraps the area of the skin the cord attaches to around a pebble called a pippin. After completing that, the maker uses a crescent shaped knife called a \"lunarium\" or \"lunellum\" to clean any surviving hairs. Once the skin completely dries, the maker gives it a deep clean and processes it into sheets. The number of sheets from a piece of skin depends on the size of the skin and the final product dimensions. For example, the average calfskin can provide three and half medium sheets of writing material. This can be doubled when folded into two conjoint leaves, also known as a \"bifolium\". Historians have found evidence of manuscripts where the scribe wrote down the medieval instructions now followed by modern membrane makers. Defects can often be found in the membrane, whether from the original animal, human error during the preparation period, or from when the animal was killed. Defects can also appear during the writing process. Unless it is kept in perfect condition, defects can appear later in the manuscript’s life as well.\n\nFirst the membrane must be prepared. The first step is to set up the quires. The quire is a group of several sheets put together. Raymond Clemens and Timothy Graham point out, in \"Introduction to Manuscript Studies\", that “the quire was the scribe’s basic writing unit throughout the Middle Ages”. They note “Pricking is the process of making holes in a sheet of parchment (or membrane) in preparation of it ruling. The lines were then made by ruling between the prick marks...The process of entering ruled lines on the page to serve as a guide for entering text. Most manuscripts were ruled with horizontal lines that served as the baselines on which the text was entered and with vertical bounding lines that marked the boundaries of the columns.”\n\nFrom the Carolingian period and all the way up to the Middle Ages, different styles of folding the quire came about. For example, in mainland Europe throughout the Middle Ages, the quire was put into a system in which each side folded on to the same style. The hair side met the hair side and the flesh side to the flesh side. This was not the same style used in the British Isles, where the membrane was folded so that it turned out an eight-leaf quire, with single leaves in the third and sixth positions. The next stage was tacking the quire. Tacking is when the scribe would hold together the leaves in quire with thread. Once threaded together, the scribe would then sew a line of parchment up the “spine” of the manuscript, as to protect the tacking.\n\n\n\n", "id": "5691", "title": "Codex"}
{"url": "https://en.wikipedia.org/wiki?curid=5692", "text": "Calf\n\nA calf (plural, calves) is the young of domestic cattle. Calves are reared to become adult cattle, or are slaughtered for their meat, called veal, and for their calfskin.\n\nThe term \"calf\" is also used for some other species. See \"Other animals\" below.\n\n\"Calf\" is the term used from birth to weaning, when it becomes known as a \"weaner\" or \"weaner calf\", though in some areas the term \"calf\" may be used until the animal is a yearling. The birth of a calf is known as \"calving\". A calf that has lost its mother is an orphan calf, also known as a \"poddy\" or \"poddy-calf\" in British English. \"Bobby calves\" are young calves which are to be slaughtered for human consumption. A \"vealer\" is a fat calf weighing less than about which is at about eight to nine months of age. A young female calf from birth until she has had a calf of her own is called a \"heifer\"\n(). In the American Old West, a motherless or small, runty calf was sometimes referred to as a \"dogie,\" (pronounced with a long \"o\").\n\nThe term \"calf\" is also used for some other species. See \"Other animals\" below.\n\n \nCalves may be produced by natural means, or by artificial breeding using artificial insemination or embryo transfer.\n\nCalves are born after a gestation of nine months. They usually stand within a few minutes of calving, and suckle within an hour. However, for the first few days they are not easily able to keep up with the rest of the herd, so young calves are often left hidden by their mothers, who visit them several times a day to suckle them. By a week old the calf is able to follow the mother all the time.\n\nSome calves are ear tagged soon after birth, especially those that are stud cattle in order to correctly identify their dams (mothers), or in areas (such as the EU) where tagging is a legal requirement for cattle. A calf must have the very best of everything until it is at least eight months old if it is to reach its maximum potential. Typically when the calves are about two months old they are branded, ear marked, castrated and vaccinated.\n\nThe \"single suckler\" system of rearing calves is similar to that occurring naturally in wild cattle, where each calf is suckled by its own mother until it is weaned at about nine months old. This system is commonly used for rearing beef cattle throughout the world.\n\nCows kept on poor forage (as is typical in subsistence farming) produce a limited amount of milk. A calf left with such a mother all the time can easily drink all the milk, leaving none for human consumption. For dairy production under such circumstances, the calf's access to the cow must be limited, for example by penning the calf and bringing the mother to it once a day after partly milking her. The small amount of milk available for the calf under such systems may mean that it takes a longer time to rear, and in subsistence farming it is therefore common for cows to calve only in alternate years.\n\nIn more intensive dairy farming, cows can easily be bred and fed to produce far more milk than one calf can drink. In the \"multi-suckler\" system, several calves are fostered onto one cow in addition to her own, and these calves' mothers can then be used wholly for milk production. More commonly, calves of dairy cows are fed formula milk from a bottle or bucket from soon after birth.\n\nPurebred female calves of dairy cows are reared as replacement dairy cows. Most purebred dairy calves are produced by artificial insemination (AI). By this method each bull can serve very many cows, so only a very few of the purebred dairy male calves are needed to provide bulls for breeding. The remainder of the male calves may be reared for beef or veal; however, some extreme dairy breeds carry so little muscle that rearing the purebred male calves may be uneconomic, and in this case they are often killed soon after birth and disposed of. Only a proportion of purebred heifers are needed to provide replacement cows, so often some of the cows in dairy herds are put to a beef bull to produce crossbred calves suitable for rearing as beef.\n\nVeal calves may be reared entirely on milk formula and killed at about 18 or 20 weeks as \"white\" veal, or fed on grain and hay and killed at 22 to 35 weeks to produce red or pink veal.\n\nA commercial steer or bull calf is expected to put on about per month. A nine-month-old steer or bull is therefore expected to weigh about . Heifers will weigh at least at eight months of age.\n\nCalves are usually weaned at about eight to nine months of age, but depending on the season and condition of the dam, they might be weaned earlier. They may be paddock weaned, often next to their mothers, or weaned in stockyards. The latter system is preferred by some as it accustoms the weaners to the presence of people and they are trained to take feed other than grass. Small numbers may also be weaned with their dams with the use of weaning nose rings or nosebands which results in the mothers rejecting the calves' attempts to suckle. Many calves are also weaned when they are taken to the large weaner auction sales that are conducted in the south eastern states of Australia. Victoria and New South Wales have yardings of up to 8,000 weaners (calves) for auction sale in one day. The best of these weaners may go to the butchers. Others will be purchased by re-stockers to grow out and fatten on grass or as potential breeders. In the United States these weaners may be known as \"feeders\" and would be placed directly into feedlots.\n\nAt about 12 months old a beef heifer reaches puberty if she is well grown.\n\nCalves suffer from few congenital abnormalities but the Akabane virus is widely distributed in temperate to tropical regions of the world. The virus is a teratogenic pathogen which causes abortions, stillbirths, premature births and congenital abnormalities, but occurs only during some years.\n\nCalf meat for human consumption is called veal; also eaten are calf's brains and calf liver. The hide is used to make calfskin, or tanned into leather and called calf leather, or sometimes in the US \"novillo\", the Spanish term. The fourth compartment of the stomach of slaughtered milk-fed calves is the source of rennet. The intestine is used to make Goldbeater's skin, and is the source of Calf Intestinal Alkaline Phosphatase (CIP).\n\nDairy cows can only produce milk after having calved, and so every dairy cow is forced to produce one calf each year throughout her productive life. On average one of these calves will become a replacement dairy cow, and some of the rest may be reared for beef or veal; however some are effectively produced solely to allow the cow to produce milk.\n\nIn English the term \"calf\" is used by extension for the young of various other large species of mammal. In addition to other cattle (such as bison, yak and water buffalo), these include the young of camels, dolphins, elephants, giraffes, hippopotamuses, larger deer (such as moose, elk (wapiti) and red deer), rhinoceroses, porpoises, whales, walruses and larger seals. But common domestic species tend to have their own names, such as foal and lamb.\n\n", "id": "5692", "title": "Calf"}
{"url": "https://en.wikipedia.org/wiki?curid=5693", "text": "Claude Shannon\n\nClaude Elwood Shannon (April 30, 1916 – February 24, 2001) was an American mathematician, electrical engineer, and cryptographer known as \"the father of information theory\".\n\nShannon is noted for having founded information theory with a landmark paper, \"A Mathematical Theory of Communication\", that he published in 1948. He is, perhaps, equally well known for founding digital circuit design theory in 1937, when—as a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT)—he wrote his thesis demonstrating that electrical applications of Boolean algebra could construct any logical, numerical relationship. Shannon contributed to the field of cryptanalysis for national defense during World War II, including his fundamental work on codebreaking and secure telecommunications.\n\nShannon was born in Petoskey, Michigan and grew up in Gaylord, Michigan. His father, Claude, Sr. (1862–1934), a descendant of early settlers of New Jersey, was a self-made businessman, and for a while, a Judge of Probate. Shannon's mother, Mabel Wolf Shannon (1890–1945), was a language teacher, and also served as the principal of Gaylord High School.\n\nMost of the first 16 years of Shannon's life were spent in Gaylord, where he attended public school, graduating from Gaylord High School in 1932. Shannon showed an inclination towards mechanical and electrical things. His best subjects were science and mathematics. At home he constructed such devices as models of planes, a radio-controlled model boat and a wireless telegraph system to a friend's house a half-mile away. While growing up, he also worked under Andrew Coltrey as a messenger for the Western Union company.\n\nHis childhood hero was Thomas Edison, who he later learned was a distant cousin. Both were descendants of John Ogden (1609–1682), a colonial leader and an ancestor of many distinguished people.\n\nShannon was apolitical and an atheist.\n\nIn 1932, Shannon entered the University of Michigan, where he was introduced to the work of George Boole. He graduated in 1936 with two bachelor's degrees: one in electrical engineering and the other in mathematics.\n\nIn 1936, Shannon began his graduate studies in electrical engineering at MIT, where he worked on Vannevar Bush's differential analyzer, an early analog computer.\nWhile studying the complicated \"ad hoc\" circuits of this analyzer, Shannon designed switching circuits based on Boole's concepts. In 1937, he wrote his master's degree thesis, \"A Symbolic Analysis of Relay and Switching Circuits\", A paper from this thesis was published in 1938. In this work, Shannon proved that his switching circuits could be used to simplify the arrangement of the electromechanical relays that were used then in telephone call routing switches. Next, he expanded this concept, proving that these circuits could solve all problems that Boolean algebra could solve. In the last chapter, he presents diagrams of several circuits, including a 4-bit full adder.\n\nUsing this property of electrical switches to implement logic is the fundamental concept that underlies all electronic digital computers. Shannon's work became the foundation of digital circuit design, as it became widely known in the electrical engineering community during and after World War II. The theoretical rigor of Shannon's work superseded the \"ad hoc\" methods that had prevailed previously. Howard Gardner called Shannon's thesis \"possibly the most important, and also the most noted, master's thesis of the century.\"\n\nShannon received his Ph.D. degree from MIT in 1940. Vannevar Bush suggested that Shannon should work on his dissertation at the Cold Spring Harbor Laboratory, in order to develop a mathematical formulation for Mendelian genetics. This research resulted in Shannon's Ph.D. thesis, called \"An Algebra for Theoretical Genetics.\"\n\nIn 1940, Shannon became a National Research Fellow at the Institute for Advanced Study in Princeton, New Jersey. In Princeton, Shannon had the opportunity to discuss his ideas with influential scientists and mathematicians such as Hermann Weyl and John von Neumann, and he also had occasional encounters with Albert Einstein and Kurt Gödel. Shannon worked freely across disciplines, and this ability may have contributed to his later development of mathematical Information theory.\n\nShannon then joined Bell Labs to work on fire-control systems and cryptography during World War II, under a contract with section D-2 (Control Systems section) of the National Defense Research Committee (NDRC).\n\nShannon is credited with the invention of signal-flow graphs, in 1942. He discovered the topological gain formula while investigating the functional operation of an analog computer.\n\nFor two months early in 1943, Shannon came into contact with the leading British mathematician Alan Turing. Turing had been posted to Washington to share with the U.S. Navy's cryptanalytic service the methods used by the British Government Code and Cypher School at Bletchley Park to break the ciphers used by the Kriegsmarine U-boats in the north Atlantic Ocean. He was also interested in the encipherment of speech and to this end spent time at Bell Labs. Shannon and Turing met at teatime in the cafeteria. Turing showed Shannon his 1936 paper that defined what is now known as the \"Universal Turing machine\"; this impressed Shannon, as many of its ideas complemented his own.\n\nIn 1945, as the war was coming to an end, the NDRC was issuing a summary of technical reports as a last step prior to its eventual closing down. Inside the volume on fire control, a special essay titled \"Data Smoothing and Prediction in Fire-Control Systems\", coauthored by Shannon, Ralph Beebe Blackman, and Hendrik Wade Bode, formally treated the problem of smoothing the data in fire-control by analogy with \"the problem of separating a signal from interfering noise in communications systems.\" In other words, it modeled the problem in terms of data and signal processing and thus heralded the coming of the Information Age.\n\nShannon's work on cryptography was even more closely related to his later publications on communication theory. At the close of the war, he prepared a classified memorandum for Bell Telephone Labs entitled \"A Mathematical Theory of Cryptography,\" dated September 1945. A declassified version of this paper was published in 1949 as \"Communication Theory of Secrecy Systems\" in the \"Bell System Technical Journal\". This paper incorporated many of the concepts and mathematical formulations that also appeared in his \"A Mathematical Theory of Communication\". Shannon said that his wartime insights into communication theory and cryptography developed simultaneously and that \"they were so close together you couldn’t separate them\". In a footnote near the beginning of the classified report, Shannon announced his intention to \"develop these results … in a forthcoming memorandum on the transmission of information.\"\n\nWhile he was at Bell Labs, Shannon proved that the cryptographic one-time pad is unbreakable in his classified research that was later published in October 1949. He also proved that any unbreakable system must have essentially the same characteristics as the one-time pad: the key must be truly random, as large as the plaintext, never reused in whole or part, and be kept secret.\n\nLater on in the American Venona project, a supposed \"one-time pad\" system by the Soviets was partially broken by the National Security Agency, but this was because of misuses of the one-time pads by Soviet cryptographic technicians in the United States and Canada. The Soviet technicians made the mistake of sometimes using the same pads more than once, and this was noticed by American cryptanalysts.\n\nIn 1948, the promised memorandum appeared as \"A Mathematical Theory of Communication,\" an article in two parts in the July and October issues of the \"Bell System Technical Journal\". This work focuses on the problem of how best to encode the information a sender wants to transmit. In this fundamental work, he used tools in probability theory, developed by Norbert Wiener, which were in their nascent stages of being applied to communication theory at that time. Shannon developed information entropy as a measure of the uncertainty in a message while essentially inventing the field of information theory.\n\nThe book, co-authored with Warren Weaver, \"The Mathematical Theory of Communication\", reprints Shannon's 1948 article and Weaver's popularization of it, which is accessible to the non-specialist. Warren Weaver pointed out that the word information in communication theory is not related to what you do say, but to what you could say. That is, information is a measure of one's freedom of choice when one selects a message. Shannon's concepts were also popularized, subject to his own proofreading, in John Robinson Pierce's \"Symbols, Signals, and Noise\".\n\nInformation theory's fundamental contribution to natural language processing and computational linguistics was further established in 1951, in his article \"Prediction and Entropy of Printed English\", showing upper and lower bounds of entropy on the statistics of English – giving a statistical foundation to language analysis. In addition, he proved that treating whitespace as the 27th letter of the alphabet actually lowers uncertainty in written language, providing a clear quantifiable link between cultural practice and probabilistic cognition.\n\nAnother notable paper published in 1949 is \"Communication Theory of Secrecy Systems\", a declassified version of his wartime work on the mathematical theory of cryptography, in which he proved that all theoretically unbreakable ciphers must have the same requirements as the one-time pad. He is also credited with the introduction of sampling theory, which is concerned with representing a continuous-time signal from a (uniform) discrete set of samples. This theory was essential in enabling telecommunications to move from analog to digital transmissions systems in the 1960s and later.\n\nHe returned to MIT to hold an endowed chair in 1956.\n\nIn 1956 Shannon joined the MIT faculty to work in the Research Laboratory of Electronics (RLE). He continued to serve on the MIT faculty until 1978.\n\nShannon developed Alzheimer's disease and spent the last few years of his life in a nursing home in Massachusetts oblivious to the marvels of the digital revolution he had helped create. He died in 2001. He was survived by his wife, Mary Elizabeth Moore Shannon, his son, Andrew Moore Shannon, his daughter, Margarita Shannon, his sister, Catherine Shannon Kay, and his two granddaughters. His wife stated in his obituary that, had it not been for Alzheimer's disease, \"He would have been bemused\" by it all.\n\nOutside of his academic pursuits, Shannon was interested in juggling, unicycling, and chess. He also invented many devices, including a Roman numeral computer called THROBAC, juggling machines, and a flame-throwing trumpet. One of his more humorous devices was a box kept on his desk called the \"Ultimate Machine\", based on an idea by Marvin Minsky. Otherwise featureless, the box possessed a single switch on its side. When the switch was flipped, the lid of the box opened and a mechanical hand reached out, flipped off the switch, then retracted back inside the box. Renewed interest in the \"Ultimate Machine\" has emerged on YouTube and Thingiverse. In addition, he built a device that could solve the Rubik's Cube puzzle.\n\nShannon designed the Minivac 601, a digital computer trainer to teach business people about how computers functioned. It was sold by the Scientific Development Corp starting in 1961.\n\nHe is also considered the co-inventor of the first wearable computer along with Edward O. Thorp. The device was used to improve the odds when playing roulette.\n\nShannon met his wife Betty when she was a numerical analyst at Bell Labs. They were married in 1949.\n\nShannon had three children, Robert James Shannon, Andrew Moore Shannon, and Margarita Shannon, and raised his family in Winchester, Massachusetts. His oldest son, Robert Shannon, died in 1998 at the age of 45.\n\nTo commemorate Shannon's achievements, there were celebrations of his work in 2001.\n\nThere are currently six statues of Shannon sculpted by Eugene Daub: one at the University of Michigan; one at MIT in the Laboratory for Information and Decision Systems; one in Gaylord, Michigan; one at the University of California, San Diego; one at Bell Labs; and another at AT&T Shannon Labs. After the breakup of the Bell System, the part of Bell Labs that remained with AT&T Corporation was named Shannon Labs in his honor.\n\nAccording to Neil Sloane, an AT&T Fellow who co-edited Shannon's large collection of papers in 1993, the perspective introduced by Shannon's communication theory (now called information theory) is the foundation of the digital revolution, and every device containing a microprocessor or microcontroller is a conceptual descendant of Shannon's publication in 1948: \"He's one of the great men of the century. Without him, none of the things we know today would exist. The whole digital revolution started with him.\" The unit shannon is named after Claude Shannon.\n\n\"Theseus\", created in 1950, was a magnetic mouse controlled by an electromechanical relay circuit that enabled it to move around a labyrinth of 25 squares. Its dimensions were the same as those of an average mouse. The maze configuration was flexible and it could be modified arbitrarily by rearranging movable partitions. The mouse was designed to search through the corridors until it found the target. Having travelled through the maze, the mouse could then be placed anywhere it had been before, and because of its prior experience it could go directly to the target. If placed in unfamiliar territory, it was programmed to search until it reached a known location and then it would proceed to the target, adding the new knowledge to its memory and learning new behavior. Shannon's mouse appears to have been the first artificial learning device of its kind.\n\nIn 1950, Shannon published a paper on computer chess entitled \"Programming a Computer for Playing Chess\". It describes how a machine or computer could be made to play a reasonable game of chess. His process for having the computer decide on which move to make was a minimax procedure, based on an evaluation function of a given chess position. Shannon gave a rough example of an evaluation function in which the value of the black position was subtracted from that of the white position. \"Material\" was counted according to the usual chess piece relative value (1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen). He considered some positional factors, subtracting ½ point for each doubled pawn, backward pawn, and isolated pawn. Another positional factor in the evaluation function was \"mobility\", adding 0.1 point for each legal move available. Finally, he considered checkmate to be the capture of the king, and gave the king the artificial value of 200 points. Quoting from the paper:\n\nThe evaluation function was clearly for illustrative purposes, as Shannon stated. For example, according to the function, pawns that are doubled as well as isolated would have no value at all, which is clearly unrealistic.\n\nShannon and his wife Betty also used to go on weekends to Las Vegas with MIT mathematician Ed Thorp, and made very successful forays in blackjack using game theory type methods co-developed with fellow Bell Labs associate John L. Kelly Jr., a physicist, based on principles of information theory. His method, known as the High-Low method, a level 1 count methodology, works by adding 1, 0, or -1 depending on the cards that appear. Shannon and Thorp also invented a small, concealable computer to help them calculate odds while gambling. They made a fortune, as detailed in the book \"Fortune's Formula\" by William Poundstone and corroborated by the writings of Elwyn Berlekamp, Kelly's research assistant in 1960 and 1962. Shannon and Thorp also applied the same theory, later known as the \"Kelly criterion\", to the stock market with even better results. Claude Shannon's card count techniques were explained in \"Bringing Down the House\", the best-selling book published in 2003 about the MIT Blackjack Team by Ben Mezrich. In 2008, the book was adapted into a drama film titled \"21\".\n\nShannon formulated a version of Kerckhoffs' principle as \"The enemy knows the system\". In this form it is known as \"Shannon's maxim\".\n\nThe Shannon Centenary, 2016, marked the life and influence of Claude Elwood Shannon on the hundredth anniversary of his birth on 30 April 1916. It was inspired in part by the Alan Turing Year. An ad hoc committee of the IEEE Information Theory Society including Christina Fragouli, Rüdiger Urbanke, Michelle Effros, Lav Varshney and Sergio Verdú, coordinated worldwide events. The initiative was announced in the History Panel at the 2015 IEEE Information Theory Workshop Jerusalem and the IEEE Information Theory Society Newsletter.\n\nA detailed listing of confirmed events was available on the website of the IEEE Information Theory Society.\n\nSome of the planned activities included:\n\nThe Claude E. Shannon Award was established in his honor; he was also its first recipient, in 1972.\n\n\n\n", "id": "5693", "title": "Claude Shannon"}
{"url": "https://en.wikipedia.org/wiki?curid=5694", "text": "Cracking\n\nCracking may refer to:\n\n\nIn computing:\n\n\n", "id": "5694", "title": "Cracking"}
{"url": "https://en.wikipedia.org/wiki?curid=5695", "text": "Community\n\nA community is a small or large social unit (a group of people) who have something in common, such as norms, religion, values, or identity. Often - but not always - communities share a sense of place that is situated in a given geographical area (e.g. a country, village, town, or neighborhood). Durable relations that extend beyond immediate genealogical ties also define a sense of community. People tend to define those social ties as important to their identity, practice, and roles in social institutions like family, home, work, government, society, or humanity, at large. Although communities are usually small relative to personal social ties (micro-level), \"community\" may also refer to large group affiliations (or macro-level), such as national communities, international communities, and virtual communities.\n\nThe word \"community\" derives from the Old French \"comuneté\", which comes from the Latin \"communitas\" \"community\", \"public spirit\" (from Latin \"communis\", \"shared in common\").\n\nHuman communities may share intent, belief, resources, preferences, needs, and risks in common, affecting the identity of the participants and their degree of cohesiveness.\n\nUrban sociologists contest the significance of place in shaping community. The anonymity and impersonal characterizing life in modern city spaces tend to be devoid of the collective connectedness associated with the idea of \"community”.\n\nIn archaeological studies of social communities the term \"community\" is used in two ways, paralleling usage in other areas. The first is an informal definition of community as a place where people used to live. In this sense it is synonymous with the concept of an ancient settlement, whether a hamlet, village, town, or city. The second meaning is similar to the usage of the term in other social sciences: a community is a group of people living near one another who interact socially. Social interaction on a small scale can be difficult to identify with archaeological data. Most reconstructions of social communities by archaeologists rely on the principle that social interaction is conditioned by physical distance. Therefore, a small village settlement likely constituted a social community, and spatial subdivisions of cities and other large settlements may have formed communities. Archaeologists typically use similarities in material culture—from house types to styles of pottery—to reconstruct communities in the past. This is based on the assumption that people or households will share more similarities in the types and styles of their material goods with other members of a social community than they will with outsiders.\n\nIn ecology, a community is an assemblage of populations of different species, interacting with one another. Community ecology is the branch of ecology that studies interactions between and among species. It considers how such interactions, along with interactions between species and the abiotic environment, affect community structure and species richness, diversity and patterns of abundance. Species interact in three ways: competition, predation and mutualism. Competition typically results in a double negative—that is both species lose in the interaction. Predation is a win/lose situation with one species winning. Mutualism, on the other hand, involves both species cooperating in some way, with both winning.\n\nIn \"Gemeinschaft und Gesellschaft\" (1887), German sociologist Ferdinand Tönnies described two types of human association: \"Gemeinschaft\" (usually translated as \"community\") and \"Gesellschaft\" (\"society\" or \"association\"). Tönnies proposed the \"Gemeinschaft–Gesellschaft\" dichotomy as a way to think about social ties. No group is exclusively one or the other. \"Gemeinschaft\" stress personal social interactions, and the roles, values, and beliefs based on such interactions. \"Gesellschaft\" stress indirect interactions, impersonal roles, formal values, and beliefs based on such interactions.\n\nGroups of people are complex, in ways that make those groups hard to form and hard to sustain; much of the shape of traditional institutions is a response to those difficulties. New social tools relieve some of those burdens, allowing for new kinds of group-forming, like using simple sharing to anchor the creation of new groups.\n\nOne simple form of cooperation, almost universal with social tools, is conversation; when people are in one another's company, even virtually, they like to talk. Conversation creates more of a sense of community than sharing does.\n\nCollaborative production is a more involved form of cooperation, as it increases the tension between individual and group goals. The litmus test for collaborative production is simple: no one person can take credit for what gets created, and the project could not come into being without the participation of many.\n\nAn online community builds weaker bonds and allows users to be anonymous. Clay Shirky, a researcher on digital media, states in reference to the audience of an online community, \"An audience isn’t just a big community; it can be more anonymous, with many fewer ties among users. A community isn’t just a small audience either; it has a social density that audiences lack.\" The sites that offer online communities, like Myspace, Twitter, Facebook, Instagram, Tumblr, and Pinterest allow users to \"stalk\" their community and act anonymously.\n\nEffective communication practices in group and organizational settings are very important to the formation and maintenance of communities. The ways that ideas and values are communicated within communities are important to the induction of new members, the formulation of agendas, the selection of leaders and many other aspects. Organizational communication is the study of how people communicate within an organizational context and the influences and interactions within organizational structures. Group members depend on the flow of communication to establish their own identity within these structures and learn to function in the group setting. Although organizational communication, as a field of study, is usually geared toward companies and business groups, these may also be seen as communities. The principles of organizational communication can also be applied to other types of communities.\n\nPublic administration is the province of local, state and federal governments, with local governments responsible for units in towns, cities, villages, and counties, among others. The most well known \"community department\" is housing and community development which has responsibility for both economic development initiatives, and as public housing and community infrastructure (e.g., business development).\n\nIn a seminal 1986 study, McMillan and Chavis identify four elements of \"sense of community\":\n\nThey give the following example of the interplay between these factors:\n\nSomeone puts an announcement on the dormitory bulletin board about the formation of an intramural dormitory basketball team. People attend the organizational meeting as strangers out of their individual needs (integration and fulfillment of needs). The team is bound by place of residence (membership boundaries are set) and spends time together in practice (the contact hypothesis). They play a game and win (successful shared valent event). While playing, members exert energy on behalf of the team (personal investment in the group). As the team continues to win, team members become recognized and congratulated (gaining honor and status for being members), Influencing new members to join and continue to do the same. Someone suggests that they all buy matching shirts and shoes (common symbols) and they do so (influence).\nA \"Sense of Community Index\" (SCI) has been developed by Chavis and colleagues and revised and adapted by others. Although originally designed to assess sense of community in neighborhoods, the index has been adapted for use in schools, the workplace, and a variety of types of communities.\n\nStudies conducted by the APPA show substantial evidence that young adults who feel a sense of belonging in a community, particularly small communities, develop fewer psychiatric and depressive disorders than those who do not have the feeling of love and belonging.\n\nThe process of learning to adopt the behavior patterns of the community is called socialization. The most fertile time of socialization is usually the early stages of life, during which individuals develop the skills and knowledge and learn the roles necessary to function within their culture and social environment. For some psychologists, especially those in the psychodynamic tradition, the most important period of socialization is between the ages of one and ten. But socialization also includes adults moving into a significantly different environment, where they must learn a new set of behaviors.\n\nSocialization is influenced primarily by the family, through which children first learn community norms. Other important influences include schools, peer groups, people, mass media, the workplace, and government. The degree to which the norms of a particular society or community are adopted determines one's willingness to engage with others. The norms of tolerance, reciprocity, and trust are important \"habits of the heart,\" as de Tocqueville put it, in an individual's involvement in community.\n\nCommunity development is often linked with community work or community planning, and may involve stakeholders, foundations, governments, or contracted entities including non-government organisations (NGOs), universities or government agencies to progress the social well-being of local, regional and, sometimes, national communities. More grassroots efforts, called community building or community organizing, seek to empower individuals and groups of people by providing them with the skills they need to effect change in their own communities. These skills often assist in building political power through the formation of large social groups working for a common agenda. Community development practitioners must understand both how to work with individuals and how to affect communities' positions within the context of larger social institutions. Public administrators, in contrast, need to understand community development in the context of rural and urban development, housing and economic development, and community, organizational and business development.\n\nFormal accredited programs conducted by universities, as part of degree granting institutions, are often used to build a knowledge base to drive curricula in public administration, sociology and community studies. The General Social Survey from the National Opinion Research Center at the University of Chicago and the Saguaro Seminar at the John F. Kennedy School of Government at Harvard University are examples of national community development in the United States. The Maxwell School of Citizenship and Public Affairs at Syracuse University in New York State offers core courses in community and economic development, and in areas ranging from non-profit development to US budgeting (federal to local, community funds). In the United Kingdom, Oxford University has led in providing extensive research in the field through its \" Community Development Journal,\" used worldwide by sociologists and community development practitioners.\n\nAt the intersection between community \"development\" and community \"building\" are a number of programs and organizations with community development tools. One example of this is the program of the Asset Based Community Development Institute of Northwestern University. The institute makes available downloadable tools to assess community assets and make connections between non-profit groups and other organizations that can help in community building. The Institute focuses on helping communities develop by \"mobilizing neighborhood assets\" — building from the inside out rather than the outside in. In the disability field, community building was prevalent in the 1980s and 1990s with roots in John McKnight's approaches.\n\nIn \"The Different Drum: Community-Making and Peace,\" Scott Peck argues that the almost accidental sense of community that exists at times of crisis can be consciously built. Peck believes that conscious community building is a process of deliberate design based on the knowledge and application of certain rules. He states that this process goes through four stages:\n\nMore recently Peck remarked that building a sense of community is easy but maintaining this sense of community is difficult in the modern world. Community building can use a wide variety of practices, ranging from simple events such as potlucks and small book clubs to larger-scale efforts such as mass festivals and construction projects that involve local participants rather than outside contractors.\n\nCommunity building that is geared toward citizen action is usually termed \"community organizing.\" In these cases, organized community groups seek accountability from elected officials and increased direct representation within decision-making bodies. Where good-faith negotiations fail, these constituency-led organizations seek to pressure the decision-makers through a variety of means, including picketing, boycotting, sit-ins, petitioning, and electoral politics. The ARISE Detroit! coalition and the Toronto Public Space Committee are examples of activist networks committed to shielding local communities from government and corporate domination and inordinate influence.\n\nCommunity organizing is sometimes focused on more than just resolving specific issues. Organizing often means building a widely accessible power structure, often with the end goal of distributing power equally throughout the community. Community organizers generally seek to build groups that are open and democratic in governance. Such groups facilitate and encourage consensus decision-making with a focus on the general health of the community rather than a specific interest group. The three basic types of community organizing are grassroots organizing, coalition building, and \"institution-based community organizing,\" (also called \"broad-based community organizing,\" an example of which is faith-based community organizing, or Congregation-based Community Organizing).\n\nIf communities are developed based on something they share in common, whether that be location or values, then one challenge for developing communities is how to incorporate individuality and differences. Indeed, as Rebekah Nathan suggests in her book, \"My Freshman Year\", we are actually drawn to developing communities totally based on sameness, despite stated commitments to diversity, such as those found on university websites. Nathan states that certain commonalities allow college students to cohere: \"What holds students together, really, is age, pop culture, a handful of (recent) historical events, and getting a degree\" (qtd. In Barrios 229). Universities may try to create community through all freshman reads, freshman seminars, and school pride; however, Nathan argues students will only form communities based on the attributes, such as age and pop culture, that they bring with them to college. Nathan's point, then, is that people come to college and don't expand their social horizons and cultural tolerance, which can prevent the development of your social community. (Barrios, Barclay. \"Emerging: Contemporary Readings for Writers\". New York: Bedford St. Martins, 2010.)\n\nSome communities have developed their own \"Local Exchange Trading Systems\" (LETS) and local currencies, such as the Ithaca Hours system, to encourage economic growth and an enhanced sense of community. Community currencies have recently proven valuable in meeting the needs of people living in various South American nations, particularly Argentina, that recently suffered as a result of the collapse of the Argentinian national currency.\n\nCommunity services is a term that refers to a wide range of community institutions, governmental and non-governmental services, voluntary, third sector organizations, and grassroots and neighborhood efforts in local communities, towns, cities, and suburban-exurban areas. In line with governmental and community thinking, volunteering and unpaid services are often preferred (e.g., altruism, beneficence) to large and continued investments in infrastructure and community services personnel, with private-public partnerships often common.\n\nNon-profit organizations from youth services, to family and neighborhood centers, recreation facilities, civic clubs, and employment, housing and poverty agencies are often the foundation of community services programs, but it may also be undertaken under the auspices of government (which funds all NGOs), one or more businesses, or by individuals or newly formed collaboratives. Community services is also the broad term given to health and human services in local communities and was specifically used as the framework for deinstitutionalization and community integration to homes, families and local communities (e.g., community residential services).\n\nIn a broad discussion of community services, schools, hospitals, clinics, rehabilitation and criminal justice institutions also view themselves as community planners and decisionmakers together with governmental leadership (e.g., city and county offices, state-regional offices). However, while many community services are voluntary, some may be part of alternative sentencing approaches in a justice system and it can be required by educational institutions as part of internships, employment training, and post-graduation plans.\n\nCommunity services may be paid for through different revenue streams which include targeted federal funds, taxpayer contributions, state and local grants and contracts, voluntary donations, Medicaid or health care funds, community development block grants, targeted education funds, and so forth. In the 2000s, the business sector began to contract with government, and also consult on government policies, and has shifted the framework of community services to the for-profit domains.\n\nHowever, by the 1990s, the call was to return to community and to go beyond community services to belonging, relationships, community building and welcoming new population groups and diversity in community life.\n\nBangladesh Community, is a rapidly expanding and extending community in Canada with its professionals, students and families. In Alberta, Bangladesh Heritage and Ethnic\nSociety (BHESA), a not-for-profit socio-cultural & heritage association known to lead to greater understanding of culture and heritage of Bangladesh, and characterized by planning, action and mobilization of community, the promotion of\nmulticultural changes and, ultimately, influence within larger systems. Through establishing its link to the larger or more extended communities with national, international, and virtual community.\n^BHESA celebrates Bangladesh culture,\n^MJMF supports Bangladeshi and Canadian Youth,\n^International Mother Language Day Celebration 2015, ^BHESA,^MJMF,\n\nA number of ways to categorize types of community have been proposed. One such breakdown is as follows:\n\nThe usual categorizations of community relations have a number of problems: 1. they tend to give the impression that a particular community can be defined as just this kind or another; 2. they tend to conflate modern and customary community relations; 3. they tend to take sociological categories such as ethnicity or race as given, forgetting that different ethnically defined persons live in different kinds of communities — grounded, interest-based, diasporic, etc.\n\nIn response to these problems, Paul James and his colleagues have developed a taxonomy that maps community relations, and recognizes that actual communities can be characterized by different kinds of relations at the same time:\n\nIn these terms, communities can be nested and/or intersecting; one community can contain another—for example a location-based community may contain a number of ethnic communities. Both lists above can used in a cross-cutting matrix in relation to each other.\n\nPossibly the most common usage of the word \"\"community\"\" indicates a large group living in close proximity. Examples of local community include:\n\n\nIn some contexts, \"\"community\"\" indicates a group of people with a common identity other than location. Members often interact regularly. Common examples in everyday usage include:\n\nSome communities share both location and other attributes. Members choose to live near each other because of one or more common interests.\n\nDefinitions of community as \"organisms inhabiting a common environment and interacting with one another,\" while scientifically accurate, do not convey the richness, diversity and complexity of human communities. Their classification, likewise is almost never precise. Untidy as it may be, community is vital for humans. M. Scott Peck expresses this in the following way: \"There can be no vulnerability without risk; there can be no community without vulnerability; there can be no peace, and ultimately no life, without community.\"\n\n\n", "id": "5695", "title": "Community"}
{"url": "https://en.wikipedia.org/wiki?curid=5696", "text": "Community college\n\nA community college is a type of educational institution. The term can have different meanings in different countries, but usually refers to an educational institution that provides tertiary education and continuing education supplemental to traditional universities and colleges.\n\nIn Australia, the term \"community college\" refers to small private businesses running short (e.g. 6 weeks) courses generally of a self-improvement or hobbyist nature. Equivalent to the American notion of community colleges are Tertiary And Further Education colleges or TAFEs; these are institutions mostly regulated at state and territory level. There are also an increasing number of private providers, which are colloquially called \"colleges\".\n\nTAFEs and other providers carry on the tradition of adult education, which was established in Australia around the mid 19th century when evening classes were held to help adults enhance their numeracy and literacy skills. Most Australian universities can also be traced back to such forerunners, although obtaining a university charter has always changed their nature. In TAFEs and colleges today, courses are designed for personal development of an individual and/or for employment outcomes. Educational programs cover a variety of topics such as arts, languages, business and lifestyle, and are usually timetabled to run two, three or four days of the week, depending on the level of the course undertaken. A Certificate I may only run for 4 hours twice a week for a term of 9 weeks. A full-time Diploma course might have classes 4 days per week for a year (36 weeks). Some courses may be offered in the evenings or weekends to accommodate people working full-time. Funding for colleges may come from government grants and course fees, and many are not-for-profit organisations. There are located in metropolitan, regional and rural locations of Australia.\n\nLearning offered by TAFEs and colleges has changed over the years. By the 1980s many colleges had recognised a community need for computer training and since then thousands of people have been up-skilled through IT courses. The majority of colleges by the late 20th century had also become Registered Training Organisations, recognising the need to offer individuals a nurturing, non-traditional education venue to gain skills that would better prepare them for the workplace and potential job openings. TAFEs and colleges have not traditionally offered bachelor's degrees, instead providing pathway arrangements with universities to continue towards degrees. The American innovation of the associate degree is emerging at some institutions. Certificate courses I to IV, diplomas and advanced diplomas are typically offered, the latter deemed equivalent to an undergraduate qualification, albeit typically in more vocational areas. Recently, some TAFE institutes (and private providers) have also become higher education providers in their own right and are now starting to offer bachelor's degrees programs.\n\nIn Canada, community colleges are adult educational institutions that provide higher education and tertiary education, and grant certificates and diplomas. Each province has its own educational system, as prescribed by the Canadian federalism model of governance. Most Canadian colleges began in the mid-1960s and early 1970s to provide practical education and training for the emerging baby boom generation, and for immigrants from around the world who were entering Canada in increasing numbers at that time. A formative trend was the merging of the then separate vocational training and adult education (night school) institutions.\n\nCanadian colleges are either publicly funded or private post-secondary institutions (run for profit). There are 150 institutions that are generally equivalent to the US community college in certain contexts. They are usually referred to simply as \"colleges\" since in common usage a degree-granting institution is almost exclusively a university.\n\nIn addition to graduate degrees, universities generally grant Associate's degrees and Bachelor's degrees, but in some regions and/or courses of study, colleges and universities collaborate so college students can earn transfer credits toward undergraduate university degrees.\n\nUniversity degrees are usually attained through four years of study. The term associate degree is used in western Canada to refer to a two-year college arts or science degree, similar to how the term is used in the United States. In other parts of Canada the term advanced degree is used to indicate a 3- or 4-year college program.\n\nIn the province of Quebec, three years is the norm for a university degree because a year of credit is earned in the CEGEP (college) system. Even when speaking in English, people often refer to all colleges as Cégeps, however the term is an acronym more correctly applied specifically to the French-language public system: Collège d'enseignement général et professionnel (CEGEP); in English: College of General and Vocational Education. The word College can also refer to a private High School in Quebec.\n\n\nIn India, 98 community colleges are recognized by the University Grants Commission. The courses offered by these colleges are diplomas, advance diplomas and certificate courses. The duration of these courses usually ranges from six months to two years.\n\nCommunity colleges in Malaysia are a network of educational institutions whereby vocational and technical skills training could be provided at all levels for school leavers before they entered the workforce. The community colleges also provide an infrastructure for rural communities to gain skills training through short courses as well as providing access to a post-secondary education.\n\nAt the moment, most community colleges award qualifications up to Level 3 in the Malaysian Qualifications Framework (Certificate 3) in both the Skills sector (Sijil Kemahiran Malaysia or the Malaysian Skills Certificate) as well as the Vocational and Training sector but the number of community colleges that are starting to award Level 4 qualifications (Diploma) are increasing. This is two levels below a bachelor's degree (Level 6 in the MQF) and students within the system who intend to further their studies to that level will usually seek entry into Advanced Diploma programs in public universities, polytechnics or accredited private providers.\n\nIn the Philippines, a community school functions as elementary or secondary school at daytime and towards the end of the day convert into a community college. This type of institution offers night classes under the supervision of the same principal, and the same faculty members who are given part-time college teaching load.\n\nThe concept of community college dates back to the time of the former Minister of Education, Culture and Sports (MECS) that had under its wings the Bureaus of Elementary Education, Secondary Education, Higher Education and Vocational-Technical Education. MECS Secretary, Dr. Cecilio Putong, who in 1971 wrote that a community school is a school established in the community, by the community, and for the community itself. Dr. Pedro T. Orata of Pangasinan shared the same idea, hence the establishment of a community college, now called the City College of Urdaneta.\n\nA community college like the one in Abuyog, Leyte can operate with only a PHP 124,000 annual budget in a two-storey structure housing more than 700 students.\n\nIn the United Kingdom, except for Scotland, a community college is a school which not only provides education for the school-age population (11–18) of the locality, but also additional services and education to adults and other members of the community. This education includes but is not limited to sports, adult literacy and lifestyle education. Usually at the age of 16 when students finish their secondary school studies, they move on to a sixth form college where they study for their A-levels (although some secondary schools have integrated sixth forms). After the two-year A-level period, they may then proceed to a college of further education or a university. This is also known as a technical college.\n\nIn the United States, community colleges, sometimes called junior colleges, technical colleges, two-year colleges, or city colleges, are primarily two-year public institutions providing higher education and lower-level tertiary education, granting certificates, diplomas, and associate degrees. Many also offer continuing and adult education. After graduating from a community college, some students transfer to a four-year liberal arts college or university for two to three years to complete a bachelor's degree.\n\nBefore the 1970s, community colleges in the United States were more commonly referred to as junior colleges, and that term is still used at some institutions. However, the term \"junior college\" has evolved to describe private two-year institutions, whereas the term \"community college\" has evolved to describe publicly funded two-year institutions. The name derives from the fact that community colleges primarily attract and accept students from the local community, and are often supported by local tax revenue.\n\nThere are research organizations and publications which focus upon the activities of community college, junior college, and technical college institutions. Many of these institutions and organizations present the most current research and practical outcomes at annual community college conferences.\n\nSeveral peer-reviewed journals extensively publish research on community colleges:\n\n\n\n\n\n\n\n", "id": "5696", "title": "Community college"}
{"url": "https://en.wikipedia.org/wiki?curid=5697", "text": "Civil Rights Memorial\n\nThe Civil Rights Memorial is a memorial in Montgomery, Alabama, to 41 people who died in the struggle for the equal and integrated treatment of all people, regardless of race, during the Civil Rights Movement in the United States. The memorial is sponsored by the Southern Poverty Law Center.\n\nThe names included in the memorial belong to those who died between 1954 and 1968. Those dates were chosen because in 1954 the U.S. Supreme Court ruled that racial segregation in schools was unlawful and 1968 is the year of Martin Luther King's assassination. The monument was created by Maya Lin, who is best known for creating the Vietnam Veterans Memorial in Washington, D.C. The Civil Rights Memorial was dedicated in 1989.\n\nThe concept of Maya Lin's design is based on the soothing and healing effect of water. It was inspired by Martin Luther King Jr.'s paraphrase \"\"... we will not be satisfied until justice rolls down like waters and righteousness like a mighty stream. ...\"\", from the \"I Have a Dream\" speech, delivered at the Lincoln Memorial, Washington D.C. on August 28, 1963. This passage in King's speech is a direct reference to Amos , as translated in the American Standard Version of the Bible. The memorial is a fountain in the form of an asymmetric inverted stone cone. A film of water flows over the base of the cone, which contains the 41 names included. It is possible to touch the smooth film of water and temporarily alter the surface film, which quickly returns to smoothness. As such, the memorial represents the aspirations of the American Civil Rights Movement to end legal segregation.\n\nThe memorial is located downtown at 400 Washington Avenue in an open plaza in front of the Civil Rights Memorial Center, which was formerly the offices of the Southern Poverty Law Center and which moved across the street into a new building in 2001. The memorial may be visited freely 24 hours a day, seven days a week.\n\nThe Civil Rights Memorial Center offers guided group tours lasting approximately one hour. Tours are available by appointment, Monday through Saturday.\n\nThe memorial is only a few blocks from other historic sites, including the Dexter Avenue King Memorial Baptist Church, the Alabama State Capitol, the Alabama Department of Archives and History, the corners where in 1955 Claudette Colvin and Rosa Parks boarded buses on which they would later refuse to give up their seats, and the Rosa Parks Library and Museum.\n\nThe 41 names included in the Civil Rights Memorial are those of:\n\"The Forgotten\" are 74 men and women who are identified in a display at the Civil Rights Memorial Center. These names were not inscribed on the Memorial because there was insufficient information about their deaths at the time the Memorial was created. However, it is thought that these people died as a result of racially motivated violence between 1952 and 1968.\n\n", "id": "5697", "title": "Civil Rights Memorial"}
{"url": "https://en.wikipedia.org/wiki?curid=5698", "text": "Charles Babbage\n\nCharles Babbage (; 26 December 1791 – 18 October 1871) was an English polymath. A mathematician, philosopher, inventor and mechanical engineer, Babbage is best remembered for originating the concept of a digital programmable computer.\n\nConsidered by some to be a \"father of the computer\", Babbage is credited with inventing the first mechanical computer that eventually led to more complex electronic designs, though all the essential ideas of modern computers are to be found in Babbage's analytical engine. His varied work in other fields has led him to be described as \"pre-eminent\" among the many polymaths of his century.\n\nParts of Babbage's uncompleted mechanisms are on display in the Science Museum in London. In 1991, a perfectly functioning difference engine was constructed from Babbage's original plans. Built to tolerances achievable in the 19th century, the success of the finished engine indicated that Babbage's machine would have worked.\n\nBabbage's birthplace is disputed, but according to the \"Oxford Dictionary of National Biography\" he was most likely born at 44 Crosby Row, Walworth Road, London, England. A blue plaque on the junction of Larcom Street and Walworth Road commemorates the event.\n\nHis date of birth was given in his obituary in \"The Times\" as 26 December 1792; but then a nephew wrote to say that Babbage was born one year earlier, in 1791. The parish register of St. Mary's Newington, London, shows that Babbage was baptised on 6 January 1792, supporting a birth year of 1791.\nBabbage was one of four children of Benjamin Babbage and Betsy Plumleigh Teape. His father was a banking partner of William Praed in founding Praed's & Co. of Fleet Street, London, in 1801. In 1808, the Babbage family moved into the old Rowdens house in East Teignmouth. Around the age of eight, Babbage was sent to a country school in Alphington near Exeter to recover from a life-threatening fever. For a short time he attended King Edward VI Grammar School in Totnes, South Devon, but his health forced him back to private tutors for a time.\n\nBabbage then joined the 30-student Holmwood academy, in Baker Street, Enfield, Middlesex, under the Reverend Stephen Freeman. The academy had a library that prompted Babbage's love of mathematics. He studied with two more private tutors after leaving the academy. The first was a clergyman near Cambridge; through him Babbage encountered Charles Simeon and his evangelical followers, but the tuition was not what he needed. He was brought home, to study at the Totnes school: this was at age 16 or 17. The second was an Oxford tutor, under whom Babbage reached a level in Classics sufficient to be accepted by Cambridge.\n\nBabbage arrived at Trinity College, Cambridge, in October 1810. He was already self-taught in some parts of contemporary mathematics; he had read in Robert Woodhouse, Joseph Louis Lagrange, and Marie Agnesi. As a result, he was disappointed in the standard mathematical instruction available at the university.\n\nBabbage, John Herschel, George Peacock, and several other friends formed the Analytical Society in 1812; they were also close to Edward Ryan. As a student, Babbage was also a member of other societies such as The Ghost Club, concerned with investigating supernatural phenomena, and the Extractors Club, dedicated to liberating its members from the madhouse, should any be committed to one.\n\nIn 1812 Babbage transferred to Peterhouse, Cambridge. He was the top mathematician there, but did not graduate with honours. He instead received a degree without examination in 1814. He had defended a thesis that was considered blasphemous in the preliminary public disputation; but it is not known whether this fact is related to his not sitting the examination.\n\nConsidering his reputation, Babbage quickly made progress. He lectured to the Royal Institution on astronomy in 1815, and was elected a Fellow of the Royal Society in 1816. After graduation, on the other hand, he applied for positions unsuccessfully, and had little in the way of career. In 1816 he was a candidate for a teaching job at Haileybury College; he had recommendations from James Ivory and John Playfair, but lost out to Henry Walter. In 1819, Babbage and Herschel visited Paris and the Society of Arcueil, meeting leading French mathematicians and physicists. That year Babbage applied to be professor at the University of Edinburgh, with the recommendation of Pierre Simon Laplace; the post went to William Wallace.\n\nWith Herschel, Babbage worked on the electrodynamics of Arago's rotations, publishing in 1825. Their explanations were only transitional, being picked up and broadened by Michael Faraday. The phenomena are now part of the theory of eddy currents, and Babbage and Herschel missed some of the clues to unification of electromagnetic theory, staying close to Ampère's force law.\n\nBabbage purchased the actuarial tables of George Barrett, who died in 1821 leaving unpublished work, and surveyed the field in 1826 in \"Comparative View of the Various Institutions for the Assurance of Lives\". This interest followed a project to set up an insurance company, prompted by Francis Baily and mooted in 1824, but not carried out. Babbage did calculate actuarial tables for that scheme, using Equitable Society mortality data from 1762 onwards.\n\nDuring this whole period Babbage depended awkwardly on his father's support, given his father's attitude to his early marriage, of 1814: he and Edward Ryan wedded the Whitmore sisters. He made a home in Marylebone in London, and founded a large family. On his father's death in 1827, Babbage inherited a large estate (value around £100,000, equivalent to £ in today's pounds), making him independently wealthy. After his wife's death in the same year he spent time travelling. In Italy he met Leopold II, Grand Duke of Tuscany, foreshadowing a later visit to Piedmont. In April 1828 he was in Rome, and relying on Herschel to manage the difference engine project, when he heard that he had become professor at Cambridge, a position he had three times failed to obtain (in 1820, 1823 and 1826).\n\nBabbage was instrumental in founding the Astronomical Society in 1820. Its initial aims were to reduce astronomical calculations to a more standard form, and to circulate data. These directions were closely connected with Babbage's ideas on computation, and in 1824 he won its Gold Medal, cited \"for his invention of an engine for calculating mathematical and astronomical tables\".\n\nBabbage's motivation to overcome errors in tables by mechanisation has been a commonplace since Dionysius Lardner wrote about it in 1834 in the \"Edinburgh Review\" (under Babbage's guidance). The context of these developments is still debated. Babbage's own account of the origin of the difference engine begins with the Astronomical Society's wish to improve \"The Nautical Almanac\". Babbage and Herschel were asked to oversee a trial project, to recalculate some part of those tables. With the results to hand, discrepancies were found. This was in 1821 or 1822, and was the occasion on which Babbage formulated his idea for mechanical computation. The issue of the \"Nautical Almanac\" is now described as a legacy of a polarisation in British science caused by attitudes to Sir Joseph Banks, who had died in 1820.\n\nBabbage studied the requirements to establish a modern postal system, with his friend Thomas Frederick Colby, concluding there should be a uniform rate that was put into effect with the introduction of the Uniform Fourpenny Post supplanted by the Uniform Penny Post in 1839 and 1840. Colby was another of the founding group of the Society. He was also in charge of the Survey of Ireland. Herschel and Babbage were present at a celebrated operation of that survey, the remeasuring of the Lough Foyle baseline.\n\nThe Analytical Society had initially been no more than an undergraduate provocation. During this period it had some more substantial achievements. In 1816 Babbage, Herschel and Peacock published a translation from French of the lectures of Sylvestre Lacroix, which was then the state-of-the-art calculus textbook.\n\nReference to Lagrange in calculus terms marks out the application of what are now called formal power series. British mathematicians had used them from about 1730 to 1760. As re-introduced, they were not simply applied as notations in differential calculus. They opened up the fields of functional equations (including the difference equations fundamental to the difference engine) and operator (D-module) methods for differential equations. The analogy of difference and differential equations was notationally changing Δ to D, as a \"finite\" difference becomes \"infinitesimal\". These symbolic directions became popular, as operational calculus, and pushed to the point of diminishing returns. The Cauchy concept of limit was kept at bay. Woodhouse had already founded this second \"British Lagrangian School\" with its treatment of Taylor series as formal.\n\nIn this context function composition is complicated to express, because the chain rule is not simply applied to second and higher derivatives. This matter was known to Woodhouse by 1803, who took from Louis François Antoine Arbogast what is now called Faà di Bruno's formula (a misnomer). In essence it was known to Abraham De Moivre (1697). Herschel found the method impressive, Babbage knew of it, and it was later noted by Ada Lovelace as compatible with the analytical engine. In the period to 1820 Babbage worked intensively on functional equations in general, and resisted both conventional finite differences and Arbogast's approach (in which Δ and D were related by the simple additive case of the exponential map). But via Herschel he was influenced by Arbogast's ideas in the matter of iteration, i.e. composing a function with itself, possibly many times. Writing in a major paper on functional equations in the \"Philosophical Transactions\" (1815/6), Babbage said his starting point was work of Gaspard Monge.\n\nFrom 1828 to 1839 Babbage was Lucasian Professor of Mathematics at Cambridge. Not a conventional resident don, and inattentive to teaching, he wrote three topical books during this period of his life. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1832. Babbage was out of sympathy with colleagues: George Biddell Airy, his predecessor as Lucasian Professor of Mathematics at Trinity College, Cambridge, thought an issue should be made of his lack of interest in lecturing. Babbage planned to lecture in 1831 on political economy. Babbage's reforming direction looked to see university education more inclusive, universities doing more for research, a broader syllabus and more interest in applications; but William Whewell found the programme unacceptable. A controversy Babbage had with Richard Jones lasted for six years. He never did give a lecture.\n\nIt was during this period that Babbage tried to enter politics. Simon Schaffer writes that his views of the 1830s included disestablishment of the Church of England, a broader political franchise, and inclusion of manufacturers as stakeholders. He twice stood for Parliament as a candidate for the borough of Finsbury. In 1832 he came in third among five candidates, missing out by some 500 votes in the two-member constituency when two other reformist candidates, Thomas Wakley and Christopher Temple, split the vote. In his memoirs Babbage related how this election brought him the friendship of Samuel Rogers: his brother Henry Rogers wished to support Babbage again, but died within days. In 1834 Babbage finished last among four. In 1832, Babbage, Herschel and Ivory were appointed Knights of the Royal Guelphic Order, however they were not subsequently made knights bachelor to entitle them to the prefix \"Sir\", which often came with appointments to that foreign order (though Herschel was later created a baronet).\n\nBabbage now emerged as a polemicist. One of his biographers notes that all his books contain a \"campaigning element\". His \"Reflections on the Decline of Science and some of its Causes\" (1830) stands out, however, for its sharp attacks. It aimed to improve British science, and more particularly to oust Davies Gilbert as President of the Royal Society, which Babbage wished to reform. It was written out of pique, when Babbage hoped to become the junior secretary of the Royal Society, as Herschel was the senior, but failed because of his antagonism to Humphry Davy. Michael Faraday had a reply written, by Gerrit Moll, as \"On the Alleged Decline of Science in England\" (1831). On the front of the Royal Society Babbage had no impact, with the bland election of the Duke of Sussex to succeed Gilbert the same year. As a broad manifesto, on the other hand, his \"Decline\" led promptly to the formation in 1831 of the British Association for the Advancement of Science (BAAS).\n\nThe \"Mechanics' Magazine\" in 1831 identified as Declinarians the followers of Babbage. In an unsympathetic tone it pointed out David Brewster writing in the \"Quarterly Review\" as another leader; with the barb that both Babbage and Brewster had received public money.\n\nIn the debate of the period on statistics (\"qua\" data collection) and what is now statistical inference, the BAAS in its Statistical Section (which owed something also to Whewell) opted for data collection. This Section was the sixth, established in 1833 with Babbage as chairman and John Elliot Drinkwater as secretary. The foundation of the Statistical Society followed. Babbage was its public face, backed by Richard Jones and Robert Malthus.\n\nBabbage published \"On the Economy of Machinery and Manufactures\" (1832), on the organisation of industrial production. It was an influential early work of operational research. John Rennie the Younger in addressing the Institute of Civil Engineers on manufacturing in 1846 mentioned mostly surveys in encyclopaedias, and Babbage's book was first an article in the \"Encyclopædia Metropolitana\", the form in which Rennie noted it, in the company of related works by John Farey, Jr., Peter Barlow and Andrew Ure. From \"An essay on the general principles which regulate the application of machinery to manufactures and the mechanical arts\" (1827), which became the \"Encyclopædia Metropolitana\" article of 1829, Babbage developed the schematic classification of machines that, combined with discussion of factories, made up the first part of the book. The second part considered the \"domestic and political economy\" of manufactures.\n\nThe book sold well, and quickly went to a fourth edition (1836). Babbage represented his work as largely a result of actual observations in factories, British and abroad. It was not, in its first edition, intended to address deeper questions of political economy; the second (late 1832) did, with three further chapters including one on piece rate. The book also contained ideas on rational design in factories, and profit sharing.\n\nIn \"Economy of Machinery\" was described what is now called the \"Babbage principle\". It pointed out commercial advantages available with more careful division of labour. As Babbage himself noted, it had already appeared in the work of Melchiorre Gioia in 1815. The term was introduced in 1974 by Harry Braverman. Related formulations are the \"principle of multiples\" of Philip Sargant Florence, and the \"balance of processes\".\n\nWhat Babbage remarked is that skilled workers typically spend parts of their time performing tasks that are below their skill level. If the labour process can be divided among several workers, labour costs may be cut by assigning only high-skill tasks to high-cost workers, restricting other tasks to lower-paid workers. He also pointed out that training or apprenticeship can be taken as fixed costs; but that returns to scale are available by his approach of standardisation of tasks, therefore again favouring the factory system. His view of human capital was restricted to minimising the time period for recovery of training costs.\n\nAnother aspect of the work was its detailed breakdown of the cost structure of book publishing. Babbage took the unpopular line, from the publishers' perspective, of exposing the trade's profitability. He went as far as to name the organisers of the trade's restrictive practices. Twenty years later he attended a meeting hosted by John Chapman to campaign against the Booksellers Association, still a cartel.\n\nIt has been written that \"what Arthur Young was to agriculture, Charles Babbage was to the factory visit and machinery\". Babbage's theories are said to have influenced the layout of the 1851 Great Exhibition, and his views had a strong effect on his contemporary George Julius Poulett Scrope. Karl Marx argued that the source of the productivity of the factory system was exactly the combination of the division of labour with machinery, building on Adam Smith, Babbage and Ure. Where Marx picked up on Babbage and disagreed with Smith was on the motivation for division of labour by the manufacturer: as Babbage did, he wrote that it was for the sake of profitability, rather than productivity, and identified an impact on the concept of a trade.\n\nJohn Ruskin went further, to oppose completely what manufacturing in Babbage's sense stood for. Babbage also affected the economic thinking of John Stuart Mill. George Holyoake saw Babbage's detailed discussion of profit sharing as substantive, in the tradition of Robert Owen and Charles Fourier, if requiring the attentions of a benevolent captain of industry, and ignored at the time.\n\nWorks by Babbage and Ure were published in French translation in 1830; \"On the Economy of Machinery\" was translated in 1833 into French by Édouard Biot, and into German the same year by Gottfried Friedenberg. The French engineer and writer on industrial organisation Léon Lalanne was influenced by Babbage, but also by the economist Claude Lucien Bergery, in reducing the issues to \"technology\". William Jevons connected Babbage's \"economy of labour\" with his own labour experiments of 1870. The Babbage principle is an inherent assumption in Frederick Winslow Taylor's scientific management.\n\nIn 1837, responding to the series of eight \"Bridgewater Treatises\", Babbage published his \"Ninth Bridgewater Treatise\", under the title \"On the Power, Wisdom and Goodness of God, as manifested in the Creation\". In this work Babbage weighed in on the side of uniformitarianism in a current debate. He preferred the conception of creation in which a God-given natural law dominated, removing the need for continuous \"contrivance\".\n\nThe book is a work of natural theology, and incorporates extracts from related correspondence of Herschel with Charles Lyell. Babbage put forward the thesis that God had the omnipotence and foresight to create as a divine legislator. In this book, Babbage dealt with relating interpretations between science and religion; on the one hand, he insisted that \"\"there exists no fatal collision between the words of Scripture and the facts of nature;\"\" on the one hand, he wrote the Book of Genesis was not meant to be read literally in relation to scientific terms. Against those who said these were in conflict, he wrote \"\"that the contradiction they have imagined can have no real existence, and that whilst the testimony of Moses remains unimpeached, we may also be permitted to confide in the testimony of our senses.\"\"\n\nThe Ninth Bridgewater Treatise was quoted extensively in \"Vestiges of the Natural History of Creation\". The parallel with Babbage's computing machines is made explicit, as allowing plausibility to the theory that transmutation of species could be pre-programmed.\nJonar Ganeri, author of \"Indian Logic\", believes Babbage may have been influenced by Indian thought; one possible route would be through Henry Thomas Colebrooke. Mary Everest Boole argues that Babbage was introduced to Indian thought in the 1820s by her uncle George Everest:\nSome time about 1825, [Everest] came to England for two or three years, and made a fast and lifelong friendship with Herschel and with Babbage, who was then quite young. I would ask any fair-minded mathematician to read Babbage's Ninth Bridgewater Treatise and compare it with the works of his contemporaries in England; and then ask himself whence came the peculiar conception of the nature of miracle which underlies Babbage's ideas of Singular Points on Curves (Chap, viii) – from European Theology or Hindu Metaphysic? Oh! how the English clergy of that day hated Babbage's book!\n\nBabbage was raised in the Protestant form of the Christian faith, his family having inculcated in him an orthodox form of worship. He explained:\n\nRejecting the Athanasian Creed as a \"direct contradiction in terms\", in his youth he looked to Samuel Clarke's works on religion, of which \"Being and Attributes of God\" (1704) exerted a particularly strong influence on him. Later in life, Babbage concluded that \"the true value of the Christian religion rested, not on speculative theology, but on \"those doctrines of kindness and benevolence which that religion claims and enforces, not merely in favour of man himself but of every creature susceptible of pain or of happiness.\"\n\nIn his autobiography \"Passages from the Life of a Philosopher\" (1864), Babbage wrote a whole chapter on the topic of religion, where he identified three sources of divine knowledge:\nHe stated, on the basis of the design argument, that studying the works of nature had been the more appealing evidence, and the one which led him to actively profess the existence of God. Advocating for natural theology, he wrote:\nLike Samuel Vince, Babbage also wrote a defense of the belief in divine miracles. Against objections previously posed by David Hume, Babbage advocated for the belief of divine agency, stating \"we must not measure the credibility or incredibility of an event by the narrow sphere of our own experience, nor forget that there is a Divine energy which overrides what we familiarly call the laws of nature.\" He alluded to the limits of human experience, expressing: \"all that we see in a miracle is an effect which is new to our observation, and whose cause is concealed. The cause may be beyond the sphere of our observation, and would be thus beyond the familiar sphere of nature; but this does not make the event a violation of any law of nature. The limits of man's observation lie within very narrow boundaries, and it would be arrogance to suppose that the reach of man's power is to form the limits of the natural world.\"\n\nThe British Association was consciously modelled on the Deutsche Naturforscher-Versammlung, founded in 1822. It rejected romantic science as well as metaphysics, and started to entrench the divisions of science from literature, and professionals from amateurs. Belonging as he did to the \"Wattite\" faction in the BAAS, represented in particular by James Watt the younger, Babbage identified closely with industrialists. He wanted to go faster in the same directions, and had little time for the more gentlemanly component of its membership. Indeed, he subscribed to a version of conjectural history that placed industrial society as the culmination of human development (and shared this view with Herschel). A clash with Roderick Murchison led in 1838 to his withdrawal from further involvement. At the end of the same year he sent in his resignation as Lucasian professor, walking away also from the Cambridge struggle with Whewell. His interests became more focussed, on computation and metrology, and on international contacts.\n\nA project announced by Babbage was to tabulate all physical constants (referred to as \"constants of nature\", a phrase in itself a neologism), and then to compile an encyclopaedic work of numerical information. He was a pioneer in the field of \"absolute measurement\". His ideas followed on from those of Johann Christian Poggendorff, and were mentioned to Brewster in 1832. There were to be 19 categories of constants, and Ian Hacking sees these as reflecting in part Babbage's \"eccentric enthusiasms\". Babbage's paper \"On Tables of the Constants of Nature and Art\" was reprinted by the Smithsonian Institution in 1856, with an added note that the physical tables of Arnold Henry Guyot \"will form a part of the important work proposed in this article\".\n\nExact measurement was also key to the development of machine tools. Here again Babbage is considered a pioneer, with Henry Maudslay, William Sellers, and Joseph Whitworth.\n\nThrough the Royal Society Babbage acquired the friendship of the engineer Marc Brunel. It was through Brunel that Babbage knew of Joseph Clement, and so came to encounter the artisans whom he observed in his work on manufactures. Babbage provided an introduction for Isambard Kingdom Brunel in 1830, for a contact with the proposed Bristol & Birmingham Railway. He carried out studies, around 1838, to show the superiority of the broad gauge for railways, used by Brunel's Great Western Railway.\n\nIn 1838, Babbage invented the pilot (also called a cow-catcher), the metal frame attached to the front of locomotives that clears the tracks of obstacles; he also constructed a dynamometer car. His eldest son, Benjamin Herschel Babbage, worked as an engineer for Brunel on the railways before emigrating to Australia in the 1850s.\n\nBabbage also invented an ophthalmoscope, which he gave to Thomas Wharton Jones for testing. Jones, however, ignored it. The device only came into use after being independently invented by Hermann von Helmholtz.\n\nBabbage achieved notable results in cryptography, though this was still not known a century after his death. Letter frequency was category 18 of Babbage's tabulation project. Joseph Henry later defended interest in it, in the absence of the facts, as relevant to the management of movable type.\n\nAs early as 1845, Babbage had solved a cipher that had been posed as a challenge by his nephew Henry Hollier, and in the process, he made a discovery about ciphers that were based on Vigenère tables. Specifically, he realized that enciphering plain text with a keyword rendered the cipher text subject to modular arithmetic. During the Crimean War of the 1850s, Babbage broke Vigenère's autokey cipher as well as the much weaker cipher that is called Vigenère cipher today. His discovery was kept a military secret, and was not published. Credit for the result was instead given to Friedrich Kasiski, a Prussian infantry officer, who made the same discovery some years later. However, in 1854, Babbage published the solution of a Vigenère cipher, which had been published previously in the \"Journal of the Society of Arts\". In 1855, Babbage also published a short letter, \"Cypher Writing\", in the same journal. Nevertheless, his priority wasn't established until 1985.\n\nBabbage involved himself in well-publicised but unpopular campaigns against public nuisances. He once counted all the broken panes of glass of a factory, publishing in 1857 a \"Table of the Relative Frequency of the Causes of Breakage of Plate Glass Windows\": Of 464 broken panes, 14 were caused by \"drunken men, women or boys\".\n\nBabbage's distaste for commoners (\"the Mob\") included writing \"Observations of Street Nuisances\" in 1864, as well as tallying up 165 \"nuisances\" over a period of 80 days. He especially hated street music, and in particular the music of organ grinders, against whom he railed in various venues. The following quotation is typical: \n\nBabbage was not alone in his campaign. A convert to the cause was the MP Michael Thomas Bass.\n\nIn the 1860s, Babbage also took up the anti-hoop-rolling campaign. He blamed hoop-rolling boys for driving their iron hoops under horses' legs, with the result that the rider is thrown and very often the horse breaks a leg. Babbage achieved a certain notoriety in this matter, being denounced in debate in Commons in 1864 for \"commencing a crusade against the popular game of tip-cat and the trundling of hoops.\"\n\nBabbage's machines were among the first mechanical computers. That they were not actually completed was largely because of funding problems and clashes of personality, most notably with Airy, the Astronomer Royal.\n\nBabbage directed the building of some steam-powered machines that achieved some modest success, suggesting that calculations could be mechanised. For more than ten years he received government funding for his project, which amounted to £17,000, but eventually the Treasury lost confidence in him.\n\nWhile Babbage's machines were mechanical and unwieldy, their basic architecture was similar to a modern computer. The data and program memory were separated, operation was instruction-based, the control unit could make conditional jumps, and the machine had a separate I/O unit.\n\nJohn Tucker, Professor of Computer Science at Swansea University, argues that it was the Welsh mathematician Robert Recorde who first laid down the foundations of these concepts.\n\nIn Babbage's time, printed mathematical tables were calculated by human computers; in other words, by hand. They were central to navigation, science and engineering, as well as mathematics. Mistakes were known to occur in transcription as well as calculation.\n\nAt Cambridge, Babbage saw the fallibility of this process, and the opportunity of adding mechanisation into its management. His own account of his path towards mechanical computation references a particular occasion:\n\nIn 1812 he was sitting in his rooms in the Analytical Society looking at a table of logarithms, which he knew to be full of mistakes, when the idea occurred to him of computing all tabular functions by machinery. The French government had produced several tables by a new method. Three or four of their mathematicians decided how to compute the tables, half a dozen more broke down the operations into simple stages, and the work itself, which was restricted to addition and subtraction, was done by eighty computers who knew only these two arithmetical processes. Here, for the first time, mass production was applied to arithmetic, and Babbage was seized by the idea that the labours of the unskilled computers could be taken over completely by machinery which would be quicker and more reliable.\n\nThere was another period, seven years later, when his interest was aroused by the issues around computation of mathematical tables. The French official initiative by Gaspard de Prony, and its problems of implementation, were familiar to him. After the Napoleonic Wars came to a close, scientific contacts were renewed on the level of personal contact: in 1819 Charles Blagden was in Paris looking into the printing of the stalled de Prony project, and lobbying for the support of the Royal Society. In works of the 1820s and 1830s, Babbage referred in detail to de Prony's project.\n\nBabbage began in 1822 with what he called the difference engine, made to compute values of polynomial functions. It was created to calculate a series of values automatically. By using the method of finite differences, it was possible to avoid the need for multiplication and division.\n\nFor a prototype difference engine, Babbage brought in Joseph Clement to implement the design, in 1823. Clement worked to high standards, but his machine tools were particularly elaborate. Under the standard terms of business of the time, he could charge for their construction, and would also own them. He and Babbage fell out over costs around 1831.\n\nSome parts of the prototype survive in the Museum of the History of Science, Oxford. This prototype evolved into the \"first difference engine.\" It remained unfinished and the finished portion is located at the Science Museum in London. This first difference engine would have been composed of around 25,000 parts, weigh fifteen tons (13,600 kg), and would have been tall. Although Babbage received ample funding for the project, it was never completed. He later (1847–1849) produced detailed drawings for an improved version,\"Difference Engine No. 2\", but did not receive funding from the British government. His design was finally constructed in 1989–1991, using his plans and 19th century manufacturing tolerances. It performed its first calculation at the Science Museum, London, returning results to 31 digits.\n\nNine years later, in 2000, the Science Museum completed the printer Babbage had designed for the difference engine.\n\nThe Science Museum has constructed two Difference Engines according to Babbage's plans for the Difference Engine No 2. One is owned by the museum. The other, owned by the technology multimillionaire Nathan Myhrvold, went on exhibition at the Computer History Museum in Mountain View, California on 10 May 2008. The two models that have been constructed are not replicas; Myhrvold's engine is the first design by Babbage, and the Science Museum's is a later model.\n\nAfter the attempt at making the first difference engine fell through, Babbage worked to design a more complex machine called the Analytical Engine. He hired C. G. Jarvis, who had previously worked for Clement as a draughtsman. The Analytical Engine marks the transition from mechanised arithmetic to fully-fledged general purpose computation. It is largely on it that Babbage's standing as computer pioneer rests.\n\nThe major innovation was that the Analytical Engine was to be programmed using punched cards: the Engine was intended to use loops of Jacquard's punched cards to control a mechanical calculator, which could use as input the results of preceding computations. The machine was also intended to employ several features subsequently used in modern computers, including sequential control, branching and looping. It would have been the first mechanical device to be, in principle, Turing-complete. The Engine was not a single physical machine, but rather a succession of designs that Babbage tinkered with until his death in 1871.\nAda Lovelace corresponded with him during his development of the Analytical Engine. She is credited with developing an algorithm for the Analytical Engine to calculate a sequence of Bernoulli numbers. Although there is disagreement over how much of the ideas were Lovelace's own, she is often described as the first computer programmer. She also translated and wrote literature supporting the project. With respect to the engine's programming by punch cards, she once wrote: \"We may say most aptly that the Analytical Engine weaves algebraical patterns just as the Jacquard-loom weaves flowers and leaves.\"\n\nBabbage visited Turin in 1840 at the invitation of Giovanni Plana. In 1842 Charles Wheatstone approached Lovelace to translate a paper of Luigi Menabrea, who had taken notes of Babbage's Turin talks; and Babbage asked her to add something of her own. Fortunato Prandi who acted as interpreter in Turin was an Italian exile and follower of Giuseppe Mazzini.\n\nPer Georg Scheutz wrote about the difference engine in 1830, and experimented in automated computation. After 1834 and Lardner's \"Edinburgh Review\" article he set up a project of his own, doubting whether Babbage's initial plan could be carried out. This he pushed through with his son, Edvard Scheutz. Another Swedish engine was that of Martin Wiberg (1860).\n\nIn 2011, researchers in Britain proposed a multimillion-pound project, \"Plan 28\", to construct Babbage's Analytical Engine. Since Babbage's plans were continually being refined and were never completed, they intended to engage the public in the project and crowd-source the analysis of what should be built. It would have the equivalent of 675 bytes of memory, and run at a clock speed of about 7 Hz. They hope to complete it by the 150th anniversary of Babbage's death, in 2021.\n\nAdvances in MEMs and nanotechnology have led to recent high-tech experiments in mechanical computation. The benefits suggested include operation in high radiation or high temperature environments. These modern versions of mechanical computation were highlighted in \"The Economist\" in its special \"end of the millennium\" black cover issue in an article entitled \"Babbage's Last Laugh\".\n\nDue to his association with the town Babbage was chosen in 2007 to appear on the 5 Totnes pound note. An image of Babbage features in the British cultural icons section of the newly designed British passport in 2015.\n\nOn 25 July 1814, Babbage married Georgiana Whitmore at St. Michael's Church in Teignmouth, Devon; her sister Louisa married Edward Ryan. The couple lived at Dudmaston Hall, Shropshire (where Babbage engineered the central heating system), before moving to 5 Devonshire Street, London in 1815.\n\nCharles and Georgiana had eight children, but only four – Benjamin Herschel, Georgiana Whitmore, Dugald Bromhead and Henry Prevost – survived childhood. Charles' wife Georgiana died in Worcester on 1 September 1827, the same year as his father, their second son (also named Charles) and their newborn son Alexander.\n\nHis youngest surviving son, Henry Prevost Babbage (1824–1918), went on to create six small demonstration pieces for Difference Engine No. 1 based on his father's designs, one of which was sent to Harvard University where it was later discovered by Howard H. Aiken, pioneer of the Harvard Mark I. Henry Prevost's 1910 Analytical Engine Mill, previously on display at Dudmaston Hall, is now on display at the Science Museum.\n\nBabbage lived and worked for over 40 years at 1 Dorset Street, Marylebone, where he died, at the age of 79, on 18 October 1871; he was buried in London's Kensal Green Cemetery. According to Horsley, Babbage died \"of renal inadequacy, secondary to cystitis.\" He had declined both a knighthood and baronetcy. He also argued against hereditary peerages, favouring life peerages instead.\n\nIn 1983 the autopsy report for Charles Babbage was discovered and later published by his great-great-grandson. A copy of the original is also available. Half of Babbage's brain is preserved at the Hunterian Museum in the Royal College of Surgeons in London. The other half of Babbage's brain is on display in the Science Museum, London.\n\nThere is a black plaque commemorating the 40 years Babbage spent at 1 Dorset Street, London. Locations, institutions and other things named after Babbage include::\n\n\nBabbage frequently appears in steampunk works; he has been called an iconic figure of the genre. Other works in which Babbage appears include:\n\n\n\n\n", "id": "5698", "title": "Charles Babbage"}
{"url": "https://en.wikipedia.org/wiki?curid=5700", "text": "Cross-dressing\n\nCross-dressing is the act of wearing items of clothing and other accoutrements commonly associated with the opposite sex within a particular society. Cross-dressing has been used for purposes of disguise, comfort, and self-discovery in modern times and throughout history.\n\nAlmost every human society throughout history has had expected norms for each gender relating to style, color, or type of clothing they are expected to wear, and likewise most societies have had a set of guidelines, views or even laws defining what type of clothing is appropriate for each gender.\n\nThe term \"cross-dressing\" denotes an action or a behavior without attributing or implying any specific causes for that behavior. It is often assumed that the connotation is directly correlated with behaviors of transgender identity or sexual, fetishist, and homosexual behavior, but the term itself does not imply any motives and is not synonymous to one's gender identity.\n\nCross-dressing has been practiced throughout much of recorded history and in many societies. There are many examples in Greek, Norse, and Hindu mythology. A reasonable number of historical figures are known to have cross-dressed to varying degrees and for a variety of reasons. There is a rich history of cross-dressing found in folklore, literature, theater, and music. Examples include Kabuki and Korean shamanism.\n\nCross dressing in early modern Spain was prevalent among theaters and happened to be the most popular form of entertainment at the time for Spanish theater audiences. There was a fascination with female cross dressers particularly,and happened to be \"extremely popular\" in the \"Golden age Comedia\". Spain viewed this cross dressing phenomenon as a threat, and passed laws targeting female transvestites throughout the 1600s. Despite the negative reactions and disapproval, it continued to remain the most popular form of theatrics and a huge contribution to Spanish comedy.\n\nThere are many different kinds of cross-dressing and many different reasons why an individual might engage in cross-dressing behavior. Some people cross-dress as a matter of comfort or style, out of personal preference for clothing associated with the opposite sex. In this case, a person's cross-dressing may or may not be apparent to other people. Some people cross-dress to shock others or challenge social norms.\n\nGender disguise has been used by women and girls to pass as male in society and by men and boys to pass themselves off as female. Gender disguise has also been used as a plot device in storytelling and is a recurring motif in literature, theater, and film. It is a common plot device in narrative ballads. Historically, some women have cross-dressed to take up male-dominated or male-exclusive professions, such as military service. Conversely, some men have cross-dressed to escape from mandatory military service or as a disguise to assist in political or social protest, as men did in the Rebecca Riots.\n\nSingle-sex theatrical troupes often have some performers who cross-dress to play roles written for members of the opposite sex (travesti). Cross-dressing, particularly the depiction of males wearing dresses, is often used for comic effect onstage and on-screen.\n\nDrag is a special form of performance art based on the act of cross-dressing. A drag queen is usually a male-assigned person who performs as an exaggeratedly feminine character, in heightened costuming sometimes consisting of a showy dress, high-heeled shoes, obvious make-up, and wig. A drag queen may imitate famous female film or pop-music stars. A faux queen is a female-assigned person employing the same techniques.\n\nA drag king is a counterpart of the drag queen but usually for much different audiences, and is defined as a female-assigned person who adopts a masculine persona in performance or imitates a male film or pop-music star. Some female-assigned people undergoing gender reassignment therapy also self-identify as \"drag kings\" although this use of \"drag king\" would generally be considered inaccurate.\nA transvestic fetishist is a person (typically a heterosexual male) who cross-dresses as part of a sexual fetish.\n\nThe term \"underdressing\" is used by male cross-dressers to describe wearing female undergarments under their male clothes. The famous low-budget film-maker Edward D. Wood, Jr. said he often wore women's underwear under his military uniform during World War II.\n\nSome people who cross-dress may endeavor to project a complete impression of belonging to another gender, including mannerisms, speech patterns, and emulation of sexual characteristics. This is referred to as passing or \"trying to pass\" depending how successful the person is. An observer who sees through the cross-dresser's attempt to pass is said to have \"read\" or \"clocked\" them. There are videos, books, and magazines on how a man may look more like a woman.\n\n\"Female masking\" is a form of cross-dressing in which men wear masks that present them as female.\n\nSometimes either member of a heterosexual couple will cross-dress in order to arouse the other. For example, the male might wear skirts or lingerie and/or the female will wear boxers or other male clothing. (See also forced feminization)\n\nOthers may choose to take a mixed approach, adopting some feminine traits and some masculine traits in their appearance. For instance, a man might wear both a dress and a beard. This is sometimes known as \"genderfuck\".\n\nThe actual determination of cross-dressing is largely socially constructed. For example, in Western society, trousers have long been adopted for usage by women, and it is no longer regarded as cross-dressing. In cultures where men have traditionally worn skirt-like garments such as the kilt or sarong, these are not seen as female clothing, and wearing them is not seen as cross-dressing for men. As societies are becoming more global in nature, both men's and women's clothing are adopting styles of dress associated with other cultures.\n\nIt was once considered taboo in Western society for women to wear clothing traditionally associated with men, except when done in certain circumstances such as cases of necessity (as per St. Thomas Aquinas's guidelines in \"Summa Theologiae II\").\nWhile this prohibition remained in force in general throughout the Middle Ages and early modern era, this is no longer the case and Western women are often seen wearing trousers, ties, and men's hats. Nevertheless, many cultures around the world still prohibit women from wearing trousers or other traditionally male clothing.\n\nCosplaying may also involve cross-dressing, for some females may wish to dress as a male, and vice versa (see Crossplay). Breast binding (for females) is not uncommon and is one of the things likely needed to cosplay a male character.\n\nIn most parts of the world it remains socially disapproved for men to wear clothes traditionally associated with women. Attempts are occasionally made, e.g. by fashion designers, to promote the acceptance of skirts as everyday wear for men. Cross-dressers have complained that society permits women to wear pants or jeans and other masculine clothing, while condemning any man who wants to wear clothing sold for women.\n\nWhile creating a more feminine figure, male cross-dressers will often utilize different types and styles of breast forms, which are silicone prostheses traditionally used by women who have undergone mastectomies to recreate the visual appearance of a breast.\n\nWhile most male cross-dressers utilize clothing associated with modern women, there are some who are involved in subcultures that involve dressing as little girls or in vintage clothing. Some such men have written that they enjoy dressing as femininely as possible, so they will wear frilly dresses with lace and ribbons, bridal gowns complete with veils, as well as multiple petticoats, corsets, girdles and/or garter belts with nylon stockings.\n\nCross-dressers may begin wearing clothing associated with the opposite sex in childhood, using the clothes of a sibling, parent, or friend. Some parents have said they allowed their children to cross-dress and, in many cases, the child stopped when they became older. The same pattern often continues into adulthood, where there may be confrontations with a spouse. Married cross-dressers experience considerable anxiety and guilt if their spouse objects to their behavior. Sometimes cross-dressers have periodically disposed of all their clothing, a practice called \"purging\", only to start collecting other gender's clothing again.\n\nThe historical associations of maleness with power and femaleness with submission and frivolity mean that in the present time a woman dressing in men's clothing and a man dressing in women's clothing evoke very different responses. A woman dressing in men's clothing is considered to be a more acceptable activity.\n\nAdvocacy for social change has done much to relax the constrictions of gender roles on men and women, but they are still subject to prejudice from some people. It is noticeable that as 'transgender' is becoming more socially accepted as a normal human condition, the prejudices against cross-dressing are changing quite quickly, just as the similar prejudices against homosexuals have changed rapidly in recent decades.\n\nThe reason it is so hard to have statistics for female-assigned cross-dressers is that the line where cross-dressing stops and cross-dressing begins has become blurred, whereas the same line for men is as well defined as ever. This is one of the many issues being addressed by third wave feminism as well as the modern-day masculist movement.\n\nCulture has very mixed views about cross-dressing. A woman who wears her husband's shirt to bed is considered attractive while a man who wears his wife's nightgown to bed may be considered transgressive. Marlene Dietrich in a tuxedo was considered very erotic; Jack Lemmon in a dress was considered ridiculous. All this may result from an overall gender role rigidity for males; that is, because of the prevalent gender dynamic throughout the world, men frequently encounter discrimination when deviating from masculine gender norms, particularly violations of heteronormativity. A man's adoption of feminine clothing is often considered a going down in the gendered social order whereas a woman's adoption of what are traditionally men's clothing (at least in the English-speaking world) has less of an impact because women have been traditionally subordinate to men, unable to affect serious change through style of dress. Thus when a male cross-dresser puts on his clothes, he transforms into the quasi-female and thereby becomes an embodiment of the conflicted gender dynamic. Following the work of Butler, gender proceeds along through ritualized performances, but in male cross-dressing it becomes a performative \"breaking\" of the masculine and a \"subversive repetition\" of the feminine.\n\nPsychoanalysts today do not regard cross-dressing by itself as a psychological problem, unless it interferes with a person's life. \"For instance,\" said Dr. Joseph Merlino, senior editor of \"Freud at 150: 21st Century Essays on a Man of Genius\", \"[suppose that]...I'm a cross-dresser and I don't want to keep it confined to my circle of friends, or my party circle, and I want to take that to my wife and I don't understand why she doesn't accept it, or I take it to my office and I don't understand why they don't accept it, then it's become a problem because it's interfering with my relationships and environment.\"\n\n\n\nNotes\nFurther reading\n\n", "id": "5700", "title": "Cross-dressing"}
{"url": "https://en.wikipedia.org/wiki?curid=5702", "text": "Channel Tunnel\n\nThe Channel Tunnel (; also nicknamed and shortened to Chunnel) is a rail tunnel linking Folkestone, Kent, in the United Kingdom, with Coquelles, Pas-de-Calais, near Calais in northern France, beneath the English Channel at the Strait of Dover. At its lowest point, it is deep below the sea bed, and below sea level. At , the tunnel has the longest undersea portion of any tunnel in the world, although the Seikan Tunnel in Japan is both longer overall at and deeper at below sea level. The speed limit for trains in the tunnel is .\n\nThe tunnel carries high-speed Eurostar passenger trains, the Eurotunnel Shuttle for road vehicles—the largest such transport in the world—and international freight trains. The tunnel connects end-to-end with the LGV Nord and High Speed 1 high-speed railway lines.\n\nIdeas for a cross-Channel fixed link appeared as early as 1802, but British political and press pressure over the compromising of national security stalled attempts to construct a tunnel. An early attempt at building a Channel Tunnel was made in the late 19th century, on the English side \"in the hope of forcing the hand of the English Government\". The eventual successful project, organised by Eurotunnel, began construction in 1988 and opened in 1994. At £5.5 billion (1985 prices), it was at the time the most expensive construction project ever proposed. The cost finally came in at £9 billion ($21 billion), well over its predicted budget. Since its construction, the tunnel has faced several problems. Both fires and cold weather have temporarily disrupted its operation. Illegal immigrants have attempted to use the tunnel to enter the UK, causing a minor diplomatic disagreement over the siting of the refugee camp at Sangatte, which was eventually closed in 2002. Migrants have also died attempting to cross through the tunnel.\n\nIn 1802, Albert Mathieu, a French mining engineer, put forward a proposal to tunnel under the English Channel, with illumination from oil lamps, horse-drawn coaches, and an artificial island mid-Channel for changing horses.\n\nIn\n1839, Aimé Thomé de Gamond, a Frenchman, performed the first geological and hydrographical surveys on the Channel, between Calais and Dover. Thomé de Gamond explored several schemes and, in 1856, he presented a proposal to Napoleon III for a mined railway tunnel from Cap Gris-Nez to Eastwater Point with a port/airshaft on the Varne sandbank at a cost of 170 million francs, or less than £7 million.\n\nIn 1865, a deputation led by George Ward Hunt proposed the idea of a tunnel to the Chancellor of the Exchequer of the day, William Ewart Gladstone.\n\nAround 1866, William Low and Sir John Hawkshaw promoted ideas, but apart from preliminary geological studies none were implemented. An official Anglo-French protocol was established in 1876 for a cross-Channel railway tunnel. In 1881, the British railway entrepreneur Sir Edward Watkin and Alexandre Lavalley, a French Suez Canal contractor, were in the Anglo-French Submarine Railway Company that conducted exploratory work on both sides of the Channel. On the English side a diameter Beaumont-English boring machine dug a pilot tunnel from Shakespeare Cliff. On the French side, a similar machine dug from Sangatte. The project was abandoned in May 1882, owing to British political and press campaigns asserting that a tunnel would compromise Britain's national defences. These early works were encountered more than a century later during the TML project.\n\nIn 1919, during the Paris Peace Conference, the British prime minister, David Lloyd George, repeatedly brought up the idea of a Channel tunnel as a way of reassuring France about British willingness to defend against another German attack. The French did not take the idea seriously and nothing came of Lloyd George's proposal.\n\nIn 1929 there was another proposal but nothing came of this discussion and the idea was shelved. Proponents estimated construction to be about US$150 million. The engineers had addressed the concerns of both nations' military leaders by designing two sumps—one near the coast of each country—that could be flooded at will to block the tunnel. This design feature did not override the concerns of both nations' military leaders, and other concerns about hordes of undesirable tourists who would disrupt English habits of living. Military fears continued during World War II. After the fall of France, as Britain prepared for an expected German invasion, a Royal Navy officer in the Directorate of Miscellaneous Weapons Development calculated that Hitler could use slave labour to build two Channel tunnels in 18 months. The estimate caused rumours that Germany had already begun digging.\n\nIn 1935, a British film from Gaumont Studios, \"The Tunnel\", also called \"TransAtlantic Tunnel\", was released as a futuristic science fiction project concerning the creation of a transatlantic tunnel. It referred briefly to its protagonist, a Mr. McAllan, as having completed a British Channel tunnel successfully in 1940, five years into the future of the film's release.\n\nBy 1955, defence arguments had become less relevant due to the dominance of air power, and both the British and French governments supported technical and geological surveys. In 1958 the 1881 workings were cleared in preparation for a £100,000 geological survey by the Channel Tunnel Study Group. 30% of the funding came from the Channel Tunnel Co Ltd, the largest shareholder of which was the British Transport Commission, as successor to the South Eastern Railway. A detailed geological survey was carried out in 1964–65.\n\nAlthough the two countries agreed to build a tunnel in 1964, the phase 1 initial studies and signing of a second agreement to cover phase 2 took until 1973. Construction work of this government-funded project to create two tunnels designed to accommodate car shuttle wagons on either side of a service tunnel started on both sides of the Channel in 1974.\n\nOn 20 January 1975, to the dismay of their French partners, the now-governing Labour Party in Britain cancelled the project due to uncertainty about EEC membership, doubling cost estimates and the general economic crisis at the time. By this time the British tunnel boring machine was ready and the Ministry of Transport was able to do a experimental drive. This short tunnel was reused as the starting and access point for tunnelling operations from the British side. The cancellation costs were estimated to be £17 million.\n\nOpposition to the tunnel over the decades reflected the high value the British placed on their insularity, and their preference for imperial links that they controlled directly. Only after the British Empire collapsed in the 1950s, and air travel replaced sea travel, could they appreciate the desirability of closer ties to the continent. With opposition fading, the government could more carefully consider the long-term economic and strategic value, and the new sense of a European identity. The British government's attitude toward a tunnel changed from hostility in 1948 to acceptance and promotion in 1964. This change reflected not only a more favourable view of being part of European unity, but also the calculation that the tunnel would provide economic advantages, especially if Britain ever joined the European Economic Community. By the 1960s, British attitudes toward the tunnel also reflected a realistic reappraisal of the country's international status: after Suez 1956 everyone realized the islands were no longer a super-power. Britain's prestige and security now seemed safest when tied closely to the continent.\n\nIn 1979, the \"Mouse-hole Project\" was suggested when the Conservatives came to power in Britain. The concept was a single-track rail tunnel with a service tunnel, but without shuttle terminals. The British government took no interest in funding the project, but Margaret Thatcher, the prime minister, said she had no objection to a privately funded project. In 1981 Thatcher and François Mitterrand, the French president, agreed to establish a working group to evaluate a privately funded project. In June 1982 the Franco-British study group favoured a twin tunnel to accommodate conventional trains and a vehicle shuttle service. In April 1985 promoters were invited to submit scheme proposals. Four submissions were shortlisted:\n\n\nThe cross-Channel ferry industry protested under the name \"Flexilink\". In 1975 there was no campaign protesting against a fixed link, with one of the largest ferry operators (Sealink) being state-owned. Flexilink continued rousing opposition throughout 1986 and 1987. Public opinion strongly favoured a drive-through tunnel, but ventilation issues, concerns about accident management, and fear of driver mesmerisation led to the only shortlisted rail submission, CTG/F-M, being awarded the project in January 1986. Among reasons given for the selection was that it caused least disruption to shipping in the Channel, least environmental disruption, was the best protected against terrorism, and was the most likely to attract sufficient private finance.\n\nThe British \"Channel Tunnel Group\" consisted of two banks and five construction companies, while their French counterparts, \"France–Manche\", consisted of three banks and five construction companies. The role of the banks was to advise on financing and secure loan commitments. On 2 July 1985, the groups formed Channel Tunnel Group/France–Manche (CTG/F–M). Their submission to the British and French governments was drawn from the 1975 project, including 11 volumes and a substantial environmental impact statement.\n\nThe design and construction was done by the ten construction companies in the CTG/F-M group. The French terminal and boring from Sangatte was undertaken by the five French construction companies in the joint venture group \"GIE Transmanche Construction\". The English Terminal and boring from Shakespeare Cliff was undertaken by the five British construction companies in the \"Translink Joint Venture\". The two partnerships were linked by TransManche Link (TML), a bi-national project organisation. The Maître d'Oeuvre was a supervisory engineering body employed by Eurotunnel under the terms of the concession that monitored project activity and reported back to the governments and banks.\n\nIn France, with its long tradition of infrastructure investment, the project garnered widespread approval. In April the French National Assembly gave unanimous support and, in June 1987, after a public inquiry, the Senate gave unanimous support. In Britain, select committees examined the proposal, making history by holding hearings away from Westminster, in Kent. In February 1987, the third reading of the Channel Tunnel Bill took place in the House of Commons, and was carried by 94 votes to 22. The Channel Tunnel Act gained Royal assent and passed into law in July. Parliamentary support for the project came partly from provincial members of Parliament on the basis of promises of regional Eurostar through train services that never materialised; the promises were repeated in 1996 when the contract for construction of the Channel Tunnel Rail Link was awarded.\n\nThe tunnel is a build-own-operate-transfer (BOOT) project with a concession.<ref name=\"Flyvbjerg p. 96/97\">Flyvbjerg et al. pp. 96–97</ref> TML would design and build the tunnel, but financing was through a separate legal entity, Eurotunnel. Eurotunnel absorbed CTG/F-M and signed a construction contract with TML, but the British and French governments controlled final engineering and safety decisions, now in the hands of the Channel Tunnel Safety Authority. The British and French governments gave Eurotunnel a 55-year operating concession (from 1987; extended by 10 years to 65 years in 1993) to repay loans and pay dividends. A Railway Usage Agreement was signed between Eurotunnel, British Rail and SNCF guaranteeing future revenue in exchange for the railways obtaining half of the tunnel's capacity.\n\nPrivate funding for such a complex infrastructure project was of unprecedented scale. An initial equity of £45 million was raised by CTG/F-M, increased by £206 million private institutional placement, £770 million was raised in a public share offer that included press and television advertisements, a syndicated bank loan and letter of credit arranged £5 billion. Privately financed, the total investment costs at 1985 prices were £2600 million. At the 1994 completion actual costs were, in 1985 prices, £4650 million: an 80% cost overrun. The cost overrun was partly due to enhanced safety, security, and environmental demands. Financing costs were 140% higher than forecast.\n\nWorking from both the English side and the French side of the Channel, eleven tunnel boring machines or TBMs cut through chalk marl to construct two rail tunnels and a service tunnel. The vehicle shuttle terminals are at Cheriton (part of Folkestone) and Coquelles, and are connected to the English M20 and French A16 motorways respectively.\n\nTunnelling commenced in 1988, and the tunnel began operating in 1994. In 1985 prices, the total construction cost was £4.650 billion (equivalent to £ billion today), an 80% cost overrun. At the peak of construction 15,000 people were employed with daily expenditure over £3 million. Ten workers, eight of them British, were killed during construction between 1987 and 1993, most in the first few months of boring.\n\nA two-inch (50-mm) diameter pilot hole allowed the service tunnel to break through without ceremony on 30 October 1990. On 1 December 1990, Englishman Graham Fagg and Frenchman Phillippe Cozette broke through the service tunnel with the media watching. Eurotunnel completed the tunnel on time, and it was officially opened, one year later than originally planned, by Queen Elizabeth II and the French president, François Mitterrand, in a ceremony held in Calais on 6 May 1994. The Queen travelled through the tunnel to Calais on a Eurostar train, which stopped nose to nose with the train that carried President Mitterrand from Paris. Following the ceremony President Mitterrand and the Queen travelled on Le Shuttle to a similar ceremony in Folkestone. A full public service did not start for several months.\n\nThe Channel Tunnel Rail Link (CTRL), now called High Speed 1, runs from St Pancras railway station in London to the tunnel portal at Folkestone in Kent. It cost £5.8 billion. On 16 September 2003 the prime minister, Tony Blair, opened the first section of High Speed 1, from Folkestone to north Kent. On 6 November 2007 the Queen officially opened High Speed 1 and St Pancras International station, replacing the original slower link to Waterloo International railway station. High Speed 1 trains travel at up to , the journey from London to Paris taking 2 hours 15 minutes, to Brussels 1 hour 51 minutes.\n\nIn 1994, the American Society of Civil Engineers elected the tunnel as one of the seven modern Wonders of the World. In 1995, the American magazine \"Popular Mechanics\" published the results.\n\nSurveying undertaken in the 20 years before construction confirmed earlier speculations that a tunnel could be bored through a chalk marl stratum. The chalk marl was conducive to tunnelling, with impermeability, ease of excavation and strength. On the English side the chalk marl ran along the entire length of the tunnel, but on the French side a length of had variable and difficult geology. The tunnel consists of three bores: two diameter rail tunnels, apart, in length with a diameter service tunnel in between. There are also cross-passages and piston relief ducts. The service tunnel was used as a pilot tunnel, boring ahead of the main tunnels to determine the conditions. English access was provided at Shakespeare Cliff, French access from a shaft at Sangatte. The French side used five tunnel boring machines (TBMs), the English side six. The service tunnel uses Service Tunnel Transport System (STTS) and Light Service Tunnel Vehicles (LADOGS). Fire safety was a critical design issue.\n\nBetween the portals at Beussingue and Castle Hill the tunnel is long, with under land on the French side and on the UK side, and under sea. It is the third-longest rail tunnel in the world, behind the Gotthard Base Tunnel in Switzerland and the Seikan Tunnel in Japan, but with the longest under-sea section. The average depth is below the seabed. On the UK side, of the expected of spoil approximately was used for fill at the terminal site, and the remainder was deposited at Lower Shakespeare Cliff behind a seawall, reclaiming of land. This land was then made into the Samphire Hoe Country Park. Environmental impact assessment did not identify any major risks for the project, and further studies into safety, noise, and air pollution were overall positive. However, environmental objections were raised over a high-speed link to London.\n\nSuccessful tunnelling required a sound understanding of the topography and geology and the selection of the best rock strata through which to dig. The geology of this site generally consists of northeasterly dipping Cretaceous strata, part of the northern limb of the Wealden-Boulonnais dome. Characteristics include:\n\n\nOn the English side, the stratum dip is less than 5°; on the French side this increases to 20°. Jointing and faulting are present on both sides. On the English side, only minor faults of displacement less than exist; on the French side, displacements of up to are present owing to the Quenocs anticlinal fold. The faults are of limited width, filled with calcite, pyrite and remoulded clay. The increased dip and faulting restricted the selection of route on the French side. To avoid confusion, microfossil assemblages were used to classify the chalk marl. On the French side, particularly near the coast, the chalk was harder, more brittle and more fractured than on the English side. This led to the adoption of different tunnelling techniques on the two sides.\n\nThe Quaternary undersea valley Fosse Dangaered, and Castle Hill landslip at the English portal, caused concerns. Identified by the 1964–65 geophysical survey, the Fosse Dangaered is an infilled valley system extending below the seabed, south of the tunnel route in mid-channel. A 1986 survey showed that a tributary crossed the path of the tunnel, and so the tunnel route was made as far north and deep as possible. The English terminal had to be located in the Castle Hill landslip, which consists of displaced and tipping blocks of lower chalk, glauconitic marl and gault debris. Thus the area was stabilised by buttressing and inserting drainage adits. The service tunnel acted as a pilot preceding the main ones, so that the geology, areas of crushed rock, and zones of high water inflow could be predicted. Exploratory probing took place in the service tunnel, in the form of extensive forward probing, vertical downward probes and sideways probing.\n\nMarine soundings and samplings by Thomé de Gamond were carried out during 1833–67, establishing the seabed depth at a maximum of and the continuity of geological strata (layers). Surveying continued over many years, with 166 marine and 70 land-deep boreholes being drilled and over 4,000-line-kilometres of marine geophysical survey completed. Surveys were undertaken in 1958–1959, 1964–1965, 1972–1974 and 1986–1988.\n\nThe surveying in 1958–59 catered for immersed tube and bridge designs as well as a bored tunnel, and thus a wide area was investigated. At this time, marine geophysics surveying for engineering projects was in its infancy, with poor positioning and resolution from seismic profiling. The 1964–65 surveys concentrated on a northerly route that left the English coast at Dover harbour; using 70 boreholes, an area of deeply weathered rock with high permeability was located just south of Dover harbour.\n\nGiven the previous survey results and access constraints, a more southerly route was investigated in the 1972–73 survey, and the route was confirmed to be feasible. Information for the tunnelling project also came from work before the 1975 cancellation. On the French side at Sangatte, a deep shaft with adits was made. On the English side at Shakespeare Cliff, the government allowed of diameter tunnel to be driven. The actual tunnel alignment, method of excavation and support were essentially the same as the 1975 attempt. In the 1986–87 survey, previous findings were reinforced, and the characteristics of the gault clay and the tunnelling medium (chalk marl that made up 85% of the route) were investigated. Geophysical techniques from the oil industry were employed.\n\nTunnelling was a major engineering challenge, with the only precedent being the undersea Seikan Tunnel in Japan. A serious risk with underwater tunnels is major water inflow due to the pressure from the sea above, under weak ground conditions. The tunnel also had the challenge of time: being privately funded, early financial return was paramount.\n\nThe objective was to construct two rail tunnels, apart, in length; a service tunnel between the two main ones; pairs of cross-passages linking the rail tunnels to the service one at spacing; piston relief ducts in diameter connecting the rail tunnels apart; two undersea crossover caverns to connect the rail tunnels, with the service tunnel always preceding the main ones by at least to ascertain the ground conditions. There was plenty of experience with excavating through chalk in the mining industry, while the undersea crossover caverns were a complex engineering problem. The French one was based on the Mount Baker Ridge freeway tunnel in Seattle; the UK cavern was dug from the service tunnel ahead of the main ones, to avoid delay.\n\nPrecast segmental linings in the main TBM drives were used, but two different solutions were used. On the French side, neoprene and grout sealed bolted linings made of cast iron or high-strength reinforced concrete were used; on the English side, the main requirement was for speed so bolting of cast-iron lining segments was only carried out in areas of poor geology. In the UK rail tunnels, eight lining segments plus a key segment were used; in the French side, five segments plus a key. On the French side, a diameter deep grout-curtained shaft at Sangatte was used for access. On the English side, a marshalling area was below the top of Shakespeare Cliff, the New Austrian Tunnelling method (NATM) was first applied in the chalk marl here. On the English side, the land tunnels were driven from Shakespeare Cliff - same place as the marine tunnels - not from Folkestone. The platform at the base of the cliff was not large enough for all of the drives and, despite environmental objections, tunnel spoil was placed behind a reinforced concrete seawall, on condition of placing the chalk in an enclosed lagoon, to avoid wide dispersal of chalk fines. Owing to limited space, the precast lining factory was on the Isle of Grain in the Thames estuary, which used Scottish granite aggregate delivered by ship from the Foster Yeoman coastal super quarry at Glensanda in Loch Linnhe on the west coast of Scotland.\n\nOn the French side, owing to the greater permeability to water, earth pressure balance TBMs with open and closed modes were used. The TBMs were of a closed nature during the initial , but then operated as open, boring through the chalk marl stratum. This minimised the impact to the ground, allowed high water pressures to be withstood and it also alleviated the need to grout ahead of the tunnel. The French effort required five TBMs: two main marine machines, one main land machine (the short land drives of allowed one TBM to complete the first drive then reverse direction and complete the other), and two service tunnel machines. On the English side, the simpler geology allowed faster open-faced TBMs. Six machines were used, all commenced digging from Shakespeare Cliff, three marine-bound and three for the land tunnels. Towards the completion of the undersea drives, the UK TBMs were driven steeply downwards and buried clear of the tunnel. These buried TBMs were then used to provide an electrical earth. The French TBMs then completed the tunnel and were dismantled. A gauge railway was used on the English side during construction.\n\nIn contrast to the English machines, which were given alphanumeric names, the French tunnelling machines were all named after women: Brigitte, Europa, Catherine, Virginie, Pascaline, Séverine.\n\nAt the end of the tunnelling, one machine was on display at the side of the M20 motorway in Folkestone until Eurotunnel sold it on eBay for £39,999 to a scrap metal merchant. Another machine (T4 \"Virginie\") still survives on the French side, adjacent to Junction 41 on the A16, in the middle of the D243E3/D243E4 roundabout. On it are the words \"hommage aux batisseurs du tunnel\", meaning \"tribute to the builders of the tunnel\".\n\nThere are three communication systems: concession radio (CR) for mobile vehicles and personnel within Eurotunnel's Concession (terminals, tunnels, coastal shafts); track-to-train radio (TTR) for secure speech and data between trains and the railway control centre; Shuttle internal radio (SIR) for communication between shuttle crew and to passengers over car radios. This service was discontinued within one year of opening because of drivers' difficulty setting their radios to the correct frequency (88.8 MHz).\n\nPower is delivered to the locomotives via an overhead line (catenary) at . All tunnel services run on electricity, shared equally from English and French sources. There are two sub-stations fed at 400 kV at each terminal, but in an emergency the tunnel's lighting (about 20,000 light fittings) and plant can be powered solely from either England or France.\n\nThe traditional railway south of London uses a 750 V DC third rail to deliver electricity, but since the opening of High Speed 1 there is no longer any need for tunnel trains to use the third rail system. High Speed 1, the tunnel and the LGV Nord all have power provided via overhead catenary at 25 kV 50 Hz. The railways on \"classic\" lines in Belgium are also electrified by overhead wires, but at 3000 V DC.\n\nA cab signalling system gives information directly to train drivers on a display. There is a train protection system that stops the train if the speed exceeds that indicated on the in-cab display. TVM430, as used on LGV Nord and High Speed 1, is used in the tunnel. The TVM signalling is interconnected with the signalling on the high-speed lines either side, allowing trains to enter and exit the tunnel system without stopping. The maximum speed is .\n\nSignalling in the tunnel is coordinated from two control centres: The main control centre at the Folkestone terminal, and a backup at the Calais terminal, which is staffed at all times and can take over all operations in the event of a breakdown or emergency.\n\nConventional ballasted tunnel-track was ruled out owing to the difficulty of maintenance and lack of stability and precision. The Sonneville International Corporation's track system was chosen based on reliability and cost-effectiveness based on good performance in Swiss tunnels and worldwide. The type of track used is known as Low Vibration Track (LVT). Like ballasted track the LVT is of the free floating type, held in place by gravity and friction. Reinforced concrete blocks of 100 kg support the rails every 60 cm and are held by 12 mm thick closed cell polymer foam pads placed at the bottom of rubber boots. The latter separate the blocks' mass movements from the lean encasement concrete. Ballastless track provides extra overhead clearance necessary for the passage of larger trains. The corrugated rubber walls of the boots add a degree of isolation of horizontal wheel-rail vibrations, and are insulators of the track signal circuit in the humid tunnel environment. UIC60 (60 kg/m) rails of 900A grade rest on rail pads, which fit the RN/Sonneville bolted dual leaf-springs. The rails, LVT-blocks and their boots with pads were assembled outside the tunnel, in a fully automated process developed by the LVT inventor, Mr. Roger Sonneville. About 334,000 Sonneville blocks were made on the Sangatte site.\n\nMaintenance activities are less than projected. Initially the rails were ground on a yearly basis or after approximately 100MGT of traffic. Ride quality continues to be noticeably smooth and of low noise. Maintenance is facilitated by the existence of two tunnel junctions or crossover facilities, allowing for two-way operation in each of the six tunnel segments thereby created, and thus providing safe access for maintenance of one isolated tunnel segment at a time. The two crossovers are the largest artificial undersea caverns ever built; 150 m long, 10 m high and 18 m wide. The English crossover is from Shakespeare Cliff, and the French crossover is from Sangatte.\n\nThe ventilation system maintains the air pressure in the service tunnel higher than in the rail tunnels, so that in the event of a fire, smoke does not enter the service tunnel from the rail tunnels. Two cooling water pipes in each rail tunnel circulate chilled water to remove heat generated by the rail traffic. Pumping stations remove water in the tunnels from rain, seepage, and so on.\n\nInitially 38 Le Shuttle locomotives were commissioned, with one at each end of a shuttle train. The shuttles have two separate halves: single and double deck. Each half has two loading/unloading wagons and 12 carrier wagons. Eurotunnel's original order was for nine tourist shuttles.\n\nHeavy goods vehicle (HGV) shuttles also have two halves, with each half containing one loading wagon, one unloading wagon and 14 carrier wagons. There is a club car behind the leading locomotive. Eurotunnel originally ordered six HGV shuttle rakes.\n\nForty-six Class 92 locomotives for hauling freight trains and overnight passenger trains (the Nightstar project, which was abandoned) were commissioned, running on both overhead AC and third-rail DC power. However, RFF does not let these run on French railways, so there are plans to certify Alstom Prima II locomotives for use in the tunnel.\n\nThirty-one Eurostar trains, based on the French TGV, built to UK loading gauge with many modifications for safety within the tunnel, were commissioned, with ownership split between British Rail, French national railways (SNCF) and Belgian national railways (SNCB). British Rail ordered seven more for services north of London. Around 2010, Eurostar ordered ten trains from Siemens based on its Velaro product.\n\nGermany (DB) has since around 2005 tried to get permission to run train services to London. At the end of 2009, extensive fire-proofing requirements were dropped and DB received permission to run German Intercity-Express (ICE) test trains through the tunnel. In June 2013 DB was granted access to the tunnel. In June 2014 the plans were shelved, because there are special safety rules that requires custom made trains (DB calls them Class 407).\n\nDiesel locomotives for rescue and shunting work are Eurotunnel Class 0001 and Eurotunnel Class 0031.\n\nThe following chart presents the estimated number of passengers and tonnes of freight, respectively, annually transported through the Channel Tunnel since 1994, in millions:\n\nTransport services offered by the tunnel are as follows:\n\n\nBoth the freight and passenger traffic forecasts that led to the construction of the tunnel were overestimated; in particular, Eurotunnel's commissioned forecasts were over-predictions. Although the captured share of Channel crossings was forecast correctly, high competition (especially from budget airlines which expanded rapidly in the 1990s and 2000s) and reduced tariffs led to low revenue. Overall cross-Channel traffic was overestimated.\n\nWith the EU's liberalisation of international rail services, the tunnel and High Speed 1 have been open to competition since 2010. There have been a number of operators interested in running trains through the tunnel and along High Speed 1 to London. In June 2013, after several years, DB obtained a license to operate Frankfurt – London trains, not expected to run before 2016 because of delivery delays of the custom-made trains.\n\nCross-tunnel passenger traffic volumes peaked at 18.4 million in 1998, dropped to 14.9 million in 2003, then rose to 21.0 million in 2014.\n\nAt the time of the decision about building the tunnel, 15.9 million passengers were predicted for Eurostar trains in the opening year. In 1995, the first full year, actual numbers were a little over 2.9 million, growing to 7.1 million in 2000, then dropping to 6.3 million in 2003. Eurostar was initially limited by the lack of a high-speed connection on the British side. After the completion of High Speed 1 in two stages in 2003 and 2007, traffic increased. In 2008, Eurostar carried 9,113,371 passengers, a 10% increase over the previous year, despite traffic limitations due to the 2008 Channel Tunnel fire. Eurostar passenger numbers continued to increase, reaching 10,397,894 in 2014.\n\nFreight volumes have been erratic, with a major decrease during 1997 due to a closure caused by a fire in a freight shuttle. Freight crossings increased over the period, indicating the substitutability of the tunnel by sea crossings. The tunnel has achieved a market share close to or above Eurotunnel's 1980s predictions but Eurotunnel's 1990 and 1994 predictions were overestimates.\n\nFor through freight trains, the first year prediction was 7.2 million gross tonnes; the actual 1995 figure was 1.3M gross tonnes. Through freight volumes peaked in 1998 at 3.1M tonnes. This fell back to 1.21M tonnes in 2007, increasing slightly to 1.24M tonnes in 2008. Together with that carried on freight shuttles, freight growth has occurred since opening, with 6.4M tonnes carried in 1995, 18.4M tonnes recorded in 2003 and 19.6M tonnes in 2007. Numbers fell back in the wake of the 2008 fire.\n\nEurotunnel's freight subsidiary is Europorte 2. In September 2006 EWS, the UK's largest rail freight operator, announced that owing to cessation of UK-French government subsidies of £52 million per annum to cover the tunnel \"Minimum User Charge\" (a subsidy of around £13,000 per train, at a traffic level of 4,000 trains per annum), freight trains would stop running after 30 November.\n\nShares in Eurotunnel were issued at £3.50 per share on 9 December 1987. By mid-1989 the price had risen to £11.00. Delays and cost overruns led to the price dropping; during demonstration runs in October 1994 it reached an all-time low. Eurotunnel suspended payment on its debt in September 1995 to avoid bankruptcy. In December 1997 the British and French governments extended Eurotunnel's operating concession by 34 years, to 2086. Financial restructuring of Eurotunnel occurred in mid-1998, reducing debt and financial charges. Despite the restructuring, \"The Economist\" reported in 1998 that to break even Eurotunnel would have to increase fares, traffic and market share for sustainability. A cost benefit analysis of the tunnel indicated that there were few impacts on the wider economy and few developments associated with the project, and that the British economy would have been better off if it had not been constructed.\n\nUnder the terms of the Concession, Eurotunnel was obliged to investigate a cross-Channel road tunnel. In December 1999 road and rail tunnel proposals were presented to the British and French governments, but it was stressed that there was not enough demand for a second tunnel. A three-way treaty between the United Kingdom, France and Belgium governs border controls, with the establishment of \"control zones\" wherein the officers of the other nation may exercise limited customs and law enforcement powers. For most purposes these are at either end of the tunnel, with the French border controls on the UK side of the tunnel and vice versa. For some city-to-city trains, the train is a control zone. A binational emergency plan coordinates UK and French emergency activities.\n\nIn 1999 Eurostar posted its first net profit, having made a loss of £925m in 1995. In 2005 Eurotunnel was described as being in a serious situation. In 2013, operating profits rose 4 per cent from 2012, to £54 million.\n\nThere is a need for full passport controls, since this is the border between the Schengen Area and the Common Travel Area. There are juxtaposed controls, meaning that passports are checked before boarding first by officials belonging to departing country and then officials of the destination country. These are only placed at the main Eurostar stations - (London, Ebbsfleet, Ashford, Calais, Lille, Brussels and Paris). There are security checks before boarding as well. For the shuttle road-vehicle trains, there are juxtaposed passport controls before boarding the trains.\n\nFor Eurostar trains travelling from places south of Paris, there is no passport and security check before departure, and those trains must stop in Lille at least 30 minutes to allow all passengers to be checked. No checks are done on board. There have been plans for services from Amsterdam, Frankfurt and Cologne to London, but a major reason to cancel them was the need for a stop in Lille.\n\nThe reason for juxtaposed controls is a wish to prevent illegal immigration before reaching British soil, and because a check of all passengers on a train can take 30 minutes, which creates long queues if done at arrival.\n\nThe terminals' sites are at Cheriton (near Folkestone in the United Kingdom) and Coquelles (near Calais in France). The terminals are designed to transfer vehicles from the motorway onto trains at a rate of 700 cars and 113 heavy vehicles per hour. The UK site uses the M20 motorway for access. The terminals are organised with the frontier controls juxtaposed with the entry to the system to allow travellers to go onto the motorway at the destination country immediately after leaving the shuttle. The area of the UK site was severely constrained and the design was challenging. The French layout was achieved more easily. To achieve design output, the shuttles accept cars on double-deck wagons; for flexibility, ramps were placed inside the shuttles to provide access to the top decks. At Folkestone there are of main-line track, 45 turnouts and eight platforms. At Calais there are of track and 44 turnouts. At the terminals the shuttle trains traverse a figure eight to reduce uneven wear on the wheels. There is a freight marshalling yard west of Cheriton at Dollands Moor Freight Yard.\n\nA 1996 report from the European Commission predicted that Kent and Nord-Pas de Calais had to face increased traffic volumes due to general growth of cross-Channel traffic and traffic attracted by the tunnel. In Kent, a high-speed rail line to London would transfer traffic from road to rail. Kent's regional development would benefit from the tunnel, but being so close to London restricts the benefits. Gains are in the traditional industries and are largely dependent on the development of Ashford International passenger station, without which Kent would be totally dependent on London's expansion. Nord-Pas-de-Calais enjoys a strong internal symbolic effect of the Tunnel which results in significant gains in manufacturing.\n\nThe removal of a bottleneck by means like the tunnel does not necessarily induce economic gains in all adjacent regions. The image of a region being connected to the European high-speed transport and active political response are more important for regional economic development. Some small-medium enterprises located in the immediate vicinity of the terminal have used the opportunity to re-brand the profile of their business with positive effect, such as \"The New Inn\" at Etchinghill which was able to commercially exploit its unique selling point as being 'the closest pub to the Channel Tunnel'. Tunnel-induced regional development is small compared to general economic growth. The South East of England is likely to benefit developmentally and socially from faster and cheaper transport to continental Europe, but the benefits are unlikely to be equally distributed throughout the region. The overall environmental impact is almost certainly negative.\n\nSince the opening of the tunnel, small positive impacts on the wider economy have been felt, but it is difficult to identify major economic successes directly attributed to the tunnel.<ref name=\"Flyvbjerg p. 68/69\">Flyvbjerg et al. p. 68–69</ref> The Eurotunnel does operate profitably, offering an alternative transportation mode unaffected by poor weather. High costs of construction did delay profitability, however, and companies involved in the tunnel's construction and operation early in operation relied on government aid to deal with debts amounted.\n\nThere have been three fires in the tunnel, all on the heavy goods vehicle (HGV) shuttles, that were significant enough to close the tunnel, as well as other more minor incidents.\n\nOn 9 December 1994, during an \"invitation only\" testing phase, a fire broke out in a Ford Escort car whilst its owner was loading it onto the upper deck of a tourist shuttle. The fire started at about 10:00, with the shuttle train stationary in the Folkestone terminal and was put out about 40 minutes later with no passenger injuries.\n\nOn 18 November 1996, a fire broke out on an HGV shuttle wagon in the tunnel, but nobody was seriously hurt. The exact cause is unknown, although it was neither a Eurotunnel equipment nor rolling stock problem; it may have been due to arson of a heavy goods vehicle. It is estimated that the heart of the fire reached , with the tunnel severely damaged over , with some affected to some extent. Full operation recommenced six months after the fire.\n\nOn 21 August 2006, the tunnel was closed for several hours when a truck on an HGV shuttle train caught fire.\n\nOn 11 September 2008, a fire occurred in the Channel Tunnel at 13:57 GMT. The incident started on an HGV shuttle train travelling towards France. The event occurred from the French entrance to the tunnel. No one was killed but several people were taken to hospitals suffering from smoke inhalation, and minor cuts and bruises. The tunnel was closed to all traffic, with the undamaged South Tunnel reopening for limited services two days later. Full service resumed on 9 February 2009 after repairs costing €60 million.\n\nOn 29 November 2012, the tunnel was closed for several hours after a truck on an HGV shuttle caught fire.\n\nOn 17 January 2015, both tunnels were closed following a lorry fire which filled the midsection of Running Tunnel North with smoke. Eurostar cancelled all services. The shuttle train had been heading from Folkestone to Coquelles and stopped adjacent to cross-passage CP 4418 just before 12:30 UTC. Thirty-eight passengers and four members of Eurotunnel staff were evacuated into the service tunnel, and then transported to France using special STTS road vehicles in the Service Tunnel. The passengers and crew were taken to the Eurotunnel Fire/Emergency Management Centre close to the French portal.\n\nOn the night of 19/20 February 1996, about 1,000 passengers became trapped in the Channel Tunnel when Eurostar trains from London broke down owing to failures of electronic circuits caused by snow and ice being deposited and then melting on the circuit boards.\n\nOn 3 August 2007, an electrical failure lasting six hours caused passengers to be trapped in the tunnel on a shuttle.\n\nOn the evening of 18 December 2009, during the December 2009 European snowfall, five London-bound Eurostar trains failed inside the tunnel, trapping 2,000 passengers for approximately 16 hours, during the coldest temperatures in eight years. A Eurotunnel spokesperson explained that snow had evaded the train's winterisation shields, and the transition from cold air outside to the tunnel's warm atmosphere had melted the snow, resulting in electrical failures. One train was turned back before reaching the tunnel; two trains were hauled out of the tunnel by Eurotunnel Class 0001 diesel locomotives. The blocking of the tunnel led to the implementation of Operation Stack, the transformation of the M20 motorway into a linear car park.\n\nThe occasion was the first time that a Eurostar train was evacuated inside the tunnel; the failing of four at once was described as \"unprecedented\". The Channel Tunnel reopened the following morning. Nirj Deva, Member of the European Parliament for South East England, had called for Eurostar chief executive Richard Brown to resign over the incidents. An independent report by Christopher Garnett (former CEO of Great North Eastern Railway) and Claude Gressier (a French transport expert) on the 18/19 December 2009 incidents was issued in February 2010, making 21 recommendations.\n\non 7 January 2010, a Brussels–London Eurostar broke down in the tunnel. The train had 236 passengers on board and was towed to Ashford; other trains that had not yet reached the tunnel were turned back.\n\nIllegal Immigrants and would-be asylum seekers have used the tunnel to attempt to enter Britain. By 1997, the problem had attracted international press attention, and in 1999, the French Red Cross opened a refugee centre at Sangatte, using a warehouse once used for tunnel construction; by 2002, it housed up to 1,500 people at a time, most of them trying to get to the UK. In 2001, most came from Afghanistan, Iraq, and Iran, but African and Eastern European countries were also represented.\n\nMost illegal immigrants and would-be asylum seekers who got into Britain found some way to ride a freight train, but others used Eurostar. They would usually get on trucks, which would then get onto the freight trains. In a few instances, groups of men claiming to be refugees were able to sneak into a tanker truck carrying liquid chocolate and managed to survive, though they did not enter the UK in one attempt. Although the facilities were fenced, airtight security was deemed impossible; refugees would even jump from bridges onto moving trains. In several incidents people were injured during the crossing; others tampered with railway equipment, causing delays and requiring repairs. Eurotunnel said it was losing £5m per month because of the problem. A dozen refugees/illegal immigrants have died in crossing attempts.\n\nIn 2001 and 2002, several riots broke out at Sangatte, and groups of refugees (up to 550 in a December 2001 incident) stormed the fences and attempted to enter \"en masse\". Immigrants have also arrived as legitimate Eurostar passengers without proper entry papers.\n\nLocal authorities in both France and the UK called for the closure of Sangatte, and Eurotunnel twice sought an injunction against the centre. The United Kingdom blamed France for allowing Sangatte to open, and France blamed the UK for its lax asylum rules and the EU for not having a uniform immigration policy. The \"cause célèbre\" nature of the problem even included journalists detained as they followed refugees onto railway property.\n\nIn 2002, after the European Commission told France that it was in breach of European Union rules on the free transfer of goods because of the delays and closures as a result of its poor security, a double fence was built at a cost of £5 million, reducing the numbers of refugees detected each week reaching Britain on goods trains from 250 to almost none. Other measures included CCTV cameras and increased police patrols. At the end of 2002, the Sangatte centre was closed after the UK agreed to take some of its refugees.\n\nOn 23 and 30 June 2015, striking workers associated with MyFerryLink damaged the sections of track by burning car tires, leading to all trains being cancelled and a backlog of vehicles. Hundreds seeking to reach Britain made use of the situation to attempt to stowaway inside and underneath transport trucks destined for the United Kingdom. Extra security measures including: £2-million upgrade of detection technology; £1 million extra for dog searches; £12 million (over three years) towards a joint fund with France for security surrounding the Port of Calais. The UK Home Office stated that approximately 19,000 attempts to cross the Channel during the first half of 2015 had been detected and prevented.\n\nOn 6 July 2015, a migrant died while attempting to climb onto a freight train while trying to reach Britain from the French side of the Channel. The previous month an Eritrean man was killed under similar circumstances.\n\nEurotunnel, the company that operates the crossing, said that it has intercepted more than 37,000 migrants since January 2015. During the night of 28 July 2015, one person aged 25–30, was found dead, after a night in which 1,500–2,000 refugees had attempted to enter the Eurotunnel terminal. According to the last official count in July 2015, about 3,000 migrants, mainly from Ethiopia, Eritrea, Sudan and Afghanistan, were living in the makeshift camps in Calais. It is estimated that about 5,000 refugees are waiting in the harbour town Calais to find a chance to get to England. Ten migrants have died near the Channel tunnel terminal since June 2015.\n\nOn 4 August 2015, a Sudanese migrant walked nearly the entire length of one of the tunnels. He was arrested close to the British side, after having walked about through the tunnel.\n\nThe Channel Tunnel Safety Authority is responsible for some aspects of safety regulation in the tunnel; it reports to the IGC.\nThe service tunnel is used for access to technical equipment in cross-passages and equipment rooms, to provide fresh-air ventilation and for emergency evacuation. The Service Tunnel Transport System (STTS) allows fast access to all areas of the tunnel. The service vehicles are rubber-tyred with a buried wire guidance system. The 24 STTS vehicles are used mainly for maintenance but also for firefighting and in emergencies. \"Pods\" with different purposes, up to a payload of , are inserted into the side of the vehicles. The vehicles cannot turn around within the tunnel, and are driven from either end. The maximum speed is when the steering is locked. A fleet of 15 Light Service Tunnel Vehicles (LADOGS) was introduced to supplement the STTSs. The LADOGS have a short wheelbase with a turning circle, allowing two-point turns within the service tunnel. Steering cannot be locked like the STTS vehicles, and maximum speed is . Pods up to 1 tonne can be loaded onto the rear of the vehicles. Drivers in the tunnel sit on the right, and the vehicles drive on the left. Owing to the risk of French personnel driving on their native right side of the road, sensors in the vehicles alert the driver if the vehicle strays to the right side.\n\nThe three tunnels contain of air that needs to be conditioned for comfort and safety. Air is supplied from ventilation buildings at Shakespeare Cliff and Sangatte, with each building capable of providing 100% standby capacity. Supplementary ventilation also exists on either side of the tunnel. In the event of a fire, ventilation is used to keep smoke out of the service tunnel and move smoke in one direction in the main tunnel to give passengers clean air. The tunnel was the first main-line railway tunnel to have special cooling equipment. Heat is generated from traction equipment and drag. The design limit was set at , using a mechanical cooling system with refrigeration plants on both sides that run chilled water circulating in pipes within the tunnel.\n\nTrains travelling at high speed create piston-effect pressure changes that can affect passenger comfort, ventilation systems, tunnel doors, fans and the structure of the trains, and which drag on the trains. Piston relief ducts of diameter were chosen to solve the problem, with 4 ducts per kilometre to give close to optimum results. Unfortunately this design led to unacceptable lateral forces on the trains so a reduction in train speed was required and restrictors were installed in the ducts.\n\nThe safety issue of a possible fire on a passenger-vehicle shuttle garnered much attention, with Eurotunnel noting that fire was the risk attracting the most attention in a 1994 safety case for three reasons: the opposition of ferry companies to passengers being allowed to remain with their cars; Home Office statistics indicating that car fires had doubled in ten years; and the long length of the tunnel. Eurotunnel commissioned the UK Fire Research Station - now part of the Building Research Establishment - to give reports of vehicle fires, and liaised with Kent Fire Brigade to gather vehicle fire statistics over one year. Fire tests took place at the French Mines Research Establishment with a mock wagon used to investigate how cars burned. The wagon door systems are designed to withstand fire inside the wagon for 30 minutes, longer than the transit time of 27 minutes. Wagon air conditioning units help to purge dangerous fumes from inside the wagon before travel. Each wagon has a fire detection and extinguishing system, with sensing of ions or ultraviolet radiation, smoke and gases that can trigger halon gas to quench a fire. Since the HGV wagons are not covered, fire sensors are located on the loading wagon and in the tunnel. A water main in the service tunnel provides water to the main tunnels at intervals. The ventilation system can control smoke movement. Special arrival sidings accept a train that is on fire, as the train is not allowed to stop whilst on fire in the tunnel, unless continuing its journey would lead to a worse outcome. Eurotunnel has banned a wide range of hazardous goods from travelling in the tunnel. Two STTS (Service Tunnel Transportation System) vehicles with firefighting pods are on duty at all times, with a maximum delay of 10 minutes before they reach a burning train.\n\nIn 2009, former F1 racing champion John Surtees drove a Ginetta G50 EV electronic sports car prototype from England to France, using the service tunnel, as part of a charity event. He was required to keep to the speed limit. To celebrate the 2014 Tour de France's transfer from its opening stages in Britain to France in July of that year, Chris Froome of Team Sky rode a bicycle through the service tunnel, becoming the first solo rider to do so. The Crossing took under an hour, reaching speeds of 40 mph–faster than most cross-channel ferries.\n\nSince 2012, French operators Bouygues Telecom, Orange and SFR have covered Running Tunnel South, the tunnel bore normally used for travel from France to Britain.\n\nIn January 2014, UK operators EE and Vodafone signed ten-year contracts with Eurotunnel for Running Tunnel North. The agreements will enable both operators' subscribers to use 2G and 3G services. Both EE and Vodafone plan to offer LTE services on the route; EE said it expected to cover the route with LTE connectivity by summer 2014. EE and Vodafone will offer Channel Tunnel network coverage for travellers from the UK to France. Eurotunnel said it also held talks with Three UK but has yet to reach an agreement with the operator.\n\nOn 6 May 2014, Eurotunnel announced that they had installed equipment from Alcatel-Lucent to cover Running Tunnel North and simultaneously to provide mobile service (GSM 900/1800 MHz and UMTS 2100 MHz) by EE, O and Vodafone. The service of EE and Vodafone commenced on the same date as the announcement. O service was expected to be available soon afterwards.\n\nOn 21 November 2014, EE announced that it had previously switched on LTE earlier in September 2014. O turned on 2G, 3G and 4G services in November 2014. Whilst Vodafone's 4G was due to go live later.\n\nAnother usage of the Channel Tunnel is the 1,000 MW high voltage direct current ElecLink connecting the electrical grids of the two countries, scheduled for 2019 at a cost of €580m.\nThe foundation stone of the Folkestone Converter Station was laid in February 2017, by Jesse Norman, Minister for Industry and Energy.\n\nIn \"A Diplomatic Incident\", the eleventh episode of the British sitcom Yes, Prime Minister, first screened in 1987, Jim Hacker decides who is to negotiate with the French government over questions such as whether French or English should come first on signs and menus, and where the legal boundary between France and England should be in the tunnel.\n\n\n\n\n\n", "id": "5702", "title": "Channel Tunnel"}
{"url": "https://en.wikipedia.org/wiki?curid=5703", "text": "Cyberpunk\n\nCyberpunk is a subgenre of science fiction in a future setting that tends to focus on society as \"high tech low life\" featuring advanced technological and scientific achievements, such as information technology and cybernetics, juxtaposed with a degree of breakdown or radical change in the social order.\n\nCyberpunk plots often center on conflict among artificial intelligences, hackers, and megacorporations, and tend to be set in a near-future Earth, rather than in the far-future settings or galactic vistas found in novels such as Isaac Asimov's \"Foundation\" or Frank Herbert's \"Dune\". The settings are usually post-industrial dystopias but tend to feature extraordinary cultural ferment and the use of technology in ways never anticipated by its original inventors (\"the street finds its own uses for things\"). Much of the genre's atmosphere echoes film noir, and written works in the genre often use techniques from detective fiction.\n\nPrimary exponents of the cyberpunk field include William Gibson, Neal Stephenson, Bruce Sterling, Bruce Bethke, Pat Cadigan, Rudy Rucker, John Shirley and Philip K. Dick (author of \"Do Androids Dream of Electric Sheep?\", from which the film \"Blade Runner\" was adapted).\n\n\"Blade Runner\" can be seen as a quintessential example of the cyberpunk style and theme. Video games, board games, and tabletop role-playing games, such as \"Cyberpunk 2020\" and \"Shadowrun\", often feature storylines that are heavily influenced by cyberpunk writing and movies. Beginning in the early 1990s, some trends in fashion and music were also labeled as cyberpunk. Cyberpunk is also featured prominently in anime and manga: \"Akira\", \"Gunnm\", \"Ghost in the Shell\", \"Cowboy Bebop\", \"Serial Experiments Lain\", \"Dennou Coil\", \"Ergo Proxy\" and \"Psycho Pass\" being among the most notable.\n\nCyberpunk writers tend to use elements from hardboiled detective fiction, film noir, and postmodernist prose to describe an often nihilistic underground side of an electronic society. The genre's vision of a troubled future is often called the antithesis of the generally utopian visions of the future popular in the 1940s and 1950s. Gibson defined cyberpunk's antipathy towards utopian SF in his 1981 short story \"The Gernsback Continuum,\" which pokes fun at and, to a certain extent, condemns utopian science fiction.\n\nIn some cyberpunk writing, much of the action takes place online, in cyberspace, blurring the line between actual and virtual reality. A typical trope in such work is a direct connection between the human brain and computer systems. Cyberpunk settings are dystopias with corruption, computers and internet connectivity. Giant, multinational corporations have for the most part replaced governments as centers of political, economic, and even military power.\n\nThe economic and technological state of Japan, in the 80s influenced Cyberpunk literature at the time. Of Japan's influence on the genre, William Gibson said, \"Modern Japan simply was cyberpunk.\" Cyberpunk is often set in urbanized, artificial landscapes, and \"city lights, receding\" was used by Gibson as one of the genre's first metaphors for cyberspace and virtual reality. The cityscapes of Hong Kong and Shanghai have had major influences in the urban backgrounds, ambiance and settings in many cyberpunk works such as Blade Runner and Shadowrun. Ridley Scott envisioned the landscape of cyberpunk Los Angeles in Blade Runner to be \"Hong Kong on a very bad day\". The streetscapes of Ghost in the Shell were based on Hong Kong. Mamoru Oshii felt that Hong Kong's strange and chaotic streets where old and new exist in confusing relationships, fit the theme of the film well. Hong Kong's Kowloon Walled City is particularly notable for its disorganized hyper-urbanization and breakdown in traditional urban planning to be an inspiration to cyberpunk landscapes.\n\nOne of the cyberpunk genre's prototype characters is Case, from Gibson's \"Neuromancer\". Case is a \"console cowboy,\" a brilliant hacker who has betrayed his organized criminal partners. Robbed of his talent through a crippling injury inflicted by the vengeful partners, Case unexpectedly receives a once-in-a-lifetime opportunity to be healed by expert medical care but only if he participates in another criminal enterprise with a new crew.\n\nLike Case, many cyberpunk protagonists are manipulated, placed in situations where they have little or no choice, and although they might see things through, they do not necessarily come out any further ahead than they previously were. These anti-heroes—\"criminals, outcasts, visionaries, dissenters and misfits\"—call to mind the private eye of detective fiction. This emphasis on the misfits and the malcontents is the \"punk\" component of cyberpunk.\n\nCyberpunk can be intended to disquiet readers and call them to action. It often expresses a sense of rebellion, suggesting that one could describe it as a type of culture revolution in science fiction. In the words of author and critic David Brin:\n\n...a closer look [at cyberpunk authors] reveals that they nearly always portray future societies in which governments have become wimpy and pathetic ...Popular science fiction tales by Gibson, Williams, Cadigan and others \"do\" depict Orwellian accumulations of power in the next century, but nearly always clutched in the secretive hands of a wealthy or corporate elite.\n\nCyberpunk stories have also been seen as fictional forecasts of the evolution of the Internet. The earliest descriptions of a global communications network came long before the World Wide Web entered popular awareness, though not before traditional science-fiction writers such as Arthur C. Clarke and some social commentators such as James Burke began predicting that such networks would eventually form.\n\nMinnesota writer Bruce Bethke coined the term in 1980 for his short story \"Cyberpunk,\" which was published in the November 1983 issue of \"Amazing Science Fiction Stories\". The term was quickly appropriated as a label to be applied to the works of William Gibson, Bruce Sterling, Pat Cadigan and others. Of these, Sterling became the movement's chief ideologue, thanks to his fanzine \"Cheap Truth.\" John Shirley wrote articles on Sterling and Rucker's significance. John Brunner's 1975 novel \"The Shockwave Rider\" is considered by many to be the first cyberpunk novel with many of the tropes commonly associated with the genre, some five years before the term was popularized by Dozois.\n\nWilliam Gibson with his novel \"Neuromancer\" (1984) is likely the most famous writer connected with the term cyberpunk. He emphasized style, a fascination with surfaces, and atmosphere over traditional science-fiction tropes. Regarded as ground-breaking and sometimes as \"the archetypal cyberpunk work,\" \"Neuromancer\" was awarded the Hugo, Nebula, and Philip K. Dick Awards. \"Count Zero\" (1986) and \"Mona Lisa Overdrive\" (1988) followed after Gibson's popular debut novel. According to the Jargon File, \"Gibson's near-total ignorance of computers and the present-day hacker culture enabled him to speculate about the role of computers and hackers in the future in ways hackers have since found both irritatingly naïve and tremendously stimulating.\"\n\nEarly on, cyberpunk was hailed as a radical departure from science-fiction standards and a new manifestation of vitality. Shortly thereafter, however, some critics arose to challenge its status as a revolutionary movement. These critics said that the SF New Wave of the 1960s was much more innovative as far as narrative techniques and styles were concerned. Furthermore, while \"Neuromancer\"<nowiki>'</nowiki>s narrator may have had an unusual \"voice\" for science fiction, much older examples can be found: Gibson's narrative voice, for example, resembles that of an updated Raymond Chandler, as in his novel \"The Big Sleep\" (1939). Others noted that almost all traits claimed to be uniquely cyberpunk could in fact be found in older writers' works—often citing J. G. Ballard, Philip K. Dick, Harlan Ellison, Stanisław Lem, Samuel R. Delany, and even William S. Burroughs. For example, Philip K. Dick's works contain recurring themes of social decay, artificial intelligence, paranoia, and blurred lines between objective and subjective realities. The influential cyberpunk movie \"Blade Runner\" (1982) is based on his book, \"Do Androids Dream of Electric Sheep?\". Humans linked to machines are found in Pohl and Kornbluth's \"Wolfbane\" (1959) and Roger Zelazny's \"Creatures of Light and Darkness\" (1968).\n\nIn 1994, scholar Brian Stonehill suggested that Thomas Pynchon's 1973 novel \"Gravity's Rainbow\" \"not only curses but precurses what we now glibly dub cyberspace.\" Other important predecessors include Alfred Bester's two most celebrated novels, \"The Demolished Man\" and \"The Stars My Destination\", as well as Vernor Vinge's novella \"True Names\".\n\nScience-fiction writer David Brin describes cyberpunk as \"the finest free promotion campaign ever waged on behalf of science fiction.\" It may not have attracted the \"real punks,\" but it did ensnare many new readers, and it provided the sort of movement that postmodern literary critics found alluring. Cyberpunk made science fiction more attractive to academics, argues Brin; in addition, it made science fiction more profitable to Hollywood and to the visual arts generally. Although the \"self-important rhetoric and whines of persecution\" on the part of cyberpunk fans were irritating at worst and humorous at best, Brin declares that the \"rebels did shake things up. We owe them a debt.\"\n\nFredric Jameson considers cyberpunk the \"supreme literary expression if not of postmodernism, then of late capitalism itself\".\n\nCyberpunk further inspired many professional writers who were not among the \"original\" cyberpunks to incorporate cyberpunk ideas into their own works, such as George Alec Effinger's \"When Gravity Fails\". \"Wired\" magazine, created by Louis Rossetto and Jane Metcalfe, mixes new technology, art, literature, and current topics in order to interest today's cyberpunk fans, which Paula Yoo claims \"proves that hardcore hackers, multimedia junkies, cyberpunks and cellular freaks are poised to take over the world.\"\n\nThe film \"Blade Runner\" (1982)—adapted from Philip K. Dick's \"Do Androids Dream of Electric Sheep?\"—is set in 2019 in a dystopian future in which manufactured beings called replicants are slaves used on space colonies and are legal prey on Earth to various bounty hunters who \"retire\" (kill) them. Although \"Blade Runner\" was largely unsuccessful in its first theatrical release, it found a viewership in the home video market and became a cult film. Since the movie omits the religious and mythical elements of Dick's original novel (e.g. empathy boxes and Wilbur Mercer), it falls more strictly within the cyberpunk genre than the novel does. William Gibson would later reveal that upon first viewing the film, he was surprised at how the look of this film matched his vision when he was working on \"Neuromancer\". The film's tone has since been the staple of many cyberpunk movies, such as \"The Matrix trilogy\" (1999-2003), which uses a wide variety of cyberpunk elements.\n\nThe number of films in the genre or at least using a few genre elements has grown steadily since \"Blade Runner\". Several of Philip K. Dick's works have been adapted to the silver screen. The films \"Johnny Mnemonic\" and \"New Rose Hotel\", both based upon short stories by William Gibson, flopped commercially and critically.\n\nIn addition, \"tech-noir\" film as a hybrid genre, means a work of combining neo-noir and science fiction or cyberpunk. It includes many cyberpunk films such as \"Blade Runner\", \"Burst City\", \"Robocop\", \"12 Monkeys\", \"The Lawnmower Man\", \"Hackers\", \"Hardware\", and \"Strange Days.\"\n\nCyberpunk themes are widely visible in anime and manga. In Japan, where cosplay is popular and not only teenagers display such fashion styles, cyberpunk has been accepted and its influence is widespread. William Gibson's \"Neuromancer,\" whose influence dominated the early cyberpunk movement, was also set in Chiba, one of Japan's largest industrial areas, although at the time of writing the novel Gibson did not know the location of Chiba and had no idea how perfectly it fit his vision in some ways. The exposure to cyberpunk ideas and fiction in the mid 1980s has allowed it to seep into the Japanese culture.\n\nCyberpunk anime and manga draw upon a futuristic vision which has elements in common with western science fiction and therefore have received wide international acceptance outside Japan. \"The conceptualization involved in cyberpunk is more of forging ahead, looking at the new global culture. It is a culture that does not exist right now, so the Japanese concept of a cyberpunk future, seems just as valid as a Western one, especially as Western cyberpunk often incorporates many Japanese elements.\" William Gibson is now a frequent visitor to Japan, and he came to see that many of his visions of Japan have become a reality:\n\nModern Japan simply was cyberpunk. The Japanese themselves knew it and delighted in it. I remember my first glimpse of Shibuya, when one of the young Tokyo journalists who had taken me there, his face drenched with the light of a thousand media-suns—all that towering, animated crawl of commercial information—said, \"You see? You see? It is \"Blade Runner\" town.\" And it was. It so evidently was.\n\nCyberpunk has influenced many anime and manga including the ground-breaking \"Akira\", \"Ghost in the Shell\", \"Ergo Proxy\", \"Battle Angel Alita\", \"Megazone 23\", \"Neo Tokyo\", \"Goku Midnight Eye\", \"Cyber City Oedo 808\", \"Bubblegum Crisis\", \"\", \"Angel Cop\", \"Extra\", \"Blame!\", \"Armitage III\", \"Texhnolyze\", \"Serial Experiments Lain\", \"Neon Genesis Evangelion\" and \"Psycho-Pass\".\n\nThere are cyberpunk video games. Popular series include the \"Metal Gear\" series, \"Megami Tensei\" series, \"Deus Ex\" series, \"Syndicate\" series, and \"System Shock\" and its sequel. Other games, like \"Blade Runner\", \"Ghost in the Shell\", and the \"Matrix\" series, are based upon genre movies, or role-playing games (for instance the various \"Shadowrun\" games). CD Projekt RED are currently developing a cyberpunk game, \"Cyberpunk 2077\".\n\nSeveral role-playing games (RPGs) called \"Cyberpunk\" exist: \"Cyberpunk\", \"Cyberpunk 2020\" and \"Cyberpunk v3\", by R. Talsorian Games, and \"GURPS Cyberpunk\", published by Steve Jackson Games as a module of the GURPS family of RPGs. \"Cyberpunk 2020\" was designed with the settings of William Gibson's writings in mind, and to some extent with his approval, unlike the approach taken by FASA in producing the transgenre \"Shadowrun\" game. Both are set in the near future, in a world where cybernetics are prominent. In addition, Iron Crown Enterprises released an RPG named \"Cyberspace\", which was out of print for several years until recently being re-released in online PDF form.\n\nIn 1990, in a convergence of cyberpunk art and reality, the United States Secret Service raided Steve Jackson Games's headquarters and confiscated all their computers. This was allegedly because the \"GURPS Cyberpunk\" sourcebook could be used to perpetrate computer crime. That was, in fact, not the main reason for the raid, but after the event it was too late to correct the public's impression. Steve Jackson Games later won a lawsuit against the Secret Service, aided by the new Electronic Frontier Foundation. This event has achieved a sort of notoriety, which has extended to the book itself as well. All published editions of \"GURPS Cyberpunk\" have a tagline on the front cover, which reads \"The book that was seized by the U.S. Secret Service!\" Inside, the book provides a summary of the raid and its aftermath.\n\nCyberpunk has also inspired several tabletop, miniature and board games such as \"Necromunda\" by Games Workshop. \"Netrunner\" is a collectible card game introduced in 1996, based on the \"Cyberpunk 2020\" role-playing game. \"Tokyo NOVA\", debuting in 1993, is a cyberpunk role-playing game that uses playing cards instead of dice.\n\nSome musicians and acts have been classified as cyberpunk due to their aesthetic style and musical content. Often dealing with dystopian visions of the future or biomechanical themes, some fit more squarely in the category than others. Bands whose music has been classified as cyberpunk include Psydoll, Front Line Assembly, Clock DVA and Sigue Sigue Sputnik. Some musicians not normally associated with cyberpunk have at times been inspired to create concept albums exploring such themes. Albums such as Gary Numan's Replicas, The Pleasure Principle and Telekon were heavily inspired by the works of Philip K. Dick. Kraftwerk's The Man-Machine and Computer World albums both explored the theme of humanity becoming dependent on technology. Nine Inch Nails' concept album Year Zero also fits into this category. Billy Idol's \"Cyberpunk\" drew heavily from cyberpunk literature and the cyberdelic counter culture in its creation. \"1. Outside\", a cyberpunk narrative fueled concept album by David Bowie, was warmly met by critics upon its release in 1995. Many musicians have also taken inspiration from specific cyberpunk works or authors, including Sonic Youth, whose albums Sister and Daydream Nation take influence from the works of Phillip K. Dick and William Gibson respectively.\n\nVaporwave and Synthwave are also influenced by cyberpunk. The former has been interpreted as a dystopian critique of capitalism in the vein of cyberpunk and the latter as a nostalgic retrofuturistic revival of aspects of cyberpunk's origins.\n\nFurthermore, many dubstep producers, such as Machine Man and Ghosthack, have found inspiration in cyberpunk themes for their works.\n\nSome Neo-Futurism artworks and cityscapes have been influenced by cyberpunk, such as the Sony Center in the Potsdamer Platz public square of Berlin, Germany.\n\nSeveral subcultures have been inspired by cyberpunk fiction. These include the cyberdelic counter culture of the late 1980s and early 90s. Cyberdelic, whose adherents referred to themselves as \"cyberpunks\", attempted to blend the psychedelic art and drug movement with the technology of cyberculture. Early adherents included Timothy Leary, Mark Frauenfelder and R. U. Sirius. The movement largely faded following the dot-com bubble implosion of 2000.\n\nCybergoth is a fashion and dance subculture which draws its inspiration from cyberpunk fiction, as well as rave and Gothic subcultures. In addition, a distinct cyberpunk fashion of its own has emerged in recent years which rejects the raver and goth influences of cybergoth, and draws inspiration from urban street fashion, \"post apocalypse\", functional clothing, high tech sports wear, tactical uniform and multifunction. This fashion goes by names like \"tech wear\", \"goth ninja\" or \"tech ninja\". Important designers in this type of fashion are ACRONYM, Demobaza, Boris Bidjan Saberi, Rick Owens and Alexander Wang.\n\nThe Kowloon Walled City in Hong Kong (demolished in 1994) is often referenced as the model cyberpunk/dystopian slum as, given its poor living conditions at the time coupled by the city's political, physical, and economic isolation has caused many in academia to be fascinated by the ingenuity of its spawning.\n\nAs a wider variety of writers began to work with cyberpunk concepts, new subgenres of science fiction emerged, some of which could be considered as playing off the cyberpunk label, others which could be considered as legitimate explorations into newer territory. These focused on technology and its social effects in different ways. One prominent subgenre is \"steampunk,\" which is set in an alternate history Victorian era that combines anachronistic technology with cyberpunk's bleak film noir world view. The term was originally coined around 1987 as a joke to describe some of the novels of Tim Powers, James P. Blaylock, and K.W. Jeter, but by the time Gibson and Sterling entered the subgenre with their collaborative novel \"The Difference Engine\" the term was being used earnestly as well.\n\nAnother subgenre is \"biopunk\" (cyberpunk themes dominated by biotechnology) from the early 1990s, a derivative style building on biotechnology rather than informational technology. In these stories, people are changed in some way not by mechanical means, but by genetic manipulation. Paul Di Filippo is seen as the most prominent biopunk writer, including his half-serious ribofunk. Bruce Sterling's Shaper/Mechanist cycle is also seen as a major influence. In addition, some people consider works such as Neal Stephenson's \"The Diamond Age\" to be postcyberpunk.\n\nCyberpunk works have been described as well-situated within postmodern literature.\n\nRole playing game publisher R. Talsorian Games, onner of the Cyberpunk 2020 franchise, trademarked the word \"Cyberpunk\" in the United States in 2012. Video game developer CD Projekt, which is developing Cyberpunk 2077, bought the U.S. trademark from R. Talsorian Games, and has filed a trademark in the European Union.\n\n", "id": "5703", "title": "Cyberpunk"}
{"url": "https://en.wikipedia.org/wiki?curid=5704", "text": "Comic strip\n\nA comic strip is a sequence of drawings arranged in interrelated panels to display brief humor or form a narrative, often serialized, with text in balloons and captions. Traditionally, throughout the 20th century and into the 21st, these have been published in newspapers and magazines, with horizontal strips printed in black-and-white in daily newspapers, while Sunday newspapers offered longer sequences in special color comics sections. With the development of the internet, they began to appear online as web comics.\nThere were more than 200 different comic strips and daily cartoon panels in American newspapers alone each day for most of the 20th century, for a total of at least 7,300,000 episodes.\n\nStrips are written and drawn by a comics artist or cartoonist. As the name implies, comic strips can be humorous (for example, \"gag-a-day\" strips such as \"Blondie\", \"Bringing Up Father\", \"Marmaduke\", and \"Pearls Before Swine\").\n\nStarting in the late 1920s, comic strips expanded from their mirthful origins to feature adventure stories, as seen in \"Popeye\", \"Captain Easy\", \"Buck Rogers\", \"Tarzan\", and \"The Adventures of Tintin\". Soap-opera continuity strips such as \"Judge Parker\" and \"Mary Worth\" gained popularity in the 1940s. All are called, generically, comic strips, though cartoonist Will Eisner has suggested that \"sequential art\" would be a better genre-neutral name.\n\nIn the UK and the rest of Europe, comic strips are also serialized in comic book magazines, with a strip's story sometimes continuing over three pages or more. Comic strips have appeared in American magazines such as \"Liberty\" and \"Boys' Life\" and also on the front covers of magazines, such as the \"Flossy Frills\" series on \"The American Weekly\" Sunday newspaper supplement.\n\nStorytelling using a sequence of pictures has existed through history. One medieval European example in textile form is the Bayeux Tapestry. Printed examples emerged in 19th-century Germany and in 18th-century England, where some of the first satirical or humorous sequential narrative drawings were produced. William Hogarth's 18th century English cartoons include both narrative sequences, such as \"A Rake's Progress\", and single panels.\n\nThe Biblia pauperum (\"Paupers' Bible\"), a tradition of picture Bibles beginning in the later Middle Ages, sometimes depicted Biblical events with words spoken by the figures in the miniatures written on scrolls coming out of their mouths—which makes them to some extent ancestors of the modern cartoon strips.\n\nIn China, with its traditions of block printing and of the incorporation of text with image, experiments with what became \"lianhuanhua\" date back to 1884.\n\nThe first newspaper comic strips appeared in North America in the late 19th century. \"The Yellow Kid\" is usually credited as one of the first newspaper strips. However, the art form combining words and pictures developed gradually and there are many examples which led up to the comic strip.\n\nSwiss author and caricature artist Rodolphe Töpffer (Geneva, 1799–1846) is considered the father of the modern comic strips. His illustrated stories such as \"Histoire de M. Vieux Bois\" (1827), first published in the USA in 1842 as \"The Adventures of Obadiah Oldbuck\" or \"Histoire de Monsieur Jabot\" (1831), inspired subsequent generations of German and American comic artists. In 1865, German painter, author, and caricaturist Wilhelm Busch created the strip \"Max and Moritz\", about two trouble-making boys, which had a direct influence on the American comic strip. \"Max and Moritz\" was a series of severely moralistic tales in the vein of German children's stories such as \"Struwwelpeter\" (\"Shockheaded Peter\"); in one, the boys, after perpetrating some mischief, are tossed into a sack of grain, run through a mill, and consumed by a flock of geese. \"Max and Moritz\" provided an inspiration for German immigrant Rudolph Dirks, who created the \"Katzenjammer Kids\" in 1897. Familiar comic-strip iconography such as stars for pain, sawing logs for snoring, speech balloons, and thought balloons originated in Dirks' strip.\n\nHugely popular, \"Katzenjammer Kids\" occasioned one of the first comic-strip copyright ownership suits in the history of the medium. When Dirks left William Randolph Hearst for the promise of a better salary under Joseph Pulitzer, it was an unusual move, since cartoonists regularly deserted Pulitzer for Hearst. In a highly unusual court decision, Hearst retained the rights to the name \"Katzenjammer Kids\", while creator Dirks retained the rights to the characters. Hearst promptly hired Harold Knerr to draw his own version of the strip. Dirks renamed his version \"Hans and Fritz\" (later, \"The Captain and the Kids\"). Thus, two versions distributed by rival syndicates graced the comics pages for decades. Dirks' version, eventually distributed by United Feature Syndicate, ran until 1979.\n\nIn the United States, the great popularity of comics sprang from the newspaper war (1887 onwards) between Pulitzer and Hearst. \"The Little Bears\" (1893–96) was the first American comic strip with recurring characters, while the first color comic supplement was published by the \"Chicago Inter-Ocean\" sometime in the latter half of 1892, followed by the \"New York Journal\"'s first color Sunday comic pages in 1897. On January 31, 1912, Hearst introduced the nation's first full daily comic page in his \"New York Evening Journal\". The history of this newspaper rivalry and the rapid appearance of comic strips in most major American newspapers is discussed by Ian Gordon. Numerous events in newspaper comic strips have reverberated throughout society at large, though few of these events occurred in recent years, owing mainly to the declining role of the newspaper comic strip as an entertainment form.\n\nThe longest running American comic strips are:\n\nNewspaper comic strips come in two different types: daily strips and Sunday strips. Most newspaper comic strips are syndicated; a syndicate hires people to write and draw a strip and then distributes it to many newspapers for a fee. A few newspaper strips are exclusive to one newspaper. For example, the \"Pogo\" comic strip by Walt Kelly originally appeared only in the \"New York Star\" in 1948 and was not picked up for syndication until the following year.\n\nIn the United States, a daily strip appears in newspapers on weekdays, Monday through Saturday, as contrasted with a Sunday strip, which typically only appears on Sundays. Daily strips usually are printed in black and white, and Sunday strips are usually in color. However, a few newspapers have published daily strips in color, and some newspapers have published Sunday strips in black and white. The two conventional formats for newspaper comics are strips and single gag panels. The strips are usually displayed horizontally, wider than they are tall. Single panels are square, circular or taller than they are wide. Strips usually, but not always, are broken up into several smaller panels with continuity from panel to panel. A horizontal strip can also be used for a single panel with a single gag, as seen occasionally in Mike Peters' \"Mother Goose and Grimm\".\n\nDuring the 1930s, the original art for a daily strip could be drawn as large as 25 inches wide by six inches high. As strips have become smaller, the number of panels have been reduced.\n\nThe popularity and accessibility of strips meant they were often clipped and saved; authors including John Updike and Ray Bradbury have written about their childhood collections of clipped strips. Often posted on bulletin boards, clipped strips had an ancillary form of distribution when they were faxed, photocopied or mailed. The \"Baltimore Sun\"'s Linda White recalled, \"I followed the adventures of \"Winnie Winkle\", \"Moon Mullins\" and \"Dondi\", and waited each fall to see how Lucy would manage to trick Charlie Brown into trying to kick that football. (After I left for college, my father would clip out that strip each year and send it to me just to make sure I didn’t miss it.)\"\n\nProof sheets were the means by which syndicates provided newspapers with black-and-white line art for the reproduction of strips (which they arranged to have colored in the case of Sunday strips). Michigan State University Comic Art Collection librarian Randy Scott describes these as \"large sheets of paper on which newspaper comics have traditionally been distributed to subscribing newspapers. Typically each sheet will have either six daily strips of a given title or one Sunday strip. Thus, a week of \"Beetle Bailey\" would arrive at the \"Lansing State Journal\" in two sheets, printed much larger than the final version and ready to be cut apart and fitted into the local comics page.\" Comic strip historian Allan Holtz described how strips were provided as mats (the plastic or cardboard trays in which molten metal is poured to make plates) or even plates ready to be put directly on the printing press. He also notes that with electronic means of distribution becoming more prevalent printed sheets \"are definitely on their way out.\"\n\nSingle panels usually, but not always, are not broken up and lack continuity. The daily \"Peanuts\" is a strip, and the daily \"Dennis the Menace\" is a single panel. J. R. Williams' long-run \"Out Our Way\" continued as a daily panel even after it expanded into a Sunday strip, \"Out Our Way with the Willets\". Jimmy Hatlo's \"They'll Do It Every Time\" was often displayed in a two-panel format with the first panel showing some deceptive, pretentious, unwitting or scheming human behavior and the second panel revealing the truth of the situation.\n\nEarly daily strips were large, often running the entire width of the newspaper, and were sometimes three or more inches high. Initially, a newspaper page included only a single daily strip, usually either at the top or the bottom of the page. By the 1920s, many newspapers had a comics page on which many strips were collected together. Over decades, the size of daily strips became smaller and smaller, until by 2000, four standard daily strips could fit in an area once occupied by a single daily strip.\n\nNEA Syndicate experimented briefly with a two-tier daily strip, \"Star Hawks\", but after a few years, \"Star Hawks\" dropped down to a single tier.\n\nIn Flanders, the two-tier strip is the standard publication style of most daily strips like \"Spike and Suzy\" and \"Nero\". They appear Monday through Saturday; until 2003 there were no Sunday papers in Flanders. In the last decades, they have switched from black and white to color.\n\nSunday newspapers traditionally included a special color section. Early Sunday strips, such as \"Thimble Theatre\" and \"Little Orphan Annie\", filled an entire newspaper page, a format known to collectors as full page. Sunday pages during the 1930s and into the 1940s often carried a secondary strip by the same artist as the main strip. No matter whether it appeared above or below a main strip, the extra strip was known as the topper, such as \"The Squirrel Cage\" which ran along with \"Room and Board\", both drawn by Gene Ahern.\n\nDuring the 1930s, the original art for a Sunday strip was usually drawn quite large. For example, in 1930, Russ Westover drew his \"Tillie the Toiler\" Sunday page at a size of 17\" × 37\". In 1937, the cartoonist Dudley Fisher launched the innovative \"Right Around Home\", drawn as a huge single panel filling an entire Sunday page.\n\nFull-page strips were eventually replaced by strips half that size. Strips such as \"The Phantom\" and \"Terry and the Pirates\" began appearing in a format of two strips to a page in full-size newspapers, such as the \"New Orleans Times Picayune\", or with one strip on a tabloid page, as in the \"Chicago Sun-Times\". When Sunday strips began to appear in more than one format, it became necessary for the cartoonist to allow for rearranged, cropped or dropped panels. During World War II, because of paper shortages, the size of Sunday strips began to shrink. After the war, strips continued to get smaller and smaller because of increased paper and printing costs. The last full-page comic strip was the \"Prince Valiant\" strip for 11 April 1971.\n\nComic strips have also been published in Sunday newspaper magazines. Russell Patterson and Carolyn Wells' \"New Adventures of Flossy Frills\" was a continuing strip series seen on Sunday magazine covers. Beginning January 26, 1941, it ran on the front covers of Hearst's \"American Weekly\" newspaper magazine supplement, continuing until March 30 of that year. Between 1939 and 1943, four different stories featuring Flossy appeared on \"American Weekly\" covers.\n\nSunday comics sections employed offset color printing with multiple print runs imitating a wide range of colors. Printing plates were created with four or more colors—traditionally, the CMYK color model: cyan, magenta, yellow and \"K\" for black. With a screen of tiny dots on each printing plate, the dots allowed an image to be printed in a halftone that appears to the eye in different gradations. The semi-opaque property of ink allows halftone dots of different colors to create an optical effect of full-color imagery.\n\nThe decade of the 1960s saw the rise of underground newspapers, which often carried comic strips, such as \"Fritz the Cat\" and \"The Fabulous Furry Freak Brothers\". \"Zippy the Pinhead\" initially appeared in underground publications in the 1970s before being syndicated. \"Bloom County\" and \"Doonesbury\" began as strips in college newspapers under different titles, and later moved to national syndication. Underground comic strips covered subjects that are usually taboo in newspaper strips, such as sex and drugs. Many underground artists, notably Vaughn Bode, Dan O'Neill, Gilbert Shelton, and Art Spiegelman went on to draw comic strips for magazines such as \"Playboy\", \"National Lampoon\", and Pete Millar's \"CARtoons\". Jay Lynch graduated from undergrounds to alternative weekly newspapers to \"Mad\" and children's books.\n\n\"Webcomics\", also known as \"online comics\" and \"internet comics\", are comics that are available to read on the Internet. Many are exclusively published online, but the majority of traditional newspaper comic strips have some Internet presence. King Features Syndicate and other syndicates often provide archives of recent strips on their websites. Some, such as Scott Adams, creator of \"Dilbert\", include an email address in each strip.\n\nMost comic strip characters do not age throughout the strip's life, but in some strips, like Lynn Johnston's award-winning \"For Better or For Worse\", the characters age as the years pass. The first strip to feature aging characters was \"Gasoline Alley\".\n\nThe history of comic strips also includes series that are not humorous, but tell an ongoing dramatic story. Examples include \"The Phantom\", \"Prince Valiant\", \"Dick Tracy\", \"Mary Worth\", \"Modesty Blaise\", \"Little Orphan Annie\", \"Flash Gordon\", and \"Tarzan\". Sometimes these are spin-offs from comic books, for example \"Superman\", \"Batman\", and \"The Amazing Spider-Man\".\n\nA number of strips have featured animals ('funny animals') as main characters. Some are non-verbal (\"Marmaduke\", \"The Angriest Dog in the World\"), some have verbal thoughts but are not understood by humans, (\"Garfield\", Snoopy in \"Peanuts\"), and some can converse with humans (\"Bloom County\", \"Calvin and Hobbes\", \"Mutts\", \"Citizen Dog\", \"Buckles\", \"Get Fuzzy\", \"Pearls Before Swine\", and \"Pooch Cafe\"). Other strips are centered entirely on animals, as in \"Pogo\" and \"Donald Duck\". Gary Larson's \"The Far Side\" was unusual, as there were no central characters. Instead \"The Far Side\" used a wide variety of characters including humans, monsters, aliens, chickens, cows, worms, amoebas, and more. John McPherson's \"Close to Home\" also uses this theme, though the characters are mostly restricted to humans and real-life situations. Wiley Miller not only mixes human, animal, and fantasy characters, but also does several different comic strip continuities under one umbrella title, \"Non Sequitur\". Bob Thaves's \"Frank & Ernest\" began in 1972 and paved the way for some of these strips, as its human characters were manifest in diverse forms — as animals, vegetables, and minerals.\n\nThe comics have long held a distorted mirror to contemporary society, and almost from the beginning have been used for political or social commentary. This ranged from the conservative slant of \"Little Orphan Annie\" to the unabashed liberalism of \"Doonesbury\". \"Pogo\" used animals to particularly devastating effect, caricaturing many prominent politicians of the day as animal denizens of Pogo's Okeefenokee Swamp. In a fearless move, Pogo's creator Walt Kelly took on Joseph McCarthy in the 1950s, caricaturing him as a bobcat named Simple J. Malarkey, a megalomaniac who was bent on taking over the characters' birdwatching club and rooting out all undesirables. Kelly also defended the medium against possible government regulation in the McCarthy era. At a time when comic books were coming under fire for supposed sexual, violent, and subversive content, Kelly feared the same would happen to comic strips. Going before the Congressional subcommittee, he proceeded to charm the members with his drawings and the force of his personality. The comic strip was safe for satire.\n\nDuring the early 20th century, comic strips were widely associated with publisher William Randolph Hearst, whose papers had the largest circulation of strips in the United States. Hearst was notorious for his practice of yellow journalism, and he was frowned on by readers of \"The New York Times\" and other newspapers which featured few or no comic strips. Hearst's critics often assumed that all the strips in his papers were fronts for his own political and social views. Hearst did occasionally work with or pitch ideas to cartoonists, most notably his continued support of George Herriman's \"Krazy Kat\". An inspiration for Bill Watterson and other cartoonists, \"Krazy Kat\" gained a considerable following among intellectuals during the 1920s and 1930s.\n\nSome comic strips, such as \"Doonesbury\" and \"The Boondocks\", may be printed on the editorial or op-ed page rather than the comics page because of their regular political commentary. For example, the August 12, 1974 \"Doonesbury\" strip was awarded a 1975 Pulitzer Prize for its depiction of the Watergate scandal. \"Dilbert\" is sometimes found in the business section of a newspaper instead of the comics page because of the strip's commentary about office politics, and Tank McNamara often appears on the sports page because of its subject matter. Lynn Johnston's \"For Better or for Worse\" created an uproar when one of its supporting characters came out of the closet and announced he was gay.\n\nThe world's longest comic strip is long and on display at Trafalgar Square as part of the London Comedy Festival. The London Cartoon Strip was created by 15 of Britain's best known cartoonists and depicts the history of London.\n\nThe Reuben, named for cartoonist Rube Goldberg, is the most prestigious award for U.S. comic strip artists. Reuben awards are presented annually by the National Cartoonists Society (NCS).\n\nToday's strip artists, with the help of the NCS, enthusiastically promote the medium, which is considered to be in decline due to fewer markets (today few strips are published in newspapers outside the United States, the United Kingdom, and Canada, mainly because of the smaller interest there, with translated versions of popular strips - particularly in Spanish - are primarily read over the internet) and ever-shrinking newspaper space. One particularly humorous example of such promotional efforts is the Great Comic Strip Switcheroonie, held in 1997 on April Fool's Day, an event in which dozens of prominent artists took over each other's strips. \"Garfield\"’s Jim Davis, for example, switched with \"Blondie\"’s Stan Drake, while Scott Adams (\"Dilbert\") traded strips with Bil Keane (\"The Family Circus\"). Even the United States Postal Service got into the act, issuing a series of commemorative stamps marking the comic-strip centennial in 1996.\n\nWhile the Switcheroonie was a one-time publicity stunt, for one artist to take over a feature from its originator is an old tradition in newspaper cartooning (as it is in the comic book industry). In fact, the practice has made possible the longevity of the genre's more popular strips. Examples include \"Little Orphan Annie\" (drawn and plotted by Harold Gray from 1924 to 1944 and thereafter by a succession of artists including Leonard Starr and Andrew Pepoy), and \"Terry and The Pirates\", started by Milton Caniff in 1934 and picked up by George Wunder.\n\nA business-driven variation has sometimes led to the same feature continuing under a different name. In one case, in the early 1940s, Don Flowers' \"Modest Maidens\" was so admired by William Randolph Hearst that he lured Flowers away from the Associated Press and to King Features Syndicate by doubling the cartoonist's salary, and renamed the feature \"Glamor Girls\" to avoid legal action by the AP. The latter continued to publish \"Modest Maidens\", drawn by Jay Allen in Flowers' style.\n\nAs newspapers have declined, the changes have affected comic strips. Jeff Reece, lifestyle editor of \"The Florida Times-Union\", wrote, \"Comics are sort of the 'third rail' of the newspaper.\"\n\nIn the early decades of the 20th century, all Sunday comics received a full page, and daily strips were generally the width of the page. The competition between papers for having more cartoons than the rest from the mid-1920s, the growth of large-scale newspaper advertising during most of the thirties, paper rationing during World War II, the decline on news readership (as television newscasts began to be more common) and inflation (which has caused higher printing costs) beginning during the fifties and sixties led to Sunday strips being published on smaller and more diverse formats. Daily strips have suffered as well, in 1910 the strips had an unlimited amount of panels, covering the entire width page, while by 1930 most \"dailies\" had four or five panels covering six of the eight columns occupied by a traditional broadsheet paper, by 1958 those four panels would be narrower, and those would have half of the space a 1910 daily strip had, and by 1998 most strips would have three panels only (with a few exceptions), or even two or one on an occasional basis, apart from strips being smaller, as most papers became slightly narrower. While most cartoonist decided to follow the tide, some cartoonists have complained about this, with \"Pogo\" ending in 1975 as a form of protest from its creators against the practice. Since then \"Calvin and Hobbes\" creator Bill Watterson has written extensively on the issue, arguing that size reduction and dropped panels reduce both the potential and freedom of a cartoonist. After a lengthy battle with his syndicator, Watterson won the privilege of making half page-sized Sunday strips where he could arrange the panels any way he liked. Many newspaper publishers and a few cartoonists objected to this, and some papers continued to print \"Calvin and Hobbes\" at small sizes. Opus won that same privilege years after \"Calvin and Hobbes\" ended, while Wiley Miller circumvented further downsizings by making his \"Non Sequitur\" Sunday strip available only in an extremely vertical (near-page-long) arrangement. Few newspapers still run half-page strips, as with \"Prince Valiant\" and \"Hägar the Horrible\" in the front page of the \"Reading Eagle\" Sunday comics section. Actually Universal Uclick and United Media practically have no half-page comics, with the remaining strips from both syndicates in this format are published only as \"thirds\", \"fourths\", and \"sixths\" (also called \"third tabs\").\n\nIn an issue related to size limitations, Sunday comics are often bound to rigid formats that allow their panels to be rearranged in several different ways while remaining readable. Such formats usually include throwaway panels at the beginning, which some newspapers will omit for space. As a result, cartoonists have less incentive to put great efforts into these panels. \"Garfield\" and \"Mutts\" were known during the mid-to-late 80s and 1990s respectively for their throwaways on their Sunday strips, however both strips now run \"generic\" title panels.\n\nWith the success of \"The Gumps\" during the 1920s, it became commonplace for strips (comedy- and adventure-laden alike) to have lengthy stories spanning weeks or months. The \"Monarch of Medioka\" story in Floyd Gottfredson's \"Mickey Mouse\" comic strip ran from September 8, 1937 to May 2, 1938. Between the 1960s and the late 1980s, as television news relegated newspaper reading to an occasional basis rather than daily, syndicators were abandoning long stories and urging cartoonists to switch to simple daily gags, or week-long \"storylines\" (with six consecutive (mostly unrelated) strips following a same subject), with longer storylines being used mainly on adventure-based and dramatic strips. Strips begun during the mid-1980s or after (such as \"Get Fuzzy\", \"Over the Hedge\", \"Monty\", and others) are known for their heavy use of storylines, lasting between one and three weeks in most cases.\n\nThe writing style of comic strips changed as well after World War II. With an increase in the number of college-educated readers, there was a shift away from slapstick comedy and towards more cerebral humor. Slapstick and visual gags became more confined to Sunday strips, because as \"Garfield\" creator Jim Davis put it, \"Children are more likely to read Sunday strips than dailies.\"\n\nMany older strips are no longer drawn by the original cartoonist, who has either died or retired. Such strips are known as \"zombie strips\". A cartoonist, paid by the syndicate or sometimes a relative of the original cartoonist, continues writing the strip, a tradition that became commonplace in the early half of the 20th century. \"Hägar the Horrible\" and \"Frank and Ernest\" are both drawn by the sons of the creators. Some strips which are still in affiliation with the original creator are produced by small teams or entire companies, such as Jim Davis' \"Garfield\", however there is some debate if these strips fall in this category.\n\nThis act is commonly criticized by modern cartoonists including Watterson and \"Pearls Before Swine\"'s Stephan Pastis. The issue was addressed in six consecutive \"Pearls\" strips in 2005. Charles Schulz, of \"Peanuts\" fame, requested that his strip not be continued by another cartoonist after his death. He also rejected the idea of hiring an inker or letterer, comparing it to a golfer hiring a man to make his putts. Schulz's family has honored his wishes and refused numerous proposals by syndicators to continue \"Peanuts\" with a new author.\n\nSince the consolidation of newspaper comics by the first quarter of the 20th century, most cartoonists have used a group of assistants (with usually one of them credited). However, quite a few cartoonists (e.g.: George Herriman and Charles Schulz, among others) have done their strips almost completely by themselves; often criticizing the use of assistants for the same reasons most have about their editors hiring anyone else to continue their work after their retirement.\n\nSince the dawn of comic strips, the ownership of them has been a recurrent issue. Traditionally, the syndicate owned the rights to the strips. However, throughout history there have been exceptions, with \"Mutt and Jeff\" being an early (if not the earliest) case in which the creator owned his works. However this was later limited to adaptations of animated properties. When it started in 1970, the Universal Press Syndicate gave cartoonists a 50-percent share on the ownership of their works, while the Creators Syndicate (founded in 1987) granted artists full rights to the strips, something that Universal Press did in 1990. followed by King Features in 1995, while before 1999 both the Tribune and United Feature services began granting rights to creators over their works; however the latter three syndicates only applied this to new strips, or to ones popular enough.\n\nStarting in the late 1940s, the national syndicates which distributed newspaper comic strips subjected them to very strict censorship. \"Li'l Abner\" was censored in September 1947 and was pulled from the Pittsburg Press by Scripps-Howard. The controversy, as reported in \"Time\", centered on Capp's portrayal of the U.S. Senate. Said Edward Leech of Scripps, \"We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables.\"\n\nAs comics are easier for children to access compared to other types of media, they have a significantly more rigid censorship code than other media. Stephan Pastis has lamented that the \"unwritten\" censorship code is still \"stuck somewhere in the 1950s.\" Generally, comics are not allowed to include such words as \"damn\", \"sucks\", \"screwed\", and \"hell\", although there have been exceptions such as the September 22, 2010 \"Mother Goose and Grimm\" in which an elderly man says, \"This nursing home food sucks,\" and a pair of \"Pearls Before Swine\" comics from January 11, 2011 with a character named Ned using the word \"crappy\". Naked backsides and shooting guns cannot be shown, according to \"Dilbert\" cartoonist Scott Adams. Such comic strip taboos were detailed in Dave Breger's book \"But That's Unprintable\" (Bantam, 1955).\n\nMany issues such as sex, narcotics, and terrorism cannot or can very rarely be openly discussed in strips, although there are exceptions, usually for satire, as in \"Bloom County\". This led some cartoonists to resort to double entendre or dialogue children do not understand, as in Greg Evans' \"Luann\". Young cartoonists have claimed commonplace words, images, and issues should be allowed in the comics. Some of the taboo words and topics are mentioned daily on television and other forms of visual media. Web comics and comics distributed primarily to college newspapers are much freer in this respect.\n\n\n\n\n", "id": "5704", "title": "Comic strip"}
{"url": "https://en.wikipedia.org/wiki?curid=5705", "text": "Continuum hypothesis\n\nIn mathematics, the continuum hypothesis (abbreviated CH) is a hypothesis about the possible sizes of infinite sets. It states:\n\nThe continuum hypothesis was advanced by Georg Cantor in 1878, and\nestablishing its truth or falsehood is the first of Hilbert's 23 problems presented in 1900. Τhe answer to this problem is independent of ZFC set theory (that is, Zermelo–Fraenkel set theory with the axiom of choice included), so that either the continuum hypothesis or its negation can be added as an axiom to ZFC set theory, with the resulting theory being consistent if and only if ZFC is consistent. This independence was proved in 1963 by Paul Cohen, complementing earlier work by Kurt Gödel in 1940.\n\nThe name of the hypothesis comes from the term \"the continuum\" for the real numbers.\n\nTwo sets are said to have the same \"cardinality\" or \"cardinal number\" if there exists a bijection (a one-to-one correspondence) between them. Intuitively, for two sets \"S\" and \"T\" to have the same cardinality means that it is possible to \"pair off\" elements of \"S\" with elements of \"T\" in such a fashion that every element of \"S\" is paired off with exactly one element of \"T\" and vice versa. Hence, the set has the same cardinality as .\n\nWith infinite sets such as the set of integers or rational numbers, this becomes more complicated to demonstrate. The rational numbers seemingly form a counterexample to the continuum hypothesis: the integers form a proper subset of the rationals, which themselves form a proper subset of the reals, so intuitively, there are more rational numbers than integers and more real numbers than rational numbers. However, this intuitive analysis does not take account of the fact that all three sets are infinite. It turns out the rational numbers can actually be placed in one-to-one correspondence with the integers, and therefore the set of rational numbers is the same size (\"cardinality\") as the set of integers: they are both countable sets.\n\nCantor gave two proofs that the cardinality of the set of integers is strictly smaller than that of the set of real numbers (see Cantor's first uncountability proof and Cantor's diagonal argument). His proofs, however, give no indication of the extent to which the cardinality of the integers is less than that of the real numbers. Cantor proposed the continuum hypothesis as a possible solution to this question.\n\nThe hypothesis states that the set of real numbers has minimal possible cardinality which is greater than the cardinality of the set of integers – or, equivalently, that the cardinality of the integers is formula_1 (\"aleph-naught\") and the cardinality of the real numbers is formula_2 (i.e. it equals the cardinality of the power set of the integers). Specifically, the continuum hypothesis says that there is no set formula_3 for which\n\nAssuming the axiom of choice, there is a smallest cardinal number formula_4 greater than formula_1, and the continuum hypothesis is in turn equivalent to the equality\n\nA consequence of the continuum hypothesis is that every infinite subset of the real numbers either has the same cardinality as the integers or the same cardinality as the entire set of the reals.\n\nThere is also a generalization of the continuum hypothesis called the generalized continuum hypothesis (GCH) which says that for all ordinals formula_6\n\nThat is, GCH asserts that the cardinality of the power set of any infinite set is the smallest cardinality greater than that of the set.\n\nCantor believed the continuum hypothesis to be true and tried for many years to prove it, in vain . It became the first on David Hilbert's list of important open questions that was presented at the International Congress of Mathematicians in the year 1900 in Paris. Axiomatic set theory was at that point not yet formulated.\n\nKurt Gödel showed in 1940 that the continuum hypothesis (CH for short) cannot be disproved from the standard Zermelo–Fraenkel set theory (ZF), even if the axiom of choice is adopted (ZFC) (). Paul Cohen showed in 1963 that CH cannot be proven from those same axioms either ( & ). Hence, CH is \"independent\" of ZFC. Both of these results assume that the Zermelo–Fraenkel axioms are consistent; this assumption is widely believed to be true. Cohen was awarded the Fields Medal in 1966 for his proof.\n\nThe continuum hypothesis is closely related to many statements in analysis, point set topology and measure theory. As a result of its independence, many substantial conjectures in those fields have subsequently been shown to be independent as well.\n\nSo far, CH appears to be independent of all known \"large cardinal axioms\" in the context of ZFC. ()\n\nThe independence from ZFC means that proving or disproving the CH within ZFC is impossible. However, Gödel and Cohen's negative results are not universally accepted as disposing of the hypothesis. Hilbert's problem remains an active topic of research; see and for an overview of the current research status.\n\nThe continuum hypothesis was not the first statement shown to be independent of ZFC. An immediate consequence of Gödel's incompleteness theorem, which was published in 1931, is that there is a formal statement (one for each appropriate Gödel numbering scheme) expressing the consistency of ZFC that is independent of ZFC, assuming that ZFC is consistent. The continuum hypothesis and the axiom of choice were among the first mathematical statements shown to be independent of ZF set theory. These proofs of independence were not completed until Paul Cohen developed forcing in the 1960s. They all rely on the assumption that ZF is consistent. These proofs are called proofs of relative consistency (see \"Forcing (mathematics))\".\n\nA result of Solovay, proved shortly after Cohen's result on the independence of the continuum hypothesis, shows that in any model of ZFC, if formula_7 is a cardinal of uncountable cofinality, then there is a forcing extension in which formula_8. However, it is not consistent to assume formula_2 is formula_10 or formula_11 or any cardinal with cofinality formula_12.\n\nGödel believed that CH is false, and that his proof that CH is consistent with ZFC only shows that the Zermelo–Fraenkel axioms do not adequately characterize the universe of sets. Gödel was a platonist and therefore had no problems with asserting the truth and falsehood of statements independent of their provability. Cohen, though a formalist , also tended towards rejecting CH.\n\nHistorically, mathematicians who favored a \"rich\" and \"large\" universe of sets were against CH, while those favoring a \"neat\" and \"controllable\" universe favored CH. Parallel arguments were made for and against the axiom of constructibility, which implies CH. More recently, Matthew Foreman has pointed out that ontological maximalism can actually be used to argue in favor of CH, because among models that have the same reals, models with \"more\" sets of reals have a better chance of satisfying CH (Maddy 1988, p. 500).\n\nAnother viewpoint is that the conception of set is not specific enough to determine whether CH is true or false. This viewpoint was advanced as early as 1923 by Skolem, even before Gödel's first incompleteness theorem. Skolem argued on the basis of what is now known as Skolem's paradox, and it was later supported by the independence of CH from the axioms of ZFC since these axioms are enough to establish the elementary properties of sets and cardinalities. In order to argue against this viewpoint, it would be sufficient to demonstrate new axioms that are supported by intuition and resolve CH in one direction or another. Although the axiom of constructibility does resolve CH, it is not generally considered to be intuitively true any more than CH is generally considered to be false (Kunen 1980, p. 171).\n\nAt least two other axioms have been proposed that have implications for the continuum hypothesis, although these axioms have not currently found wide acceptance in the mathematical community. In 1986, Chris Freiling presented an argument against CH by showing that the negation of CH is equivalent to Freiling's axiom of symmetry, a statement about probabilities. Freiling believes this axiom is \"intuitively true\" but others have disagreed. A difficult argument against CH developed by W. Hugh Woodin has attracted considerable attention since the year 2000 (Woodin 2001a, 2001b). Foreman (2003) does not reject Woodin's argument outright but urges caution.\n\nSolomon Feferman (2011) has made a complex philosophical argument that CH is not a definite mathematical problem. He proposes a theory of \"definiteness\" using a semi-intuitionistic subsystem of ZF that accepts classical logic for bounded quantifiers but uses intuitionistic logic for unbounded ones, and suggests that a proposition formula_13 is mathematically \"definite\" if the semi-intuitionistic theory can prove formula_14. He conjectures that CH is not definite according to this notion, and proposes that CH should, therefore, be considered not to have a truth value. Peter Koellner (2011b) wrote a critical commentary on Feferman's article.\n\nJoel David Hamkins proposes a multiverse approach to set theory and argues that \"the continuum hypothesis is settled on the multiverse view by our extensive knowledge about how it behaves in the multiverse, and, as a result, it can no longer be settled in the manner formerly hoped for.\" (Hamkins 2012). In a related vein, Saharon Shelah wrote that he does \"not agree with the pure Platonic view that the interesting problems in set theory can be decided, that we just have to discover the additional axiom. My mental picture is that we have many possible set theories, all conforming to ZFC.\" (Shelah 2003).\n\nThe \"generalized continuum hypothesis\" (GCH) states that if an infinite set's cardinality lies between that of an infinite set \"S\" and that of the power set of \"S\", then it either has the same cardinality as the set \"S\" or the same cardinality as the power set of \"S\". That is, for any infinite cardinal formula_15 there is no cardinal formula_16 such that formula_17 GCH is equivalent to:\nThe beth numbers provide an alternate notation for this condition: formula_20 for every ordinal formula_19\n\nThis is a generalization of the continuum hypothesis since the continuum has the same cardinality as the power set of the integers. It was first suggested by .\n\nLike CH, GCH is also independent of ZFC, but Sierpiński proved that ZF + GCH implies the axiom of choice (AC) (and therefore the negation of the axiom of determinacy, AD), so choice and GCH are not independent in ZF; there are no models of ZF in which GCH holds and AC fails. To prove this, Sierpiński showed GCH implies that every cardinality n is smaller than some Aleph number, and thus can be ordered. This is done by showing that n is smaller than formula_22 which is smaller than its own Hartogs number — this uses the equality formula_23; for the full proof, see Gillman (2002).\n\nKurt Gödel showed that GCH is a consequence of ZF + V=L (the axiom that every set is constructible relative to the ordinals), and is therefore consistent with ZFC. As GCH implies CH, Cohen's model in which CH fails is a model in which GCH fails, and thus GCH is not provable from ZFC. W. B. Easton used the method of forcing developed by Cohen to prove Easton's theorem, which shows it is consistent with ZFC for arbitrarily large cardinals formula_24 to fail to satisfy formula_25 Much later, Foreman and Woodin proved that (assuming the consistency of very large cardinals) it is consistent that formula_26 holds for every infinite cardinal formula_27 Later Woodin extended this by showing the consistency of formula_28 for every formula_16. showed that, for each \"n\" ≥ 1, it is consistent with ZFC that for each κ, 2 is the \"n\"th successor of κ. On the other hand, proved, that if γ is an ordinal and for each infinite cardinal κ, 2 is the γth successor of κ, then γ is finite.\n\nFor any infinite sets A and B, if there is an injection from A to B then there is an injection from subsets of A to subsets of B. Thus for any infinite cardinals A and B,\nIf A and B are finite, the stronger inequality\nholds. GCH implies that this strict, stronger inequality holds for infinite cardinals as well as finite cardinals.\n\nAlthough the generalized continuum hypothesis refers directly only to cardinal exponentiation with 2 as the base, one can deduce from it the values of cardinal exponentiation in all cases. It implies that formula_32 is (see: Hayden & Kennison (1968), page 147, exercise 76):\n\n\n\n", "id": "5705", "title": "Continuum hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=5706", "text": "Çevik Bir\n\nÇevik Bir (born 1939) is a retired Turkish army general. He was a member of the Turkish General Staff in the 1990s. He took a major part in several important international missions in the Middle East and North Africa. He was born in Buca, Izmir Province, in 1939 and is married with one child.\n\nHe graduated from the Turkish Military Academy as an engineer officer in 1958, from the Army Staff College in 1970 and from the Armed Forces College in 1971. He graduated from NATO Defense College, Rome, Italy in 1973.\n\nFrom 1973 to 1985, he served at SHAPE, NATO's headquarters in Belgium. He was promoted to brigadier general and commanded an armed brigade and division in Turkey. From 1987 to 1991, he served as major general, and then was promoted to lieutenant general.\n\nAfter the dictator Siad Barre’s ousting, conflicts between the General Farah Aidid's party and other clans in Somalia had led to famine and lawlessness throughout the country. An estimated 300,000 people had died from starvation. A combined military force of United States and United Nations (under the name \"UNOSOM\") were deployed to Mogadishu, to monitor the ceasefire and deliver food and supplies to the starving people of Somali. Çevik Bir, who was then a lieutenant-general of Turkey, became the force commander of UNOSOM II in 1993. Despite the retreat of US and UN forces after several deaths due to local hostilities mainly led by Aidid, the introduction of a powerful military force opened the transportation routes, enabling the provision of supplies and ended the famine quickly.\n\nHe became a four-star general and served three years as vice chairman of the Turkish Armed Forces, then appointed commander of the Turkish First Army, in Istanbul. While he was vice chairman of the TAF, he signed the Turkish-Israeli Military Coordination agreement in 1996.\n\nÇevik Bir became the Turkish army's deputy chief of general staff shortly after the Somali operation and played a vital role in establishing a Turkish-Israeli entente against the emerging fundamentalism in the Middle East.\nÇevik Bir retired from the army on August 30, 1999. He is a former member of the Association for the Study of the Middle East and Africa (ASMEA).\n\nOn April 12, 2012, Bir and 30 other officers were taken in custody for their role in the 1997 military memorandum that forced the then Turkish government, led by the Refah Partisi (Welfare Party), to step down.\n\n\n \n", "id": "5706", "title": "Çevik Bir"}
{"url": "https://en.wikipedia.org/wiki?curid=5708", "text": "Collectivism\n\nCollectivism is the moral stance, political philosophy, ideology, or social outlook that emphasizes the group and its interests. Collectivism is the opposite of individualism. Collectivists focus on communal, societal, or national interests in various types of political, economic and educational systems.\n\nCollectivism has been characterized as \"horizontal collectivism\", wherein equality is emphasized and people engage in sharing and cooperation, or \"vertical collectivism\", wherein hierarchy is emphasized and people submit to specific authorities. Horizontal collectivism is based on the assumption that each individual is more or less equal, while vertical collectivism assumes that individuals are fundamentally different from each other. Social anarchist Alexander Berkman, who was a horizontal collectivist, argued that equality does not imply a lack of unique individuality, but an equal amount of freedom and equal opportunity to develop one's own skills and talents.\n\nHorizontal collectivists tend to favor democratic decision-making, while vertical collectivists believe in a more strict chain of command. Horizontal collectivism stresses common goals, interdependence and sociability. Vertical collectivism stresses the integrity of the in-group (e.g. the family or the nation, for example), expects individuals to sacrifice themselves for the in-group if necessary, and promotes competition between different in-groups.\n\nCollectivism is often portrayed as the polar opposite of individualism, the economic, political, social or cultural autonomy of the individual within society; but given the different interpretations of individualism, from egocentric perspectives to more integrative ones, this apparent opposition is not necessarily true. For example, worker cooperatives operate on a collective basis but require the direct input of each individual member. While the ideas of holism posit that a sum is greater than its parts, this does not necessarily imply that a collectivity is greater or more powerful than the individuals that make it up, but instead that the collective energies of all individuals involved produce something that goes beyond each person (whereas, in authoritarian collectivities, power accrues to a person or group who is supposed to embody the collective). Theoretically, collectivism goes beyond considering the individual as the prime mover of society, but instead considers the numerous associations individuals voluntarily form as society's basis. In doing so it recognizes society as a \"collection\" of individuals and so remains with the understanding that any collective organization is fundamentally composed of individuals.\n\nDepending on how conscious a collectivity is of this reality determines how genuinely it maintains respect for individuality. On the other hand, individualism which encourages individuality at the expense of others cannot be considered collectivist, nor even individualist, since individualism is not the same as egotism.\n\nResearch has proposed that collectivism and individualism came about through an evolutionary adaptation, which resulted from the need to be protected from parasites. This theory is commonly known as the parasite-stress theory, whereby different pathogens in different geographical locations will lead to residents of those locations having an immune-system specifically designed to protect against diseases in those particular areas.\n\nSome societies are on the whole more collectivist and some on the whole more individualist. In collectivist societies, the group is considered more important than any one individual and groups in such societies are expected to \"take care\" of their members and individuals are expected to \"take care\" of the group (usually called an \"in-group\") that they are a member of. Harmony within these groups is considered paramount. For example, it may be considered \"inappropriate\" for a member of an in-group to openly criticize another in public (though they are often allowed to do so in private).\n\nThere are two main objections to collectivism from the ideas of individualism. One is that collectivism stifles individuality and diversity by insisting upon a common social identity, such as nationalism or some other group focus. The other is that collectivism is linked to statism and the diminution of freedom when political authority is used to advance collectivist goals.\n\nCriticism of collectivism comes from liberal individualists, such as classical liberals, libertarians, Objectivists, and individualist anarchists. A notable modern criticism of economic collectivism is the one put forward by Friedrich Hayek in his book \"The Road to Serfdom\", published in 1944.\n\nAustrian School economist Ludwig von Mises wrote on collectivism:\nOn the other hand the application of the basic ideas of collectivism cannot result in anything but social disintegration and the perpetuation of armed conflict. It is true that every variety of collectivism promises eternal peace starting with the day of its own decisive victory and the final overthrow and extermination of all other ideologies and their supporters. ... As soon as a faction has succeeded in winning the support of the majority of citizens and thereby attained control of the government machine, it is free to deny to the minority all those democratic rights by means of which it itself has previously carried on its own struggle for supremacy.\n\nMany socialists, particularly libertarian socialists, individualist anarchists, and De Leonists criticise the concept of collectivism. Some anti-collectivists often argue that all authoritarian and totalitarian societies are (vertically) collectivist in nature. Socialists argue that modern capitalism and private property, which is based on joint-stock or corporate ownership structures, is a form of organic collectivism that sharply contrasts with the perception that capitalism is a system of free individuals exchanging commodities. Socialists sometimes argue that true individualism can only exist when individuals are free from coercive social structures to pursue their own interests, which can only be accomplished by common ownership of socialized, productive assets and free access to the means of life so that no individual has coercive power over other individuals.\n\nGeorge Orwell, a dedicated democratic socialist, believed that collectivism resulted in the empowerment of a minority of individuals that led to further oppression of the majority of the population in the name of some ideal such as freedom.\n\nIt cannot be said too often – at any rate, it is not being said nearly often enough – that collectivism is not inherently democratic, but, on the contrary, gives to a tyrannical minority such powers as the Spanish Inquisitors never dreamt of.\nYet in the subsequent sentence he also warns of the tyranny of private ownership over the means of production:\n... that a return to 'free' competition means for the great mass of people a tyranny probably worse, because more irresponsible, than that of the state.\nMarxists criticize this use of the term \"collectivism,\" on the grounds that all societies are based on class interests and therefore all societies could be considered \"collectivist.\" The liberal ideal of the free individual is seen from a Marxist perspective as a smokescreen for the collective interests of the capitalist class. Social anarchists argue that \"individualism\" is a front for the interests of the upper class. As anarchist Emma Goldman wrote:\n'rugged individualism'... is only a masked attempt to repress and defeat the individual and his individuality. So-called Individualism is the social and economic laissez-faire: the exploitation of the masses by the [ruling] classes by means of legal trickery, spiritual debasement and systematic indoctrination of the servile spirit ... That corrupt and perverse 'individualism' is the straitjacket of individuality. ... [It] has inevitably resulted in the greatest modern slavery, the crassest class distinctions driving millions to the breadline. 'Rugged individualism' has meant all the 'individualism' for the masters, while the people are regimented into a slave caste to serve a handful of self-seeking 'supermen.' ... Their 'rugged individualism' is simply one of the many pretenses the ruling class makes to mask unbridled business and political extortion.\n\nIn response to criticism made by various pro-capitalist groups that claim that public ownership or common ownership of the means of production is a form of collectivism, socialists maintain that common ownership over productive assets does not infringe upon the individual, but is instead a liberating force that transcends the false dichotomy of individualism and collectivism. Socialists maintain that these critiques conflate the concept of private property in the means of production with personal possessions and individual production.\n\nAyn Rand, creator of the philosophy of Objectivism and a particularly vocal opponent of collectivism, argued that it led to totalitarianism. She argued that \"collectivism means the subjugation of the individual to a group,\" and that \"throughout history, no tyrant ever rose to power except on the claim of representing \"the common good\".\" She further claimed that \"horrors which no man would dare consider for his own selfish sake are perpetrated with a clear conscience by \"altruists\" who justify themselves by the common good.\" (The \"altruists\" Rand refers to are not those who practice simple benevolence or charity, but rather those who believe in Auguste Comte's ethical doctrine of altruism which holds that there is \"a moral and political obligation of the individual to sacrifice his own interests for the sake of a greater social good.\").\n", "id": "5708", "title": "Collectivism"}
{"url": "https://en.wikipedia.org/wiki?curid=5711", "text": "Nepeta\n\nNepeta is a genus of flowering plants in the family Lamiaceae also known as catmints. The genus name is reportedly in reference to Nepete, an ancient Etruscan city. There are about 250 species.\n\nThe genus is native to Europe, Asia, and Africa, and has also naturalized in North America.\n\nSome members of this group are known as catnip or catmint because of their effect on house cats – the nepetalactone contained in some \"Nepeta\" species binds to the olfactory receptors of cats, typically resulting in temporary euphoria.\n\nMost of the species are herbaceous perennial plants, but some are annuals. They have sturdy stems with opposite heart-shaped, green to gray-green leaves. \"Nepeta\" plants are usually aromatic in foliage and flowers.\n\nThe tubular flowers can be lavender, blue, white, pink, or lilac, and spotted with tiny lavender-purple dots. The flowers are located in verticillasters grouped on spikes; or the verticillasters are arranged in opposite cymes, racemes, or panicles – toward the tip of the stems.\n\nThe calyx is tubular or campanulate, they are slightly curved or straight, and the limbs are often 2-lipped with five teeth. The lower lip is larger, with 3-lobes, and the middle lobe is the largest. The flowers have 4 hairless stamens that are nearly parallel, and they ascend under the upper lip of the corolla. Two stamen are longer and stamens of pistillate flowers are rudimentary. The style protrudes outside of the mouth of the flowers.\n\nThe fruits are nutlets, which are oblong-ovoid, ellipsoid, ovoid, or obovoid in shape. The surfaces of the nutlets can be slightly ribbed, smooth or warty.\n\n\nSpecies include:\n\nSome \"Nepeta\" species are cultivated as ornamental plants. They can be drought tolerant – water conserving, often deer repellent, with long bloom periods from late spring to autumn. Some species also have repellent properties to insect pests, including aphids and squash bugs, when planted in a garden.\n\n\"Nepeta\" species are used as food plants by the larvae of some Lepidoptera (butterfly and moth) species including \"Coleophora albitarsella\", and as nectar sources for pollinators, such as honeybees and hummingbirds.\n\n\n\n", "id": "5711", "title": "Nepeta"}
{"url": "https://en.wikipedia.org/wiki?curid=5714", "text": "Cornish Nationalist Party\n\nThe Cornish Nationalist Party (CNP), , is a political party, founded by Dr James Whetter, who campaigned for independence for Cornwall. It was formed by people who left Cornwall's main nationalist party Mebyon Kernow on 28 May 1975, but it is no longer for independence.\n\nA separate party with a similar name (Cornish National Party) existed from 1969.\n\nThe split with Mebyon Kernow was based on the same debate that was occurring in most of the other political parties campaigning for autonomy from the United Kingdom at the time (such as the Scottish National Party and Plaid Cymru): whether to be a centre-left party, appealing to the electorate on a social democratic line, or whether to appeal emotionally on a centre-right cultural line. Originally, another subject of the split was whether to embrace devolution as a first step to full independence (or as the sole step if this was what the electorate wished) or for it to be \"all or nothing\".\n\nThe CNP essentially represented a more right-wing outlook from those who disagree that economic arguments were more likely to win votes than cultural. The CNP worked to preserve the identity of Cornwall and improve its economy, and encouraged links with Cornish people overseas and with other regions with distinct identities. It also gave support to the Cornish language and commemorated Thomas Flamank, a leader of the Cornish Rebellion in 1497, at an annual ceremony at Bodmin on 27 June each year.\n\nWhile the CNP is not a racist organisation, there was a perceived image problem from the similarly-styled BNP and NF (the nativist British National Party and National Front). The CNP was for some time seen as more of a pressure group, as it did not put up candidates for any elections, although its visibility and influence within Cornwall is negligible. , it is now registered on the UK political parties register, and so Mebyon Kernow is no longer the only registered political party based in Cornwall. In April 2009, a news story reported that the CNP had re-formed following a conference in Bodmin; however, it did not contest any elections that year.\n\nWhetter and the CNP still publish a quarterly journal, \"The Cornish Banner\" (\"An Baner Kernewek\"), within the actions of the Roseland Institute.\n\nA newspaper article and a revamp of the party website in October 2014 state that the party is now to contest elections once more.\nJohn Le Bretton, vice-chairman of the party, said: \"The CNP supports the retention of Cornwall council as a Cornwall-wide authority running Cornish affairs and we call for the British government in Westminster to devolve powers to the council so that decisions affecting Cornwall can be made in Cornwall\".\n\nThe party's policies include the following:\n\n\n\n", "id": "5714", "title": "Cornish Nationalist Party"}
{"url": "https://en.wikipedia.org/wiki?curid=5715", "text": "Cryptanalysis\n\nCryptanalysis (from the Greek \"kryptós\", \"hidden\", and \"analýein\", \"to loosen\" or \"to untie\") is the study of analyzing information systems in order to study the hidden aspects of the systems. Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.\n\nIn addition to mathematical analysis of cryptographic algorithms, cryptanalysis includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.\n\nEven though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.\n\nGiven some encrypted data (\"\"ciphertext\"\"), the goal of the \"cryptanalyst\" is to gain as much information as possible about the original, unencrypted data (\"\"plaintext\"\").\n\nAttacks can be classified based on what type of information the attacker has available. As a basic starting point it is normally assumed that, for the purposes of analysis, the general algorithm is known; this is Shannon's Maxim \"the enemy knows the system\"—in its turn, equivalent to Kerckhoffs' principle. This is a reasonable assumption in practice — throughout history, there are countless examples of secret algorithms falling into wider knowledge, variously through espionage, betrayal and reverse engineering. (And on occasion, ciphers have been reconstructed through pure deduction; for example, the German Lorenz cipher and the Japanese Purple code, and a variety of classical schemes):\n\nAttacks can also be characterised by the resources they require. Those resources include:\n\nIt's sometimes difficult to predict these quantities precisely, especially when the attack isn't practical to actually implement for testing. But academic cryptanalysts tend to provide at least the estimated \"order of magnitude\" of their attacks' difficulty, saying, for example, \"SHA-1 collisions now 2.\"\n\nBruce Schneier notes that even computationally impractical attacks can be considered breaks: \"Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2 encryptions; an attack requiring 2 encryptions would be considered a break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised.\"\n\nThe results of cryptanalysis can also vary in usefulness. For example, cryptographer Lars Knudsen (1998) classified various types of attack on block ciphers according to the amount and quality of secret information that was discovered:\n\nAcademic attacks are often against weakened versions of a cryptosystem, such as a block cipher or hash function with some rounds removed. Many, but not all, attacks become exponentially more difficult to execute as rounds are added to a cryptosystem, so it's possible for the full cryptosystem to be strong even though reduced-round variants are weak. Nonetheless, partial breaks that come close to breaking the original cryptosystem may mean that a full break will follow; the successful attacks on DES, MD5, and SHA-1 were all preceded by attacks on weakened versions.\n\nIn academic cryptography, a \"weakness\" or a \"break\" in a scheme is usually defined quite conservatively: it might require impractical amounts of time, memory, or known plaintexts. It also might require the attacker be able to do things many real-world attackers can't: for example, the attacker may need to choose particular plaintexts to be encrypted or even to ask for plaintexts to be encrypted using several keys related to the secret key. Furthermore, it might only reveal a small amount of information, enough to prove the cryptosystem imperfect but too little to be useful to real-world attackers. Finally, an attack might only apply to a weakened version of cryptographic tools, like a reduced-round block cipher, as a step towards breaking of the full system.\n\nCryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography—new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.\n\nSuccessful cryptanalysis has undoubtedly influenced history; the ability to read the presumed-secret thoughts and plans of others can be a decisive advantage. For example, in England in 1587, Mary, Queen of Scots was tried and executed for treason as a result of her involvement in three plots to assassinate Elizabeth I of England. The plans came to light after her coded correspondence with fellow conspirators was deciphered by Thomas Phelippes.\n\nIn World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers — including the Enigma machine and the Lorenz cipher — and Japanese ciphers, particularly 'Purple' and JN-25. 'Ultra' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by 'Magic' intelligence.\n\nGovernments have long recognized the potential benefits of cryptanalysis for intelligence, both military and diplomatic, and established dedicated organizations devoted to breaking the codes and ciphers of other nations, for example, GCHQ and the NSA, organizations which are still very active today. In 2004, it was reported that the United States had broken Iranian ciphers. (It is unknown, however, whether this was pure cryptanalysis, or whether other factors were involved:).\n\nAlthough the actual word \"\"cryptanalysis\"\" is relatively recent (it was coined by William Friedman in 1920), methods for breaking codes and ciphers are much older. The first known recorded explanation of cryptanalysis was given by 9th-century Arabian polymath, Al-Kindi (also known as \"Alkindus\" in Europe), in \"A Manuscript on Deciphering Cryptographic Messages\". This treatise includes a description of the method of frequency analysis (Ibrahim Al-Kadi, 1992- ref-3). Italian scholar Giambattista della Porta was author of a seminal work on cryptanalysis \"\"De Furtivis Literarum Notis\".\"\n\nFrequency analysis is the basic tool for breaking most classical ciphers. In natural languages, certain letters of the alphabet appear more often than others; in English, \"E\" is likely to be the most common letter in any sample of plaintext. Similarly, the digraph \"TH\" is the most likely pair of letters in English, and so on. Frequency analysis relies on a cipher failing to hide these statistics. For example, in a simple substitution cipher (where each letter is simply replaced with another), the most frequent letter in the ciphertext would be a likely candidate for \"E\". Frequency analysis of such a cipher is therefore relatively easy, provided that the ciphertext is long enough to give a reasonably representative count of the letters of the alphabet that it contains.\n\nIn Europe during the 15th and 16th centuries, the idea of a polyalphabetic substitution cipher was developed, among others by the French diplomat Blaise de Vigenère (1523–96). For some three centuries, the Vigenère cipher, which uses a repeating key to select different encryption alphabets in rotation, was considered to be completely secure (\"le chiffre indéchiffrable\"—\"the indecipherable cipher\"). Nevertheless, Charles Babbage (1791–1871) and later, independently, Friedrich Kasiski (1805–81) succeeded in breaking this cipher. During World War I, inventors in several countries developed rotor cipher machines such as Arthur Scherbius' Enigma, in an attempt to minimise the repetition that had been exploited to break the Vigenère system.\n\nCryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war's end as describing Ultra intelligence as having been \"decisive\" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war \"by not less than two years and probably by four years\"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.\n\nIn practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers — the first electronic digital computers to be controlled by a program.\n\nWith reciprocal machine ciphers such as the Lorenz cipher and the Enigma machine used by Nazi Germany during World War II, each message had its own key. Usually, the transmitting operator informed the receiving operator of this message key by transmitting some plaintext and/or ciphertext before the enciphered message. This is termed the \"indicator\", as it indicates to the receiving operator how to set his machine to decipher the message.\n\nPoorly designed and implemented indicator systems allowed first the Poles and then the British at Bletchley Park to break the Enigma cipher system. Similar poor indicator systems allowed the British to identify \"depths\" that led to the diagnosis of the Lorenz SZ40/42 cipher system, and the comprehensive breaking of its messages without the cryptanalysts seeing the cipher machine.\n\nSending two or more messages with the same key is an insecure process. To a cryptanalyst the messages are then said to be \"\"in depth.\"\" This may be detected by the messages having the same \"indicator\" by which the sending operator informs the receiving operator about the key generator initial settings for the message.\n\nGenerally, the cryptanalyst may benefit from lining up identical enciphering operations among a set of messages. For example, the Vernam cipher enciphers by bit-for-bit combining plaintext with a long key using the \"exclusive or\" operator, which is also known as \"modulo-2 addition\" (symbolized by ⊕ ):\nDeciphering combines the same key bits with the ciphertext to reconstruct the plaintext:\n(In modulo-2 arithmetic, addition is the same as subtraction.) When two such ciphertexts are aligned in depth, combining them eliminates the common key, leaving just a combination of the two plaintexts:\nThe individual plaintexts can then be worked out linguistically by trying \"probable words\" (or phrases), also known as \"\"cribs,\"\" at various locations; a correct guess, when combined with the merged plaintext stream, produces intelligible text from the other plaintext component:\nThe recovered fragment of the second plaintext can often be extended in one or both directions, and the extra characters can be combined with the merged plaintext stream to extend the first plaintext. Working back and forth between the two plaintexts, using the intelligibility criterion to check guesses, the analyst may recover much or all of the original plaintexts. (With only two plaintexts in depth, the analyst may not know which one corresponds to which ciphertext, but in practice this is not a large problem.) When a recovered plaintext is then combined with its ciphertext, the key is revealed:\nKnowledge of a key of course allows the analyst to read other messages encrypted with the same key, and knowledge of a set of related keys may allow cryptanalysts to diagnose the system used for constructing them.\n\nEven though computation was used to great effect in Cryptanalysis of the Lorenz cipher and other systems during World War II, it also made possible new methods of cryptography orders of magnitude more complex than ever before. Taken as a whole, modern cryptography has become much more impervious to cryptanalysis than the pen-and-paper systems of the past, and now seems to have the upper hand against pure cryptanalysis. The historian David Kahn notes:\n\nKahn goes on to mention increased opportunities for interception, bugging, side channel attacks, and quantum computers as replacements for the traditional means of cryptanalysis. In 2010, former NSA technical director Brian Snow said that both academic and government cryptographers are \"moving very slowly forward in a mature field.\"\n\nHowever, any postmortems for cryptanalysis may be premature. While the effectiveness of cryptanalytic methods employed by intelligence agencies remains unknown, many serious attacks against both academic and practical cryptographic primitives have been published in the modern era of computer cryptography:\n\n\nThus, while the best modern ciphers may be far more resistant to cryptanalysis than the Enigma, cryptanalysis and the broader field of information security remain quite active.\n\n\nAsymmetric cryptography (or public key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on \"hard\" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.\n\nAsymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie-Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA's security depends (in part) upon the difficulty of integer factorization — a breakthrough in factoring would impact the security of RSA.\n\nIn 1980, one could factor a difficult 50-digit number at an expense of 10 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 10 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore's law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.\n\nAnother distinguishing feature of asymmetric schemes is that, unlike attacks on symmetric cryptosystems, any cryptanalysis has the opportunity to make use of knowledge gained from the public key.\n\n\n\nQuantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.\n\nBy using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.\n\n\n\n\n\n", "id": "5715", "title": "Cryptanalysis"}
