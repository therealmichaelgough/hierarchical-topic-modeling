{"url": "https://en.wikipedia.org/wiki?curid=9926", "text": "ETA (separatist group)\n\nETA (, ), an acronym for Euskadi Ta Askatasuna (; \"Basque Homeland and Liberty\"), is a formerly armed leftist Basque nationalist and separatist organization in the Basque Country (in northern Spain and southwestern France). The group was founded in 1959 and later evolved from a group promoting traditional Basque culture to a paramilitary group engaged in a violent campaign of bombing, assassinations and kidnappings in the Southern Basque Country and throughout Spanish territory. Its goal was gaining independence for the Basque Country. ETA is the main group within the Basque National Liberation Movement and is the most important Basque participant in the Basque conflict.\n\nSince 1968, it has killed over 820 people (including 340 civilians) and injured thousands more. ETA is proscribed as a terrorist group by Spain, France, the United Kingdom, the United States, and the European Union. This convention is followed by a plurality of domestic and international media, which also refer to the group as \"terrorists\". There are more than 300 imprisoned members of the group in Spain, France, and other countries.\n\nETA declared ceasefires in 1989, 1996, 1998 and 2006. On 5 September 2010, ETA declared a new ceasefire that is still in force, and on 20 October 2011, ETA announced a \"definitive cessation of its armed activity\". On 24 November 2012, it was reported that the group was ready to negotiate a \"definitive end\" to its operations and disband completely. The group announced on 7 April 2017 that it had given up all its weapons and explosives and would be officially a disarmed organization as of the following day.\n\nETA's motto is (\"Keep up on both\"), referring to the two figures in its symbol, a snake (representing politics) wrapped around an axe (representing armed struggle).\n\nETA has changed its internal structure on several occasions, commonly for security reasons. The group used to have a very hierarchical organization with a leading figure at the top, delegating into three substructures: the logistical, military and political sections. Reports from Spanish and French police point towards significant changes in ETA's structures in recent years. ETA has divided the three substructures into a total of eleven. The change was a response to recent captures, and possible infiltration, by the different law enforcement agencies. ETA's intention is to disperse its members and reduce the impact of detentions.\n\nThe leading committee comprises 7 to 11 individuals, and ETA's internal documentation refers to it as Zuba, an abbreviation of \"Zuzendaritza Batzordea\" (directorial committee). There is another committee named \"Zuba-hitu\" that functions as an advisory committee. The eleven different substructures are: logistics, politics, international relations with fraternal organisations, military operations, reserves, prisoner support, expropriation, information, recruitment, negotiation, and treasury.\n\nETA's armed operations are organized in different \"taldes\" (\"groups\") or \"commandos\", generally composed of three to five members, whose objective is to conduct attacks in a specific geographic zone. The \"taldes\" are coordinated by the \"cúpula militar\" (\"military cupola\"). To supply the \"taldes\", support groups maintain safe houses and \"zulos\" (small rooms concealed in forests, garrets or underground, used to store arms, explosives or, sometimes, kidnapped people; the Basque word \"zulo\" literally means \"hole\"). The small cellars used to hide the people kidnapped are named by ETA and ETA's supporters \"people's jails\". Currently the most common \"commandos\" are itinerant, not linked to any specific area, and thus are more difficult to capture.\n\nAmong its members, ETA distinguishes between \"legales/legalak\" (\"legal ones\"), those members who do not have police records and live apparently normal lives; \"liberados\" (\"liberated\") members known to the police that are on ETA's payroll and working full-time for ETA; and \"apoyos\" (\"support\") who just give occasional help and logistics support to the group when required.\n\nThere are also the imprisoned members of the group, serving time scattered across Spain and France, that sometimes still have significant influence inside the organisation; and finally the \"quemados\" (\"burnt out\"), members freed after having been imprisoned or those that are suspected by the group of being under police vigilance. In the past there was also the figure of the deportees, expelled by the French government to remote countries where they live freely. France has since stopped the practice of deporting ETA members to other places than to Spain to be judged. ETA's internal bulletin is named \"Zutabe\" (\"Column\"), replacing the earlier one (1962) \"Zutik\" (\"Standing\").\n\nETA also promotes the \"kale borroka\" (\"street fight\"), that is, violent acts against public transportation, political parties offices or cultural buildings, destruction of private property of politicians, police, military, journalist, council members, and anyone voicing criticism against ETA, bank offices, menaces, graffiti of political mottoes, and general rioting, usually using Molotov cocktails. These groups are made up mostly of young people, who are directed through youth organisations (such as Jarrai, Haika and Segi). Many of the present-day members of ETA started their collaboration with the group as participants in the \"kale borroka\".\n\nThe former political party Batasuna, disbanded in 2003, pursued the same political goals as ETA and did not condemn ETA's use of violence. Formerly known as Euskal Herritarrok and \"Herri Batasuna\", it is presently banned by the Spanish Supreme Court as an anti-democratic organisation following the Political Parties Law (\"Ley de Partidos Políticos\"), It generally received 8% to 15% of the vote in the Basque Autonomous Community.\n\nBatasuna's political status is controversial. It was considered to be the political wing of ETA. Moreover, after the investigations on the nature of the relationship between Batasuna and ETA by Judge Baltasar Garzón, who suspended the activities of the political organisation and ordered police to shut down its headquarters, the Supreme Court of Spain finally declared Batasuna illegal on 18 March 2003. The court considered proven that Batasuna had links with ETA and that it constituted in fact part of ETA's structure. In 2003, the Constitutional Tribunal upheld the legality of the law.\n\nHowever, the party itself denies being the political wing of ETA, although double membership – simultaneous or alternative – between Batasuna and ETA is often recorded, such as with the cases of prominent Batasuna leaders like Josu Ternera, Arnaldo Otegi, Jon Salaberria and others.\n\nThe Spanish Cortes (the Spanish Parliament) began the process of declaring the party illegal in August 2002 by issuing a bill entitled the \"Ley de Partidos Políticos\" which bars political parties that use violence to achieve political goals, promotes hatred against different groups or seek to destroy the democratic system. The bill passed the Cortes with a 304 to 16 vote. Many within the Basque nationalistic movement strongly disputed the Law, which they consider too draconian or even unconstitutional; alleging that any party could be made illegal almost by choice, simply for not clearly stating their opposition to an attack.\n\nDefenders of the new law argue that the \"Ley de Partidos\" does not necessarily require responses to individual acts of violence, but rather a declaration of principles explicitly rejecting violence as a means of achieving political goals. Defenders also argue that the ban of a political party is subject to judicial process, with all the guarantees of the State of Law. Batasuna has failed to produce such a statement. other political parties linked to organizations such as \"Partido Comunista de España (reconstituido)\" have also been declared illegal, and Acción Nacionalista Vasca and Communist Party of the Basque Lands (EHAK/PCTV, \"Euskal Herrialdeetako Alderdi Komunista/Partido Comunista de las Tierras Vascas\") were declared illegal in September 2008.\n\nA new party called Aukera Guztiak \"(All the Options)\" was formed expressly for the elections to the Basque Parliament of April 2005. Its supporters claimed no heritage from Batasuna, asserting that their aim was to allow Basque citizens to freely express their political ideas, even those of independence. On the matter of political violence, Aukera Guztiak stated their right not to condemn some kinds of violence more than others if they did not see fit (in this regard, the Basque National Liberation Movement (MLNV) regards present police actions as violence, torture and state terrorism). Nevertheless, most of their members and certainly most of their leadership were former Batasuna supporters or affiliates. The Spanish Supreme Court unanimously considered the party to be a sequel to Batasuna and declared a ban on it.\n\nAfter Aukera Guztiak had been banned, and less than two weeks before the election, another political group appeared born from an earlier schism from Herri Batasuna, the Communist Party of the Basque Lands (EHAK/PCTV, \"Euskal Herrialdeetako Alderdi Komunista/Partido Comunista de las Tierras Vascas\"), a formerly unknown political party which had no representation in the Autonomous Basque Parliament. EHAK made the announcement that they would apply the votes they obtained to sustain the political programme of the now banned Aukera Guztiak platform.\n\nThis move left no time for the Spanish courts to investigate EHAK in compliance with the \"Ley de Partidos\" before the elections were held. The bulk of Batasuna supporters voted in this election for PCTV, a virtually unknown political formation until then. PCTV obtained 9 seats of 75 (12.44% of votes) at the Basque Parliament.\nThe election of EHAK representatives eventually allowed the programme of the now-illegal Batasuna to continue being represented without having condemned violence as required by the \"Ley de Partidos\".\n\nIn February 2011, Sortu, a party described as \"the new Batasuna\", was launched. Unlike predecessor parties, Sortu explicitly rejects politically motivated violence, including that of ETA. However, on 23 March 2011, the Spanish Supreme Court banned Sortu from registering as a political party on the grounds that it was linked to ETA.\n\nThe Spanish transition to democracy from 1975 on and ETA's progressive radicalisation have resulted in a steady loss of support, which became especially apparent at the time of their 1997 kidnapping and countdown assassination of Miguel Ángel Blanco. Their loss of sympathisers has been reflected in an erosion of support for the political parties identified with them. In the 1998 Basque parliament elections Euskal Herritarrok, formerly Batasuna, polled 17.7% of the votes. However, by 2001 the party's support had fallen to 10.0%. There were also concerns that Spain's \"judicial offensive\" against alleged ETA supporters (two Basque political parties and one NGO were banned in September 2008) constitute a threat to human rights. Strong evidence was seen that a legal network had grown so wide as to lead to the arrest of numerous innocent people. According to Amnesty International, torture was still \"persistent\", though not \"systematic.\" Inroads could be undermined by judicial short-cuts and abuses of human rights.\n\nThe Euskobarometro, the survey carried out by the Universidad del País Vasco (University of the Basque Country), asking about the views of ETA within the Basque population, obtained these results in May 2009: 64% rejected ETA totally, 13% identified themselves as former ETA sympathisers who no longer support the group. Another 10% agreed with ETA's ends, but not their means. 3% said that their attitude towards ETA was mainly one of fear, 3% expressed indifference and 3% were undecided or did not answer. About 3% gave ETA \"justified, with criticism\" support (supporting the group but criticising some of their actions) and only 1% gave ETA total support. Even within Batasuna voters, at least 48% rejected ETA's violence.\n\nA poll taken by the Basque Autonomous Government in December 2006 during ETA's \"permanent\" ceasefire showed that 88% of the Basques thought that it was necessary for all political parties to launch a dialogue, including a debate on the political framework for the Basque Country (86%). 69% support the idea of ratifying the results of this hypothetical multiparty dialogue through a referendum. This poll also reveals that the hope of a peaceful resolution to the issue of the constitutional status of the Basque region has fallen to 78% (from 90% in April).\n\nThese polls did not cover Navarre, where support for Basque nationalist electoral options is weaker (around 25% of population) or the Northern Basque Country where support is even weaker (around 15% of population).\n\nETA grew out of a student group called Ekin, founded in the early 1950s, which published a magazine and undertook direct action. ETA was founded on 31 July 1959 as Euskadi Ta Askatasuna (Basque Homeland and Freedom) by students frustrated by the moderate stance of the Basque Nationalist Party. (Originally, the name for the organisation used the word \"Aberri\" instead of \"Euskadi\", creating the acronym \"ATA\". However, in some Basque dialects, \"ata\" means \"duck\", so the name was changed.)\n\nETA held their first assembly in Bayonne, France, in 1962, during which a \"declaration of principles\" was formulated and following which a structure of activist cells was developed. Subsequently, Marxist and third-worldist perspectives developed within ETA, becoming the basis for a political programme set out in Federico Krutwig's 1963 book \"Vasconia\", which is considered to be the defining text of the movement. In contrast to previous Basque nationalist platforms, Krutwig's vision was anti-religious and based upon language and culture rather than race. ETA's third and fourth assemblies, held in 1964 and 1965, adopted an anti-capitalist and anti-imperialist position, seeing nationalism and the class struggle as intrinsically connected.\n\nSome sources attribute the 1960 bombing of the Amara station in Donostia-San Sebastian (which killed a 22-month-old child) to ETA, but statistics published by the Spanish Ministry of the Interior have always showed that ETA's first victim was killed in 1968. The 1960 attack was claimed by the Portuguese and Spanish left-wing group DRIL (together with four other very similar bombings committed that same day across Spain, all of them attributed to DRIL), and the attribution to ETA has been considered to be unfounded by the researchers. And police documents dating from 1961, released in 2013, show that the DRIL was indeed the author of the bombing.\n\nETA's first killing occurred on 7 June 1968, when Guardia Civil member José Pardines Arcay was shot dead after he tried to halt ETA member Txabi Etxebarrieta during a routine road check. Etxebarrieta was chased down and killed as he tried to flee. This led to retaliation in the form of the first planned ETA assassination: that of Melitón Manzanas, chief of the secret police in San Sebastián and associated with a long record of tortures inflicted on detainees in his custody. In December 1970, several members of ETA were condemned to death in the \"Proceso de Burgos\" (\"Burgos Trial\"), but international pressure resulted in their sentences being commuted (a process which, however, had by that time already been applied to some other members of ETA).\n\nIn early December 1970, ETA kidnapped the German consul in San Sebastian, Eugen Beilh, in order to exchange him for the Burgos defendants. He was released unharmed on 24 December.\n\nNationalists who refused to follow the tenets of Marxism–Leninism and who sought to create a united front appeared as ETA-V, but lacked the support to challenge ETA.\n\nThe most significant assassination performed by ETA during Franco's dictatorship was Operación Ogro, the December 1973 bomb assassination in Madrid of Admiral Luis Carrero Blanco, Franco's chosen successor and president of the government (a position roughly equivalent to being a prime minister). The assassination had been planned for months and was executed by placing a bomb in a tunnel dug below the street where Carrero Blanco's car passed every day. The bomb blew up beneath the politician's car and threw it five stories into the air and over the top of a nearby building onto a balcony in a nearby courtyard.\n\nFor some in the Spanish opposition, Carrero Blanco's assassination i.e. the elimination of Franco's chosen successor was an instrumental step for the subsequent establishment of democracy. The government responded with new anti-terrorist laws which gave police greater powers and empowered military tribunals to pass death sentences against those found guilty. However, the last use of capital punishment in Spain when two ETA members were executed in September 1975, eight weeks before Franco's death, sparked massive domestic and international protests against the Spanish government.\n\nDuring the Spanish transition to democracy which began following Franco's death, ETA split into two separate groups: ETA political-military or ETA(pm), and ETA military or ETA(m).\n\nBoth ETA(m) and ETA(pm) refused offers of amnesty, and instead pursued and intensified their violent struggle. The years 1978–80 were to prove ETA's most deadly, with 68, 76, and 98 fatalities, respectively.\n\nDuring the Franco dictatorship, ETA was able to take advantage of tolerance by the French government, which allowed its members to move freely through French territory, believing that in this manner they were contributing to the end of Franco's regime. There is much controversy over the degree to which this policy of \"sanctuary\" continued even after the transition to democracy, but it is generally agreed that currently the French authorities collaborate closely with the Spanish government against ETA.\n\nIn the 1980s, ETA(pm) accepted the Spanish government's offer of individual pardons to all ETA prisoners, even those who had committed violent crimes, who publicly abandoned the policy of violence. This caused a new division in ETA(pm) between the seventh and eighth assemblies. ETA VII accepted this partial amnesty granted by the now democratic Spanish government and integrated into the political party \"Euskadiko Ezkerra\" (\"Left of the Basque Country\").\n\nETA VIII, after a brief period of independent activity, eventually integrated into ETA(m). With no factions existing anymore, ETA(m) revamped the original name of Euskadi Ta Askatasuna.\n\nDuring the 1980s a \"dirty war\" ensued by means of the Grupos Antiterroristas de Liberación (GAL, \"Antiterrorist Liberation Groups\"), a paramilitary group which billed themselves as counter-terrorist, active between 1983 and 1987. The GAL committed assassinations, kidnappings and torture, not only of ETA members but of civilians supposedly related to those, some of whom turned out to have nothing to do with ETA. 27 people were killed by GAL. GAL activities were a follow-up of similar dirty war actions by death squads, actively supported by members of Spanish security forces and secret services, using names such as Batallón Vasco Español active from 1975 to 1981. They were responsible for the killing of about 48 people.\n\nOne consequence of GAL's activities in France was the decision in 1984 by interior minister Pierre Joxe to permit the extradition of ETA suspects to Spain. Reaching this decision had taken 25 years and was critical in curbing ETA's capabilities by denial of previously safe territory in France.\n\nThe airing of the state-sponsored \"dirty war\" scheme and the imprisonment of officials responsible for GAL in the early 1990s led to a political scandal in Spain. The group's connections with the state were unveiled by the Spanish journal El Mundo, with an investigative series leading to the GAL plot being discovered and a national trial initiated. As a consequence, the group's attacks since the revelation have generally been dubbed state terrorism.\n\nIn 1997 the Spanish Audiencia Nacional court finished its trial, which resulted in convictions and imprisonment of several individuals related to the GAL, including civil servants up to the highest levels of the Spanish Socialist Workers' Party (PSOE) government, such as former Homeland Minister José Barrionuevo. Premier Felipe González was quoted as saying that the constitutional state has to defend itself \"also in the sewers\" (\"El Estado de derecho también se defiende en las cloacas\") something which, for some, indicated at least his knowledge of the scheme. However, his involvement with the GAL could never be proven.\n\nThese events marked the end of the armed \"counter-terrorist\" period in Spain and no major cases of foul play on the part of the Spanish government after 1987 (when GAL ceased to operate) have been proven in courts.\n\nAccording to the radical nationalist group, Euskal Memoria, between 1960 and 2010 there were 465 deaths in the Basque Country due to (primarily Spanish) \"state violence\". This figure is considerably higher than those given elsewhere, which are usually between 250 and 300. Critics of ETA cite only 56 members of that organisation killed by state forces since 1975.\n\nETA members and supporters routinely claim torture at the hands of Spanish police forces. While these claims are hard to verify, some convictions are based on confessions while prisoners are held \"incomunicado\" and without access to a lawyer of their choice, for a maximum of five days. These confessions are routinely repudiated by the defendants during trials as having been extracted under torture. There have been some successful prosecutions of proven tortures during the \"dirty war\" period of the mid-1980s, although the penalties have been considered by Amnesty International as unjustifiably light and lenient with co-conspirators and enablers.\n\nIn this regard, Amnesty International has shown concern for the continuous disregard on the recommendations issued by the agency to prevent the alleged abuses to possibly take place. Also in this regard, ETA's manuals have been found instructing its members and supporters to claim routinely that they had been tortured while detained. Unai Romano's case has been very controversial. Pictures of him with a symmetrically swollen face of uncertain etiology were published after his incommunicado period leading to claims of police abuse and torture. Martxelo Otamendi, the ex-director of the Basque newspaper Euskaldunon Egunkaria, decided to bring charges in September 2008 against the Spanish Government in Strasbourg Court for \"not inspecting properly\" torture denounced cases.\n\nAs a result of ETA's violence, threats and killings of journalists, Reporters Without Borders has included Spain in all six editions of its annual watchlist on press freedom. Thus, this NGO has included ETA in its watchlist \"Predators of Press Freedom\".\n\nETA performed their first car bomb assassination in Madrid in September 1985, resulting in one death (American citizen Eugene Kent Brown, Johnson & Johnson employee) and sixteen injuries; the Plaza República Dominicana bombing in July 1986 killed 12 members of the Guardia Civil and injured 50; on 19 June 1987, the Hipercor bombing was an attack in a shopping center in Barcelona, killing twenty-one and injuring forty-five; in the last case, entire families were killed. The horror caused then was so striking that ETA felt compelled to issue a communiqué stating that they had given warning of the Hipercor bomb, but that the police had declined to evacuate the area. The police claim that the warning came only a few minutes before the bomb exploded.\n\nIn 1986 Gesto por la Paz (known in English as Association for Peace in the Basque Country) was founded; they began to convene silent demonstrations in communities throughout the Basque Country the day after any violent killing, whether by ETA or by GAL. These were the first systematic demonstrations in the Basque Country against political violence. Also in 1986, in Ordizia, ETA shot down María Dolores Katarain, known as \"Yoyes\", while she was walking with her infant son. Yoyes was a former member of ETA who had abandoned the armed struggle and rejoined civil society: they accused her of \"desertion\" because of her taking advantage of the Spanish reinsertion policy which granted amnesty to those prisoners who publicly refused political violence (see below).\n\nOn 12 January 1988, all Basque political parties except ETA-affiliated Herri Batasuna signed the Ajuria-Enea pact with the intent of ending ETA's violence. Weeks later on 28 January, ETA announced a 60-day \"ceasefire\", later prolonged several times. Negotiations known as the Mesa de Argel (\"Algiers Table\") took place between the ETA representative Eugenio Etxebeste (\"Antxon\"), and the then PSOE government of Spain but no successful conclusion was reached, and ETA eventually resumed the use of violence.\n\nDuring this period, the Spanish government had a policy referred to as \"reinsertion\", under which imprisoned ETA members whom the government believed had genuinely abandoned violence could be freed and allowed to rejoin society. Claiming a need to prevent ETA from coercively impeding this reinsertion, the PSOE government decided that imprisoned ETA members, who previously had all been imprisoned within the Basque Country, would instead be dispersed to prisons throughout Spain, some as far from their families as in the Salto del Negro prison in the Canary Islands. France has taken a similar approach.\n\nIn the event, the only clear effect of this policy was to incite social protest, especially from nationalists and families of the prisoners, claiming cruelty of separating family members from the insurgents. Much of the protest against this policy runs under the slogan \"\"Euskal Presoak – Euskal Herrira\"\" (\"Basque prisoners to the Basque Country\", by \"Basque prisoners\" only ETA members are meant). It has to be noted that almost in any Spanish jail there is a group of ETA prisoners, as the number of ETA prisoners makes it difficult to disperse them.\n\n\"Gestoras pro-Amnistía/Amnistiaren Aldeko Batzordeak\" (\"Pro-Amnesty Managing Assemblies\", currently illegal), later \"Askatasuna\" (\"Freedom\") and \"Senideak\" (\"The family members\") provided support for prisoners and families. The Basque Government and several Nationalist town halls granted money on humanitarian reasons for relatives to visit prisoners. The long road trips have caused accidental deaths that are protested against by Nationalist Prisoner's Family supporters.\n\nDuring the ETA ceasefire of the late 1990s, the PSOE government brought back to the mainland the prisoners on the islands and in Africa. Since the end of the ceasefire, ETA prisoners have not been sent back to overseas prisons. Some Basque authorities have established grants for the expenses of visiting families.\n\nAnother Spanish \"counter-terrorist\" law puts suspected terrorist cases under the central tribunal \"Audiencia Nacional\" in Madrid, due to the threats by the group over the Basque courts. Under Article 509 suspected terrorists are subject to being held \"incommunicado\" for up to thirteen days, during which they have no contact with the outside world other than through the court-appointed lawyer, including informing their family of their arrest, consultation with private lawyers or examination by a physician other than the coroners. In comparison, the habeas corpus term for other suspects is three days.\n\nIn 1992, ETA's three top leaders—\"military\" leader Francisco Mujika Garmendia (\"Pakito\"), political leader José Luis Alvarez Santacristina (\"Txelis\") and logistical leader José María Arregi Erostarbe (\"Fiti\"), often referred to collectively as the \"cúpula\" of ETA or as the Artapalo collective—were arrested in the northern Basque town of Bidart, which led to changes in ETA's leadership and direction.\n\nAfter a two-month truce, ETA adopted even more radical positions. The principal consequence of the change appears to have been the creation of the \"\"Y Groups\"\", formed by young militants of ETA parallel groups (generally minors), dedicated to so-called \"\"kale borroka\"\"—street struggle—and whose activities included burning buses, street lamps, benches, ATMs, garbage containers, and throwing Molotov cocktails. The appearance of these groups was attributed by many to the supposed weakness of ETA, which obliged them to resort to minors to maintain or augment their impact on society after arrests of leading militants, including the \"cupola\". ETA also began to menace leaders of other parties besides rival Basque nationalist parties.\n\nIn 1995, the armed group again launched a peace proposal. The so-called \"Democratic Alternative\" replaced the earlier KAS Alternative as a minimum proposal for the establishment of Euskal Herria. The Democratic Alternative offered the cessation of all armed ETA activity if the Spanish-government would recognize the Basque people as having sovereignty over Basque territory, the right to self-determination and that it freed all ETA members in prison. The Spanish government ultimately rejected this peace offer as it would go against the Spanish Constitution of 1978. Changing the constitution was not considered.\n\nAlso in 1995 came a failed ETA car bombing attempt directed against José María Aznar, a conservative politician who was leader of the then-opposition Partido Popular (PP) and was shortly after elected to the presidency of the government; there was also an abortive attempt in Majorca on the life of King Juan Carlos I. Still, the act with the largest social impact came the following year. 10 July 1997, PP council member Miguel Ángel Blanco was kidnapped in the Basque town of Ermua, with the separatist group threatening to assassinate him unless the Spanish government met ETA's demand of starting to bring all ETA's inmates to prisons of the Basque Country within two days after the kidnapping.\n\nThis demand was not met by the Spanish government and after three days Miguel Ángel Blanco was found shot dead when the deadline expired. More than six million people took out to the streets to demand his liberation, with massive demonstrations occurring as much in the Basque regions as elsewhere in Spain, chanting cries of \"Assassins\" and \"Basques yes, ETA no\". This response came to be known as the \"Spirit of Ermua\".\n\nLater came acts of violence such as the 6 November 2001 car bomb in Madrid which injured 65 people, and attacks on football stadiums and tourist destinations throughout Spain.\n\nThe 11 September 2001 attacks in the USA appeared to have dealt a hard blow to ETA, owing to the worldwide toughening of \"anti-terrorist\" measures (such as the freezing of bank accounts), the increase in international police coordination, and the end of the toleration some countries had, up until then, extended to ETA. In addition, in 2002 the Basque nationalist youth movement, \"Jarrai\", was outlawed and the law of parties was changed outlawing Herri Batasuna, the \"political arm\" of ETA (although even before the change in law, Batasuna had been largely paralysed and under judicial investigation by judge Baltasar Garzón).\n\nWith ever-increasing frequency, attempted ETA actions have been frustrated by Spanish security forces.\n\nOn 24 December 2003, in San Sebastián and in Hernani, National Police arrested two ETA members who had left dynamite in a railroad car prepared to explode in Chamartín Station in Madrid. On 1 March 2004, in a place between Alcalá de Henares and Madrid, a light truck with 536 kg of explosives was discovered by the Guardia Civil.\n\nETA was initially blamed for the 2004 Madrid bombings by the outgoing government and large sections of the press. However, the group denied responsibility and Islamic fundamentalists from Morocco were eventually convicted. The judicial investigation currently states that there is no relationship between ETA and the Madrid bombings.\n\nIn the context of negotiation with the Spanish government, ETA has declared what it has described as \"truce\" a number of times since its creation.\n\nOn 22 March 2006, ETA sent a DVD message to the Basque Network Euskal Irrati-Telebista and the journals \"Gara\" and Berria with a communiqué from the group announcing what it called a \"permanent ceasefire\" that was broadcast over Spanish TV.\n\nTalks with the group were then officially opened by Spanish \"Presidente del Gobierno\" José Luis Rodríguez Zapatero.\n\nThese took place all over 2006, not free from incidents such as an ETA cell stealing some 300 handguns, ammunition and spare parts in France in October 2006. or a series of warnings made by ETA such as the one of 23 September, when masked ETA militants declared that the group would \"keep taking up arms\" until achieving \"independence and socialism in the Basque country\", which were regarded by some as a way to increase pressure on the talks, by others as a tactic to reinforce ETA's position in the negotiations.\n\nFinally, on 30 December 2006 ETA detonated a van bomb after three confusing warning calls, in a parking building at the Madrid Barajas international airport. The explosion caused the collapse of the building and killed two Ecuadorian immigrants who were napping inside their cars in the parking building. At , José Luis Rodríguez Zapatero released a statement stating that the \"peace process\" had been discontinued.\n\nIn January 2008, ETA stated that its call for independence is similar to that of the Kosovo status and Scotland.\nIn the week of 8 September 2008, two Basque political parties were banned by a Spanish court for their secretive links to ETA. In another case in the same week, 21 people were convicted whose work on behalf of ETA prisoners actually belied secretive links to the armed separatists themselves.ETA reacted to these actions by placing three major car bombs in less than 24 hours in northern Spain.\n\nIn April 2009 Jurdan Martitegi was arrested, making him the fourth consecutive ETA military chief to be captured within a single year, an unprecedented police record further weakening the group.\nThe group, and therefore the violence, surged in the middle of 2009, with several ETA attacks leaving three people dead and dozens injured around Spain.\n\nThe Basque newspaper \"Gara\" published an article that suggested that ETA member Jon Anza could have been killed and buried by Spanish police in April 2009. The central prosecutor in the French town of Bayonne, Anne Kayanakis, announced, as the official version, that the autopsy carried out on the body of Jon Anza – a suspected member of the armed Basque group ETA, missing since April 2009 – revealed no signs of having been beaten, wounded or shot, which should rule out any suspicions that he died from unnatural causes. Nevertheless, that very magistrate denied the demand of the family asking for the presence of a family doctor during the autopsy. After this Jon Anza's family members asked for a second autopsy to be carried on.\n\nIn December 2009, Spain raised its terror alert after warning that ETA could be planning major attacks or high-profile kidnappings during Spain's European Union presidency. The next day, after being asked by the opposition, Alfredo Pérez Rubalcaba said that warning was part of a strategy.\n\nOn 5 September 2010, ETA declared a new ceasefire, its third, after two previous ceasefires were ended by the group. A spokesperson speaking on a video announcing the ceasefire said the group wished to use \"peaceful, democratic means\" to achieve its aims, though it was not specified whether the ceasefire was considered permanent by the group. ETA claimed that it had made the decision to initiate a ceasefire several months prior to the announcement. In part of the video, the spokesperson said that the group was \"prepared today as yesterday to agree to the minimum democratic conditions necessary to put in motion a democratic process, if the Spanish government is willing.\"\n\nThe announcement was met with a mixed reaction; Basque nationalist politicians responded positively, and said that the Spanish and international governments should do the same, while the Spanish interior counselor of Basque, Rodolfo Ares, said that the commitment did not go far enough. He said that he considered ETA's statement \"absolutely insufficient\" because it did not commit to a complete termination of what Ares considered \"terrorist activity\" by the group.\n\nOn 10 January 2011, ETA declared that their September 2010 ceasefire would be permanent and verifiable by international observers. Observers urged caution, pointing out that ETA had broken permanent ceasefires in the past, whereas Prime Minister José Luis Rodríguez Zapatero (who left office in December 2011) demanded that ETA declare that it had given up violence once and for all. After the declaration, Spanish press started speculating of a possible Real IRA-type split within ETA, with hardliners forming a new more violent offshoot led by \"Dienteputo\".\n\nOn 21 October 2011, ETA announced a cessation of armed activity via video clip sent to media outlets following the Donostia-San Sebastián International Peace Conference, which was attended by former UN Secretary-General Kofi Annan, former Taoiseach of Ireland Bertie Ahern, former Prime Minister of Norway Gro Harlem Brundtland (an international leader in sustainable development and public health), former Interior Minister of France Pierre Joxe, president of Sinn Féin Gerry Adams (a Teachta Dála in Dáil Éireann), and British diplomat Jonathan Powell, who served as the first Downing Street Chief of Staff.\n\nThey all signed a final declaration that was supported also by former UK Prime Minister Tony Blair, the former US President and 2002 Nobel Peace Prize winner Jimmy Carter, and the former US senator and former US Special Envoy for Middle East Peace George J. Mitchell. The meeting did not include Spanish or French government representatives.\n\nThe day after the ceasefire, in a contribution piece to the \"New York Times\", Tony Blair indicated that lessons in dealing with paramilitary separatist groups can be learned from the way in which the Spanish administration handled ETA. Blair wrote, “governments must firmly defend themselves, their principles and their people against terrorists. This requires good police and intelligence work as well as political determination. [However], firm security pressure on terrorists must be coupled with offering them a way out when they realize that they cannot win by violence. Terrorist groups are rarely defeated by military means alone”. Blair also suggested that Spain will need to discuss weapon decommissioning, peace strategies, reparations for victims, and security with ETA, as Britain has discussed with the Provisional IRA.\n\nETA has declared ceasefires many times before, most significantly in 1999 and 2006, but the Spanish government and media outlets expressed particularly hopeful opinions regarding the permanence of this proclamation. Spanish premier José Luis Rodríguez Zapatero described the move as \"a victory for democracy, law and reason\". Additionally, the effort of security and intelligence forces in Spain and France are cited by politicians as the primary instruments responsible for the weakening of ETA. The optimism may come as a surprise considering ETA’s failure to renounce the independence movement, which has been one of the Spanish government’s requirements. ETA’s ceasefire video ended with the assertion that the struggle for the Basque homeland continues.\n\nLess optimistically, Spanish Prime Minister Mariano Rajoy of the center-right People's Party expressed the need to push for the full dissolution of ETA. The People's Party has emphasized the obligation of the state to refuse negotiations with separatist movements since former Prime Minister José María Aznar was in office. Aznar was responsible for banning media outlets seen as subversive to the state and Batasuna, the political party of ETA. Additionally, in preparation for his party’s manifesto, on 30 October 2011, Rajoy declared that the People's Party would not negotiate with ETA under threats of violence nor announcements of the group’s termination, but would instead focus party efforts on remembering and honoring victims of separatist violence.\n\nWhile ETA pledged to refrain from a violent separatist movement, the separatist movement was not denounced. The ETA announcement reinforced the struggle for the Basque homeland, but through the use of democratic means. This event may not alter the goals of the Basque separatist movement, but will change the method of the fight for a more autonomous state. Negotiations with the newly elected administration may prove difficult with the return to the center-right People's Party, which is replacing Socialist control, due to pressure from within the party to refuse all ETA negotiations.\n\nIn September 2016, French police stated that they did not believe ETA had made progress in giving up arms. In March 2017, well-known French-Basque activist was quoted as having told \"Le Monde\", \"ETA has made us responsible for the disarmament of its arsenal, and by the afternoon of 8 April, ETA will be completely unarmed.\" On 7 April, the BBC reported that ETA would disarm \"tomorrow\", including a photo of a stamped ETA letter attesting to this. The French police found 3.5 tonnes of weapons on 8 April, the following day, at the caches handed over by ETA.\n\nETA's targets have expanded from the former military or police-related personnel and their families, to a wider array, which today includes the following:\n\n\n\nETA's tactics have included:\n\nThese bombs have sometimes killed family members of ETA's target victim and bystanders. When the bombs are large car-bombs seeking to produce large damage and terror, they were generally announced by one or more telephone calls made to newspapers speaking in the name of ETA. Charities (usually Detente Y Ayuda—DYA) have also been used to announce the threat if the bomb is in a populated area. The type of explosives used in these attacks were initially Goma-2 or self-produced ammonal. After a number of successful robberies in France, ETA began using Titadyne.\n\nWith its attacks against what they consider \"enemies of the Basque people\", ETA has killed over 820 people since 1968 to date, including more than 340 civilians. It has maimed hundreds more and kidnapped dozens.\n\nIts ability to inflict violence has declined steadily since the group was at its strongest during the late 1970s and 1980 (when it killed 92 people in a single year). After decreasing peaks in the fatal casualties in 1987 and 1991, 2000 remains to date as the last year when ETA killed more than 20 in a single year. Since 2002 to date, the yearly number of ETA's fatal casualties has been reduced to single digits.\n\nSimilarly, over the 1990s and, especially, during the 2000s (decade), fluid cooperation between the French and Spanish police, state-of-the-art tracking devices and techniques and, apparently, police infiltration have allowed increasingly repeating blows to ETA's leadership and structure (between May 2008 and April 2009 no less than four consecutive \"military chiefs\" were arrested).\n\nETA operates mainly in Spain, particularly in the Basque Country, Navarre, and (to a lesser degree) Madrid, Barcelona, and the tourist areas of the Spanish Mediterranean coast. To date, about 65% of ETA's killings have been committed in the Basque Country, followed by Madrid with roughly 15%. Navarre and Catalonia also register significant numbers.\n\nActions in France usually consist of assaults on arsenals or military industries in order to steal weapons or explosives; these are usually stored in large quantities in hide-outs located in the French Basque Country rather than Spain. The French judge Laurence Le Vert has been threatened by ETA and a plot arguably aiming to assassinate her was unveiled. Only very rarely have ETA members engaged in shootings with the French Gendarmerie. This has often occurred mainly when members of the group were confronted at checkpoints.\n\nIn spite of this, ETA killed in France on 1 December 2007, two Spanish Civil Guards on counter-terrorist surveillance duties in Capbreton, Landes. This has been its first killing after it ended its 2006 declaration of \"permanent ceasefire\" and the first killing committed by ETA in France of a Spanish police agent ever since 1976, when they kidnapped, tortured and assassinated two Spanish inspectors in Hendaye.\n\nMore recently, 2007 police reports point out that, after the serious blows suffered by ETA and its political counterparts during the 2000s (decade), its budget would have been adjusted to 2,000,000 euros annually.\n\nAlthough ETA used robbery as a means of financing its activities in its early days, it has since been accused both of arms trafficking and of benefiting economically from its political counterpart Batasuna. Extortion remains ETA's main source of funds.\n\nETA is considered to form part of what is informally known as the Basque National Liberation Movement, a movement born much after ETA's creation. This loose term refers to a range of political organizations that are ideologically akin, comprising several distinct organizations that promote a type of leftist Basque nationalism that is often referred to by the Basque-language term \"Ezker Abertzalea\" (Nationalist Left). Other groups typically considered to belong to this independentist movement are: the political party Batasuna, the nationalist youth organization Segi, the labour union Langile Abertzaleen Batzordeak (LAB), and Askatasuna among others. There are often strong interconnections between these groups, double or even triple membership are not infrequent.\n\nThere are Basque nationalist parties with similar goals as those of ETA (namely, independence) but who openly reject their violent means. They are: EAJ-PNV, Eusko Alkartasuna, Aralar and, in the French Basque country, Abertzaleen Batasuna. In addition, a number of left-wing parties, such as Ezker Batua, Batzarre and some sectors of the EAJ-PNV party, also support self-determination but are not in favour of independence.\n\nHistorically, members of ETA have taken refuge in France, particularly the French Basque Country. The leadership have typically chosen to live for security reasons in France, where police pressure is much less than in Spain. Accordingly, ETA's tactical approach had been to downplay the issue of independence of the French Basque country so as to get French acquiescence for their activities. The French government quietly tolerated the group, especially during Franco's regime, when ETA members could face the death penalty in Spain. In the 1980s, the advent of the GAL still hindered counter-terrorist cooperation between the France and Spain, with the French government considering ETA a Spanish domestic problem. At the time, ETA members often travelled between the two countries using the French sanctuary as a base for operations.\n\nWith the disbanding of the GAL, the French government changed its position in the matter and initiated in the 1990s the ongoing period of active cooperation with the Spanish government against ETA, including fast-track transfers of detainees to Spanish tribunals that are regarded as fully compliant with European Union legislation in human rights and the legal representation of detainees. Virtually all of the highest ranks within ETA –including their successive \"military\", \"political\" or finances chiefs– have been captured in French territory, from where they had been plotting their activities after having crossed the border from Spain.\n\nIn response to the new situation, ETA carried out attacks against French policemen and made threats to some French judges and prosecutors. This implied a change from the group's previous low-profile in the French Basque Country, which successive ETA leaders had used to discreetly manage their activities in Spain.\n\nETA considers its prisoners political prisoners. Until 2003, ETA consequently forbade them to ask penal authorities for progression to \"tercer grado\" (a form of open prison that allows single-day or weekend furloughs) or parole. Before that date, those who did so were menaced and expelled from the group. Some were assassinated by ETA for leaving the group and going through reinsertion programs.\n\nThe Spanish Government passed the \"Ley de Partidos Políticos\". This is a law barring political parties which support violence and do not condemn terrorist actions or are involved with terrorist groups.<ref name=\"Ley 6/2002\">Ley Orgánica 6/2002, de 27 de junio, de Partidos Políticos. Noticias.juridicas.com (21 January 2011). Retrieved on 30 January 2011.</ref> The law resulted in the banning of Herri Batasuna and its successor parties unless they explicitly condemned terrorist actions and, at times, imprisoning or trying some of its leaders who have been indicted for cooperation with ETA.\n\nJudge Baltasar Garzón has initiated a judicial procedure (coded as \"18/98\"), aimed towards the support structure of ETA. This procedure started in 1998 with the preventive closure of the newspaper \"Egin\" (and its associated radio-station \"Egin Irratia\"), accused of being linked to ETA, and temporary imprisoning the editor of its \"investigative unit\", Pepe Rei, under similar accusations. In August 1999 Judge Baltasar Garzón authorized the reopening of the newspaper and the radio, but they could not reopen due to economic difficulties.\n\nJudicial procedure 18/98 has many ramifications, including the following:\n\n\n, indicted members of the youth movements Haika, Segi and Jarrai have been found guilty (January 2007) of a crime of connivance with terrorism. Most of the other trials are still under process.\n\nOn Tuesday 20 May 2008, leading ETA figures were arrested in Bordeaux, France. Francisco Javier López Peña, also known as 'Thierry,' had been on the run for twenty years before his arrest. A final total of arrests brought in six people, including ETA members and supporters, including the ex-Mayor of Andoain, José Antonio Barandiarán, who is rumoured to have led police to 'Thierry'. The Spanish Interior Ministry claimed the relevance of the arrests would come in time with the investigation. Furthermore, the Interior Minister said that those members of ETA now arrested had ordered the latest attacks, and that senior ETA member Francisco Javier López Peña was \"not just another arrest because he is, in all probability, the man who has most political and military weight in the terrorist group.\"\n\nAfter Lopez Pena's arrest, along with the Basque referendum being put on hold, police work has been on the rise. On 22 July 2008, Spanish police dismantled the most active cell of ETA by detaining nine suspected members of the group. Interior Minister Alfredo Perez Rubalcaba said about the arrests: \"We can't say this is the only ETA unit but it was the most active, most dynamic and of course the most wanted one.\" Four days later French police also arrested two suspects believed to be tied to the same active cell. The two suspects were: Asier Eceiza, considered a top aide to a senior ETA operative still sought by police, and Olga Comes, whom authorities have linked to the ETA suspects.\n\nThe European Union and the United States list ETA as a terrorist group in their relevant watch lists. The United Kingdom lists ETA as a terrorist group under the Terrorism Act 2000. The Canadian Parliament listed ETA as a terrorist group in 2003.\n\nFrance and Spain have often shown co-operation in the fight against ETA, after France's lack of co-operation during the Franco era. In late 2007, two Spanish guards were shot to death in France when on a joint operation with their French counterparts. Furthermore, in May 2008, the arrests of four people in Bordeaux led to a major breakthrough against ETA, according to the Spanish Interior Ministry.\n\nOn 2 October 2008, as ETA activity increased, France increased its pressure on ETA by arresting more ETA suspects, including Unai Fano, María Lizarraga on 23 September. and Esteban Murillo Zubiri, brother of Gabriel Zuburi, in Bidarrain. He had been wanted by the Spanish authorities since 2007 when a Europol arrest warrant was issued against him. French judicial authorities had already ordered that he be held in prison on remand.\n\nSpain has also sought cooperation from the United Kingdom in dealing with ETA-IRA ties. In November 2008, this came to light after Iñaki de Juana Chaos, whose release from prison was canceled on appeal, had moved to Belfast. He was thought to be staying at an IRA safe house while being sought by the Spanish authorities. Interpol notified the judge, Eloy Velasco, that he was in either the Republic of Ireland or Northern Ireland.\n\n\n\n\n\n\n\n\n\n\n\n", "id": "9926", "title": "ETA (separatist group)"}
{"url": "https://en.wikipedia.org/wiki?curid=9927", "text": "Endomembrane system\n\nThe endomembrane system is composed of the different membranes that are suspended in the cytoplasm within a eukaryotic cell. These membranes divide the cell into functional and structural compartments, or organelles. In eukaryotes the organelles of the endomembrane system include: the nuclear membrane, the endoplasmic reticulum, the Golgi apparatus, lysosomes, vesicles, endosomes and the cell membrane, among others. The system is defined more accurately as the set of membranes that form a single functional and developmental unit, either being connected directly, or exchanging material through vesicle transport. Importantly, the endomembrane system does not include the membranes of chloroplasts or mitochondria, but might have evolved from the latter (see below: Evolution).\n\nThe nuclear membrane contains two lipid bilayers that encompass the contents of the nucleus. The endoplasmic reticulum (ER) is a synthesis and transport organelle that branches into the cytoplasm in plant and animal cells. The Golgi apparatus is a series of multiple compartments where molecules are packaged for delivery to other cell components or for secretion from the cell. Vacuoles, which are found in both plant and animal cells (though much bigger in plant cells), are responsible for maintaining the shape and structure of the cell as well as storing waste products. A vesicle is a relatively small, membrane-enclosed sac that stores or transports substances. The cell membrane is a protective barrier that regulates what enters and leaves the cell. There is also an organelle known as the Spitzenkörper that is only found in fungi, and is connected with hyphal tip growth.\n\nIn prokaryotes endomembranes are rare, although in many photosynthetic bacteria the plasma membrane is highly folded and most of the cell cytoplasm is filled with layers of light-gathering membrane. These light-gathering membranes may even form enclosed structures called chlorosomes in green sulfur bacteria.\n\nThe organelles of the endomembrane system are related through direct contact or by the transfer of membrane segments as vesicles. Despite these relationships, the various membranes are not identical in structure and function. The thickness, molecular composition, and metabolic behavior of a membrane are not fixed, they may be modified several times during the membrane's life. One unifying characteristic the membranes share is a lipid bilayer, with proteins attached to either side or traversing them.\n\nMost lipids are synthesized in yeast either in the endoplasmic reticulum, lipid particles, or the mitochondrion, with little or no lipid synthesis occurring in the plasma membrane or nuclear membrane. Sphingolipid biosynthesis begins in the endoplasmic reticulum, but is completed in the Golgi apparatus. The situation is similar on mammals, with the exception of the first few steps in ether lipid biosynthesis, which occur in peroxisomes. The various membranes that enclose the other subcellular organelles must therefore be constructed by transfer of lipids from these sites of synthesis. However, although it is clear that lipid transport is a central process in organelle biogenesis, the mechanisms by which lipids are transported through cells remain poorly understood.\n\nThe first proposal that the membranes within cells form a single system that exchanges material between its components was by Morré and Mollenhauer in 1974. This proposal was made as a way of explaining how the various lipid membranes are assembled in the cell, with these membranes being assembled through \"lipid flow\" from the sites of lipid synthesis. The idea of lipid flow through a continuous system of membranes and vesicles was an alternative to the various membranes being independent entities that are formed from transport of free lipid components, such as fatty acids and sterols, through the cytosol. Importantly, the transport of lipids through the cytosol and lipid flow through a continuous endomembrane system are not mutually exclusive processes and both may occur in cells.\n\nThe nuclear envelope surrounds the nucleus, separating its contents from the cytoplasm. It has two membranes, each a lipid bilayer with associated proteins. The outer nuclear membrane is continuous with the rough endoplasmic reticulum membrane, and like that structure, features ribosomes attached to the surface. The outer membrane is also continuous with the inner nuclear membrane since the two layers are fused together at numerous tiny holes called nuclear pores that perforate the nuclear envelope. These pores are about 120 nm in diameter and regulate the passage of molecules between the nucleus and cytoplasm, permitting some to pass through the membrane, but not others. Since the nuclear pores are located in an area of high traffic, they play an important role in the physiology of cells. The space between the outer and inner membranes is called the perinuclear space and is joined with the lumen of the rough ER.\n\nThe nuclear envelope's structure is determined by a network of intermediate filaments (protein filaments). This network is organized into lining similar to mesh called the nuclear lamina, which binds to chromatin, integral membrane proteins, and other nuclear components along the inner surface of the nucleus. The nuclear lamina is thought to help materials inside the nucleus reach the nuclear pores and in the disintegration of the nuclear envelope during mitosis and its reassembly at the end of the process.\n\nThe nuclear pores are highly efficient at selectively allowing the passage of materials to and from the nucleus, because the nuclear envelope has a considerable amount of traffic. RNA and ribosomal subunits must be continually transferred from the nucleus to the cytoplasm. Histones, gene regulatory proteins, DNA and RNA polymerases, and other substances essential for nuclear activities must be imported from the cytoplasm. The nuclear envelope of a typical mammalian cell contains 3000–4000 pore complexes. If the cell is synthesizing DNA each pore complex needs to transport about 100 histone molecules per minute. If the cell is growing rapidly, each complex also needs to transport about 6 newly assembled large and small ribosomal subunits per minute from the nucleus to the cytosol, where they are used to synthesize proteins.\n\nThe endoplasmic reticulum (ER) is a membranous synthesis and transport organelle that is an extension of the nuclear envelope. More than half the total membrane in eukaryotic cells is accounted for by the ER. The ER is made up of flattened sacs and branching tubules that are thought to interconnect, so that the ER membrane forms a continuous sheet enclosing a single internal space. This highly convoluted space is called the ER lumen and is also referred to as the ER cisternal space. The lumen takes up about ten percent of the entire cell volume. The endoplasmic reticulum membrane allows molecules to be selectively transferred between the lumen and the cytoplasm, and since it is connected to the nuclear envelope, it provides a channel between the nucleus and the cytoplasm.\n\nThe ER has a central role in producing, processing, and transporting biochemical compounds for use inside and outside of the cell. Its membrane is the site of production of all the transmembrane proteins and lipids for most of the cell's organelles, including the ER itself, the Golgi apparatus, lysosomes, endosomes, mitochondria, peroxisomes, secretory vesicles, and the plasma membrane. Furthermore, almost all of the proteins that will exit the cell, plus those destined for the lumen of the ER, Golgi apparatus, or lysosomes, are originally delivered to the ER lumen. Consequently, many of the proteins found in the cisternal space of the endoplasmic reticulum lumen are there only temporarily as they pass on their way to other locations. Other proteins, however, constantly remain in the lumen and are known as endoplasmic reticulum resident proteins. These special proteins contain a specialized retention signal made up of a specific sequence of amino acids that enables them to be retained by the organelle. An example of an important endoplasmic reticulum resident protein is the chaperon protein known as BiP which identifies other proteins that have been improperly built or processed and keeps them from being sent to their final destinations.\n\nThe ER is involved in cotranslational sorting of proteins. A polypeptide which contains an ER signal sequence is recognised by a signal recognition protein which halts the production of the protein. the SRP transports the polypeptide to the ER membrane where its released in through a membrane pore and translation resumes.\n\nThere are two distinct, though connected, regions of ER that differ in structure and function: smooth ER and rough ER. The rough endoplasmic reticulum is so named because the cytoplasmic surface is covered with ribosomes, giving it a bumpy appearance when viewed through an electron microscope. The smooth ER appears smooth since its cytoplasmic surface lacks ribosomes.\n\nIn the great majority of cells, smooth ER regions are scarce and are often partly smooth and partly rough. They are sometimes called transitional ER because they contain ER exit sites from which transport vesicles carrying newly synthesized proteins and lipids bud off for transport to the Golgi apparatus. In certain specialized cells, however, the smooth ER is abundant and has additional functions. The smooth ER of these specialized cells functions in diverse metabolic processes, including synthesis of lipids, metabolism of carbohydrates, and detoxification of drugs and poisons.\n\nEnzymes of the smooth ER are vital to the synthesis of lipids, including oils, phospholipids, and steroids. Sex hormones of vertebrates and the steroid hormones secreted by the adrenal glands are among the steroids produced by the smooth ER in animal cells. The cells that synthesize these hormones are rich in smooth ER.\n\nLiver cells are another example of specialized cells that contain an abundance of smooth ER. These cells provide an example of the role of smooth ER in carbohydrate metabolism. Liver cells store carbohydrates in the form of glycogen. The breakdown of glycogen eventually leads to the release of glucose from the liver cells, which is important in the regulation of sugar concentration in the blood. However, the primary product of glycogen breakdown is glucose-1-phosphate. This is converted to glucose-6-phosphate and then an enzyme of the liver cell's smooth ER removes the phosphate from the glucose, so that it can then leave the cell.\n\nEnzymes of the smooth ER can also help detoxify drugs and poisons. Detoxification usually involves the addition of a hydroxyl group to a drug, making the drug more soluble and thus easier to purge from the body. One extensively studied detoxification reaction is carried out by the cytochrome P450 family of enzymes, which catalyze water-insoluble drugs or metabolites that would otherwise accumulate to toxic levels in cell membrane.\n\nMuscle cells have another specialized function of smooth ER. The ER membrane pumps calcium ions from the cytosol into the cisternal space. When a muscle cell becomes stimulated by a nerve impulse, calcium goes back across the ER membrane into the cytosol and generates the contraction of the muscle cell.\n\nMany types of cells export proteins produced by ribosomes attached to the rough ER. The ribosomes assemble amino acids into protein units, which are carried into the rough ER for further adjustments. These proteins may be either transmembrane proteins, which become embedded in the membrane of the endoplasmic reticulum, or water-soluble proteins, which are able to pass through the membrane into the lumen. Those that reach the inside of the endoplasmic reticulum are folded into the correct three-dimensional conformation. Chemicals, such as carbohydrates or sugars, are added, then the endoplasmic reticulum either transports the completed proteins, called secretory proteins, to areas of the cell where they are needed, or they are sent to the Golgi apparatus for further processing and modification.\n\nOnce secretory proteins are formed, the ER membrane separates them from the proteins that will remain in the cytosol. Secretory proteins depart from the ER enfolded in the membranes of vesicles that bud like bubbles from the transitional ER. These vesicles in transit to another part of the cell are called transport vesicles. An alternative mechanism for transport of lipids and proteins out of the ER are through lipid transfer proteins at regions called membrane contact sites where the ER becomes closely and stably associated with the membranes of other organelles, such as the plasma membrane, Golgi or lysosomes.\n\nIn addition to making secretory proteins, the rough ER makes membranes that grows in place from the addition of proteins and phospholipids. As polypeptides intended to be membrane proteins grow from the ribosomes, they are inserted into the ER membrane itself and are kept there by their hydrophobic portions. The rough ER also produces its own membrane phospholipids; enzymes built into the ER membrane assemble phospholipids. The ER membrane expands and can be transferred by transport vesicles to other components of the endomembrane system.\n\nThe Golgi apparatus (also known as the Golgi body and the Golgi complex) is composed of interconnected sacs called cisternae. Its shape is similar to a stack of pancakes. The number of these stacks varies with the specific function of the cell. The Golgi apparatus is used by the cell for further protein modification. The section of the Golgi apparatus that receives the vesicles from the ER is known as the cis face, and is usually near the ER. The opposite end of the Golgi apparatus is called the trans face, this is where the modified compounds leave. The trans face is usually facing the plasma membrane, which is where most of the substances the Golgi apparatus modifies are sent.\n\nVesicles sent off by the ER containing proteins are further altered at the Golgi apparatus and then prepared for secretion from the cell or transport to other parts of the cell. Various things can happen to the proteins on their journey through the enzyme covered space of the Golgi apparatus. The modification and synthesis of the carbohydrate portions of glycoproteins is common in protein processing. The Golgi apparatus removes and substitutes sugar monomers, producing a large variety of oligosaccharides. In addition to modifying proteins, the Golgi also manufactures macromolecules itself. In plant cells, the Golgi produces pectins and other polysaccharides needed by the plant structure.\n\nOnce the modification process is completed, the Golgi apparatus sorts the products of its processing and sends them to various parts of the cell. Molecular identification labels or tags are added by the Golgi enzymes to help with this. After everything is organized, the Golgi apparatus sends off its products by budding vesicles from its trans face.\n\nVacuoles, like vesicles, are membrane-bound sacs within the cell. They are larger than vesicles and their specific function varies. The operations of vacuoles are different for plant and animal vacuoles.\n\nIn plant cells, vacuoles cover anywhere from 30% to 90% of the total cell volume. Most mature plant cells contain one large central vacuole encompassed by a membrane called the tonoplast. Vacuoles of plant cells act as storage compartments for the nutrients and waste of a cell. The solution that these molecules are stored in is called the cell sap. Pigments that color the cell are sometime located in the cell sap. Vacuoles can also increase the size of the cell, which elongates as water is added, and they control the turgor pressure (the osmotic pressure that keeps the cell wall from caving in). Like lysosomes of animal cells, vacuoles have an acidic pH and contain hydrolytic enzymes. The pH of vacuoles enables them to perform homeostatic procedures in the cell. For example, when the pH in the cells environment drops, the H ions surging into the cytosol can be transferred to a vacuole in order to keep the cytosol's pH constant.\n\nIn animals, vacuoles serve in exocytosis and endocytosis processes. Endocytosis refers to when substances are taken into the cell, whereas for exocytosis substances are moved from the cell into the extracellular space. Material to be taken-in is surrounded by the plasma membrane, and then transferred to a vacuole. There are two types of endocytosis, phagocytosis (cell eating) and pinocytosis (cell drinking). In phagocytosis, cells engulf large particles such as bacteria. Pinocytosis is the same process, except the substances being ingested are in the fluid form.\n\nVesicles are small membrane-enclosed transport units that can transfer molecules between different compartments. Most vesicles transfer the membranes assembled in the endoplasmic reticulum to the Golgi apparatus, and then from the Golgi apparatus to various locations.\n\nThere are various types of vesicles each with a different protein configuration. Most are formed from specific regions of membranes. When a vesicle buds off from a membrane it contains specific proteins on its cytosolic surface. Each membrane a vesicle travels to contains a marker on its cytosolic surface. This marker corresponds with the proteins on the vesicle traveling to the membrane. Once the vesicle finds the membrane, they fuse.\n\nThere are three well known types of vesicles. They are clathrin-coated, COPI-coated, and COPII-coated vesicles. Each performs different functions in the cell. For example, clathrin-coated vesicles transport substances between the Golgi apparatus and the plasma membrane. COPI- and COPII-coated vesicles are frequently used for transportation between the ER and the Golgi apparatus.\n\nLysosomes are organelles that contain hydrolytic enzymes that are used for intracellular digestion. The main functions of a lysosome are to process molecules taken in by the cell and to recycle worn out cell parts. The enzymes inside of lysosomes are acid hydrolases which require an acidic environment for optimal performance. Lysosomes provide such an environment by maintaining a pH of 5.0 inside of the organelle. If a lysosome were to rupture, the enzymes released would not be very active because of the cytosol's neutral pH. However, if numerous lysosomes leaked the cell could be destroyed from autodigestion.\n\nLysosomes carry out intracellular digestion, in a process called phagocytosis (from the Greek phagein, to eat and kytos, vessel, referring here to the cell), by fusing with a vacuole and releasing their enzymes into the vacuole. Through this process, sugars, amino acids, and other monomers pass into the cytosol and become nutrients for the cell. Lysosomes also use their hydrolytic enzymes to recycle the cell's obsolete organelles in a process called autophagy. The lysosome engulfs another organelle and uses its enzymes to take apart the ingested material. The resulting organic monomers are then returned to the cytosol for reuse. The last function of a lysosome is to digest the cell itself through autolysis.\n\nThe spitzenkörper is a component of the endomembrane system found only in fungi, and is associated with hyphal tip growth. It is a phase-dark body that is composed of an aggregation of membrane-bound vesicles containing cell wall components, serving as a point of assemblage and release of such components intermediate between the Golgi and the cell membrane. The spitzenkörper is motile and generates new hyphal tip growth as it moves forward.\n\nThe plasma membrane is a phospholipid bilayer membrane that separates the cell from its environment and regulates the transport of molecules and signals into and out of the cell. Embedded in the membrane are proteins that perform the functions of the plasma membrane. The plasma membrane is not a fixed or rigid structure, the molecules that compose the membrane are capable of lateral movement. This movement and the multiple components of the membrane are why it is referred to as a fluid mosaic. Smaller molecules such as carbon dioxide, water, and oxygen can pass through the plasma membrane freely by diffusion or osmosis. Larger molecules needed by the cell are assisted by proteins through active transport.\n\nThe plasma membrane of a cell has multiple functions. These include transporting nutrients into the cell, allowing waste to leave, preventing materials from entering the cell, averting needed materials from leaving the cell, maintaining the pH of the cytosol, and preserving the osmotic pressure of the cytosol. Transport proteins which allow some materials to pass through but not others are used for these functions. These proteins use ATP hydrolysis to pump materials against their concentration gradients.\n\nIn addition to these universal functions, the plasma membrane has a more specific role in multicellular organisms. Glycoproteins on the membrane assist the cell in recognizing other cells, in order to exchange metabolites and form tissues. Other proteins on the plasma membrane allow attachment to the cytoskeleton and extracellular matrix; a function that maintains cell shape and fixes the location of membrane proteins. Enzymes that catalyze reactions are also found on the plasma membrane. Receptor proteins on the membrane have a shape that matches with a chemical messenger, resulting in various cellular responses.\n\nThe origin of the endomembrane system is linked to the origin of eukaryotes themselves and the origin of eukaryoties to the endosymbiotic origin of mitochondria. Many models have been put forward to explain the origin of the endomembrane system (reviewed in). The most recent concept suggests that the endomembrane system evolved from outer membrane vesicles the endosymbiotic mitochondrion secreted. This OMV-based model for the origin of the endomembrane system is currently the one that requires the least amount of novel inventions at eukaryote origin and explains the many connections of mitochondria with other compartments of the cell.\n", "id": "9927", "title": "Endomembrane system"}
{"url": "https://en.wikipedia.org/wiki?curid=9928", "text": "Ethnology\n\nEthnology (from the Greek \"ἔθνος\", \"ethnos\" meaning \"nation\") is the branch of anthropology that compares and analyzes the characteristics of different peoples and the relationship between them (cf. cultural, social, or sociocultural anthropology).\n\nCompared to ethnography, the study of single groups through direct contact with the culture, ethnology takes the research that ethnographers have compiled and then compares and contrasts different cultures.\nThe term \"ethnologia\" (\"ethnology\") is credited to Adam Franz Kollár (1718-1783) who used and defined it in his \"Historiae ivrisqve pvblici Regni Vngariae amoenitates\" published in Vienna in 1783. as: “the science of nations and peoples, or, that study of learned men in which they inquire into the origins, languages, customs, and institutions of various nations, and finally into the fatherland and ancient seats, in order to be able better to judge the nations and peoples in their own times.” \n\nKollár's interest in linguistic and cultural diversity was aroused by the situation in his native multi-ethnic and multilingual Kingdom of Hungary and his roots among its Slovaks, and by the shifts that began to emerge after the gradual retreat of the Ottoman Empire in the more distant Balkans.\n\nAmong the goals of ethnology have been the reconstruction of human history, and the formulation of cultural invariants, such as the incest taboo and culture change, and the formulation of generalizations about \"human nature\", a concept which has been criticized since the 19th century by various philosophers (Hegel, Marx, structuralism, etc.). In some parts of the world ethnology has developed along independent paths of investigation and pedagogical doctrine, with \"cultural anthropology\" becoming dominant especially in the United States, and \"social anthropology\" in Great Britain. The distinction between the three terms is increasingly blurry. Ethnology has been considered an academic field since the late 18th century especially in Europe and is sometimes conceived of as any comparative study of human groups.\nThe 15th-century exploration of America by European explorers had an important role in formulating new notions of the Occidental, such as, the notion of the \"Other\". This term was used in conjunction with \"savages\", which was either seen as a brutal barbarian, or alternatively, as \"noble savage\". Thus, civilization was opposed in a dualist manner to barbary, a classic opposition constitutive of the even more commonly shared ethnocentrism. The progress of ethnology, for example with Claude Lévi-Strauss's structural anthropology, led to the criticism of conceptions of a linear progress, or the pseudo-opposition between \"societies with histories\" and \"societies without histories\", judged too dependent on a limited view of history as constituted by accumulative growth.\n\nLévi-Strauss often referred to Montaigne's essay on cannibalism as an early example of ethnology. Lévi-Strauss aimed, through a structural method, at discovering universal invariants in human society, chief among which he believed to be the incest taboo. However, the claims of such cultural universalism have been criticized by various 19th and 20th century social thinkers, including Marx, Nietzsche, Foucault, Derrida, Althusser and Deleuze.\n\nThe French school of ethnology was particularly significant for the development of the discipline since the early 1950s with Paul Rivet, Marcel Griaule, Germaine Dieterlen, Claude Lévi-Strauss and Jean Rouch.\n\n\n\n\n\n", "id": "9928", "title": "Ethnology"}
{"url": "https://en.wikipedia.org/wiki?curid=9929", "text": "Espagnole sauce\n\nIn cooking, espagnole sauce () is one of Auguste Escoffier's five mother sauces that are the basis of sauce-making in classic French cooking. These types of sauce were already gathered in different Spanish cooking handbooks of the late 19th century. Escoffier popularized the recipe, which is still followed today.\n\nEspagnole has a strong taste and is rarely used directly on food. As a mother sauce it serves as the starting point for many derivatives, such as Sauce Africaine, Sauce Bigarade, Sauce Bourguignonne, sauce aux champignons, sauce charcutière, sauce chasseur, Sauce Chevreuil and demi-glace. There are hundreds of other derivatives in the classical French repertoire.\n\nEscoffier included a recipe for a Lenten espagnole sauce, using fish stock and mushrooms, in \"Le Guide culinaire\", but doubted its necessity.\n\nThe basic method of making espagnole is to prepare a very dark brown roux, to which veal stock or water is added, along with browned bones, pieces of beef, vegetables, and various seasonings. This blend is allowed to slowly reduce while being frequently skimmed. The classic recipe calls for additional veal stock to be added as the liquid gradually reduces, but today water is generally used instead. Tomato paste or pureed tomatoes are added towards the end of the process, and the sauce is further reduced.\n\nAlthough \"espagnole\" is the French word for \"Spanish\", the sauce's connection to Spanish cuisine is argued by French cooks. According to Louis Diat, the creator of vichyssoise and the author of the classic \"Gourmet's Basic French Cookbook\": \"There is a story that explains why the most important basic brown sauce in French cuisine is called \"sauce espagnole\", or Spanish sauce. According to the story, the Spanish cooks of Louis XIII's bride, Anne, helped to prepare their wedding feast, and insisted upon improving the rich brown sauce of France with Spanish tomatoes. This new sauce was an instant success, and was gratefully named in honor of its creators.\"\n\nIn \"Kettner's Book of the Table\", published in 1877, there is an entirely different explanation:\n\nThe name \"Kettner\" in the title refers to Auguste Kettner, former chef to Napoleon III, who emigrated to England and in 1867 opened a restaurant in Soho—\"Kettner's\"—one of the oldest restaurants in London.\n\n\n", "id": "9929", "title": "Espagnole sauce"}
{"url": "https://en.wikipedia.org/wiki?curid=9931", "text": "Amplifier\n\nAn amplifier, electronic amplifier or (informally) amp is an electronic device that can increase the power of a signal (a time-varying voltage or current). An amplifier functions by using electric power from a power supply to increase the amplitude of the voltage or current signal. An amplifier is effectively the opposite of an attenuator: while an amplifier provides gain, an attenuator provides loss.\n\nAn amplifier can either be a separate piece of equipment or an electrical circuit contained within another device. Amplification is fundamental to modern electronics, and amplifiers are widely used in almost all electronic equipment. Amplifiers can be categorized in different ways. One is by the frequency of the electronic signal being amplified; audio amplifiers amplify signals in the audio (sound) range of less than 20 kHz, RF amplifiers amplify frequencies in the radio frequency range between 20 kHz and 300 GHz. Another is which quantity, voltage or current is being amplified; amplifiers can be divided into voltage amplifiers, current amplifiers, transconductance amplifiers, and transresistance amplifiers. A further distinction is whether the output is a linear or nonlinear representation of the input. Amplifiers can also be categorized by their physical placement in the signal chain.\n\nThe first practical electronic device that could amplify was the triode vacuum tube, invented in 1906 by Lee De Forest, which led to the first amplifiers around 1912. Vacuum tubes were used in almost all amplifiers until the 1960s–1970s when the transistor, invented in 1947, replaced them. Today most amplifiers use transistors, but vacuum tubes continue to be used in some applications.\n\nThe development of audio communication technology; the telephone and intercom around 1880 and the first AM radio transmitters and receivers around 1905 created a need to somehow make an electrical audio signal \"louder\". \nBefore the invention of electronic amplifiers, earphone drivers mechanically coupled to carbon microphones were used as crude amplifiers in telephone repeaters. After the turn of the century it was found that negative resistance mercury lamps could amplify, and were also tried in repeaters. \nThe first practical electronic device that could amplify was the Audion (triode) vacuum tube, invented in 1906 by Lee De Forest, which led to the first amplifiers around 1912. The terms \"amplifier\" and \"amplification\" (from the Latin \"amplificare\", 'to enlarge or expand') were first used for this new capability around 1915 when triodes became widespread.\n\nThe amplifying vacuum tube revolutionized electrical technology, creating the new field of electronics, the technology of active electrical devices. It made possible long distance telephone lines, public address systems, radio broadcasting, talking motion pictures, practical audio recording, radar, television, and the first computers. For 50 years virtually all consumer electronic devices used vacuum tubes. Early tube amplifiers often had positive feedback (regeneration), which could increase gain but also make the amplifier unstable and prone to oscillation. Much of the mathematical theory of amplifiers was developed at Bell Telephone Laboratories during the 1920s to 1940s. Distortion levels in early amplifiers were high, usually around 5%, until 1934, when Harold Black developed negative feedback; this allowed the distortion levels to be greatly reduced, at the cost of lower gain. Other advances in the theory of amplification were made by Harry Nyquist and Hendrik Wade Bode.\n\nThe vacuum tube was the only amplifying device (besides specialized power devices such as the magnetic amplifier and amplidyne) for 40 years, and dominated electronics until 1947, when the first transistor, the BJT, was invented. The replacement of bulky, fragile vacuum tubes with transistors during the 1960s and 1970s created another revolution in electronics, making possible the first really portable electronic devices, such as the transistor radio developed in 1954. Today most amplifiers use transistors, but vacuum tubes are still used in some high power applications such as radio transmitters.\n\nAmplifier quality is characterized by a list of specifications that include:\n\nAmplifiers are described according to the properties of their inputs, their outputs, and how they relate. All amplifiers have gain, a multiplication factor that relates the magnitude of some property of the output signal to a property of the input signal. The gain may be specified as the ratio of output voltage to input voltage (voltage gain), output power to input power (power gain), or some combination of current, voltage, and power. In many cases the property of the output that varies is dependent on the same property of the input, making the gain unitless (though often expressed in decibels (dB)).\n\nMost amplifiers are designed to be linear. That is, they provide constant gain for any normal input level and output signal. If an amplifier's gain is not linear, the output signal can become distorted. There are, however, cases where variable gain is useful. Certain signal processing applications use exponential gain amplifiers.\n\nAmplifiers are usually designed to function well in a specific application, for example: radio and television transmitters and receivers, high-fidelity (\"hi-fi\") stereo equipment, microcomputers and other digital equipment, and guitar and other instrument amplifiers. Every amplifier includes at least one active device, such as a vacuum tube or transistor.\n\nAll amplifiers include some form of active device: this is the device that does the actual amplification. The active device can be a vacuum tube, discrete solid state component, such as a single transistor, or part of an integrated circuit, as in an op-amp).\n\nTransistor amplifiers (or solid state amplifiers) are the most common type of amplifier in use today. A transistor is used as the active element. The gain of the amplifier is determined by the properties of the transistor itself as well as the circuit it is contained within.\n\nCommon active devices in transistor amplifiers include bipolar junction transistors (BJTs) and metal oxide semiconductor field-effect transistors (MOSFETs).\n\nApplications are numerous, some common examples are audio amplifiers in a home stereo or public address system, RF high power generation for semiconductor equipment, to RF and microwave applications such as radio transmitters.\n\nTransistor-based amplification can be realized using various configurations: for example a bipolar junction transistor can realize common base, common collector or common emitter amplification; a MOSFET can realize common gate, common source or common drain amplification. Each configuration has different characteristics.\n\nVacuum-tube amplifiers (also known as tube amplifiers or valve amplifiers) use a vacuum tube as the active device. While semiconductor amplifiers have largely displaced valve amplifiers for low power applications, valve amplifiers can be much more cost effective in high power applications such as radar, countermeasures equipment, and communications equipment. Many microwave amplifiers are specially designed valve amplifiers, such as the klystron, gyrotron, traveling wave tube, and crossed-field amplifier, and these microwave valves provide much greater single-device power output at microwave frequencies than solid-state devices. Vacuum tubes remain in use in some high end audio equipment, as well as in musical instrument amplifiers, due to a preference for \"tube sound\".\n\nMagnetic amplifiers are devices somewhat similar to a transformer where one winding is used to control the saturation of a magnetic core and hence alter the impedance of the other winding.\n\nThey have largely fallen out of use due to development in semiconductor amplifiers but are still useful in HVDC control, and in nuclear power control circuitry to their not being affected by radioactivity.\n\nNegative resistances can be used as amplifiers, such as the tunnel diode amplifier.\n\nAmplifiers can also be categorised by the way they amplify the input signal.\n\nA power amplifier is an amplifier designed primarily to increase the power available to a load. In practice, amplifier power gain depends on the source and load impedances, as well as the inherent voltage and current gain. A radio frequency (RF) amplifier design typically optimizes impedances for power transfer, while audio and instrumentation amplifier designs normally optimize input and output impedance for least loading and highest signal integrity. An amplifier that is said to have a gain of 20 dB might have a voltage gain of 20 dB and an available power gain of much more than 20 dB (power ratio of 100)—yet actually deliver a much lower power gain if, for example, the input is from a 600 Ω microphone and the output connects to a 47 kΩ input socket for a power amplifier. In general the power amplifier is the last 'amplifier' or actual circuit in a signal chain (the output stage) and is the amplifier stage that requires attention to power efficiency. Efficiency considerations lead to the various classes of power amplifier based on the biasing of the output transistors or tubes: see power amplifier classes below.\n\n\nPower amplifier circuits include the following types:\n\nAn operational amplifier is an amplifier circuit which typically has very high open loop gain and differential inputs. Op amps have become very widely used as standardized \"gain blocks\" in circuits due to their versatility; their gain, bandwidth and other characteristics can be controlled by feedback through an external circuit. Though the term today commonly applies to integrated circuits, the original operational amplifier design used valves, and later designs used discrete transistor circuits.\n\nA fully differential amplifier is similar to the operational amplifier, but also has differential outputs. These are usually constructed using BJTs or FETs.\n\nA differential amplifier is the first stage of an op-amp, a differential amplifier consists of two transistors which are emitter coupled.\nTypes of differential amplifiers:\n\n\nformula_1\n\n\nIt is the average between the input voltages formula_2 and formula_3\n\nformula_4\n\nThese use balanced transmission lines to separate individual single stage amplifiers, the outputs of which are summed by the same transmission line. The transmission line is a balanced type with the input at one end and on one side only of the balanced transmission line and the output at the opposite end is also the opposite side of the balanced transmission line. The gain of each stage adds linearly to the output rather than multiplies one on the other as in a cascade configuration. This allows a higher bandwidth to be achieved than could otherwise be realised even with the same gain stage elements.\n\nThese nonlinear amplifiers have much higher efficiencies than linear amps, and are used where the power saving justifies the extra complexity. Class-D amplifiers are the main example of this type of amplification—see below.\n\nVideo amplifiers are designed to process video signals and have varying bandwidths depending on whether the video signal is for SDTV, EDTV, HDTV 720p or 1080i/p etc.. The specification of the bandwidth itself depends on what kind of filter is used—and at which point ( or for example) the bandwidth is measured. Certain requirements for step response and overshoot are necessary for an acceptable TV image.\n\nThese deal with video signals that drive an oscilloscope display tube, and can have bandwidths of about . The specifications on step response, rise time, overshoot, and aberrations can make designing these amplifiers difficult. One of the pioneers in high bandwidth vertical amplifiers was the Tektronix company.\n\nTraveling wave tube amplifiers (TWTAs) are used for high power amplification at low microwave frequencies. They typically can amplify across a broad spectrum of frequencies; however, they are usually not as tunable as klystrons.\n\nKlystrons are specialized linear-beam vacuum-devices, designed to provide high power, widely tunable amplification of millimetre and sub-millimetre waves. Klystrons are designed for large scale operations and despite having a narrower bandwidth than TWTAs, they have the advantage of coherently amplifying a reference signal so its output may be precisely controlled in amplitude, frequency and phase.\n\nAn audio power amplifier is usually used to amplify signals such as music or speech. In the mid 1960s, guitar and bass amplifiers began to gain popularity because of their relatively low price ($50) and guitars being the most popular instruments as well. Several factors are especially important in the selection of musical instrument amplifiers (such as guitar amplifiers) and other audio amplifiers (although the whole of the sound system components such as microphones to loudspeakers affect these parameters):\n\n\nBefore coming onto the music scene, amplifiers were heavily used in cinema. In the premiere of \"Noah's Ark\" in 1929, the movie's director (Michael Kurtiz) used the amplifier for a festival following the movie's premiere.\n\nMany alternative classifications address different aspects of amplifier designs, and they all express some particular perspective relating the design parameters to the objectives of the circuit. Amplifier design is always a compromise of numerous factors, such as cost, power consumption, real-world device imperfections, and a multitude of performance specifications. Below are several different approaches to classification:\n\nElectronic amplifiers use one variable presented as either a current and voltage. Either current or voltage can be used as input and either as output, leading to four types of amplifiers. In idealized form they are represented by each of the four types of dependent source used in linear analysis, as shown in the figure, namely:\n\nEach type of amplifier in its ideal form has an ideal input and output resistance that is the same as that of the corresponding dependent source:\n\nIn practice the ideal impedances are not possible to achieve. For any particular circuit, a small-signal analysis is often used to find the actual impedance. A small-signal AC test current \"I\" is applied to the input or output node, all external sources are set to AC zero, and the corresponding alternating voltage \"V\" across the test current source determines the impedance seen at that node as \"R = V / I\".\n\nAmplifiers designed to attach to a transmission line at input and output, especially RF amplifiers, do not fit into this classification approach. Rather than dealing with voltage or current individually, they ideally couple with an input or output impedance matched to the transmission line impedance, that is, match \"ratios\" of voltage to current. Many real RF amplifiers come close to this ideal. Although, for a given appropriate source and load impedance, RF amplifiers can be characterized as amplifying voltage or current, they fundamentally are amplifying power.\n\nOne set of classifications for amplifiers is based on which device terminal is common to both the input and the output circuit. In the case of bipolar junction transistors, the three classes are common emitter, common base, and common collector. For field-effect transistors, the corresponding configurations are common source, common gate, and common drain; for vacuum tubes, common cathode, common grid, and common plate.\nThe common emitter (or common source, common cathode, etc.) is most often configured to provide amplification of a voltage applied between base and emitter, and the output signal taken between collector and emitter is inverted, relative to the input. The common collector arrangement applies the input voltage between base and collector, and to take the output voltage between emitter and collector. This causes negative feedback, and the output voltage tends to follow the input voltage. This arrangement is also used as the input presents a high impedance and does not load the signal source, though the voltage amplification is less than one. The common-collector circuit is, therefore, better known as an emitter follower, source follower, or cathode follower.\n\nAn amplifier whose output exhibits no feedback to its input side is described as 'unilateral'. The input impedance of a unilateral amplifier is independent of load, and output impedance is independent of signal source impedance.\n\nAn amplifier that uses feedback to connect part of the output back to the input is a \"bilateral\" amplifier. Bilateral amplifier input impedance depends on the load, and output impedance on the signal source impedance.\nAll amplifiers are bilateral to some degree; however they may often be modeled as unilateral under operating conditions where feedback is small enough to neglect for most purposes, simplifying analysis (see the common base article for an example).\n\nAn amplifier design often deliberately applies negative feedback to tailor amplifier behavior. Some feedback, positive or negative, is unavoidable and often undesirable—introduced, for example, by parasitic elements, such as inherent capacitance between input and output of devices such as transistors, and capacitive coupling of external wiring. Excessive frequency-dependent positive feedback can turn an amplifier into an oscillator.\n\nLinear unilateral and bilateral amplifiers can be represented as two-port networks.\n\nAnother way to classify amplifiers is by the phase relationship of the input signal to the output signal. An 'inverting' amplifier produces an output 180 degrees out of phase with the input signal (that is, a polarity inversion or mirror image of the input as seen on an oscilloscope). A 'non-inverting' amplifier maintains the phase of the input signal waveforms. An emitter follower is a type of non-inverting amplifier, indicating that the signal at the emitter of a transistor is following (that is, matching with unity gain but perhaps an offset) the input signal. Voltage follower is also non inverting type of amplifier having unity gain.\n\nThis description can apply to a single stage of an amplifier, or to a complete amplifier system.\n\nOther amplifiers may be classified by their function or output characteristics. These functional descriptions usually apply to complete amplifier systems or sub-systems and rarely to individual stages.\nThe performance of an op-amp with these characteristics is entirely defined by the (usually passive) components that form a negative feedback loop around it. The amplifier itself does not affect the output\".\nAll real-world op-amps fall short of the idealised specification above—but some modern components have remarkable performance and come close in some respects.\n\nAmplifiers are sometimes classified by the coupling method of the signal at the input, output, or between stages. Different types of these include:\n\nDepending on the frequency range and other properties amplifiers are designed according to different principles.\n\n\nThe frequency range handled by an amplifier might be specified in terms of bandwidth (normally implying a response that is 3 dB down when the frequency reaches the specified bandwidth), or by specifying a frequency response that is within a certain number of decibels between a lower and an upper frequency (e.g. \"20 Hz to 20 kHz plus or minus 1 dB\").\n\nPower amplifier circuits (output stages) are classified as A, B, AB and C for analog designs—and class D and E for switching designs—based on the proportion of each input cycle (conduction angle) during which an amplifying device passes current. The image of the conduction angle derives from amplifying a sinusoidal signal. If the device is always on, the conducting angle is 360°. If it is on for only half of each cycle, the angle is 180°. The angle of flow is closely related to the amplifier power efficiency. The various classes are introduced below, followed by a more detailed discussion under their individual headings further down.\n\nIn the illustrations below, a bipolar junction transistor is shown as the amplifying device. However the same attributes are found with MOSFETs or vacuum tubes.\n\n\nA \"Class D\" amplifier uses some form of pulse-width modulation to control the output devices; the conduction angle of each device is no longer related directly to the input signal but instead varies in pulse width. These are sometimes called \"digital\" amplifiers because the output device is switched fully on or off, and not carrying current proportional to the signal amplitude.\n\n\nAmplifying devices operating in class A conduct over the entire range of the input cycle. A \"class-A amplifier\" is distinguished by the output stage devices being biased for class A operation. Subclass A2 is sometimes used to refer to vacuum-tube class-A stages that drive the grid slightly positive on signal peaks for slightly more power than normal class A (A1; where the grid is always negative). This, however, incurs higher signal distortion.\n\n\n\nClass-A power amplifier designs have largely been superseded by more efficient designs, though their simplicity makes them popular with some hobbyists. There is a market for expensive high fidelity class-A amps considered a \"cult item\" among audiophiles mainly for their absence of crossover distortion and reduced odd-harmonic and high-order harmonic distortion. Class A power amps are also used in some \"boutique\" guitar amplifiers due to their perceived tonal qualities.\n\nSome hobbyists who prefer class-A amplifiers also prefer the use of thermionic valve (tube) designs instead of transistors, for several reasons:\n\n\nTransistors are much cheaper, and so more elaborate designs that give greater efficiency but use more parts are still cost-effective. A classic application for a pair of class-A devices is the long-tailed pair, which is exceptionally linear, and forms the basis of many more complex circuits, including many audio amplifiers and almost all op-amps.\n\nClass-A amplifiers may be used in output stages of op-amps (although the accuracy of the bias in low cost op-amps such as the 741 may result in class A or class AB or class B performance, varying from device to device or with temperature). They are sometimes used as medium-power, low-efficiency, and high-cost audio power amplifiers. The power consumption is unrelated to the output power. At idle (no input), the power consumption is essentially the same as at high output volume. The result is low efficiency and high heat dissipation.\n\nClass-B amplifiers only amplify half of the input wave cycle, thus creating a large amount of distortion, but their efficiency is greatly improved and is much better than class A. Class-B amplifiers are also favoured in battery-operated devices, such as transistor radios. Class B has a maximum theoretical efficiency of π/4 (≈ 78.5%). This is because the amplifying element is switched off altogether half of the time, and so cannot dissipate power. A single class-B element is rarely found in practice, though it has been used for driving the loudspeaker in the early IBM Personal Computers with beeps, and it can be used in RF power amplifier where the distortion levels are less important. However, class C is more commonly used for this.\n\nA practical circuit using class-B elements is the push–pull stage, such as the very simplified complementary pair arrangement shown below. Here, complementary or quasi-complementary devices are each used for amplifying the opposite halves of the input signal, which is then recombined at the output. This arrangement gives excellent efficiency, but can suffer from the drawback that there is a small mismatch in the cross-over region at the \"joins\" between the two halves of the signal, as one output device has to take over supplying power exactly as the other finishes. This is called crossover distortion. An improvement is to bias the devices so they are not completely off when they are not in use. This approach is called \"class AB\" operation.\n\nClass B amplifiers offer higher efficiency than class A amplifier using a single active device.\n\nClass AB is widely considered a good compromise for amplifiers, since much of the time the music signal is quiet enough that the signal stays in the \"class A\" region, where it is amplified with good fidelity, and by definition if passing out of this region, is large enough that the distortion products typical of class B are relatively small. The crossover distortion can be reduced further by using negative feedback.\n\nIn class-AB operation, each device operates the same way as in class B over half the waveform, but also conducts a small amount on the other half. As a result, the region where both devices simultaneously are nearly off (the \"dead zone\") is reduced. The result is that when the waveforms from the two devices are combined, the crossover is greatly minimised or eliminated altogether. The exact choice of quiescent current (the standing current through both devices when there is no signal) makes a large difference to the level of distortion (and to the risk of thermal runaway, that may damage the devices). Often, bias voltage applied to set this quiescent current must be adjusted with the temperature of the output transistors. (For example, in the circuit at the beginning of the article, the diodes would be mounted physically close to the output transistors, and specified to have a matched temperature coefficient.) Another approach (often used with thermally tracking bias voltages) is to include small value resistors in series with the emitters.\n\nClass AB sacrifices some efficiency over class B in favor of linearity, thus is less efficient (below 78.5% for full-amplitude sine waves in transistor amplifiers, typically; much less is common in class-AB vacuum-tube amplifiers). It is typically much more efficient than class A.\n\nSometimes a numeral is added for vacuum-tube stages. If grid current is not permitted to flow, the class is AB. If grid current is allowed to flow (adding more distortion, but giving slightly higher output power) the class is AB.\n\nClass-C amplifiers conduct less than 50% of the input signal and the distortion at the output is high, but high efficiencies (up to 90%) are possible. The usual application for class-C amplifiers is in RF transmitters operating at a single fixed carrier frequency, where the distortion is controlled by a tuned load on the amplifier. The input signal is used to switch the active device causing pulses of current to flow through a tuned circuit forming part of the load.\n\nThe class-C amplifier has two modes of operation: tuned and untuned. The diagram shows a waveform from a simple class-C circuit without the tuned load. This is called untuned operation, and the analysis of the waveforms shows the massive distortion that appears in the signal. When the proper load (e.g., an inductive-capacitive filter plus a load resistor) is used, two things happen. The first is that the output's bias level is clamped with the average output voltage equal to the supply voltage. This is why tuned operation is sometimes called a \"clamper\". This restores the waveform to its proper shape, despite the amplifier having only a one-polarity supply. This is directly related to the second phenomenon: the waveform on the center frequency becomes less distorted. The residual distortion is dependent upon the bandwidth of the tuned load, with the center frequency seeing very little distortion, but greater attenuation the farther from the tuned frequency that the signal gets.\n\nThe tuned circuit resonates at one frequency, the fixed carrier frequency, and so the unwanted frequencies are suppressed, and the wanted full signal (sine wave) is extracted by the tuned load. The signal bandwidth of the amplifier is limited by the Q-factor of the tuned circuit but this is not a serious limitation. Any residual harmonics can be removed using a further filter.\n\nIn practical class-C amplifiers a tuned load is invariably used. In one common arrangement the resistor shown in the circuit above is replaced with a parallel-tuned circuit consisting of an inductor and capacitor in parallel, whose components are chosen to resonate the frequency of the input signal. Power can be coupled to a load by transformer action with a secondary coil wound on the inductor. The average voltage at the collector is then equal to the supply voltage, and the signal voltage appearing across the tuned circuit varies from near zero to near twice the supply voltage during the RF cycle. The input circuit is biased so that the active element (e.g., transistor) conducts for only a fraction of the RF cycle, usually one third (120 degrees) or less.\n\nThe active element conducts only while the collector voltage is passing through its minimum. By this means, power dissipation in the active device is minimised, and efficiency increased. Ideally, the active element would pass only an instantaneous current pulse while the voltage across it is zero: it then dissipates no power and 100% efficiency is achieved. However practical devices have a limit to the peak current they can pass, and the pulse must therefore be widened, to around 120 degrees, to obtain a reasonable amount of power, and the efficiency is then 60–70%.\n\nIn the class-D amplifier the active devices (transistors) function as electronic switches instead of linear gain devices; they are either on or off. The analog signal is converted to a stream of pulses that represents the signal by pulse-width modulation, pulse-density modulation, delta-sigma modulation or a related modulation technique before being applied to the amplifier. The time average power value of the pulses is directly proportional to the analog signal, so after amplification the signal can be converted back to an analog signal by a passive low-pass filter.\nThe purpose of the output filter is to smooth the pulse stream to an analog signal, removing the high frequency spectral components of the pulses. The frequency of the output pulses is typically ten or more times the highest frequency in the input signal to amplify, so that the filter can adequately reduce the unwanted harmonics and accurately reproduce the input.\n\nThe main advantage of a class-D amplifier is power efficiency. Because the output pulses have a fixed amplitude, the switching elements (usually MOSFETs, but vacuum tubes, and at one time bipolar transistors, were used) are switched either completely on or completely off, rather than operated in linear mode. A MOSFET operates with the lowest resistance when fully on and thus (excluding when fully off) has the lowest power dissipation when in that condition. Compared to an equivalent class-AB device, a class-D amplifier's lower losses permit the use of a smaller heat sink for the MOSFETs while also reducing the amount of input power required, allowing for a lower-capacity power supply design. Therefore, class-D amplifiers are typically smaller than an equivalent class-AB amplifier.\n\nAnother advantage of the class-D amplifier is that it can operate from a digital signal source without requiring a digital-to-analog converter (DAC) to convert the signal to analog form first. If the signal source is in digital form, such as in a digital media player or computer sound card, the digital circuitry can convert the binary digital signal directly to a pulse-width modulation signal that is applied to the amplifier, simplifying the circuitry considerably.\n\nClass-D amplifiers are widely used to control motors—but are now also used as power amplifiers, with extra circuitry that converts analogue to a much higher frequency pulse width modulated signal. Switching power supplies have even been modified into crude class-D amplifiers (though typically these only reproduce low-frequencies with acceptable accuracy).\n\nHigh quality class-D audio power amplifiers have now appeared on the market. These designs have been said to rival traditional AB amplifiers in terms of quality. An early use of class-D amplifiers was high-power subwoofer amplifiers in cars. Because subwoofers are generally limited to a bandwidth of no higher than 150 Hz, switching speed for the amplifier does not have to be as high as for a full range amplifier, allowing simpler designs. Class-D amplifiers for driving subwoofers are relatively inexpensive in comparison to class-AB amplifiers.\n\nThe letter \"D\" used to designate this amplifier class is simply the next letter after \"C\" and, although occasionally used as such, does not stand for \"digital\". Class-D and class-E amplifiers are sometimes mistakenly described as \"digital\" because the output waveform superficially resembles a pulse-train of digital symbols, but a class-D amplifier merely converts an input waveform into a continuously pulse-width modulated analog signal. (A digital waveform would be pulse-code modulated.)\n\nThe class-E/F amplifier is a highly efficient switching power amplifier, typically used at such high frequencies that the switching time becomes comparable to the duty time. As said in the class-D amplifier, the transistor is connected via a serial LC circuit to the load,\nand connected via a large L (inductor) to the supply voltage. The supply voltage is connected to ground via a large capacitor to prevent any RF signals leaking into the supply. The class-E amplifier adds a C (capacitor) between the transistor and ground and uses a defined L to connect to the supply voltage.\n\nThe following description ignores DC, which can be added easily afterwards.\nThe above-mentioned C and L are in effect a parallel LC circuit to ground. When the transistor is on, it pushes through the serial LC circuit into the load and some current begins to flow to the parallel LC circuit to ground. Then the serial LC circuit swings back and compensates the current into the parallel LC circuit. At this point the current through the transistor is zero and it is switched off. Both LC circuits are now filled with energy in C and L. The whole circuit performs a damped oscillation. The damping by the load has been adjusted so that some time later the energy from the Ls is gone into the load, but the energy in both C peaks at the original value to in turn restore the original voltage so that the voltage across the transistor is zero again and it can be switched on.\n\nWith load, frequency, and duty cycle (0.5) as given parameters and the constraint that the voltage is not only restored, but peaks at the original voltage, the four parameters (L, L, C and C) are determined. The class-E amplifier takes the finite on resistance into account and tries to make the current touch the bottom at zero. This means that the voltage and the current at the transistor are symmetric with respect to time. The Fourier transform allows an elegant formulation to generate the complicated LC networks and says that the first harmonic is passed into the load, all even harmonics are shorted and all higher odd harmonics are open.\n\nClass E uses a significant amount of second-harmonic voltage. The second harmonic can be used to reduce the overlap with edges with finite sharpness. For this to work, energy on the second harmonic has to flow from the load into the transistor, and no source for this is visible in the circuit diagram. In reality, the impedance is mostly reactive and the only reason for it is that class E is a class F (see below) amplifier with a much simplified load network and thus has to deal with imperfections.\n\nIn many amateur simulations of class-E amplifiers, sharp current edges are assumed nullifying the very motivation for class E and measurements near the transit frequency of the transistors show very symmetric curves, which look much similar to class-F simulations.\n\nThe class-E amplifier was invented in 1972 by Nathan O. Sokal and Alan D. Sokal, and details were first published in 1975. Some earlier reports on this operating class have been published in Russian and Polish.\n\nIn push–pull amplifiers and in CMOS, the even harmonics of both transistors just cancel. Experiment shows that a square wave can be generated by those amplifiers. Theoretically square waves consist of odd harmonics only. In a class-D amplifier, the output filter blocks all harmonics; i.e., the harmonics see an open load. So even small currents in the harmonics suffice to generate a voltage square wave. The current is in phase with the voltage applied to the filter, but the voltage across the transistors is out of phase. Therefore, there is a minimal overlap between current through the transistors and voltage across the transistors. The sharper the edges, the lower the overlap.\n\nWhile in class D, transistors and the load exist as two separate modules, class F admits imperfections like the parasitics of the transistor and tries to optimise the global system to have a high impedance at the harmonics. Of course there must be a finite voltage across the transistor to push the current across the on-state resistance. Because the combined current through both transistors is mostly in the first harmonic, it looks like a sine. That means that in the middle of the square the maximum of current has to flow, so it may make sense to have a dip in the square or in other words to allow some overswing of the voltage square wave. A class-F load network by definition has to transmit below a cutoff frequency and reflect above.\n\nAny frequency lying below the cutoff and having its second harmonic above the cutoff can be amplified, that is an octave bandwidth. On the other hand, an inductive-capacitive series circuit with a large inductance and a tunable capacitance may be simpler to implement. By reducing the duty cycle below 0.5, the output amplitude can be modulated. The voltage square waveform degrades, but any overheating is compensated by the lower overall power flowing. Any load mismatch behind the filter can only act on the first harmonic current waveform, clearly only a purely resistive load makes sense, then the lower the resistance, the higher the current.\n\nClass F can be driven by sine or by a square wave, for a sine the input can be tuned by an inductor to increase gain. If class F is implemented with a single transistor, the filter is complicated to short the even harmonics. All previous designs use sharp edges to minimise the overlap.\n\nThere is a variety of amplifier designs that enhance class-AB output stages with more efficient techniques to achieve greater efficiency with low distortion. These designs are common in large audio amplifiers since the heatsinks and power transformers would be prohibitively large (and costly) without the efficiency increases. The terms \"class G\" and \"class H\" are used interchangeably to refer to different designs, varying in definition from one manufacturer or paper to another.\n\nClass-G amplifiers (which use \"rail switching\" to decrease power consumption and increase efficiency) are more efficient than class-AB amplifiers. These amplifiers provide several power rails at different voltages and switch between them as the signal output approaches each level. Thus, the amplifier increases efficiency by reducing the wasted power at the output transistors. Class-G amplifiers are more efficient than class AB but less efficient when compared to class D, however, they do not have the electromagnetic interference effects of class D.\n\nClass-H amplifiers take the idea of class G one step further creating an infinitely variable supply rail. This is done by modulating the supply rails so that the rails are only a few volts larger than the output signal at any given time. The output stage operates at its maximum efficiency all the time. Switched-mode power supplies can be used to create the tracking rails. Significant efficiency gains can be achieved but with the drawback of more complicated supply design and reduced THD performance. In common designs, a voltage drop of about 10V is maintained over the output transistors in Class H circuits. The picture above shows positive supply voltage of the output stage and the voltage at the speaker output. The boost of the supply voltage is shown for a real music signal.\n\nThe voltage signal shown is thus a larger version of the input, but has been changed in sign (inverted) by the amplification. Other arrangements of amplifying device are possible, but that given (that is, common emitter, common source or common cathode) is the easiest to understand and employ in practice. If the amplifying element is linear, the output is a faithful copy of the input, only larger and inverted. In practice, transistors are not linear, and the output only approximates the input. nonlinearity from any of several sources is the origin of distortion within an amplifier. The class of amplifier (A, B, AB or C) depends on how the amplifying device is biased. The diagrams omit the bias circuits for clarity.\n\nAny real amplifier is an imperfect realization of an ideal amplifier. An important limitation of a real amplifier is that the output it generates is ultimately limited by the power available from the power supply. An amplifier saturates and clips the output if the input signal becomes too large for the amplifier to reproduce or exceeds operational limits for the device.\n\nThe Doherty amplifier is a hybrid configuration. It was invented in 1934 by William H. Doherty for Bell Laboratories—whose sister company, Western Electric, manufactured radio transmitters. The Doherty amplifier consists of a class-B \"primary\" or \"carrier\" stages in parallel with a class-C \"auxiliary\" or \"peak\" stage. The input signal splits to drive the two amplifiers, and a combining network sums the two output signals. Phase shifting networks are used in inputs and outputs. During periods of low signal level, the class-B amplifier efficiently operates on the signal and the class-C amplifier is cutoff and consumes little power. During periods of high signal level, the class-B amplifier delivers its maximum power and the class-C amplifier delivers up to its maximum power. The efficiency of previous AM transmitter designs was proportional to modulation but, with average modulation typically around 20%, transmitters were limited to less than 50% efficiency. In Doherty's design, even with zero modulation, a transmitter could achieve at least 60% efficiency.\n\nAs a successor to Western Electric for broadcast transmitters, the Doherty concept was considerably refined by Continental Electronics Manufacturing Company of Dallas, TX. Perhaps, the ultimate refinement was the screen-grid modulation scheme invented by Joseph B. Sainton. The Sainton amplifier consists of a class-C primary or carrier stage in parallel with a class-C auxiliary or peak stage. The stages are split and combined through 90-degree phase shifting networks as in the Doherty amplifier. The unmodulated radio frequency carrier is applied to the control grids of both tubes. Carrier modulation is applied to the screen grids of both tubes. The bias point of the carrier and peak tubes is different, and is established such that the peak tube is cutoff when modulation is absent (and the amplifier is producing rated unmodulated carrier power) whereas both tubes contribute twice the rated carrier power during 100% modulation (as four times the carrier power is required to achieve 100% modulation). As both tubes operate in class C, a significant improvement in efficiency is thereby achieved in the final stage. In addition, as the tetrode carrier and peak tubes require very little drive power, a significant improvement in efficiency within the driver stage is achieved as well (317C, et al.). The released version of the Sainton amplifier employs a cathode-follower modulator, not a push–pull modulator. Previous Continental Electronics designs, by James O. Weldon and others, retained most of the characteristics of the Doherty amplifier but added screen-grid modulation of the driver (317B, et al.).\n\nThe Doherty amplifier remains in use in very-high-power AM transmitters, but for lower-power AM transmitters, vacuum-tube amplifiers in general were eclipsed in the 1980s by arrays of solid-state amplifiers, which could be switched on and off with much finer granularity in response to the requirements of the input audio. However, interest in the Doherty configuration has been revived by cellular-telephone and wireless-Internet applications where the sum of several constant envelope users creates an aggregate AM result. The main challenge of the Doherty amplifier for digital transmission modes is in aligning the two stages and getting the class-C amplifier to turn on and off very quickly.\n\nRecently, Doherty amplifiers have found widespread use in cellular base station transmitters for GHz frequencies. Implementations for transmitters in mobile devices have also been demonstrated.\n\nAmplifiers are implemented using active elements of different kinds:\n\nFor special purposes, other active elements have been used. For example, in the early days of the satellite communication, parametric amplifiers were used. The core circuit was a diode whose capacitance was changed by an RF signal created locally. Under certain conditions, this RF signal provided energy that was modulated by the extremely weak satellite signal received at the earth station.\n\nThe practical amplifier circuit to the right could be the basis for a moderate-power audio amplifier. It features a typical (though substantially simplified) design as found in modern amplifiers, with a class-AB push–pull output stage, and uses some overall negative feedback. Bipolar transistors are shown, but this design would also be realizable with FETs or valves.\n\nThe input signal is coupled through capacitor C1 to the base of transistor Q1. The capacitor allows the AC signal to pass, but blocks the DC bias voltage established by resistors R1 and R2 so that any preceding circuit is not affected by it. Q1 and Q2 form a differential amplifier (an amplifier that multiplies the difference between two inputs by some constant), in an arrangement known as a long-tailed pair. This arrangement is used to conveniently allow the use of negative feedback, which is fed from the output to Q2 via R7 and R8.\n\nThe negative feedback into the difference amplifier allows the amplifier to compare the input to the actual output. The amplified signal from Q1 is directly fed to the second stage, Q3, which is a common emitter stage that provides further amplification of the signal and the DC bias for the output stages, Q4 and Q5. R6 provides the load for Q3 (a better design would probably use some form of active load here, such as a constant-current sink). So far, all of the amplifier is operating in class A. The output pair are arranged in class-AB push–pull, also called a complementary pair. They provide the majority of the current amplification (while consuming low quiescent current) and directly drive the load, connected via DC-blocking capacitor C2. The diodes D1 and D2 provide a small amount of constant voltage bias for the output pair, just biasing them into the conducting state so that crossover distortion is minimized. That is, the diodes push the output stage firmly into class-AB mode (assuming that the base-emitter drop of the output transistors is reduced by heat dissipation).\n\nThis design is simple, but a good basis for a practical design because it automatically stabilises its operating point, since feedback internally operates from DC up through the audio range and beyond. Further circuit elements would probably be found in a real design that would roll-off the frequency response above the needed range to prevent the possibility of unwanted oscillation. Also, the use of fixed diode bias as shown here can cause problems if the diodes are not both electrically and thermally matched to the output transistors if the output transistors turn on too much, they can easily overheat and destroy themselves, as the full current from the power supply is not limited at this stage.\n\nA common solution to help stabilise the output devices is to include some emitter resistors, typically one ohm or so. Calculating the values of the circuit's resistors and capacitors is done based on the components employed and the intended use of the amp.\n\nTwo most common circuits:\n\nFor the basics of radio frequency amplifiers using valves, see Valved RF amplifiers.\n\nReal world amplifiers are imperfect.\n\nDifferent power supply types result in many different methods of bias. Bias is a technique by which active devices are set to operate in a particular region, or by which the DC component of the output signal is set to the midpoint between the maximum voltages available from the power supply. Most amplifiers use several devices at each stage; they are typically matched in specifications except for polarity. Matched inverted polarity devices are called complementary pairs. Class-A amplifiers generally use only one device, unless the power supply is set to provide both positive and negative voltages, in which case a dual device symmetrical design may be used. Class-C amplifiers, by definition, use a single polarity supply.\n\nAmplifiers often have multiple stages in cascade to increase gain. Each stage of these designs may be a different type of amp to suit the needs of that stage. For instance, the first stage might be a class-A stage, feeding a class-AB push–pull second stage, which then drives a class-G final output stage, taking advantage of the strengths of each type, while minimizing their weaknesses.\n\n", "id": "9931", "title": "Amplifier"}
{"url": "https://en.wikipedia.org/wiki?curid=9932", "text": "Escort carrier\n\nThe escort carrier or escort aircraft carrier (hull classification symbol CVE), also called a \"jeep carrier\" or \"baby flattop\" in the United States Navy (USN) or \"Woolworth Carrier\" by the Royal Navy, was a small and slow type of aircraft carrier used by the Royal Navy, the Imperial Japanese Navy and Imperial Japanese Army Air Force, and the United States Navy in World War II. They were typically half the length and a third the displacement of larger fleet carriers. While they were slower, carried fewer planes and were less well armed and armored, escort carriers were cheaper and could be built quickly, which was their principal advantage. Escort carriers could be completed in greater numbers as a stop-gap when fleet carriers were scarce. However, the lack of protection made escort carriers particularly vulnerable and several were sunk with great loss of life. The light carrier (hull classification symbol CVL) was a similar concept to escort carriers in most respects, but were capable of higher speeds to allow operation alongside fleet carriers.\n\nMost often built on a commercial ship hull, escort carriers were too slow to keep up with the main forces consisting of fleet carriers, battleships, and cruisers. Instead, they were used to escort convoys, defending them from enemy threats such as submarines and planes. In the invasions of mainland Europe and Pacific islands, escort carriers provided air support to ground forces during amphibious operations. Escort carriers also served as backup aircraft transports for fleet carriers and ferried aircraft of all military services to points of delivery.\n\nIn the Battle of the Atlantic, escort carriers were used to protect convoys against U-boats. Initially escort carriers accompanied the merchant ships and helped to fend off attacks from aircraft and submarines. As numbers increased later in the war, escort carriers also formed part of hunter-killer groups that sought out submarines instead of being attached to a particular convoy.\n\nIn the Pacific theater, CVEs provided air support of ground troops in the Battle of Leyte Gulf. They lacked the speed and weapons to counter enemy fleets, relying on the protection of a Fast Carrier Task Force. However, at the Battle off Samar, one U.S. task force of escort carriers managed to successfully defend itself against a much larger Japanese force of battleships and cruisers. The Japanese met a furious defense of carrier aircraft, screening destroyers, and destroyer escorts, proving that CVEs could appear to have the same striking power as full CVs.\n\nOf the 151 aircraft carriers built in the U.S. during World War II, 122 were escort carriers. Though no examples survive to this day, the \"Casablanca\" class was the most numerous class of aircraft carrier, with 50 launched. Second was the \"Bogue\" class, with 45 launched.\n\nIn the early 1920s, the Washington Naval Treaty imposed limits on the maximum size and total tonnage of aircraft carriers for the five main naval powers. Later treaties largely kept these provisions. As a result, construction between the World Wars had been insufficient to meet operational needs for aircraft carriers as World War II expanded from Europe. Too few fleet carriers were available to simultaneously transport aircraft to distant bases, support amphibious invasions, offer carrier landing training for replacement pilots, conduct anti-submarine patrols, and provide defensive air cover for deployed battleships and cruisers. The foregoing mission requirements limited use of fleet carriers′ unique offensive strike capability demonstrated at the Battle of Taranto and the Attack on Pearl Harbor. Conversion of existing ships (and hulls under construction for other purposes) provided additional aircraft carriers until new construction became available.\n\nConversions of cruisers and passenger liners with speed similar to fleet carriers were identified by the U.S. as \"light aircraft carriers\" (hull classification symbol CVL) able to operate at battle fleet speeds. Slower conversions were classified as \"escort carriers\" and were considered naval auxiliaries suitable for pilot training and transport of aircraft to distant bases.\n\nThe Royal Navy had recognized a need for carriers to defend its trade routes in the 1930s. While designs had been prepared for \"trade protection carriers\" and five suitable liners identified for conversion, nothing further was done mostly because there were insufficient aircraft for even the fleet carriers under construction at the time. However, by 1940 the need had become urgent and was converted from the captured German merchant ship MV \"Hannover\" and commissioned in July 1941. For defense from German aircraft, convoys were supplied first with fighter catapult ships and CAM ships that could carry a single (disposable) fighter. In the interim, before escort carriers could be supplied, they also brought in merchant aircraft carriers that could operate four aircraft.\n\nIn 1940, Admiral William Halsey recommended construction of naval auxiliaries for pilot training. In early 1941 the British asked the US to build on their behalf six carriers of an improved \"Audacity\" design but the US had already begun their own escort carrier. On 1 February 1941, the United States Chief of Naval Operations gave priority to construction of naval auxiliaries for aircraft transport. U.S. ships built to meet these needs were initially referred to as auxiliary aircraft escort vessels (AVG) in February 1942 and then auxiliary aircraft carrier (ACV) on 5 August 1942. The first U.S. example of the type was . Operation Torch and North Atlantic anti-submarine warfare proved these ships capable aircraft carriers for ship formations moving at the speed of trade or amphibious invasion convoys. U.S. classification revision to escort aircraft carrier (CVE) on 15 July 1943 reflected upgraded status from auxiliary to combatant. They were informally known as \"Jeep carriers\" or \"baby flattops\". It was quickly found that the escort carriers had better performance than light carriers, which tended to pitch badly in moderate to high seas. The \"Commencement Bay\" class was designed to incorporate the best features of American CVLs on a more stable hull with a less expensive propulsion system.\n\nAmong their crews, CVE was sarcastically said to stand for \"Combustible, Vulnerable, and Expendable\". Magazine protection was minimal in comparison to fleet aircraft carriers. was sunk within minutes by a single torpedo, and exploded from undetermined causes with very heavy loss of life. Three escort carriers—, and —were destroyed by \"kamikaze\"s, the largest ships to meet such a fate.\n\nAllied escort carriers were typically around long, not much more than half the length of the almost fleet carriers of the same era, but were less than 1/3 of the weight. A typical escort carrier displaced about , as compared to almost for a full-size fleet carrier. The aircraft hangar typically ran only 1/3 of the way under the flight deck and housed a combination of 24-30 fighters and bombers organized into one single \"composite squadron\". By comparison, a late \"Essex\"-class fleet carrier could carry a total of 103 aircraft organized into separate fighter, bomber and torpedo-bomber squadrons.\n\nThe island on these ships was small and cramped, and located well forward of the funnels (unlike on a normal-sized carrier where the funnels were integrated into the island). Although the first escort carriers had only one aircraft elevator, having two elevators (one fore and one aft) quickly, along with the single aircraft catapult, became standard. The carriers employed the same system of arresting cables and tail hooks as on the big carriers, and procedures for launch and recovery were the same as well.\n\nThe crew size was less than of that of a large carrier, but this was still a bigger complement than most naval vessels. It was large enough to justify the existence of facilities such as a permanent canteen or snack bar, called a gedunk bar, in addition to the mess. The bar was open for longer hours than the mess and sold several flavors of ice cream, along with cigarettes and other consumables. There were also several vending machines available on board.\n\nIn all, 130 Allied escort carriers were launched or converted during the war. Of these, six were British conversions of merchant ships: , , , , and . The remaining escort carriers were U.S.-built. Like the British, the first U.S. escort carriers were converted merchant vessels (or in the , converted military oilers). The \"Bogue\" class carriers were based on the hull of the Type C3 cargo ship. The last 69 escort carriers of the and classes were purpose-designed and purpose-built carriers drawing on the experience gained with the previous classes.\n\nOriginally developed at the behest of the United Kingdom to operate as part of a North Atlantic convoy escort, rather than as part of a naval strike force, many of the escort carriers produced were assigned to the Royal Navy for the duration of the war under the Lend-Lease act. They supplemented and then replaced the converted merchant aircraft carriers that were put into service by the British and Dutch as an emergency measure until the escort carriers became available. As convoy escorts, they were used by the Royal Navy to provide air scouting, to ward off enemy long-range scouting aircraft and, increasingly, to spot and hunt submarines. Often additional escort carriers also joined convoys, not as fighting ships, but as transporters, ferrying aircraft from the U.S. to Britain. In this case, the aircraft cargo could be doubled by storing aircraft on the flight deck as well as in the hangar.\n\nThe ships sent to the Royal Navy were slightly modified, partly to suit the traditions of that service. Among other things the ice cream making machines were removed, since they were considered unnecessary luxuries on ships, which served grog and other alcoholic beverages. The heavy duty washing machines of the laundry room were also removed since \"all a British sailor needs to keep clean is a bucket and a bar of soap\" (quoted from Warrilow).\n\nOther modifications were due to the need for a completely enclosed hangar when operating in the North Atlantic and in support of the Arctic convoys.\n\nMeanwhile, the U.S. discovered their own use for the escort carriers. In the North Atlantic, they supplemented the escorting destroyers by providing air support for anti-submarine warfare. One of these escort carriers, , was instrumental in the capture of off North Africa in 1944.\n\nIn the Pacific theater, escort carriers lacked the speed to sail with fast carrier attack groups, so were often tasked to escort the landing ships and troop carriers during the island-hopping campaign. In this role they provided air cover for the troopships and flew the first wave of attacks on beach fortifications in amphibious landing operations. On occasion, they even escorted the large carriers, serving as emergency airstrips and providing fighter cover for their larger sisters while these were busy readying or refueling their own planes. They also transported aircraft and spare parts from the U.S. to remote island airstrips.\n\nThe Attack on Pearl Harbor brought up an urgent need for aircraft carriers, so some T3 tankers were converted to Escort carrier, is example of how a T3 tanker hull AO-33 was rebuilt to be an escort carrier. The T3 tanker size and speed made the T3 a useful escort carrier. There were two classes of T3 hull carriers: \"Sangamon\" class and \"Commencement Bay\" class.\n\nA major battle for these escort carriers was the Battle off Samar in the Philippines on 25 October 1944. The Japanese lured Admiral William Halsey, Jr. into chasing a decoy fleet with his 3rd Fleet. This left aircraft from 16 small and slow escort carriers in three task groups armed primarily to bomb ground forces; along with their protective screen of destroyers and slower destroyer escorts, with \"Taffy 3\" bearing the brunt of the fight. They faced a Japanese force of four battleships, including the giant , eight cruisers, and 11 destroyers. The American ships held off the Japanese.\n\nThe slow carriers could not outrun cruisers. They launched their aircraft and maneuvered to avoid shellfire for over an hour. They took dozens of hits, mostly from armor-piercing rounds that passed right through their thin, unarmored hulls without exploding. , sunk in this action, was the only U.S. carrier lost to gunfire in the war, and the Japanese concentration of fire on this one carrier assisted the escape of the others. The carriers′ only substantial armament—aside from their aircraft—was a single 5 in (127 mm) dual-purpose gun mounted on the stern, but the pursuing Japanese cruisers closed to within range of these guns. One of the guns caused critical damage to the burning Japanese heavy cruiser and a subsequent bomb dropped from one of the task force′s aircraft hit the heavy cruiser′s forward machinery room, leaving her dead in the water. Several \"kamikaze\" aircraft were shot down by carrier gunners, with only lost to air attack. The Americans lost a similar number of ships and men to the Battle of Coral Sea and Battle of Midway combined.\n\nMany escort carriers were Lend-Leased to the United Kingdom, this list specifies the breakdown in service to each navy.\n\n\nIn addition, six escort carriers were produced by the British during the war (all converted from other vessels).\n\nThe table below lists escort carriers and similar ships performing the same missions. The first four were built as early fleet aircraft carriers. Merchant aircraft carriers (MAC) carried trade cargo in addition to operating aircraft. Aircraft transports carried larger numbers of planes by eliminating accommodation for operating personnel and storage of fuel and ammunition.\n\nThe years following World War II brought many revolutionary new technologies to the navy, most notably the helicopter and the jet fighter, and with this a complete rethinking of its strategies and ships′ tasks. Although several of the latest \"Commencement Bay\"-class CVE were deployed as floating airfields during the Korean War, the main reasons for the development of the escort carrier had disappeared or could be dealt with better by newer weapons. The emergence of the helicopter meant that helicopter-deck equipped frigates could now take over the CVE's role in a convoy while also performing their own traditional role as submarine hunters. Ship-mounted guided missile launchers took over much of the aircraft protection role, and in-flight refueling abolished the need for floating stopover points for transport or patrol aircraft. As a result, after the \"Commencement Bay\" class, no new escort carriers were designed, and with every downsizing of the navy, the CVEs were the first to be mothballed.\n\nSeveral escort carriers were pressed back into service during the first years of the Vietnam War because of their ability to carry large numbers of aircraft. Redesignated AKV (air transport auxiliary), they were manned by a civilian crew and used to ferry whole aircraft and spare parts from the U.S. to Army, Air Force and Marine bases in South Vietnam. However, CVEs were only useful in this role for a limited period. Once all major aircraft were equipped with refueling probes, instead of shipping a plane overseas to its pilot, it became much easier to fly the aircraft directly to its base.\n\nThe last chapter in the saga of the escort carriers consisted of two conversions: As an experiment, was converted from an aircraft carrier into a pure helicopter carrier (CVHA-1) and used by the Marine Corps to carry assault helicopters for the first wave of amphibious warfare operations. Later, \"Thetis Bay\" became a full amphibious assault ship (LHP-6). Although in service only from 1955 (the year of her conversion) to 1964, the experience gained in her training exercises greatly influenced the design of today′s amphibious assault ships.\n\nIn the second conversion, in 1961, had all her aircraft handling equipment removed and four tall radio antennas installed on her long, flat deck. In lieu of aircraft, the hangar deck now had no less than 24 military radio transmitter trucks bolted to its floor. Rechristened , the ship was used as a communication relay ship and served dutifully through the Vietnam War as a floating radio station, relaying transmissions between the forces on the ground and the command centers back home. Like \"Thetis Bay\", the experience gained before \"Annapolis\" was stricken in 1976 helped develop today′s purpose-built amphibious command ships of the .\n\nUnlike almost all other major classes of ships and patrol boats from World War II, most of which can be found in a museum or port, no escort carrier or American light carrier has survived: all were destroyed during the war or broken up in the following decades. The Dictionary of American Naval Fighting Ships records that the last former escort carrier remaining in naval service — USS \"Annapolis\" (ex- USS \"Gilbert Islands\") — was sold for scrapping 19 December 1979. The last American light carrier (the escort carrier′s faster sister type) was , which was broken up in 2002 after a decade-long attempt to preserve the vessel.\n\nThe U.S. designed the Sea Control Ship to serve a similar role; whilst none were actually built, the Spanish \"Principe de Asturias\" and the Thai \"Chakri Naruebet\" are based on the concept.\n\n\nFor complete lists see:\n\n\n", "id": "9932", "title": "Escort carrier"}
{"url": "https://en.wikipedia.org/wiki?curid=9933", "text": "Extreme sport\n\nExtreme sports is a popular term for certain activities perceived as involving a high degree of risk. These activities often involve speed, height, a high level of physical exertion, and highly specialized gear.\n\nThe definition of an extreme sport is not exact and the origin of the term is unclear, but it gained popularity in the 1990s when it was picked up by marketing companies to promote the X Games and when the Extreme Sports Channel and Extreme.com launched. More recently, the commonly used definition from research is \"a competitive (comparison or self-evaluative) activity within which the participant is subjected to natural or unusual physical and mental challenges such as speed, height, depth or natural forces and where fast and accurate cognitive perceptual processing may be required for a successful outcome\" by Dr. Rhonda Cohen (2012).\n\nWhile use of the term \"extreme sport\" has spread far and wide to describe a multitude of different activities, exactly which sports are considered 'extreme' is debatable. There are, however, several characteristics common to most extreme sports. While not the exclusive domain of youth, extreme sports tend to have a younger-than-average target demographic. Extreme sports are rarely sanctioned by schools. Extreme sports tend to be more solitary than traditional sports (rafting and paintballing are notable exceptions, as they are done in teams). In addition, beginning extreme athletes tend to work on their craft without the guidance of a coach (though some may hire a coach later).\n\nActivities categorized by media as extreme sports differ from traditional sports due to the higher number of inherently uncontrollable variables. These environmental variables are frequently weather and terrain related, including wind, snow, water and mountains. Because these natural phenomena cannot be controlled, they inevitably affect the outcome of the given activity or event.\n\nIn a traditional sporting event, athletes compete against each other under controlled circumstances. While it is possible to create a controlled sporting event such as X Games, there are environmental variables that cannot be held constant for all athletes. Examples include changing snow conditions for snowboarders, rock and ice quality for climbers, and wave height and shape for surfers.\n\nWhilst traditional sporting judgment criteria may be adopted when assessing performance (distance, time, score, etc.), extreme sports performers are often evaluated on more subjective and aesthetic criteria. This results in a tendency to reject unified judging methods, with different sports employing their own ideals and indeed having the ability to evolve their assessment standards with new trends or developments in the sports.\n\nWhile the exact definition and what is included as extreme sport is debatable, some attempted to make classification for extreme sports.\n\nOne argument is that to qualify as an \"extreme sport\" both expression terms need to be fulfilled;\n\nAlong this definition, an activity such as bungee jumping may not qualify as no skill or physical ability is required to execute a good jump (i.e., avoid poor execution). A passenger in a canyon jet boat ride will not fulfill the requirements, as the skill required pertains to the pilot, not the passengers. \"Thrill seeking\" might in these cases be a more suitable qualification than \"extreme sport\".\n\nExtreme sports may be subdivided into:\n\nThese sports require the use of snow, ice or water) sports (\"sports de glisse\" in French) and rolling sports. Another subdivision can be made along motorized and non motorized vehicle sports, resulting in the following matrix;\n\nNo vehicle is required (rock climbing, canyoning, ice climbing, parkour, psicobloc etc.)\n\nThe origin of the divergence of the term \"extreme sports\" from \"sports\" may date to the 1950s in the appearance of a phrase usually, but wrongly, attributed to Ernest Hemingway. The phrase is;\nThere are only three sports: bullfighting, motor racing, and mountaineering; all the rest are merely games.\nThe implication of the phrase was that the word \"sport\" defined an activity in which one might be killed. The other activities being termed \"games\". The phrase may have been invented by either writer Barnaby Conrad or automotive author Ken Purdy.\n\nThe Dangerous Sports Club of Oxford University, England was founded by David Kirke, Chris Baker, Ed Hulton and Alan Weston. They first came to wide public attention by inventing modern day bungee jumping, by making the first modern jumps on 1 April 1979, from the Clifton Suspension Bridge, Bristol, England. They followed the Clifton Bridge effort with a jump from the Golden Gate Bridge in San Francisco, California (including the first female bungee jump by Jane Wilmot), and with a televised leap from the Royal Gorge Suspension Bridge in Colorado, sponsored by and televised on the popular American television program \"That's Incredible!\" Bungee jumping was treated as a novelty for a few years, then became a craze for young people, and is now an established industry for thrill seekers.\nThe Club also pioneered a surrealist form of skiing, holding three events at St. Moritz, Switzerland, in which competitors were required to devise a sculpture mounted on skis and ride it down a mountain. The event reached its limits when the Club arrived in St. Moritz with a London double-decker bus, wanting to send it down the ski slopes, and the Swiss resort managers refused.\n\nOther Club activities included expedition hang gliding from active volcanoes; the launching of giant (60 ft) plastic spheres with pilots suspended in the centre (zorbing); microlight flying; and BASE jumping (in the early days of this sport).\n\nIn recent decades the term \"extreme sport\" was further promoted after the Extreme Sports Channel, Extreme.com launched and then the X Games, a multi-sport event was created and developed by ESPN. The first X Games (known as 1995 Extreme Games) were held in Newport, Providence, Mount Snow, and Vermont in the United States.\n\nCertain extreme sports clearly trace back to other extreme sports, or combinations thereof. For example, windsurfing was conceived as a result of efforts to equip a surfboard with a sailing boat's propulsion system (mast and sail). Kitesurfing on the other hand was conceived by combining the propulsion system of kite buggying (a parafoil) with the bi-directional boards used for wakeboarding. Wakeboarding is in turn derived from snowboarding and waterskiing.\n\nSome contend that the distinction between an extreme sport and a conventional one has as much to do with marketing as with the level of danger involved or the adrenaline generated. For example, rugby union is both dangerous and adrenaline-inducing but is not considered an extreme sport due to its traditional image, and because it does not involve high speed or an intention to perform stunts (the aesthetic criteria mentioned above) and also it does not have changing environmental variables for the athletes. Demolition derby racing, predominantly an adult sport, is not thought of as 'extreme' while BMX racing, a youth sport, is.\n\nOne common aspect of an extreme sport is a counter-cultural aura — a rejection of authority and of the status quo by disaffected youth. Some youth of Generation Y have seized upon activities which they can claim as their own, and have begun rejecting more traditional sports in increasing numbers.\n\nA feature of such activities in the view of some is their alleged capacity to induce an adrenaline rush in participants. However, the medical view is that the rush or high associated with the activity is not due to adrenaline being released as a response to fear, but due to increased levels of dopamine, endorphins and serotonin because of the high level of physical exertion. Furthermore, a recent study suggests that the link to adrenaline and 'true' extreme sports is tentative. The study defined 'true' extreme sports as a leisure or recreation activity where the most likely outcome of a mismanaged accident or mistake was death. This definition was designed to separate the marketing hype from the activity.\n\nEric Brymer also found that the potential of various extraordinary human experiences, many of which parallel those found in activities such as meditation, was an important part of the extreme sport experience. Those experiences put the participants outside their comfort zone and are often done in conjunction with adventure travel.\n\nSome of the sports have existed for decades and their proponents span generations, some going on to become well known personalities. Rock climbing and ice climbing have spawned publicly recognizable names such as Edmund Hillary, Chris Bonington, Wolfgang Güllich and more recently Joe Simpson. Another example is surfing, invented centuries ago by the inhabitants of Hawaii.\n\nExtreme sports by their nature can be extremely dangerous, conducive to fatalities, near-fatalities and other serious injuries, and sometimes consist in treading along the brink of death. This imminent and inherent danger in these sports has been considered a somewhat necessary part of its appeal, which is partially a result of pressure for athletes to make more money and provide maximum entertainment.\n\nMany persons with various physical disabilities participate in extreme sports. Nonprofit organizations such as Adaptive Action Sports seek to increase awareness of the participation in action sports by members of the disabled community, as well as increase access to the adaptive technologies that make participation possible and to competitions such as The X Games.\n\n", "id": "9933", "title": "Extreme sport"}
{"url": "https://en.wikipedia.org/wiki?curid=9935", "text": "Eadgyth\n\nEdith of England, also spelt Eadgyth or Ædgyth (, ; 910 – 26 January 946), a member of the House of Wessex, was German queen from 936 until her death, by her marriage with King Otto I.\n\nEdith was born to the reigning English king Edward the Elder by his second wife, Ælfflæd, and hence was a granddaughter of King Alfred the Great. In 919 her sister Eadgifu married the West Frankish king Charles the Simple. Nothing is known of Edith until at the request of the East Frankish king Henry the Fowler, who wished to stake a claim to equality and to seal the alliance between the two Saxon kingdoms, her half-brother King Æthelstan sent his sisters Edith and Edgiva to Germany. Henry's eldest son and heir to the throne Otto was instructed to choose whichever one pleased him best. Otto chose Edith, according to Hrotsvitha of Gandersheim a woman \"of pure noble countenance, graceful character and truly royal appearance\", and married her in 930. The remaining sister Edgiva was married to a \"king near the Jupiter mountains\" (the Alps), probably to Louis, brother of King Rudolph II of Burgundy. The precise identity of the husband of this sister is debated.\n\nIn 936 Henry the Fowler died and his eldest son Otto, Edith's husband, was crowned king at Aachen Cathedral. A surviving report of the ceremony by the medieval cronicler Widukind of Corvey makes no mention of his wife having been crowned at this point, but according to Bishop Thietmar of Merseburg's chronicle, Eadgyth was nevertheless anointed as queen, albeit in a separate ceremony. \n\nAs queen consort, Edith undertook the usual state duties of a \"First Lady\": when she turns up in the records it is generally in connection with gifts to the state's favoured monasteries or memorials to holy women and saints. In this respect she seems to have been more diligent than her now widowed and subsequently sainted mother-in-law, Queen Matilda, whose own charitable activities only achieve a single recorded mention from the period of Eadgyth's time as queen. There was probably rivalry between the Benedictine founded at Magdeburg by Otto and Eadgyth in 937, a year after coming to the throne, and Matilda's foundation Quedlinburg Abbey, intended by her as a memorial to her husband, the late King Henry. Edith accompanied her husband on his travels, though not during battles. While Otto fought against the rebellious dukes Eberhard of Franconia and Gilbert of Lorraine in 939, she spent the hostilities at Lorsch Abbey.\n\nLike her brother, Æthelstan, Edith was devoted to the cult of their ancestor Saint Oswald of Northumbria and was instrumental in introducing this cult into Germany after her marriage to the emperor. Her lasting influence may have caused certain monasteries and churches in the Duchy of Saxony to be dedicated to this saint.\n\nEadgyth's death at a relatively young age, in her early thirties, was unexpected. Otto apparently mourned the loss of a loved spouse he greatly esteemed. He secondly married Adelaide of Italy in 951.\n\nEdith and Otto's children were:\nboth buried in St. Alban's Abbey, Mainz.\n\nInitially buried in the St Maurice monastery, Edith's tomb since the 16th century has been located in Magdeburg Cathedral. Long regarded a cenotaph, a lead coffin inside a stone sarcophagus with her name on it was found and opened in 2008 by archaeologists during work on the building. An inscription recorded that it was the body of Eadgyth, reburied in 1510. The fragmented and incomplete bones were examined in 2009, then brought to Bristol, England, for tests in 2010. Professor Mark Horton of Bristol University said that \"this may prove to be the oldest complete remains of an English royal.\" \n\nThe investigations at Bristol, applying isotope tests on tooth enamel, checked whether she was born and brought up in Wessex and Mercia, as written history has indicated. Testing on the bones revealed that they are the remains of Eadgyth, from study made of the enamel of the teeth in her upper jaw. Testing of the enamel revealed that the individual entombed at Magdeburg had spent time as a youth in the chalky uplands of Wessex. \"Tests on these isotopes can give a precise record of where the person lived up to the age of 14,\" noted \"The Times\" of London in its story on the testing. \"In this case they showed that the woman in the casket had spent the first years of her life drinking water that came from springs on the chalk hills of southern England. This matched exactly the historical records of Eadgyth’s early life.\" The bones \"are the oldest surviving remains of an English royal burial,\" Bristol University announced in a press release.\n\nFollowing the tests the bones were solemnly re-interred in a new titanium coffin in her tomb at Magdeburg Cathedral on 22 October 2010.\n\n\n\n \n", "id": "9935", "title": "Eadgyth"}
{"url": "https://en.wikipedia.org/wiki?curid=9937", "text": "Kingdom of Essex\n\nThe kingdom of the East Saxons (; ), today referred to as the Kingdom of Essex, was one of the seven traditional kingdoms of the Anglo-Saxon Heptarchy. It was founded in the 6th century and covered the territory later occupied by the counties of Essex, Hertfordshire, Middlesex and (for a short while) Kent. Kings of Essex were frequently subservient to foreign overlords. The last king of Essex was Sigered and in 825, he ceded the kingdom to Egbert of Wessex.\n\nThe kingdom was bounded to the north by the River Stour and the Kingdom of East Anglia, to the south by the River Thames and Kent, to the east lay the North Sea and to the west Mercia. The territory included the remains of two provincial Roman capitals, Colchester and London. The early kingdom included the land of the Middle Saxons, later Middlesex, most if not all of Hertfordshire and may at times have included Surrey. For a brief period in the 8th century, the Kingdom of Essex controlled what is now Kent.\n\nThe modern English county of Essex maintains the historic northern and the southern borders, but only covers the territory east of the River Lea, the other parts being lost to neighbouring Mercia during the 8th century.\n\nIn the Tribal Hidage it is listed as containing 7000 hides.\n\nAlthough the kingdom of Essex was one of the kingdoms of the Heptarchy, its history is not well documented. It produced relatively few Anglo-Saxon charters and no version of the \"Anglo-Saxon Chronicle\"; in fact the only mention in the chronicle concerns Bishop Mellitus. As a result, the kingdom is regarded as comparatively obscure. For most of the kingdom's existence, the Essex king was subservient to an overlord - variously the kings of Kent, Anglia or Mercia.\n\nSaxon occupation of land that was to form the kingdom had begun by the early 5th century at Mucking and other locations. A large proportion of these original settlers came from Old Saxony. According to British legend (see: Historia Brittonum) the territory known later as Essex was ceded by the Britons to the Saxons following the infamous Treachery of the Long Knives, which occurred ca. 460 during the reign of High King Vortigern. Della Hooke relates the territory ruled by the kings of Essex to the pre-Roman territory of the Trinovantes.\n\nThe kingdom of Essex grew by the absorption of smaller subkingdoms or Saxon tribal groups. There are a number of suggestions for the location of these subkingdoms including:\n\nEssex emerged as a single kingdom during the 6th century. The dates, names and achievements of the Essex kings, like those of most early rulers in the Heptarchy, remain conjectural. The historical identification of the kings of Essex, including the evidence and a reconstructed genealogy are discussed extensively by Yorke. The dynasty claimed descent from Woden via Seaxnēat. A genealogy of the Essex royal house was prepared in Wessex in the 9th century. Unfortunately the surviving copy is somewhat mutilated. At times during the history of the kingdom several sub-kings within Essex appear to have been able to rule simultaneously. They may have exercised authority over different parts of the kingdom. The first recorded king, according to the East Saxon King List, was Æscwine of Essex, to which a date of 527 is given for the start of his reign, although there are some difficulties with the date of his reign, and Sledd of Essex is listed as the founder of the Essex royal house by other sources.\n\nThe Essex kings issued coins that echoed those issued by Cunobeline simultaneously asserting a link to the first century rulers while emphasising independence from Mercia.\n\nThe tomb of Sæbbi of Essex (r.664-683) was visible in Old St Paul's Cathedral until the Great Fire of London of 1666 when the cathedral and the tombs within it were lost.\n\nThe earliest English record of the kingdom dates to Bede's \"Historia ecclesiastica gentis Anglorum\", which noted the arrival of Bishop (later Saint) Mellitus in London in 604. Æthelberht (King of Kent and overlord of southern England according to Bede) was in a position to exercise some authority in Essex shortly after 604, when his intervention helped in the conversion of King Saebert of Essex (son of Sledd), his nephew, to Christianity. It was Æthelberht, and not Sæberht, who built and endowed St. Pauls in London, where St. Paul’s Cathedral now stands. Bede describes Æthelberht as Sæberht’s overlord. After the death of Saebert in AD 616, Mellitus was driven out and the kingdom reverted to paganism. This may have been the result of opposition to Kentish influence in Essex affairs rather than being specifically anti-Christian.\n\nThe kingdom reconverted to Christianity under Sigeberht II the Good following a mission by St Cedd who established monasteries at \"Tilaburg\" (probably East Tilbury, but possibly West Tilbury) and \"Ithancester\" (almost certainly Bradwell-on-Sea). A royal tomb at Prittlewell was discovered and excavated in 2003. Finds included gold foil crosses, suggesting the occupant was Christian. If the occupant was a king, it was probably either Saebert or Sigeberht (murdered AD 653). It is, however, also possible that the occupant was not royal, but simply a wealthy and powerful individual whose identity has gone unrecorded.\n\nEssex reverted to Paganism again in 660 with the ascension of the Pagan King Swithelm of Essex. He converted in 662, but died in 664. He was succeeded by his two sons, Sigehere and Sæbbi. A plague the same year caused Sigehere and his people to recant their Christianity and Essex reverted to Paganism a third time. This rebellion was suppressed by Wulfhere of Mercia who established himself as overlord. Bede describes Sigehere and Sæbbi as \"rulers … under Wulfhere, king of the Mercians\". Wulfhere sent Jaruman, the bishop of Lichfield, to reconvert the East Saxons.\n\nWine (in 666) and Erkenwald (in 675) were appointed bishops of London with spiritual authority over the East Saxon Kingdom. Although London (and the rest of Middlesex) was lost by the East Saxons in the 8th century, the bishops of London continued to exert spiritual authority over Essex as a kingdom, shire and county until 1845.\n\nDespite the comparative obscurity of the kingdom, there were strong connections between Essex and the Kentish kingdom across the river Thames which led to the marriage of King Sledd to Ricula, sister of the king, Aethelbert of Kent. For a brief period in the 8th century the kingdom encompassed the Kentish Kingdom to the South. During this period, Essex kings were issuing their own sceattas (coins), perhaps as an assertion of their own independence. However, by the mid 8th century much of the kingdom, including London, had fallen to Mercia and the rump of Essex, roughly the modern county, was now subordinate to the same. After the defeat of the Mercian king Beornwulf around AD 825, Sigered, the last king of Essex, ceded the kingdom which then became a possession of the Wessex king Egbert.\n\nThe Mercians continued to control parts of Essex and may have supported a pretender to the Essex throne since a Sigeric \"rex Orientalem Saxonum\" witnessed a Mercian charter after AD 825. During the ninth century, Essex was part of a sub-kingdom that included Sussex, Surrey and Kent. Sometime between 878 and 886, the territory was formally ceded by Wessex to the Danelaw kingdom of East Anglia, under the Treaty of Alfred and Guthrum. After the reconquest by Edward the Elder the king's representative in Essex was styled an ealdorman and Essex came to be regarded as a shire.\n\nThe following list of kings may omit whole generations.\n\nThere are a number of variations of the spelling of the names of the Kings listed above. This was a time when spellings varied widely, even within a document. Amongst these variations are the preference between þ and ð (both \"th\" - voiced or unvoiced depending on adjacent letters).\n\nThe character '⁊' was used as the ampersand '&' in contemporary Anglo-Saxon writings. The era pre-dates the emergence of forms of writing accepted today, notably minuscule, and the letters 'W' and 'U'. Anglo-Saxon used the letter wynn (Ƿ) for W, but using the Latin alphabet where W was followed by U this was generally rendered as 'VV' (which was also used for 'W' alone).\n\n", "id": "9937", "title": "Kingdom of Essex"}
{"url": "https://en.wikipedia.org/wiki?curid=9939", "text": "Eve (disambiguation)\n\nEve is the first woman created by God according to the creation narrative of Abrahamic religions.\n\nEve may also refer to:\n\n\n\n\n\nThe day before, or the evening before, a holiday, such as:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "9939", "title": "Eve (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=9941", "text": "Æthelberht of Kent\n\nÆthelberht (; also Æthelbert, Aethelberht, Aethelbert, or Ethelbert, Old English Æðelberht, ; 560 – 24 February 616) was King of Kent from about 589 until his death. The eighth-century monk Bede, in his \"Ecclesiastical History of the English People\", lists him as the third king to hold \"imperium\" over other Anglo-Saxon kingdoms. In the late ninth century \"Anglo-Saxon Chronicle\" he is referred to as a bretwalda, or \"Britain-ruler\". He was the first English king to convert to Christianity.\n\nÆthelberht was the son of Eormenric, succeeding him as king, according to the \"Chronicle\". He married Bertha, the Christian daughter of Charibert, king of the Franks, thus building an alliance with the most powerful state in contemporary Western Europe; the marriage probably took place before he came to the throne. Bertha's influence may have led to Pope Gregory I's decision to send Augustine as a missionary from Rome. Augustine landed on the Isle of Thanet in east Kent in 597. Shortly thereafter, Æthelberht converted to Christianity, churches were established, and wider-scale conversion to Christianity began in the kingdom. He provided the new church with land in Canterbury, thus establishing one of the foundation stones of what ultimately became the Anglican Communion.\n\nÆthelberht's law for Kent, the earliest written code in any Germanic language, instituted a complex system of fines; the law code is preserved in the Textus Roffensis. Kent was rich, with strong trade ties to the continent, and he may have instituted royal control over trade. Coinage probably began circulating in Kent during his reign for the first time since the Anglo-Saxon invasion. He later came to be regarded as a saint for his role in establishing Christianity among the Anglo-Saxons. His feast day was originally 24 February, but was changed to 25 February.\n\nIn the fifth century, raids on Britain by continental peoples had developed into full-scale migrations. The newcomers are known to have included Angles, Saxons, Jutes, and Frisians, and there is evidence of other groups as well. These groups captured territory in the east and south of England, but at about the end of the fifth century, a British victory at the battle of Mount Badon (Mons Badonicus) halted the Anglo-Saxon advance for fifty years. From about 550, however, the British began to lose ground once more, and within twenty-five years it appears that control of almost all of southern England was in the hands of the invaders.\n\nAnglo-Saxons probably conquered Kent before Mons Badonicus. There is both documentary and archaeological evidence that Kent was primarily colonized by Jutes, from the southern part of the Jutland peninsula. According to legend, the brothers Hengist and Horsa landed in 449 as mercenaries for a British king, Vortigern. After a rebellion over pay and Horsa's death in battle, Hengist established the Kingdom of Kent. Some historians now think the underlying story of a rebelling mercenary force may be accurate; most now date the founding of the kingdom of Kent to the middle of the fifth-century, which is consistent with the legend. This early date, only a few decades after the departure of the Romans, also suggests that more of Roman civilization may have survived into Anglo-Saxon rule in Kent than in other areas.\n\nOverlordship was a central feature of Anglo-Saxon politics which began before Æthelberht's time; kings were described as overlords as late as the ninth century. The Anglo-Saxon invasion may have involved military coordination of different groups within the invaders, with a leader who had authority over many different groups; Ælle of Sussex may have been such a leader. Once the new states began to form, conflicts among them began. Tribute from dependents could lead to wealth. A weaker state also might ask or pay for the protection of a stronger neighbour against a warlike third state.\n\nSources for this period in Kentish history include the \"Ecclesiastical History of the English People\", written in 731 by Bede, a Northumbrian monk. Bede was interested primarily in England's Christianization. Since Æthelberht was the first Anglo-Saxon king to convert to Christianity, Bede provides more substantial information about him than about any earlier king. One of Bede's correspondents was Albinus, abbot of the monastery of St. Peter and St. Paul (subsequently renamed St. Augustine's) in Canterbury. The \"Anglo-Saxon Chronicle\", a collection of annals assembled c. 890 in the kingdom of Wessex, mentions several events in Kent during Æthelberht's reign. Further mention of events in Kent occurs in the late sixth century history of the Franks by Gregory of Tours. This is the earliest surviving source to mention any Anglo-Saxon kingdom. Some of Pope Gregory the Great's letters concern the mission of St. Augustine to Kent in 597; these letters also mention the state of Kent and its relationships with neighbours. Other sources include regnal lists of the kings of Kent and early charters (land grants by kings to their followers or to the church). Although no originals survive from Æthelberht's reign, later copies exist. A law code from Æthelberht's reign also survives.\n\nAccording to Bede, Æthelberht was descended directly from Hengist. Bede gives the line of descent as follows: \"Ethelbert was son of Irminric, son of Octa, and after his grandfather Oeric, surnamed Oisc, the kings of the Kentish folk are commonly known as Oiscings. The father of Oeric was Hengist.\" An alternative form of this genealogy, found in the \"Historia Brittonum\" among other places, reverses the position of Octa and Oisc in the lineage. The first of these names that can be placed historically with reasonable confidence is Æthelberht's father, whose name now usually is spelled Eormenric. The only direct written reference to Eormenric is in Kentish genealogies, but Gregory of Tours does mention that Æthelberht's father was the king of Kent, though Gregory gives no date. Eormenric's name provides a hint of connections to the kingdom of the Franks, across the English channel; the element \"Eormen\" was rare in names of the Anglo-Saxon aristocracy, but much more common among Frankish nobles. One other member of Æthelberht's family is known: his sister, Ricole, who is recorded by both Bede and the \"Anglo-Saxon Chronicle\" as the mother of Sæberht, king of the East Saxons (i.e. Essex).\n\nThe dates of Æthelberht's birth and accession to the throne of Kent are both matters of debate. Bede, the earliest source to give dates, is thought to have drawn his information from correspondence with Albinus. Bede states that when Æthelberht died in 616 he had reigned for fifty-six years, placing his accession in 560. Bede also says that Æthelberht died twenty-one years after his baptism. Augustine’s mission from Rome is known to have arrived in 597, and according to Bede, it was this mission that converted Æthelberht. Hence Bede’s dates are inconsistent. The \"Anglo-Saxon Chronicle\", an important source for early dates, is inconsistent with Bede and also has inconsistencies among different manuscript versions. Putting together the different dates in the \"Chronicle\" for birth, death, and length of reign, it appears that Æthelberht's reign was thought to have been either 560–616, or 565–618, but that the surviving sources have confused the two traditions.\n\nIt is possible that Æthelberht was converted to Christianity before Augustine's arrival. Æthelberht's wife was a Christian and brought a Frankish bishop with her, to attend her at court, so Æthelberht would have had knowledge of Christianity before the mission reached Kent. It also is possible that Bede had the date of Æthelberht's death wrong; if, in fact, Æthelberht died in 618, this would be consistent with his baptism in 597, which is in accord with the tradition that Augustine converted the king within a year of his arrival.\n\nGregory of Tours, in his \"Historia Francorum\", writes that Bertha, daughter of Charibert, king of the Franks, married the son of the king of Kent. Bede says that Æthelberht received Bertha \"from her parents\". If Bede is interpreted literally, the marriage would have had to take place before 567, when Charibert died. The traditions for Æthelberht's reign, then, would imply that Æthelberht married Bertha before either 560 or 565.\n\nThe extreme length of Æthelberht's reign also has been regarded with skepticism by historians; it has been suggested that he died in the fifty-sixth year of his life, rather than the fifty-sixth year of his reign. This would place the year of his birth approximately at 560, and he would not then have been able to marry until the mid 570s. According to Gregory of Tours, Charibert was king when he married Ingoberg, Bertha's mother, which places that marriage no earlier than 561. It therefore is unlikely that Bertha was married much before about 580. These later dates for Bertha and Æthelberht also solve another possible problem: Æthelberht's daughter, Æthelburh, seems likely to have been Bertha's child, but the earlier dates would have Bertha aged sixty or so at Æthelburh's likely birthdate using the early dates.\n\nGregory, however, also says that he thinks that Ingoberg was seventy years old in 589; and this would make her about forty when she married Charibert. This is possible, but seems unlikely, especially as Charibert seems to have had a preference for younger women, again according to Gregory's account. This would imply an earlier birth date for Bertha. On the other hand, Gregory refers to Æthelberht at the time of his marriage to Bertha simply as \"a man of Kent\", and in the 589 passage concerning Ingoberg's death, which was written in about 590 or 591, he refers to Æthelberht as \"the son of the king of Kent\". If this does not simply reflect Gregory's ignorance of Kentish affairs, which seems unlikely given the close ties between Kent and the Franks, then some assert that Æthelberht's reign cannot have begun before 589.\n\nWhile all of the contradictions above cannot be reconciled, the most probable dates that may be drawn from available data place Æthelberht's birth at approximately 560 and, perhaps, his marriage to Bertha at 580. His reign is most likely to have begun in 589 or 590.\n\nThe later history of Kent shows clear evidence of a system of joint kingship, with the kingdom being divided into east Kent and west Kent, although it appears that there generally was a dominant king. This evidence is less clear for the earlier period, but there are early charters, known to be forged, which nevertheless imply that Æthelberht ruled as joint king with his son, Eadbald. It may be that Æthelberht was king of east Kent and Eadbald became king of west Kent; the east Kent king seems generally to have been the dominant ruler later in Kentish history. Whether or not Eadbald became a joint king with Æthelberht, there is no question that Æthelberht had authority throughout the kingdom.\n\nThe division into two kingdoms is most likely to date back to the sixth century; east Kent may have conquered west Kent and preserved the institutions of kingship as a subkingdom. This was a common pattern in Anglo-Saxon England, as the more powerful kingdoms absorbed their weaker neighbours. An unusual feature of the Kentish system was that only sons of kings appeared to be legitimate claimants to the throne, although this did not eliminate all strife over the succession.\n\nThe main towns of the two kingdoms were Rochester, for west Kent, and Canterbury, for east Kent. Bede does not state that Æthelberht had a palace in Canterbury, but he does refer to Canterbury as Æthelberht's \"metropolis\", and it is clear that it is Æthelberht's seat.\n\nThere are many indications of close relations between Kent and the Franks. Æthelberht's marriage to Bertha certainly connected the two courts, although not as equals: the Franks would have thought of Æthelberht as an under-king. There is no record that Æthelberht ever accepted a continental king as his overlord and, as a result, historians are divided on the true nature of the relationship. Evidence for an explicit Frankish overlordship of Kent comes from a letter written by Pope Gregory the Great to Theuderic, king of Orléans, and Theudebert, king of Metz. The letter concerned Augustine's mission to Kent in 597, and in it Gregory says that he believes \"that you wish your subjects in every respect to be converted to that faith in which you, their kings and lords, stand\". It may be that this is a papal compliment, rather than a description of the relationship between the kingdoms. It also has been suggested that Liudhard, Bertha's chaplain, was intended as a representative of the Frankish church in Kent, which also could be interpreted as evidence of overlordship.\n\nA possible reason for the willingness of the Franks to connect themselves with the Kentish court is the fact that a Frankish king, Chilperic I, is recorded as having conquered a people known as the Euthiones during the mid-sixth century. If, as seems likely from the name, these people were the continental remnants of the Jutish invaders of Kent, then it may be that the marriage was intended as a unifying political move, reconnecting different branches of the same people. Another perspective on the marriage may be gained by considering that it is likely that Æthelberht was not yet king at the time he and Bertha were wed: it may be that Frankish support for him, acquired via the marriage, was instrumental in gaining the throne for him.\n\nRegardless of the political relationship between Æthelberht and the Franks, there is abundant evidence of strong connections across the English Channel. There was a luxury trade between Kent and the Franks, and burial artefacts found include clothing, drink, and weapons that reflect Frankish cultural influence. The Kentish burials have a greater range of imported goods than those of the neighbouring Anglo-Saxon regions, which is not surprising given Kent's easier access to trade across the English Channel. In addition, the grave goods are both richer and more numerous in Kentish graves, implying that material wealth was derived from that trade. Frankish influences also may be detected in the social and agrarian organization of Kent. Other cultural influences may be seen in the burials as well, so it is not necessary to presume that there was direct settlement by the Franks in Kent.\n\nIn his \"Ecclesiastical History\", Bede includes his list of seven kings who held \"imperium\" over the other kingdoms south of the Humber. The usual translation for \"imperium\" is \"overlordship\". Bede names Æthelberht as the third on the list, after Ælle of Sussex and Ceawlin of Wessex. The anonymous annalist who composed one of the versions of the \"Anglo-Saxon Chronicle\" repeated Bede's list of seven kings in a famous entry under the year 827, with one additional king, Egbert of Wessex. The \"Chronicle\" also records that these kings held the title \"\"bretwalda\"\", or \"Britain-ruler\". The exact meaning of \"bretwalda\" has been the subject of much debate; it has been described as a term \"of encomiastic poetry\", but there also is evidence that it implied a definite role of military leadership.\n\nThe prior \"bretwalda\", Ceawlin, is recorded by the \"Anglo-Saxon Chronicle\" as having fought Æthelberht in 568. The entry states that Æthelberht lost the battle and was driven back to Kent. The dating of the entries concerning the West Saxons in this section of the \"Chronicle\" is thought to be unreliable and a recent analysis suggests that Ceawlin's reign is more likely to have been approximately 581–588, rather than the dates of 560–592 that are given in the \"Chronicle\". The battle was at \"Wibbandun\", which may be translated as Wibba's Mount; it is not known where this was.\n\nAt some point Ceawlin ceased to hold the title of \"bretwalda\", perhaps after a battle at Stoke Lyne, in Oxfordshire, which the \"Chronicle\" dates to 584, some eight years before he was deposed in 592 (again using the \"Chronicle's\" unreliable dating). Æthelberht certainly was a dominant ruler by 601, when Gregory the Great wrote to him: Gregory urges Æthelberht to spread Christianity among those kings and peoples subject to him, implying some level of overlordship. If the battle of Wibbandun was fought c. 590, as has been suggested, then Æthelberht must have gained his position as overlord at some time in the 590s. This dating for Wibbandun is slightly inconsistent with the proposed dates of 581–588 for Ceawlin's reign, but those dates are not thought to be precise, merely the most plausible given the available data.\n\nIn addition to the evidence of the \"Chronicle\" that Æthelberht was accorded the title of bretwalda, there is evidence of his domination in several of the southern kingdoms of the Heptarchy. In Essex, Æthelberht appears to have been in a position to exercise authority shortly after 604, when his intervention helped in the conversion of King Sæberht of Essex, his nephew, to Christianity. It was Æthelberht, and not Sæberht, who built and endowed St. Pauls in London, where St Paul's Cathedral now stands. Further evidence is provided by Bede, who explicitly describes Æthelberht as Sæberht's overlord.\n\nBede describes Æthelberht's relationship with Rædwald, king of East Anglia, in a passage that is not completely clear in meaning. It seems to imply that Rædwald retained ducatus, or military command of his people, even while Æthelberht held imperium. This implies that being a bretwalda usually included holding the military command of other kingdoms and also that it was more than that, since Æthelberht is bretwalda despite Rædwald's control of his own troops. Rædwald was converted to Christianity while in Kent but did not abandon his pagan beliefs; this, together with the fact that he retained military independence, implies that Æthelberht's overlordship of East Anglia was much weaker than his influence with the East Saxons. An alternative interpretation, however, is that the passage in Bede should be translated as \"Rædwald, king of the East Angles, who while Æthelberht lived, even conceded to him the military leadership of his people\"; if this is Bede's intent, then East Anglia firmly was under Æthelberht's overlordship.\n\nThere is no evidence that Æthelberht's influence in other kingdoms was enough for him to convert any other kings to Christianity, although this is partly due to the lack of sources—nothing is known of Sussex's history, for example, for almost all of the seventh and eighth centuries. Æthelberht was able to arrange a meeting in 602 in the Severn valley, on the northwestern borders of Wessex, however, and this may be an indication of the extent of his influence in the west. No evidence survives showing Kentish domination of Mercia, but it is known that Mercia was independent of Northumbria, so it is quite plausible that it was under Kentish overlordship.\n\nThe native Britons had converted to Christianity under Roman rule. The Anglo-Saxon invasions separated the British church from European Christianity for centuries, so the church in Rome had no presence or authority in Britain, and in fact, Rome knew so little about the British church that it was unaware of any schism in customs. However, Æthelberht would have known something about the Roman church from his Frankish wife, Bertha, who had brought a bishop, Liudhard, with her across the Channel, and for whom Æthelberht built a chapel, St Martin's.\n\nIn 596, Pope Gregory the Great sent Augustine, prior of the monastery of St. Andrew in Rome, to England as a missionary, and in 597, a group of nearly forty monks, led by Augustine, landed on the Isle of Thanet in Kent. According to Bede, Æthelberht was sufficiently distrustful of the newcomers to insist on meeting them under the open sky, to prevent them from performing sorcery. The monks impressed Æthelberht, but he was not converted immediately. He agreed to allow the mission to settle in Canterbury and permitted them to preach.\n\nIt is not known when Æthelberht became a Christian. It is possible, despite Bede's account, that he already was a Christian before Augustine's mission arrived. It is likely that Liudhard and Bertha pressed Æthelberht to consider becoming a Christian before the arrival of the mission, and it is also likely that a condition of Æthelberht's marriage to Bertha was that Æthelberht would consider conversion. Conversion via the influence of the Frankish court would have been seen as an explicit recognition of Frankish overlordship, however, so it is possible that Æthelberht's delay of his conversion until it could be accomplished via Roman influence, might have been an assertion of independence from Frankish control. It also has been argued that Augustine's hesitation—he turned back to Rome, asking to be released from the mission—is an indication that Æthelberht was a pagan at the time Augustine was sent.\n\nAt the latest, Æthelberht must have converted before 601, since that year Gregory wrote to him as a Christian king. An old tradition records that Æthelberht converted on 1 June, in the summer of the year that Augustine arrived. Through Æthelberht's influence Sæberht, king of Essex, also was converted, but there were limits to the effectiveness of the mission. The entire Kentish court did not convert: Eadbald, Æthelberht's son and heir, was a pagan at his accession. Rædwald, king of East Anglia, was only partly converted (apparently while at Æthelberht's court) and retained a pagan shrine next to the new Christian altar. Augustine also was unsuccessful in gaining the allegiance of the British clergy.\n\nSome time after the arrival of Augustine's mission, perhaps in 602 or 603, Æthelberht issued a set of laws, in ninety sections. These laws are by far the earliest surviving code composed in any of the Germanic countries, and they were almost certainly among the first documents written down in Anglo-Saxon, as literacy would have arrived in England with Augustine's mission. The only surviving early manuscript, the \"Textus Roffensis\", dates from the twelfth century, and it now resides in the Medway Studies Centre in Strood, Kent. Æthelberht's code makes reference to the church in the very first item, which enumerates the compensation required for the property of a bishop, a deacon, a priest, and so on; but overall, the laws seem remarkably uninfluenced by Christian principles. Bede asserted that they were composed \"after the Roman manner\", but there is little discernible Roman influence either. In subject matter, the laws have been compared to the Lex Salica of the Franks, but it is not thought that Æthelberht based his new code on any specific previous model.\n\nThe laws are concerned with setting and enforcing the penalties for transgressions at all levels of society; the severity of the fine depended on the social rank of the victim. The king had a financial interest in enforcement, for part of the fines would come to him in many cases, but the king also was responsible for law and order, and avoiding blood feuds by enforcing the rules on compensation for injury was part of the way the king maintained control. Æthelberht's laws are mentioned by Alfred the Great, who compiled his own laws, making use of the prior codes created by Æthelberht, as well as those of Offa of Mercia and Ine of Wessex.\n\nOne of Æthelberht's laws seems to preserve a trace of a very old custom: the third item in the code states that \"If the king is drinking at a man's home, and anyone commits any evil deed there, he is to pay twofold compensation.\" This probably refers to the ancient custom of a king traveling the country, being hosted, and being provided for by his subjects wherever he went. The king's servants retained these rights for centuries after Æthelberht's time.\n\nItems 77–81 in the code have been interpreted as a description of a woman's financial rights after a divorce or legal separation. These clauses define how much of the household goods a woman could keep in different circumstances, depending on whether she keeps custody of the children, for example. It has recently been suggested, however, that it would be more correct to interpret these clauses as referring to women who are widowed, rather than divorced.\n\nThere is little documentary evidence about the nature of trade in Æthelberht's Kent. It is known that the kings of Kent had established royal control of trade by the late seventh century, but it is not known how early this control began. There is archaeological evidence suggesting that the royal influence predates any of the written sources. It has been suggested that one of Æthelberht's achievements was to take control of trade away from the aristocracy and to make it a royal monopoly. The continental trade provided Kent access to luxury goods which gave it an advantage in trading with the other Anglo-Saxon nations, and the revenue from trade was important in itself.\n\nKentish manufacture before 600 included glass beakers and jewelry. Kentish jewellers were highly skilled, and before the end of the sixth century they gained access to gold. Goods from Kent are found in cemeteries across the channel and as far away as at the mouth of the Loire. It is not known what Kent traded for all of this wealth, although it seems likely that there was a flourishing slave trade. It may well be that this wealth was the foundation of Æthelberht's strength, although his overlordship and the associated right to demand tribute would have brought wealth in its turn.\n\nIt may have been during Æthelberht's reign that the first coins were minted in England since the departure of the Romans: none bear his name, but it is thought likely that the first coins predate the end of the sixth century. These early coins were gold, and probably were the shillings (\"scillingas\" in Old English) that are mentioned in Æthelberht's laws. The coins are also known to numismatists as \"thrymsas\".\n\nÆthelberht died on 24 February 616 and was succeeded by his son, Eadbald, who was not a Christian—Bede says he had been converted but went back to his pagan faith, although he ultimately did become a Christian king. Eadbald outraged the church by marrying his stepmother, which was contrary to Church law, and by refusing to accept baptism. Sæberht of the East Saxons also died at approximately this time, and he was succeeded by his three sons, none of whom were Christian. A subsequent revolt against Christianity and the expulsion of the missionaries from Kent may have been a reaction to Kentish overlordship after Æthelberht's death as much as a pagan opposition to Christianity.\n\nIn addition to Eadbald, it is possible that Æthelberht had another son, Æthelwald. The evidence for this is a papal letter to Justus, archbishop of Canterbury from 619 to 625, that refers to a king named Aduluald, who is apparently different from Audubald, which refers to Eadbald. There is no agreement among modern scholars on how to interpret this: \"Aduluald\" might be intended as a representation of \"Æthelwald\", and hence an indication of another king, perhaps a sub-king of west Kent; or it may be merely a scribal error which should be read as referring to Eadbald.\n\nÆthelberht was later regarded as a saint for his role in establishing Christianity among the Anglo-Saxons. His feast day was originally 24 February but was changed to 25 February. In the 2004 edition of the Roman Martyrology, he is listed under his date of death, 24 February, with the citation: 'King of Kent, converted by St Augustine, bishop, the first leader of the English people to do so'. The Roman Catholic Archdiocese of Southwark, which contains Kent, commemorates him on 25 February. He is honoured together with his wife Bertha on the liturgical calendar of The Episcopal Church on 27 May.\n\n\nPrimary sources\n\nSecondary sources\n\n", "id": "9941", "title": "Æthelberht of Kent"}
{"url": "https://en.wikipedia.org/wiki?curid=9942", "text": "Erwin Schrödinger\n\nErwin Rudolf Josef Alexander Schrödinger (; 12 August 1887 – 4 January 1961), sometimes written as ' or ', was a Nobel Prize-winning Austrian physicist who developed a number of fundamental results in the field of quantum theory, which formed the basis of wave mechanics: he formulated the wave equation (stationary and time-dependent Schrödinger equation) and revealed the identity of his development of the formalism and matrix mechanics. Schrödinger proposed an original interpretation of the physical meaning of the wave function.\n\nIn addition, he was the author of many works in various fields of physics: statistical mechanics and thermodynamics, physics of dielectrics, colour theory, electrodynamics, general relativity, and cosmology, and he made several attempts to construct a unified field theory. In his book \"What Is Life?\" Schrödinger addressed the problems of genetics, looking at the phenomenon of life from the point of view of physics. He paid great attention to the philosophical aspects of science, ancient and oriental philosophical concepts, ethics, and religion. He also wrote on philosophy and theoretical biology. He is also known for his \"Schrödinger's cat\" thought-experiment.\n\nOn 12 August 1887, Schrödinger was born in , Vienna, Austria, to (cerecloth producer, botanist) and Georgine Emilia Brenda Schrödinger (née Bauer) (daughter of , Professor of Chemistry, Technische Hochschule Vienna). He was their only child.\n\nHis mother was of half Austrian and half English descent; his father was Catholic and his mother was Lutheran. Despite being raised in a religious household as a Lutheran, he was an atheist. However, he had strong interests in Eastern religions, pantheism and used religious symbolism in his works. He also believed his scientific work was an approach to the godhead, albeit in a metaphorical sense.\n\nHe was also able to learn English outside of school, as his maternal grandmother was British. Between 1906 and 1910 Schrödinger studied in Vienna under Franz S. Exner (1849–1926) and Friedrich Hasenöhrl (1874–1915). He also conducted experimental work with Karl Wilhelm Friedrich \"Fritz\" Kohlrausch.\n\nIn 1911, Schrödinger became an assistant to Exner. At an early age, Schrödinger was strongly influenced by Arthur Schopenhauer. As a result of his extensive reading of Schopenhauer's works, he became deeply interested throughout his life in colour theory and philosophy. In his lecture \"Mind and Matter\", he said that \"The world extended in space and time is but our representation.\" This is a repetition of the first words of Schopenhauer's main work.\n\nIn 1914 Erwin Schrödinger achieved Habilitation (\"venia legendi\"). Between 1914 and 1918 he participated in war work as a commissioned officer in the Austrian fortress artillery (Gorizia, Duino, Sistiana, Prosecco, Vienna). In 1920 he became the assistant to Max Wien, in Jena, and in September 1920 he attained the position of ao. Prof. (\"ausserordentlicher Professor\"), roughly equivalent to Reader (UK) or associate professor (US), in Stuttgart. In 1921, he became o. Prof. (\"ordentlicher Professor\", i.e. full professor), in Breslau (now Wrocław, Poland).\n\nIn 1921, he moved to the University of Zürich. In 1927, he succeeded Max Planck at the Friedrich Wilhelm University in Berlin. In 1934, however, Schrödinger decided to leave Germany; he disliked the Nazis' anti-semitism. He became a Fellow of Magdalen College at the University of Oxford. Soon after he arrived, he received the Nobel Prize together with Paul Dirac. His position at Oxford did not work out well; his unconventional domestic arrangements, sharing living quarters with two women was not met with acceptance. In 1934, Schrödinger lectured at Princeton University; he was offered a permanent position there, but did not accept it. Again, his wish to set up house with his wife and his mistress may have created a problem. He had the prospect of a position at the University of Edinburgh but visa delays occurred, and in the end he took up a position at the University of Graz in Austria in 1936. He had also accepted the offer of chair position at Department of Physics, Allahabad University in India.\n\nIn the midst of these tenure issues in 1935, after extensive correspondence with Albert Einstein, he proposed what is now called the Schrödinger's cat thought experiment.\n\nIn 1938, after the Anschluss, Schrödinger had problems because of his flight from Germany in 1933 and his known opposition to Nazism. He issued a statement recanting this opposition (he later regretted doing so and explained the reason to Einstein). However, this did not fully appease the new dispensation and the University of Graz dismissed him from his job for political unreliability. He suffered harassment and received instructions not to leave the country, but he and his wife fled to Italy. From there, he went to visiting positions in Oxford and Ghent University.\nIn the same year he received a personal invitation from Ireland's Taoiseach, Éamon de Valera, to reside in Ireland and agree to help establish an Institute for Advanced Studies in Dublin. He moved to Clontarf, Dublin, became the Director of the School for Theoretical Physics in 1940 and remained there for 17 years. He became a naturalized Irish citizen in 1948, but retained his Austrian citizenship. He wrote about 50 further publications on various topics, including his explorations of unified field theory.\n\nIn 1944, he wrote \"What Is Life?\", which contains a discussion of negentropy and the concept of a complex molecule with the genetic code for living organisms. According to James D. Watson's memoir, \"DNA, the Secret of Life\", Schrödinger's book gave Watson the inspiration to research the gene, which led to the discovery of the DNA double helix structure in 1953. Similarly, Francis Crick, in his autobiographical book \"What Mad Pursuit\", described how he was influenced by Schrödinger's speculations about how genetic information might be stored in molecules.\n\nSchrödinger stayed in Dublin until retiring in 1955. He had a lifelong interest in the Vedanta philosophy of Hinduism, which influenced his speculations at the close of \"What Is Life?\" about the possibility that individual consciousness is only a manifestation of a unitary consciousness pervading the universe. A manuscript \"Fragment From An Unpublished Dialogue of Galileo\" from this time recently resurfaced at The King's Hospital boarding school, Dublin after it was written for the School's 1955 edition of their Blue Coat to celebrate his leaving of Dublin to take up his appointment as Chair of Physics at the University of Vienna.\n\nIn 1956, he returned to Vienna (chair \"ad personam\"). At an important lecture during the World Energy Conference he refused to speak on nuclear energy because of his skepticism about it and gave a philosophical lecture instead. During this period Schrödinger turned from mainstream quantum mechanics' definition of wave–particle duality and promoted the wave idea alone, causing much controversy.\n\nOn 6 April 1920, Schrödinger married Annemarie (Anny) Bertel. Schrödinger suffered from tuberculosis and several times in the 1920s stayed at a sanatorium in Arosa. It was there that he formulated his wave equation.\nAs has been noted above, Schrödinger had an unconventional personal life. When he migrated to Ireland in 1938, he obtained visas for himself, his wife and also another woman, Mrs. Hilde March. March was the wife of an Austrian colleague and Schrödinger had fathered a daughter with her in 1934. Schrödinger wrote personally to the Taoiseach, Éamon de Valera to obtain the visa for Mrs. March. In October 1939 the \"ménage à trois\" duly took up residence in Dublin. Schrödinger fathered two further daughters by two different women during his time in Ireland. His grandson, Professor Terry Rudolph, follows in Schrödinger's footsteps as a quantum physicist who teaches at Imperial College London.\nOn 4 January 1961, Schrödinger died of tuberculosis, aged 73, in Vienna. He left Anny a widow, and was buried in Alpbach, Austria, in a Catholic cemetery. Although he was not Catholic, the priest in charge of the cemetery permitted the burial after learning Schrödinger was a member of the Pontifical Academy of Sciences. His wife, Anny (born 3 December 1896) died on 3 October 1965.\n\nEarly in his life, Schrödinger experimented in the fields of electrical engineering, atmospheric electricity, and atmospheric radioactivity, but he usually worked with his former teacher Franz Exner. He also studied vibrational theory, the theory of Brownian movement, and mathematical statistics. In 1912, at the request of the editors of the \"Handbook of Electricity and Magnetism\", Schrödinger wrote an article titled \"Dieelectrism\". That same year, Schrödinger gave a theoretical estimate of the probable height distribution of radioactive substances, which is required to explain the observed radioactivity of the atmosphere, and in August 1913 executed several experiments in Zeehame that confirmed his theoretical estimate and those of Victor Franz Hess. For this work, Schrödinger was awarded the 1920 Haitinger Prize (Haitinger-Preis) of the Austrian Academy of Sciences. Other experimental studies conducted by the young researcher in 1914 were checking formulas for capillary pressure in gas bubbles and the study of the properties of soft beta-radiation appearing in the fall of gamma rays on the surface of metal. The last work he performed together with his friend Fritz Kohlrausch. In 1919, Schrödinger performed his last physical experiment on coherent light and subsequently focused on theoretical studies.\n\nIn the first years of his career Schrödinger became acquainted with the ideas of quantum theory, developed in the works of Max Planck, Albert Einstein, Niels Bohr, Arnold Sommerfeld, and others. This knowledge helped him work on some problems in theoretical physics, but the Austrian scientist at the time was not yet ready to part with the traditional methods of classical physics.\n\nThe first publications of Schrödinger about atomic theory and the theory of spectra began to emerge only from the beginning of the 1920s, after his personal acquaintance with Sommerfeld and Wolfgang Pauli and his move to Germany. In January 1921, Schrödinger finished his first article on this subject, about the framework of the Bohr-Sommerfeld effect of the interaction of electrons on some features of the spectra of the alkali metals. Of particular interest to him was the introduction of relativistic considerations in quantum theory. In autumn 1922 he analyzed the electron orbits in an atom from a geometric point of view, using methods developed by the mathematician Hermann Weyl (1885–1955). This work, in which it was shown that quantum orbits are associated with certain geometric properties, was an important step in predicting some of the features of wave mechanics. Earlier in the same year he created the Schrödinger equation of the relativistic Doppler effect for spectral lines, based on the hypothesis of light quanta and considerations of energy and momentum. He liked the idea of his teacher Exner on the statistical nature of the conservation laws, so he enthusiastically embraced the articles of Bohr, Kramers, and Slater, which suggested the possibility of violation of these laws in individual atomic processes (for example, in the process of emission of radiation). Although the experiments of Hans Geiger and Walther Bothe soon cast doubt on this, the idea of energy as a statistical concept was a lifelong attraction for Schrödinger and he discussed it in some reports and publications.\n\nIn January 1926, Schrödinger published in \"Annalen der Physik\" the paper \"\"Quantisierung als Eigenwertproblem\"\" [\"tr\". Quantization as an Eigenvalue Problem] on wave mechanics and presented what is now known as the Schrödinger equation. In this paper, he gave a \"derivation\" of the wave equation for time-independent systems and showed that it gave the correct energy eigenvalues for a hydrogen-like atom. This paper has been universally celebrated as one of the most important achievements of the twentieth century and created a revolution in most areas of quantum mechanics and indeed of all physics and chemistry. A second paper was submitted just four weeks later that solved the quantum harmonic oscillator, rigid rotor, and diatomic molecule problems and gave a new derivation of the Schrödinger equation. A third paper, published in May, showed the equivalence of his approach to that of Heisenberg and gave the treatment of the Stark effect. A fourth paper in this series showed how to treat problems in which the system changes with time, as in scattering problems. In this paper he introduced a complex solution to the Wave equation in order to prevent the occurrence of fourth and sixth order differential equations. (This was arguably the moment when quantum mechanics switched from real to complex numbers.) When he introduced complex numbers in order to lower the order of the differential equations, something magical happened, and all of wave mechanics was at his feet. (He eventually reduced the order to one.) These papers were his central achievement and were at once recognized as having great significance by the physics community.\n\nSchrödinger was not entirely comfortable with the implications of quantum theory. Schrödinger wrote about the probability interpretation of quantum mechanics, saying: \"I don't like it, and I'm sorry I ever had anything to do with it.\"\n\nFollowing his work on quantum mechanics, Schrödinger devoted considerable effort to working on a Unified Field Theory that would unite gravity, electromagnetism, and nuclear forces within the basic framework of General Relativity, doing the work with an extended correspondence with Albert Einstein. In 1947, he announced a result, \"Affine Field Theory,\" in a talk at the Royal Irish Academy, but the announcement was criticized by Einstein as \"preliminary\" and failed to lead to the desired unified theory. Following the failure of his attempt at unification, Schrödinger gave up his work on unification and turned to other topics.\n\nAs many physicists, Schrödinger had a strong interest in psychology, in particular colour perception and colorimetry (\"Farbenmetrik\"). He spent few years of his life working on these questions and published a series of papers in this area:\n\nHis work on the psychology of color perception follows the step of Newton, Maxwell and von Helmholtz in the same area. Some of these paper have been translated to English and can be found in: \"Sources of Colour Science\", Ed. David L. MacAdam, The MIT Press (1970).\n\nThe philosophical issues raised by Schrödinger's cat are still debated today and remain his most enduring legacy in popular science, while Schrödinger's equation is his most enduring legacy at a more technical level. To this day, Schrödinger is known as the father of quantum mechanics. The large crater Schrödinger, on the far side of the Moon, is named after him. The Erwin Schrödinger International Institute for Mathematical Physics was established in Vienna in 1993.\n\nSchrödinger's portrait was the main feature of the design of the 1983–97 Austrian 1000-Schilling banknote, the second-highest denomination.\n\nA building is named after him at the University of Limerick, in Limerick, Ireland, as is the 'Erwin Schrödinger Zentrum' at Adlershof in Berlin.\n\nSchrödinger's 126th birthday anniversary was celebrated with a Google Doodle.\n\n\nSchrodinger's cat is named in his honour, see also list of things named after Erwin Schrödinger.\n\n\n\n", "id": "9942", "title": "Erwin Schrödinger"}
{"url": "https://en.wikipedia.org/wiki?curid=9945", "text": "EasyWriter\n\nEasyWriter was a word processor first written for the Apple II series computer in 1979, the first word processor for that platform. Published by Information Unlimited Software (IUS), it was written by John Draper's Cap'n Software, which also produced a version of Forth, which EasyWriter was developed in. Draper developed EasyWriter while serving nights in the Alameda County Jail under a work furlough program.\n\nIt was later ported to the IBM PC and released with the new computer in August 1981 as a launch title. Many criticized EasyWriter 1.0, distributed by IBM, for being buggy and hard to use; \"PC Magazine\" told IBM executives as early as December 1981 that subscribers \"wish IBM had provided better word processing\", Its poor quality caused others to quickly provide alternatives, such as Camilo Wilson's Volkswriter. while IBM offered a free upgrade to version 1.10 to version 1.0 owners.\n\nIUS released a separate application, EasyWriter II. Completely rewritten by Basic Software Group, IUS emphasized that II—developed with C instead of Forth—\"is not an updated version of the original IBM selection or its upgrade\".\n\n\"BYTE\" in 1981 reviewed EasyWriter and EasyWriter Professional for the Apple II, stating that \"editing is a pleasure with either version\", and approving of their features, user interface, and documentation. In an early review of the IBM PC, however, the magazine in 1982 stated that EasyWriter for it or the Apple II \"didn't seem to be of the same caliber as, say, VisiCalc or the Peachtree business packages\", citing the lack of ease of use and slow scrolling as flaws, and advised those who planned to use the IBM PC primarily for word processing to buy another computer until alternative software became available. Andrew Fluegelman wrote in \"PC Magazine\" that although EasyWriter 1.0 appeared to be an easy-to-use word processor for casual users, it \"contains a few very annoying inconveniences and some very serious traps\". He cited several bugs, slow performance, and user-interface issues, and later called it \"pretty much a lemon\".\n\nIBM's Don Estridge admitted in 1983 that he \"tried to use EasyWriter 1.0 and had the same experience everybody else had\". EasyWriter 1.10 resolved most of Fluegelman's complaints. He reported that it \"performs smoothly, will handle most any routine writing and printing job, and is easy to learn and operate\", and that if IBM had released 1.10 first EasyWriter would likely have become the standard PC word processor.\n\n\"BYTE\" criticized EasyWriter II for running as a booter instead of using DOS, requiring specially formatted disks for storage and a utility to convert to DOS-formatted disks, not being compatible with double-sided drives, and using a heavily modal editing interface.\n\n\n", "id": "9945", "title": "EasyWriter"}
{"url": "https://en.wikipedia.org/wiki?curid=9946", "text": "Ed Sullivan\n\nEdward Vincent Sullivan (September 28, 1901 – October 13, 1974) was an American television personality, sports and entertainment reporter, and longtime syndicated columnist for the \"New York Daily News\" and the Chicago Tribune New York News Syndicate. He is principally remembered as the creator and host of the television variety program \"The Toast of the Town\", later popularly—and, eventually, officially—renamed \"The Ed Sullivan Show\". Broadcast for 23 years from 1948 to 1971, it set a record as the longest-running variety show in US broadcast history. \"It was, by almost any measure, the last great TV show,\" proclaimed television critic David Hinckley. \"It's one of our fondest, dearest pop culture memories.\"\n\nSullivan was a broadcasting pioneer at many levels during television's infancy. As TV critic David Bianculli wrote, \"Before MTV, Sullivan presented rock acts. Before Bravo, he presented jazz and classical music and theater. Before the Comedy Channel, even before there was \"the Tonight Show\", Sullivan discovered, anointed and popularized young comedians. Before there were 500 channels, before there was cable, Ed Sullivan was where the choice was. From the start, he was indeed 'the Toast of the Town'.\" In 1996, Sullivan was ranked number 50 on \"TV Guide\"'s \"50 Greatest TV Stars of All Time\".\n\nSullivan was born in Harlem, New York City, the son of Elizabeth F. (née Smith) and Peter Arthur Sullivan, a customs house employee, and grew up in Port Chester, New York. He was of Irish descent. The entire family loved music, and someone was always playing the piano or singing. A phonograph was a prized possession; the family loved playing all types of records on it. Sullivan was a gifted athlete in high school, earning 12 athletic letters at Port Chester High School. He played halfback in football; he was a guard in basketball; in track he was a sprinter. With the baseball team, Ed was catcher and team captain, and he led the team to several championships. Baseball made an impression on him that would affect his career as well as the culture of America. Sullivan noted that in high school sports integration was taken for granted: \"When we went up into Connecticut, we ran into clubs that had Negro players. In those days this was accepted as commonplace; and so, my instinctive antagonism years later to any theory that a Negro wasn't a worthy opponent or was an inferior person. It was just as simple as that.\"\n\nHis first job was for the The Port Chester Daily Item—for which he had written sports news while in high school and then joined the paper full-time after graduation. In 1919 he joined The Hartford Post. The newspaper folded in his first week there, but he landed another job on The New York Evening Mail as a sports reporter. After The Evening Mail closed in 1923 he bounced through a series of news jobs with The Associated Press, The Philadelphia Bulletin, The Morning World, The Morning Telegraph, The New York Bulletin and The Leader. Finally, in 1927, Mr. Sullivan joined The Evening Graphic as first sports writer and then sports editor. In 1929, when Walter Winchell moved to The Daily Mirror, Mr. Sullivan was made Broadway columnist. His theatre column was later carried in the \"New York Daily News\". His column, \"Little Old New York\", concentrated on Broadway shows and gossip, as Winchell's had, and like Winchell, he did show-business news broadcasts on radio. Again echoing Winchell, Sullivan took on yet another medium in 1933 by writing and starring in the film \"Mr. Broadway\", which has him guiding the audience around New York nightspots to meet entertainers and celebrities. Sullivan soon became a powerful starmaker in the entertainment world himself, becoming one of Winchell's main rivals, setting the El Morocco nightclub in New York as his unofficial headquarters against Winchell's seat of power at the nearby Stork Club. Sullivan continued writing for \"The News\" throughout his broadcasting career, and his popularity long outlived Winchell's.\n\nThroughout his career as a columnist, Mr. Sullivan had dabbled in entertainment—producing vaudeville shows with which he appeared as master of ceremonies in the Twenties and Thirties, directing a radio program over WABC (now WCBS) and organizing benefit reviews for various causes.\n\nIn 1941, Sullivan was host of the \"Summer Silver Theater\", a variety program on CBS, with Will Bradley as bandleader and a guest star featured each week.\n\nIn 1948, Marlo Lewis, a producer, got the CBS network to hire Sullivan to do a weekly Sunday-night TV variety show, \"Toast of the Town\", which later became \"The Ed Sullivan Show\". Debuting in June 1948, the show was originally broadcast from the Maxine Elliott Theatre on West 39th Street in New York City. In January, 1953, it moved to CBS-TV Studio 50, at 1697 Broadway (at 53rd Street) in New York City, which in 1967 was renamed the Ed Sullivan Theater (and was later the home of the \"Late Show with David Letterman\" and \"The Late Show with Stephen Colbert\"). Studio 50 was formerly a CBS Radio studio, from 1936 to 1953, and before that was the legitimate Hammerstein Theatre, built in 1927.\n\nTelevision critics gave the new show and its host poor reviews. Harriet Van Horne alleged that \"he got where he is not by having a personality, but by having \"no\" personality.\" (The host wrote to the critic, \"Dear Miss Van Horne: You bitch. Sincerely, Ed Sullivan.\") Sullivan had little acting ability; in 1967, 20 years after his show's debut, \"Time\" magazine asked, \"What exactly is Ed Sullivan's talent?\" His mannerisms on camera were so awkward that some viewers believed the host suffered from Bell's palsy. \"Time\" in 1955 stated that Sullivan resembled\n\nThe magazine concluded, however, that \"Yet, instead of frightening children, Ed Sullivan charms the whole family.\" Sullivan appeared to the audience as an average guy who brought the great acts of show business to their home televisions. \"Ed Sullivan will last\", comedian Fred Allen said, \"as long as someone else has talent\", and frequent guest Alan King said, \"Ed does nothing, but he does it better than anyone else in television.\" He had a newspaperman's instinct for what the public wanted, and programmed his variety hours with remarkable balance. There was something for everyone. A typical show would feature a vaudeville act (acrobats, jugglers, magicians, etc.), one or two popular comedians, a singing star, a hot jukebox favorite, a figure from the legitimate theater, and for the kids, a visit with puppet \"Topo Gigio, the little Italian mouse\", or a popular athlete. The bill was often international in scope, with many European performers augmenting the American artists.\n\nSullivan had a healthy sense of humor about himself and permitted—even encouraged—impersonators such as John Byner, Frank Gorshin, Rich Little, and especially Will Jordan to imitate him on his show. Johnny Carson also did a fair impression, and even Joan Rivers imitated Sullivan's unique posture. The impressionists exaggerated his stiffness, raised shoulders, and nasal tenor phrasing, along with some of his commonly used introductions, such as \"And now, right here on our stage...\", \"For all you youngsters out there...\", and \"a really big shew\" (his pronunciation of the word \"show\"). Will Jordan portrayed Sullivan in the films \"I Wanna Hold Your Hand\", \"The Buddy Holly Story\", \"The Doors\", \"Mr. Saturday Night\", \"Down with Love\", and in the 1979 TV movie \"Elvis\".\n\nSullivan inspired a song in the musical \"Bye Bye Birdie\", and in 1963, appeared as himself in the film.\n\nIn the 1950s and 1960s, Sullivan was a respected starmaker because of the number of performers who became household names after appearing on the show. He had a knack for identifying and promoting top talent and paid a great deal of money to secure that talent for his show. Sullivan was quoted by saying \"In the conduct of my own show, I've never asked a performer his religion, his race or his politics. Performers are engaged on the basis of their abilities. I believe that this is another quality of our show that has helped win it a wide and loyal audience.\"\n\nAlthough Sullivan was wary of Elvis Presley's \"bad boy\" image, and initially said that he would never book him, Presley became too big a name to ignore; in 1956, Sullivan signed him for three appearances. In August 1956, Sullivan was injured in an automobile accident near his country home in Southbury, Connecticut, and missed Presley's first appearance on September 9. Charles Laughton wound up introducing Presley on the Sullivan hour. After Sullivan got to know Presley personally, he made amends by telling his audience, \"This is a real decent, fine boy.\"\n\nSullivan's failure to scoop the TV industry with Presley made him determined to get the next big sensation first. In November 1963, while in Heathrow Airport, Sullivan witnessed Beatlemania as the band returned from Sweden. At first he was reluctant to book the Beatles because the band did not have a single released in the US at the time, but at the behest of a friend, legendary impresario Sid Bernstein, Sullivan signed the group. Their initial Sullivan show appearance on February 9, 1964, was the most-watched program in TV history to that point, and remains one of the most-watched programs of all time. The Beatles appeared three more times in person, and submitted filmed performances later. The Dave Clark Five, who claimed a \"cleaner\" image than the Beatles, made 13 appearances on the show, more than any other UK group.\n\nUnlike many shows of the time, Sullivan asked that most musical acts perform their music live, rather than lip-synching to their recordings. Examination of performances show that exceptions were made, as when a microphone could not be placed close enough to a performer for technical reasons. An example was B.J. Thomas' 1969 performance of \"Raindrops Keep Fallin' on My Head\", in which actual water was sprinkled on him as a special effect. In 1969, Sullivan presented the Jackson 5 with their first single \"I Want You Back\", which ousted the B.J. Thomas song from the top spot of \"Billboard's\" pop charts.\nSullivan appreciated African American talent. According to biographer Gerald Nachman, \"Most TV variety shows welcomed 'acceptable' black superstars like Louis Armstrong, Pearl Bailey and Sammy Davis Jr. ... but in the early 1950s, long before it was fashionable, Sullivan was presenting the much more obscure black entertainers he had enjoyed in Harlem on his uptown rounds — legends like Peg Leg Bates, Pigmeat Markham and Tim Moore ... strangers to white America.\" He hosted pioneering TV appearances by Bo Diddley, the Platters, Brook Benton, Jackie Wilson, Fats Domino, and numerous Motown acts, including the Supremes, who appeared 17 times. As the critic John Leonard wrote, \"There wasn't an important black artist who didn't appear on Ed's show.\"\n\nHe defied pressure to exclude African American entertainers, and to avoid interacting with them when they did appear. \"Sullivan had to fend off his hard-won sponsor, Ford's Lincoln dealers, after kissing Pearl Bailey on the cheek and daring to shake Nat King Cole's hand,\" Nachman wrote. According to biographer Jerry Bowles, \"Sullivan once had a Ford executive thrown out of the theatre when he suggested that Sullivan stop booking so many black acts. And a dealer in Cleveland told him 'We realize that you got to have niggers on your show. But do you have to put your arm around Bill 'Bojangles' Robinson at the end of his dance?' Sullivan had to be physically restrained from beating the man to a pulp.\" Sullivan later raised money to help pay for Robinson's funeral. “As a Catholic, it was inevitable that I would despise intolerance, because Catholics suffered more than their share of it,\" he told an interviewer. \"As I grew up, the causes of minorities were part and parcel of me. Negroes and Jews were the minority causes closest at hand. I need no urging to take a plunge in and help.”\n\nAt a time when television had not yet embraced Country and Western music, Sullivan featured Nashville performers on his program. This, in turn, paved the way for shows such as \"Hee Haw\", and variety shows hosted by Johnny Cash, Glen Campbell, and other country singers. The act that appeared most frequently through the show's run was the Canadian comedy duo of Wayne & Shuster, who made 67 appearances between 1958 and 1969.\n\nSullivan appeared as himself on other television programs, including an April 1958 episode of the Howard Duff and Ida Lupino CBS sitcom, \"Mr. Adams and Eve\". On September 14, 1958, Sullivan appeared on \"What's My Line?\" as a mystery guest, and showed his comedic side by donning a rubber mask. In 1961, Sullivan was asked by CBS to fill in for an ailing Red Skelton on \"The Red Skelton Show\". Sullivan took Skelton's roles in the various comedy sketches; Skelton's hobo character \"Freddie the Freeloader\" was renamed \"Eddie the Freeloader.\"\n\nSullivan was quick to take offense if he felt he had been crossed, and could hold a grudge for a long time. As he told biographer Gerald Nachman, \"I'm a pop-off. I flare up, then I go around apologizing.\" \"Armed with an Irish temper and thin skin,\" wrote Nachman, \"Ed brought to his feuds a hunger for combat fed by his coverage of, and devotion to, boxing.\" Bo Diddley, Buddy Holly, Jackie Mason, and Jim Morrison were parties to some of Sullivan's most storied conflicts.\n\nFor his second Sullivan appearance in 1955, Bo Diddley planned to sing his namesake hit, \"Bo Diddley\", but Sullivan told him to perform Tennessee Ernie Ford's song \"Sixteen Tons\". \"That would have been the end of my career right there,\" he told his biographer, so he sang \"Bo Diddley\" anyway. Sullivan was enraged: \"You're the first black boy that ever double-crossed me on the show,\" Diddley quoted him as saying. \"We didn't have much to do with each other after that.\" Later, Diddley resented that Elvis Presley, whom he accused of copying his revolutionary style and beat, received the attention and accolades on Sullivan's show that he felt were rightfully his. \"I am owed,\" he said, \"and I never got paid.\" \"He might have,\" wrote Nachman, \"had things gone smoother with Sullivan.\"\n\nBuddy Holly and the Crickets first appeared on the Sullivan show in 1957 to an enthusiastic response. For their second appearance in January 1958, Sullivan considered the lyrics of their chosen number \"Oh, Boy!\" too suggestive, and ordered Holly to substitute another song. Holly responded that he had already told his hometown friends in Texas that he would be singing \"Oh, Boy!\" for them. Sullivan, unaccustomed to having his instructions questioned, angrily repeated them, but Holly refused to back down. Later, when the band was slow to respond to a summons to the rehearsal stage, Sullivan commented, \"I guess the Crickets are not too excited to be on \"The Ed Sullivan Show\".\" Holly, still annoyed by Sullivan's attitude, replied, \"I hope they're damn more excited than I am.\" Sullivan retaliated by cutting them from two numbers to one, then mispronounced Holly's name during the introduction. He also saw to it that Holly's guitar amplifier was turned off. Nevertheless, the band was received so well that Sullivan was forced to invite them back; Holly responded that Sullivan did not have enough money. Archival photographs taken during the appearance show Holly smirking and ignoring a visibly angry Sullivan.\n\nDuring Jackie Mason's October 1964 performance on a show that had been shortened by ten minutes due to an address by President Lyndon Johnson, Sullivan—on-stage but off-camera—signaled Mason that he had two minutes left by holding up two fingers. Sullivan's signal distracted the studio audience, and to television viewers unaware of the circumstances, it seemed as though Mason's jokes were falling flat. Mason, in a bid to regain the audience's attention, cried, \"I'm getting fingers here!\" and made his own frantic hand gesture: \"Here's a finger for you!\" Videotapes of the incident are inconclusive as to whether Mason's upswept hand (which was just off-camera) was intended to be an indecent gesture, but Sullivan was convinced that it was, and banned Mason from future appearances on the program. Mason later insisted that he did not know what the \"middle finger\" meant, and that he did not make the gesture anyway. In September 1965, Sullivan—who, according to Mason, was \"deeply apologetic\"—brought Mason on the show for a \"surprise grand reunion\". \"He said they were old pals,\" Nachman wrote, \"news to Mason, who never got a repeat invitation.\" Mason added that his earning power \"...was cut right in half after that. I never really worked my way back until I opened on Broadway in 1986.\"\n\nWhen the Byrds performed on December 12, 1965, David Crosby got into a shouting match with the show's director. They were never asked to return.\n\nSullivan decided that \"Girl, we couldn't get much higher\", from the Doors' signature song \"Light My Fire\", was too overt a reference to drug use, and directed that the lyric be changed to \"Girl, we couldn't get much better\" for the group's September 1967 appearance. The band members \"nodded their assent\", according to Doors biographer Ben Fong-Torres, then sang the song as written. After the broadcast, producer Bob Precht told the group, \"Mr. Sullivan wanted you for six more shows, but you'll never work \"the Ed Sullivan Show\" again.\" Jim Morrison replied, \"Hey, man, we just \"did\" the \"Ed Sullivan Show\".\" Sullivan, true to his word, never invited the band back.\n\nThe Rolling Stones famously capitulated during their fifth appearance on the show, in 1967, when Mick Jagger was told to change the titular lyric of \"Let's Spend the Night Together\" to \"Let's spend some time together\". \"But Jagger prevailed,\" wrote Nachman, by deliberately calling attention to the censorship, rolling his eyes, mugging, and drawing out the word \"t-i-i-i-me\" as he sang the revised lyric. Sullivan was angered by the insubordination, but the Stones did make one additional appearance on the show, in 1969.\n\nMoe Howard of the Three Stooges recalled in 1975 that Sullivan had a memory problem of sorts: \"Ed was a very nice man, but for a showman, quite forgetful. On our first appearance, he introduced us as the Three Ritz Brothers. He got out of it by adding, 'who look more like the Three Stooges to me'.\" Joe DeRita, who worked with the Stooges after 1959, had commented that Sullivan had a personality \"like the bottom of a bird cage.\"\n\nDiana Ross, who was very fond of Sullivan, later recalled Sullivan's forgetfulness during the many occasions the Supremes performed on his show. In a 1995 appearance on the \"Late Show with David Letterman\" (taped in the Ed Sullivan Theater), Ross stated, \"he could never remember our names. He called us 'the girls'.\"\n\nIn a 1990 press conference, Paul McCartney recalled meeting Sullivan again in the early 1970s. Sullivan apparently had no idea who McCartney was. McCartney tried to remind Sullivan that he was one of the Beatles, but Sullivan obviously could not remember, and nodding and smiling, simply shook McCartney's hand and left. In an interview with Howard Stern around 2012, Joan Rivers said that Sullivan had been suffering from dementia toward the end of his life.\n\nSullivan, like many American entertainers, was pulled into the Cold War anticommunism hysteria of the late 1940s and 1950s. Tap dancer Paul Draper's scheduled January 1950 appearance on \"Toast of the Town\" met with opposition from Hester McCullough, an activist in the hunt for \"subversives\". Branding Draper a Communist Party \"sympathizer\", she demanded that Sullivan’s lead sponsor, the Ford Motor Company, cancel Draper’s appearance. Draper denied the charge, and appeared on the show as scheduled. Ford received over a thousand angry letters and telegrams, and Sullivan was obliged to promise Ford’s advertising agency, Kenyon & Eckhardt, that he would avoid controversial guests going forward. Draper was forced to move to Europe to earn a living.\n\nAfter the Draper incident, Sullivan began to work closely with Theodore Kirkpatrick of the anticommunist \"Counterattack\" newsletter. He would consult Kirkpatrick if any questions came up regarding a potential guest's political leanings. Sullivan wrote in his June 21, 1950, \"Daily News\" column that \"Kirkpatrick has sat in my living room on several occasions and listened attentively to performers eager to secure a certification of loyalty.\"\n\nCold War repercussions manifested in a different way when Bob Dylan was booked to appear in May 1963. His chosen song was \"Talkin' John Birch Paranoid Blues\", which poked fun at the ultraconservative John Birch Society and its tendency to see Communist conspiracies in many situations. No concern was voiced by anyone, including Sullivan, during rehearsals, but on the day of the broadcast, CBS's Standards and Practices department rejected the song, fearing that lyrics equating the Society’s views with those of Adolf Hitler might trigger a defamation lawsuit. Dylan was offered the opportunity to perform a different song, but he responded that if he could not sing the number of his choice, he would rather not appear at all. The story generated widespread media attention in the days that followed; Sullivan denounced the network’s decision in published interviews.\n\nSullivan butted heads with Standards and Practices on other occasions, as well. In 1956, Ingrid Bergman—who had been living in \"exile\" in Europe since 1950 in the wake of her scandalous love affair with director Roberto Rossellini while they were both married—was planning a return to Hollywood as the star of \"Anastasia\". Sullivan, confident that the American public would welcome her back, invited her to appear on his show and flew to Europe to film an interview with Bergman, Yul Brynner, and Helen Hayes on the \"Anastasia\" set. When he arrived back in New York, Standards and Practices informed Sullivan that under no circumstances would Bergman be permitted to appear on the show, either live or on film. Sullivan's prediction later proved correct, as Bergman won her second Academy Award for her portrayal, as well as the forgiveness of her fans.\n\nSullivan was engaged to champion swimmer Sybil Bauer, but she died of cancer in 1927 at the age of 23. In 1926, Ed met and began dating Sylvia Weinstein. Sylvia tried to tell her Jewish family she was dating a man named Ed Solomon, but her brother figured out she meant Ed Sullivan. With both families strongly opposed to a Catholic-Jewish marriage, the affair was on-again-off-again for three years. They were finally married on April 28, 1930, in a City Hall ceremony, and a year later Sylvia gave birth to Elizabeth (\"Betty\"), named after Sullivan’s mother, who had died that year. Despite Jewish tradition, the couple raised their daughter Catholic.\n\nEd and Sylvia were always “on the town,” eating out five nights a week at some of the trendiest clubs and restaurants – The Stork Club, Danny’s Hideaway and Jimmy Kelly’s. Ed would hobnob with the rich and famous, was friends with U.S. Presidents and would even receive audiences with various Popes. In 1952, Betty married the \"Ed Sullivan Show\"'s producer, Bob Precht. From Betty and Bob, Ed had five grandchildren, Robert Edward, Carla Elizabeth, Vincent Henry, Andrew Sullivan and Margo Elizabeth. The Sullivan/Precht family was very close. The Sullivans rented a suite of rooms at the Hotel Delmonico in 1944 after living at the Hotel Astor on Times Square for many years. Sullivan rented a suite next door to the family suite, which he used as an office until \"The Ed Sullivan Show\" was canceled in 1971. Sullivan was in the habit of calling Sylvia after every program to get her immediate critique.\n\nIn the fall of 1965, CBS began televising its weekly programs in color. Although the Sullivan show was seen live in the Central and Eastern time zones, it was taped for airing in the Pacific and Mountain time zones. Most of the taped programs, as well as some early kinescopes, were preserved, and excerpts have been released on home video.\n\nBy 1971, the show's ratings had plummeted. In an effort to refresh its lineup, CBS canceled the program along with some of its other longtime shows. Sullivan was angered, and refused to do a final show, although he remained with the network in various other capacities and hosted a 25th anniversary special in June 1973.\n\nIn early September 1974, X-rays revealed that Sullivan had advanced esophageal cancer. Doctors gave him very little time to live, and the family chose to keep the diagnosis from him. Sullivan, still believing his ailment to be yet another complication from a long-standing battle with gastric ulcers, died five weeks later on October 13, 1974, at New York's Lenox Hill Hospital. His funeral was attended by 3,000 at St. Patrick's Cathedral, New York, on a cold, rainy day. Sullivan is interred in a crypt at the Ferncliff Cemetery in Hartsdale, New York.\n\nSullivan has a star on the Hollywood Walk of Fame at 6101 Hollywood Blvd.\n\n\n", "id": "9946", "title": "Ed Sullivan"}
{"url": "https://en.wikipedia.org/wiki?curid=9948", "text": "Élisabeth Vigée Le Brun\n\nÉlisabeth Louise Vigée Le Brun (Marie Élisabeth Louise; 16 April 1755 – 30 March 1842), also known as Madame Lebrun, was a prominent French painter.\n\nHer artistic style is generally considered part of the aftermath of Rococo, while she often adopts a neoclassical style. Vigée Le Brun cannot be considered a pure Rococo or Neoclassical painter. Her subject matter and color palette can be classified as Rococo, however, her style is aligned with the emergence of Neoclassicism. Vigée Le Brun created a name for herself in Ancien Régime society by serving as the portrait painter to Marie Antoinette.\n\nVigée Le Brun left a legacy of 660 portraits and 200 landscapes. In addition to private collections, her works may be found at major museums, such as the Hermitage Museum, London's National Gallery, the Metropolitan Museum of Art in New York, and many other collections in continental Europe and the United States.\n\nBorn in Paris on 16 April 1755, Marie-Louise-Élisabeth Vigée was the daughter of a portraitist and fan painter, Louis Vigée, from whom she received her first instruction. Her mother, Jeanne (née Maissin), was a hairdresser. She was sent to live with relatives in Épernon until the age of 6, when she entered a convent, where she remained for five years. Her father died when she was 12 years old. In 1768, her mother married a wealthy jeweler, Jacques-François Le Sèvre, and shortly after, the family moved to the Rue Saint-Honoré, close to the Palais Royal. She was later patronized by the wealthy heiress Louise Marie Adélaïde de Bourbon, wife of Louis Philippe II, Duke of Orléans. During this period Louise Élisabeth benefited from the advice of Gabriel François Doyen, Jean-Baptiste Greuze, Joseph Vernet, and other masters of the period.\n\nBy the time she was in her early teens, Louise Élisabeth was painting portraits professionally. After her studio was seized for her practicing without a license, she applied to the Académie de Saint-Luc, which unwittingly exhibited her works in their Salon. In 1774, she was made a member of the Académie. On 11 January 1776 she married Jean-Baptiste-Pierre Le Brun, a painter and art dealer. Vigée Le Brun began exhibiting her work at their home in Paris, the Hôtel de Lubert, and the Salons she held here supplied her with many new and important contacts. Her husband's great-great-uncle was Charles Le Brun, the first director of the French Academy under Louis XIV. Vigée-Le Brun painted portraits of many of the nobility of the day.\n\nOn 12 February 1780, Vigée-Le Brun gave birth to a daughter, Jeanne Julie Louise, whom she called Julie.\n\nIn 1781 she and her husband toured Flanders and the Netherlands, where seeing the works of the Flemish masters inspired her to try new techniques. There, she painted portraits of some of the nobility, including the Prince of Nassau. In 1787, she caused a minor public scandal with a self-portrait, exhibited the same year, in which she was shown smiling open-mouthed – in contravention of painting conventions going back to antiquity. The court gossip-sheet \"Mémoires secrets\" commented: \"An affectation which artists, art-lovers and persons of taste have been united in condemning, and which finds no precedent among the Ancients, is that in smiling, [Madame Vigée-Lebrun] shows her teeth.\"\n\nVigée Le Brun, as her career blossomed, was invited to the Palace of Versailles granted patronage by Marie Antoinette. So pleased was the queen that during a period of six years, Vigée Le Brun would paint more than thirty portraits of the queen and her family, leading to her being commonly viewed as the official portraitist of Marie Antoinette. Vigée Le Brun helped to improve Marie Antoinette's image by painting portraits that included her children and worked towards making her more relatable to the public, in hopes to counter the bad press and judgement the queen had recently received. She received commission for the portrait \"Marie-Antoinette and her Children\" (1787) in 1785, which portrayed Marie Antoinette as a devout and loving mother figure.\n\nMarie Antoinette later worked as a helping hand in Vigée Le Brun's acceptance into the Académie Royale de Peinture et de Sculpture in 1783. Whilst of benefit during the reign of the Bourbon royals, this label was to prove problematic later.\nOn 31 May 1783, Vigée-Le Brun was accepted as a member of France's \"Académie Royale de Peinture et de Sculpture\". She submitted numerous portraits along with an allegorical history painting which she considered her \"morceau de réception  – La Paix qui ramène l'Abondance\" (Peace Bringing Back Prosperity). The Academy did not place her work within an academic category of type of painting – either history or portraiture.\n\nAdélaïde Labille-Guiard also was admitted on the same day. The admission of Vigée Le Brun was opposed on the grounds that her husband was an art dealer, but eventually they were overruled by an order from Louis XVI because Marie Antoinette put considerable pressure on her husband on behalf of her portraitist.\n\nIn 1789, she was succeeded as court painter to Marie Antoinette by Alexander Kucharsky. Vigée Le Brun was Marie Antoinette's favorite painter for a decade. She also enjoyed the patronage of European aristocrats, actors, and writers and was elected to art academies in 10 cities.\nSince Vigée Le Brun was close to the royal family, she was in danger during the French Revolution. After the arrest of the royal family during the French Revolution, Vigée Le Brun fled France with her young daughter Julie. She lived and worked for some years in Italy, Austria, and Russia, where her experience in dealing with an aristocratic clientele was still useful. In Rome, her paintings met with great critical acclaim and she was elected to the Roman Accademia di San Luca.\n\nIn Russia, she was received by the nobility and painted numerous aristocrats, including the last king of Poland Stanisław August Poniatowski and members of the family of Catherine the Great. Although the French aesthetic was widely admired in Russia, there remained various cultural differences as to what was deemed acceptable. Catherine was not initially happy with Vigée Le Brun's portrait of her granddaughters, Elena and Alexandra Pavlovna, due to the area of bare skin the short sleeved gowns revealed. In order to please the Empress, Vigée Le Brun added sleeves, thereby giving the work its characteristic look. This tactic seemed effective in pleasing Catherine, as she agreed to sit herself for Vigée Le Brun (although Catherine died of a stroke before this work was due to begin).\n\nWhile in Saint Petersburg, Vigée Le Brun was made a member of the \"Academy of Fine Arts of Saint Petersburg\". Much to Vigée Le Brun's dismay, her daughter Julie married a Russian nobleman.\n\nAfter a sustained campaign by her ex-husband and other family members to have her name removed from the list of counter-revolutionary émigrés, Vigée Le Brun was finally able to return to France during the reign of Emperor Napoleon I. In spite of being no longer labeled as émigrée, her relationship with the new regime was never totally harmonious, as might be expected given that she was a strong royalist and the former portraitist of Marie Antoinette.\n\nMuch in demand by the \"élite\" of Europe, she visited England at the beginning of the 19th century and painted the portrait of several British notables, including Lord Byron. In 1807 she traveled to Switzerland and was made an honorary member of the \"Société pour l'Avancement des Beaux-Arts\" of Geneva.\n\nShe published her memoirs in 1835 and 1837, which provide an interesting view of the training of artists at the end of the period dominated by royal academies. Her portrait of fellow neoclassical painter Hubert Robert is in Paris at the Louvre.\n\nStill very active with her painting in her fifties, she purchased a house in Louveciennes, Île-de-France, and lived there until the house was seized by the Prussian Army during the war in 1814. She stayed in Paris until her death on 30 March 1842 when her body was taken back to Louveciennes and buried in the Cimetière de Louveciennes near her old home.\n\nHer tombstone epitaph states \"\"Ici, enfin, je repose...\"\" (Here, at last, I rest...).\n\nThe Metropolitan Museum in New York held an exhibition of eighty of her works in February/May 2016, the first retrospective and only the second solo exhibition in modern times\n\nLe Brun is the central character in Joel Gross's historical drama \"Marie Antoinette: The Color of Flesh\" (premiered in 2008).\n\nIn the episode \"The Portrait\" from the BBC series \"Let Them Eat Cake\" (1999) written by Peter Learmouth and starring Dawn French and Jennifer Saunders, Madame Vigée Le Brun (Maggie Steed) paints a portrait of the Comtesse De Vache (Jennifer Saunders), weeping over a dead canary.\n\n\n\n\n", "id": "9948", "title": "Élisabeth Vigée Le Brun"}
{"url": "https://en.wikipedia.org/wiki?curid=9949", "text": "Epistle to the Galatians\n\nThe Epistle to the Galatians, often shortened to Galatians, is the ninth book of the New Testament. It is a letter from Paul the Apostle to a number of Early Christian communities in Galatia. Scholars have suggested that this is either the Roman province of Galatia in southern Anatolia, or a large region defined by an ethnic group of Celtic people in central Anatolia. Paul is principally concerned with the controversy surrounding Gentile Christians and the Mosaic Law during the Apostolic Age. Paul argues that the Gentile Galatians do not need to adhere to the tenets of the Mosaic Law, particularly circumcision, by contextualizing the role of the law in light of the revelation of Christ. Galatians has exerted enormous influence on the history of Christianity, the development of Christian theology, and the study of the apostle Paul.\n\nNo original of the letter is known to survive. The earliest reasonably complete version available to scholars today, named P, dates to approximately the year 200 AD, approximately 150 years after the original was presumably drafted. This papyrus is fragmented in a few areas, causing some of the original text carefully preserved over the years to be missing, \"however, through careful research relating to paper construction, handwriting development, and the established principles of textual criticism, scholars can be rather certain about where these errors and changes appeared and what the original text probably said.\"\n\nSome scholars date the original composition to c. 50–60 AD. Other scholars agree that Galatians was written between the late 40s and early 50s.\n\nBiblical scholars agree that Galatians is a true example of Paul's writing. The main arguments in favor of the authenticity of Galatians include its style and themes, which are common to the core letters of the Pauline corpus. Moreover, Paul's possible description of the Council of Jerusalem () gives a different point of view from the description in , if it is, in fact, describing the Jerusalem Council.\n\nThe central dispute in the letter concerns the question of how Gentiles could convert to Christianity, which shows that this letter was written at a very early stage in church history, when the vast majority of Christians were Jewish or Jewish proselytes, which historians refer to as the Jewish Christians. Another indicator that the letter is early is that there is no hint in the letter of a developed organization within the Christian community at large. This puts it during the lifetime of Paul himself.\n\nNevertheless, a small number of scholars have questioned Paul's authorship of Galatians, such as Bruno Bauer, Abraham Loman, C. H. Weisse and Frank R. McGuire.\n\nPaul's letter is addressed \"to the churches of Galatia\" (), but the location of these churches is a matter of debate. A minority of scholars have argued that the \"Galatia\" is an ethnic reference to a Celtic people living in northern Asia Minor, but most agree that it is a geographical reference to the Roman province in central Asia Minor, which had been settled by immigrant Celts in the 270s BC and retained Gaulish features of culture and language in Paul's day. Acts of the Apostles records Paul traveling to the \"region of Galatia and Phrygia\", which lies immediately west of Galatia.\n\nSome claim the New Testament says that the churches of Galatia (Antioch of Pisidia, Iconium, Lystra and Derbe) were founded by Paul himself (; ). They seem to have been composed mainly of converts from paganism (). After Paul's departure, the churches were led astray from Paul's trust/faith-centered teachings by individuals proposing \"another gospel\" (which centered on salvation through the Mosaic law, so-called legalism), whom Paul saw as preaching a \"different gospel\" from what Paul had taught (). The Galatians appear to have been receptive to the teaching of these newcomers, and the epistle is Paul's response to what he sees as their willingness to turn from his teaching.\n\nThe identity of these \"opponents\" is disputed. However, the majority of modern scholars view them as Jewish Christians, who taught that in order for converts to belong to the People of God, they must be subject to some or all of the Jewish Law, (i.e. Judaizers). The letter indicates controversy concerning circumcision, Sabbath observance, and the Mosaic Covenant. It would appear, from Paul's response, that they cited the example of Abraham, who was circumcised as a mark of receiving the covenant blessings (). They certainly appear to have questioned Paul's authority as an apostle, perhaps appealing to the greater authority of the Jerusalem church governed by James (brother of Jesus).\n\nThe North Galatian view holds that the epistle was written very soon after Paul's second visit to Galatia (). In this view, the visit to Jerusalem, mentioned in , is identical with that of Acts 15, which is spoken of as a thing of the past. Consequently, the epistle seems to have been written after the Council of Jerusalem. The similarity between this epistle and the epistle to the Romans has led to the conclusion that they were both written at roughly the same time, during Paul's stay in Macedonia in roughly 56–57. This third date takes the word \"quickly\" in Gal. 1:6 literally. John P. Meier suggests that Galatians was \"written in the middle or late 50s, only a few years after the Antiochene incident he narrates\". Eminent Biblical scholar Helmut Koester also subscribes to the \"North Galatian Hypothesis\". Koester points out that the cities of Galatia in the north consist of Ankyra, Pessinus, and Gordium (of the Gordian Knot fame of Alexander the Great).\n\nThe South Galatian view holds that Paul wrote Galatians before or shortly after the First Jerusalem Council, probably on his way to it, and that it was written to churches he had presumably planted during either his time in Tarsus (he would have traveled a short distance, since Tarsus is in Cilicia) after his first visit to Jerusalem as a Christian, or during his first missionary journey, when he traveled throughout southern Galatia. If it was written to the believers in South Galatia, it would likely have been written in 49.\n\nA third theory is that Galatians 2:1–10 describes Paul and Barnabas' visit to Jerusalem described in Acts 11:30 and 12:25. This theory holds that the epistle was written before the Council was convened, possibly making it the earliest of Paul's epistles. According to this theory, the revelation mentioned (Gal 2:2) corresponds with the prophecy of Agabus (Acts 11:27–28). This view holds that the private speaking about the gospel shared among the Gentiles precludes the Acts 15 visit, but fits perfectly with Acts 11. It further holds that continuing to remember the poor (Gal. 2: 10) fits with the purpose of the Acts 11 visit (but not Acts 15). In addition, the exclusion of any mention of the letter of Acts 15 is seen to indicate that such a letter did not yet exist, since Paul would have been likely to use it against the legalism confronted in Galatians. Finally, this view doubts Paul's confrontation of Peter (Gal. 2:11) would have been necessary after the events described in Acts 15. If this view is correct, the epistle should be dated somewhere around 47, depending on other difficult to date events, such as Paul's conversion.\n\nKirsopp Lake found this view less likely and wondered why it would be necessary for the Jerusalem Council (Acts 15) to take place at all if the issue were settled in Acts 11:30/12:25, as this view holds. Defenders of the view do not think it unlikely an issue of such magnitude would need to be discussed more than once. Renowned New Testament scholar J.B. Lightfoot also objected to this view since it \"clearly implies that his [Paul's] Apostolic office and labours were well known and recognized before this conference.\" Defenders of this view, such as Ronald Fung, disagree with both parts of Lightfoot's statement, insisting a) Paul received his \"Apostolic Office\" at his conversion (Gal. 1:15–17; Ac. 9). Fung holds, then, that Paul's apostolic mission began almost immediately in Damascus (Acts 9:20). While accepting that Paul's apostolic anointing was likely only recognized by the Apostles in Jerusalem during the events described in Gal. 2/Acts 11:30, Fung does not see this as a problem for this theory.\n\nThe citation here is based on the content of the gospel. Alternative outlines have been introduced based on the rhetorical form of the letter.\n\nGalatians\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis epistle addresses the question of whether the Gentiles in Galatia were obligated to follow Mosaic Law to be part of the Christ community. After an introductory address (), the apostle discusses the subjects which had occasioned the epistle.\n\nIn the first two chapters, Paul discusses his life before Christ and his early ministry, including interactions with other apostles in Jerusalem. This is the most extended discussion of Paul's past that we find in the Pauline letters (cf. Philippians 3:1-7). Some have read this autobiographical narrative as Paul's defense of his apostolic authority (; ). Others, however, see Paul's telling of the narrative as making an argument to the Galatians about the nature of the gospel and the Galatians' own situation.\n\nChapter 3 exhorts the Galatian believers to stand fast in the faith as it is in Jesus. Paul engages in an exegetical argument, drawing upon the figure of Abraham and the priority of his faith to the covenant of circumcision. Paul explains that the law was introduced as a temporary measure, one that is no longer efficacious now that the seed of Abraham, Christ, has come. Chapter 4 then concludes with a summary of the topics discussed and with the benediction, followed by teaching about the right use of their Christian freedom.\n\nIn the conclusion of the epistle, Paul wrote, \"See with what large letters I am writing to you with my own hand.\" (Galatians 6:11, ESV) Regarding this conclusion, Lightfoot, in his Commentary on the epistle, says:\n\n\"At this point the apostle takes the pen from his amanuensis, and the concluding paragraph is written with his own hand. From the time when letters began to be forged in his name (; ) it seems to have been his practice to close with a few words in his own handwriting, as a precaution against such forgeries... In the present case he writes a whole paragraph, summing up the main lessons of the epistle in terse, eager, disjointed sentences. He writes it, too, in large, bold characters (Gr. \"pelikois grammasin\"), that his hand-writing may reflect the energy and determination of his soul.\"\n\nAlternatively, some commentators have postulated that Paul's thorn in the flesh was poor eyesight, which caused him to write in characteristically large letters.\n\nGalatians also contains a catalogue of vices and virtues, a popular formulation of ancient Christian ethics.\n\nProbably the most famous single statement made in the Epistle, by Paul, is in chapter 3, verse 28: \"There is neither Jew nor Gentile, neither slave nor free, nor is there male and female, for you are all one in Christ Jesus.\" The debate surrounding that verse is legend and the two schools of thought are (1) this only applies to the spiritual standing of people in the eyes of God, it does not implicate social distinctions and gender roles on earth; and (2) this is not just about our spiritual standing but is also very much about how we relate to each other and treat each other in the here and now. Position (1) emphasises the immediate context of the verse and notes that it is embedded in a discussion about justification: our relationship with God. Position (2) reminds its critics that the \"whole letter context\" is very much about how people got on in the here and now together, and in fact the discussion about justification came out of an actual example of people treating other people differently (2:11ff).\n\n\nOnline translations of the Epistle to Galatians:\n\nRelated articles:\n", "id": "9949", "title": "Epistle to the Galatians"}
{"url": "https://en.wikipedia.org/wiki?curid=9950", "text": "Epistle to the Philippians\n\nThe Epistle of Paul and Timothy to the Philippians, often referred to simply as Philippians, is the eleventh book in the New Testament. Paul and Timothy first visited Philippi in Greece during Paul's second missionary journey, which occurred between approximately 49 and 51 AD. Philippi was the location of the first Christian community established in Europe.\n\nBiblical scholars are in general agreement that the letter was indeed written by Paul of Tarsus. The estimated date of the letter is 62 AD, about 10 years after Paul's first visit to Philippi.\n\nThe historical background of Philippians is traditionally gathered from two main primary New Testament sources: informative internal data from the letter itself, and related information garnered from the rest of the New Testament Canon, especially from the Acts of the Apostles and the other Pauline Epistles. Other primary information is also derived from external historical sources related to the chronological connections between Paul's association with Philippi, its political and economical setting, and its social and religio-philosophical context.\n\nAccording to the document itself, the Philippians had sent Epaphroditus, their envoy (\"messenger [\"apostolon\"] and minister [\"leitourgon\"]\" Phil 2:25), with contributions as an expression of their \"partnership\" and \"concern\" to meet the needs of Paul (, , and ).\n\nDuring the execution of his responsibilities of travel to deliver their \"gift\" (), Epaphroditus contracted some life-threatening debilitating illness (\"esthenese\", cf. ). At some point he recovers. It is at this time, whether premeditated or due to an extended stay with the apostle, various internal matters are revealed to Paul on the part of Epaphroditus (, , , , , and ).\n\nUpon Epaphroditus' return to health, Paul sends word to the Philippians through Epaphroditus of his upcoming sentence in Rome and of his optimism in the face of death (1:18b-26), along with exhortations to imitate his capacity to rejoice in the Lord despite one's circumstances (2:14–18). Moreover, Paul sends counsel regarding spiritual adversaries among the Philippians (3:1–21), and conflicts within their fellowship (4:2–3). Lastly, he provides receipt of both Epaphroditus' heroism (2:25–30) and the arrival of \"the gift\" (4:10), along with his promise of a divine accounting (4:17–20).\n\nWithin the letter is also found an optimism where Paul's belief of his release is the basis upon which he promises to send Timothy to them for ministry (3:19–23), and an anticipation to also pay them a personal visit (2:24). With this communication Epaphroditus sets out on his homeward journey (2:28–29).\n\nThere has been ongoing debate regarding where Paul was when he wrote this letter (and therefore the date of the letter's composition). Internal evidence in the letter itself points clearly to it being composed while Paul was in custody (Philippians 1:7,13), but which period of imprisonment is highly debated. Some suggest the Roman imprisonment at the end of the Book of Acts (chapter 28:30,31). Others suggest the earlier Caesarean imprisonment (Acts 23-26). Still others suggest an earlier imprisonment again, and postulate an Ephesian imprisonment during Paul's lengthy stay in that city (Acts 19). Until recently no one seems to have advocated the second period of Roman imprisonment (after the end of the book of Acts, but attested to in the writings of early church fathers). Jim Reiher considered and speculated on this theory in a 2012 article. The main reasons suggested for a later date, include:\n\nThis second Roman imprisonment theory is still to be rigorously debated in the wider theological community.\n\nThe letter begins in standard form for an ancient Hellenistic letter structure, with author – or senders – first, then recipients with a greeting (Phil. 1.1–2).\n\nThe address and the greeting is clear:\n\nPhilippians 2:5–11:\nHave this attitude in yourselves which was also in Christ Jesus, who, although He existed in the form of God, did not regard equality with God a thing to be grasped, but emptied Himself, taking the form of a bond-servant, and being made in the likeness of men. Being found in appearance as a man, He humbled Himself by becoming obedient to the point of death, even death on a cross.\nFor this reason also, God highly exalted Him, and bestowed on Him the name which is above every name, so that at the name of Jesus every knee will bow, of those who are in heaven and on earth and under the earth, and that every tongue will confess that Jesus Christ is Lord, to the glory of God the Father.\nThe letter was written to the church at Philippi, one of the earliest churches to be founded in Europe. They were very attached to Paul, just as he was very fond of them. Of all the churches, their contributions (which Paul gratefully acknowledges) are among the only ones he accepts. (Acts 20:33–35; 2 Cor. 11:7–12; 2 Thess. 3:8). The generosity of the Philippians comes out very conspicuously (Phil. 4:15). \"This was a characteristic of the Macedonian missions, as 2 Cor. 8 and 9 amply and beautifully prove. It is remarkable that the Macedonian converts were, as a class, very poor (2 Cor. 8:2), though the very first converts were of all classes (Acts 16); and the parallel facts, their poverty and their open-handed support of the great missionary and his work, are deeply harmonious.\" (Moule).\n\nAs with all epistles, the original was composed in Greek.\n\nPhilippians has been the subject of much research and Ralph P. Martin argues that Philippians 2 may be considered the beginning of the field of Christology, specifically referring to the rich analysis that Apostle Paul began in . Veronica Koperski views as the beginning of the analysis of the knowledge of Christ.\n\nWhile Paul's opening prayer is for love (1:9), based on knowledge of Christ, his final prayer is for the peace of God (4:7), which surpasses all understanding. Thus the concepts of love, knowledge and peace are jointly developed in the Epistle.\n\n\n\nOnline translations of the Epistle to the Philippians:\nOnline Study of Philippians:\n\nRelated articles:\n", "id": "9950", "title": "Epistle to the Philippians"}
{"url": "https://en.wikipedia.org/wiki?curid=9951", "text": "Epistle to the Colossians\n\nThe Epistle of Paul to the Colossians, usually referred to simply as Colossians, is the twelfth book of the New Testament. It was written, according to the text, by Paul the Apostle and Timothy to the Church in Colossae, a small Phrygian city near Laodicea and approximately 100 miles (160 km) from Ephesus in Asia Minor.\n\nScholars have increasingly questioned Paul's authorship and attributed the letter to an early follower instead. The authenticity of the letter, however, has been defended with equal strength.\n\nDuring the first generation after Jesus, Paul's epistles to various churches helped establish early Christian theology. According to Bruce Metzger, it was written in the 50s while Paul was in prison. Colossians is similar to Ephesians, also written at this time. Some critical scholars have ascribed the epistle to an early follower of Paul, writing as Paul. The epistle's description of Christ as pre-eminent over creation marks it, for some scholars, as representing an advanced christology not present during Paul's lifetime. Defenders of Pauline authorship cite the work's similarities to Philemon, which is broadly accepted as authentic.\n\nThe letter may have been written by Paul at Rome during his first imprisonment. (Acts , ) Other scholars have suggested that it was written from Caesarea or Ephesus. If the letter is not considered to be an authentic part of the Pauline corpus, then it might be dated during the late 1st century, possibly as late as AD 80.\n\nLike some of his other epistles (e.g., those to Corinth: 1 Corinthians, 2 Corinthians), this seems to have been written in consequence of information which had been conveyed to Paul of the internal state of the church there by Epaphras. A faithful minister to the Colossians, Epaphras was visiting Paul when the epistle was written.\n\nThe letter's authors claim to be Paul and Timothy, but authorship began to be authoritatively questioned during the 19th century. Pauline authorship was held to by many of the early church's prominent theologians, such as Irenaeus, Clement of Alexandria, Tertullian, Origen of Alexandria and Eusebius.\n\nHowever, as with several epistles attributed to Paul, critical scholarship disputes this claim. One ground is that the epistle's language doesn't seem to match Paul's, with 48 words appearing in Colossians that are found nowhere else in his writings and 33 of which occur nowhere else in the New Testament. A second ground is that the epistle features a strong use of liturgical-hymnic style which appears nowhere else in Paul's work to the same extent. A third is that the epistle's themes related to Christ, eschatology and the church seem to have no parallel in Paul's undisputed works.\n\nThose who are advocates of Pauline authorship defend the differences that there are between elements in this letter and those commonly considered the genuine work of Paul (e.g. 1 Thessalonians). It is argued that these differences can come by human variability, such as by growth in theological knowledge over time, different occasion for writing, as well as use of different secretaries (or amanuensis) in composition. As it is usually pointed out by the same authors who note the differences in language and style, the number of words foreign to the New Testament and Paul is no greater in Colossians than in the undisputed Pauline letters (Galatians, of similar length, has 35 hapax legomena). In regard to the style, as Norman Perrin, who argues for pseudonymity, notes, \"The letter does employ a great deal of traditional material and it can be argued that this accounts for the non-Pauline language and style. If this is the case, the non-Pauline language and style are not indications of pseudonymity.\" Not only that, but it has been noted that Colossians has indisputably Pauline stylistic characteristics, found nowhere else in the New Testament. Advocates of Pauline authorship also argue that the differences between Colossians and the rest of the New Testament are not as great as they are purported to be.\n\nColossae is in the region of the seven churches of . In there is mention of local brethren in Colossae, Laodicea, and Hierapolis. Colossae was approximately 12 miles (19 km) from Laodicea and 14 miles (23 km) from Hierapolis. Members of the congregation at Colossae had incorporated pagan elements into their practice, including worship of elemental spirits. The Epistle to the Colossians declares Christ's supremacy over the entire created universe and exhorts Christians to lead godly lives. The letter consists of two parts: first a doctrinal section, then a second regarding conduct. In both sections, false teachers who have been spreading error in the congregation are opposed.\n\nI. Introduction (1:1-14)\nII. The Supremacy of Christ (1:15-23)\n\nIII. Paul's Labor for the Church (1:24-2:7)\nIV. Freedom from Human Regulations through Life with Christ (2:8-23)\nV. Rules for Holy Living (3:1-4:6)\nVI. Final Greetings (4:7-18)\nIn its doctrinal sections, Colossians emphatically explains that Christ is begotten before all creation (not created) and is supreme over all that has been created. All things were created through him and for him, and the universe is sustained by him. God had chosen for his complete being to dwell in Christ. The \"cosmic powers\" revered by the false teachers had been \"discarded\" and \"led captive\" at Christ's death. Christ is the master of all angelic forces and the head of the church. Christ is the only mediator between God and humanity, the unique agent of cosmic reconciliation. Interestingly, it is the Father in Colossians who is said to have delivered us from the domain of darkness and transferred us to the kingdom of His beloved Son (), not the typical way of articulating salvation today. The Son is the agent of reconciliation and salvation not merely of the church, but in some sense redeems the rest of creation as well (\"all things, whether things on earth or things in heaven\" ().\n\nThe doctrinal part comprises the first two chapters. Its main theme is developed in chapter , with a warning against being drawn away from Him in whom dwelt all the fullness of the deity, and who was the head of all spiritual powers. Christ was the head of the body of which they were members; and if they were truly united to him, what more did they need?\n\nColossians praises the spiritual growth of the recipients because of their love for all the set-apart ones in Christ. It calls them to grow in wisdom and knowledge that their love might be principled love and not sentimentality. \"Christ in you is your hope of glory!\".\n\n\"Christ in you, the hope of Glory\"\n\nOne of the great themes of the doctrinal section of Colossians is promise of union with Christ through the indwelling life of God the Holy Spirit. For example, Colossians 1:27, \"To them God has chosen to make known among the Gentiles the glorious riches of this mystery, which is Christ in you, the hope of glory.\" The Apostle Paul wrote to remind them of this promise and guard them against moving their on going trust from Christ to other philosophies and traditions which did not depend on Christ.\n\nAs does 1 Corinthians 13, Colossians 1, in early verses, deals with faith, hope and love. While in the case of 1 Corinthians, the love \"does not\" statements like boasting recall the criticisms of the Corinthians earlier in the book, Colossians is different. Paul looks at the faith, hope and love of the Colossian believers as evidence and validation of their faith experience. In introductory verses, 1:6,7, Paul states that the fruit seen in them started the day they understood the grace of God in truth. Conduct is a fruit of faith.\n\nColossians denounces ascetic practices or avoiding certain foods because Christ's death put an end to such distinctions. Believers are one in Christ, not divided between circumcised and uncircumcised, slave and free, and so on. He then calls on his audience to fulfill all domestic and social obligations. A striking image of religion by such rules used by Paul is his use of the word \"shadows\". Some practices in the past may be shadows of what is to come, but Christ is the one who cast the shadow.\n\nColossians does not state with precision what heresies were being faced by the church. There are approximately 44 different theories what the heresies encountered by the church at Colossae were, including a view there was no particular heresy at all but issues typical of those faced by believers. Some argue that it is not merely fasting or feasting or particular area of emotions, conduct or intellect that are denounced but doing them with an independent spirit, not connected to Christ, the head of the body or lack of appreciation of the gospel of grace.\n\nThe practical part of the Epistle, , enforces various duties naturally flowing from the doctrines expounded. They are exhorted to mind things that are above , to mortify every evil principle of their nature, and to put on the new man. Many special duties of the Christian life are also insisted upon as the fitting evidence of the Christian character. The letter ends with customary prayer, instruction, and greetings.\n\nColossians is often categorized as one of the \"prison epistles\" that include Ephesians, Colossians, Philippians, and Philemon. Colossians has some close parallels with the letter to Philemon — names of some of the same people (e.g., Timothy, Aristarchus, Archippus, Mark, Epaphras, Luke, Onesimus, and Demas) appear in both epistles, and both were written by Paul.\n\nTychicus is named as the bearer of the letter, just as he is in Ephesians and Philemon, and he is to tell the recipients of the state of the apostle. After friendly greetings, he bids them exchange this letter with the one he had sent to the neighbouring Laodicean Church. (The Epistle to the Laodiceans was a possible lost letter of Paul which some scholars identify with the canonical Epistle to the Ephesians, others dispute this view.) He then closes the letter with the usual salutation.\n\nColossians calls, in several places, for faithfulness to be recognized:\n\nIt is striking that the apostle writes in Col 1:28 [ESV]: \"Him we proclaim, warning everyone and teaching everyone with all wisdom, that we may present everyone mature in Christ. For this I [Paul] toil, struggling with all his energy that he powerfully works within me.\" Then in Col 3:16, Paul's goal is that the Colossians themselves to in some manner continue in this ministry and admonish and test each other, \"one another\" as Paul did for them. Col 3:16: \"Let the word of Christ dwell in you [Colossian believers] richly, teaching and admonishing one another in all wisdom, singing psalms and hymns and spiritual songs, with thankfulness in your hearts to God.\" In some sense, Paul would have the teaching and admonishing ministry continue by members of the church in Colossae.\n\nColossians has some interesting interpersonal dynamics. Epaphras, who first shared the good news to Colossae, near the opening of the book is presented as a beloved fellow servant—\"one of us\" from Paul's ministry point of view. In contrast, towards the closing of the book he is called \"one of you\". Onesimus is also referred to as \"one of you\" and an important helper of Paul who was the run away slave in the book of Philemon who became a believer. His past is not brought up. Mark and Barnabus parted company with Paul over Mark leaving the mission after violent opposition in the book of Acts, but here is reconciled to Paul and spoken of as a comfort. Epharoditus, Paul refers to as a fellow prisoner, we recognize as the person in the book of Acts who was literally dragged through the streets into court on charges. Paul acknowledges that most never saw his face in Colossae or Laodicea, yet was concerned throughout the book they were aware of the events going on with himself, and appointed Tychicus to inform them so that they should understand how great Paul's suffering for them has been.\n\n\n\nOnline translations of the Epistle to the Colossians:\n", "id": "9951", "title": "Epistle to the Colossians"}
{"url": "https://en.wikipedia.org/wiki?curid=9952", "text": "First Epistle to the Thessalonians\n\nThe First Epistle to the Thessalonians, usually referred to simply as First Thessalonians (written 1 Thessalonians and abbreviated 1 Thess. or 1 Thes.), is a book from the New Testament of the Christian Bible. The first letter to the Thessalonians was likely the first of Paul's letters, probably written by the end of AD 52, making it the oldest book in the New Testament.\n\nMost New Testament scholars believe Paul the Apostle wrote this letter from Corinth, although information appended to this work in many early manuscripts (e.g., Codices Alexandrinus, Mosquensis, and Angelicus) state that Paul wrote it in Athens after Timothy had returned from Macedonia with news of the state of the church in Thessalonica (; ). For the most part, the letter is personal in nature, with only the final two chapters spent addressing issues of doctrine, almost as an aside. Paul's main purpose in writing is to encourage and reassure the Christians there. Paul urges them to go on working quietly while waiting in hope for the return of Christ.\n\nUnlike all subsequent Pauline epistles, 1 Thessalonians does not focus on justification by faith or questions of Jewish–Gentile relations, themes that are covered in all other letters. Many scholars see this as an indication that this letter was written before the Epistle to the Galatians, where Paul's positions on these matters were formed and elucidated.\n\nThe majority of New Testament scholars hold 1 Thessalonians to be authentic, although a number of scholars in the mid-19th century contested its authenticity, most notably Clement Schrader and F.C. Baur. 1 Thessalonians matches other accepted Pauline letters, both in style and in content, and its authorship is also affirmed by 2 Thessalonians. \n\n\nIt is also sometimes suggested that 1 Thes. 5:1–11 is a post-Pauline insertion that has many features of Lukan language and theology that serves as an apologetic correction to Paul's imminent expectation of the Second Coming in 1 Thes. 4:13–18.\n\nOther scholars, such as Schmithals, Eckhart, Demke and Munro, have developed complicated theories involving redaction and interpolation in 1 and 2 Thessalonians.\n\nPaul claimed the title of the \"Apostle to the Gentiles\", and established gentile churches in several important cities in the Roman Empire.\n\nAccording to Bart D. Ehrman, the Acts of the Apostles tells a different story of Paul's career, but in this case it reports that, while there were \"some\" Jews converted during Paul's initial preaching in Thessalonica, the gentiles who were converted were \"a large number\" and the Jews as a body fiercely opposed Paul's work there.\n\n\nPaul, speaking for himself, Silas, and Timothy, gives thanks for the news about their faith and love; he reminds them of the kind of life he had lived while he was with them. Paul stresses how honorably he conducted himself, reminding them that he had worked to earn his keep, taking great pains not to burden anyone. He did this, he says, even though he could have used his status as an apostle to impose upon them.\n\nPaul goes on to explain that the dead will be resurrected prior to those still living, and both groups will greet the Lord in the air.\n\n\n", "id": "9952", "title": "First Epistle to the Thessalonians"}
{"url": "https://en.wikipedia.org/wiki?curid=9953", "text": "Epistle to Titus\n\nThe Epistle of Paul to Titus, usually referred to simply as Titus, is one of the three Pastoral Epistles (along with 1 Timothy and 2 Timothy) traditionally attributed to Paul the Apostle and is part of the New Testament. It is addressed to Saint Titus and describes the requirements and duties of elders and bishops. Some consider it, along with 2 Timothy, to be Paul's final instructions to early church leaders before his final departure.\n\nNot mentioned in the Acts of the Apostles, Saint Titus was noted in Galatians (cf. Gal. 2:1, 3) where Paul wrote of journeying to Jerusalem with Barnabas, accompanied by Titus. He was then dispatched to Corinth, Greece, where he successfully reconciled the Christian community there with Paul, its founder. Titus was later left on the island of Crete to help organize the Church, and later met back with the Apostle Paul in the Nicopolis. He soon went to Dalmatia (now Croatia). According to Eusebius of Caesarea in the \"Ecclesiastical History\", he served as the first bishop of Crete and remained there in his old years. He was buried in Cortyna (Gortyna), Crete; his head was later removed to Venice during the invasion of Crete by the Saracens in 832 and was enshrined in St Mark's Basilica, Venice, Italy.\n\nScholars are not unanimous about the authenticity of the pastoral epistles, but it is considered pseudepigraphic by about 80% of scholars. Titus is usually one of the three Pastoral epistles attributed to Paul. Titus has a very close affinity with 1 Timothy, sharing similar phrases and expressions and similar subject matter.\n\nThe author of \"Titus\" identifies himself as \"Paul, a servant of God and an apostle of Jesus Christ.\" According to \"Easton's Bible Dictionary\"(1893), The Epistle was probably written about the same time as the First Epistle to Timothy, with which it has many affinities.\"\n\nScholars who believe Paul wrote \"Titus\" date its composition from the circumstance that it was written after Paul's visit to Crete (Titus 1:5). That visit could not be the one referred to in the Acts of the Apostles 27:7, when Paul was on his voyage to Rome as a prisoner, and where he continued a prisoner for two years. Thus traditional exegesis supposes that after his release Paul sailed from Rome into Asia, passing Crete by the way, and that there he left Titus \"to set in order the things that were wanting.\" Thence he would have gone to Ephesus, where he left Timothy, and from Ephesus to Macedonia, where he wrote the \"First Epistle to Timothy\", and thence, according to the subscription of this epistle, to \"Nicopolis of Macedonia\", from which place he wrote to Titus, about 66 or 67.\nHowever, works written under a false name would have been very problematic since the early church clearly excluded from the apostolic canon any works they thought to be pseudonymous. While critics point to the common practice of pseudonymous writing in the ancient world, they usually fail to point out that this practice, though common in the culture, was not common in personal letters, and was categorically rejected by the early church (cf. 2 Thess. 2:2; 3:17; also Muratorian Canon 64–67; Eusebius, Ecclesiastical History 6.12.3). Tertullian (c. a.d. 160–225) wrote that when it was discovered that a church elder had composed a pseudonymous work, The Acts of Paul (which included a purported Pauline letter, 3 Corinthians), the offending elder \"was removed from his office\" (On Baptism 17).\n\nThe Pastoral epistles are regarded by majority of scholars as being pseudepigraphical. On the basis of the language and content of the pastoral epistles, these scholars today doubt that they were written by Paul and believe that they were written after his death. The early Church did not agree. Critics claim the vocabulary and style of the Pauline letters could not have been written by Paul according to available biographical information and reflect the views of the emerging Church rather than the apostle's. These scholars date the epistle from the 80s CE up to the end of the 2nd century. The Church of England's Common Worship Lectionary Scripture Commentary concurs with this view: \"the proportioning of the theological and practical themes is one factor that leads us to think of these writings as coming from the post-Pauline church world of the late first or early second century\".\n\nOne of the secular peculiarities of the Epistle to Titus is the reference to the Epimenides paradox: \"One of the Cretans, a prophet of their own, said, 'Cretans are always liars'.\" The statement by a member of a group that all members are liars is a famous logic problem, applicable also to .\n\n\nOnline translations of the Epistle to Titus:\n\nExegetical papers on Titus:\n", "id": "9953", "title": "Epistle to Titus"}
{"url": "https://en.wikipedia.org/wiki?curid=9954", "text": "Eurovision Song Contest\n\nThe Eurovision Song Contest (), sometimes popularly called Eurovision but not to be confused with the Eurovision network that broadcasts it, is the longest-running annual international TV song competition, held, primarily, among the member countries of the European Broadcasting Union (EBU) since 1956. The competition was based upon the existing Sanremo Music Festival held in Italy since 1951.\n\nEach participating country submits an original song to be performed on live television and radio and then casts votes for the other countries' songs to determine the most popular song in the competition. The contest has been broadcast every year for sixty years, since its inauguration in 1956, and is one of the longest-running television programmes in the world. It is also one of the most watched non-sporting events in the world, with audience figures having been quoted in recent years as anything between 100 million and 600 million internationally. Eurovision has also been broadcast outside Europe to several countries that do not compete, such as the United States, Canada, New Zealand, and China. An exception was made in 2015, when Australia was allowed to compete as a guest entrant as part of the celebration of the 60th anniversary of the event. In November 2015, the EBU announced that Australia was invited back as a participant in the 2016 contest after their success in 2015. Following their success again in 2016, Australia will compete again in 2017. Since 2000, the contest has also been broadcast over the Internet via the Eurovision website.\n\nWinning the Eurovision Song Contest provides a short-term boost to the winning artists' career, but rarely results in long-term success. Notable exceptions are ABBA (winner in 1974 for Sweden), Bucks Fizz (winner in 1981 for the United Kingdom) and (winner in 1988 for Switzerland), all of whom launched successful worldwide careers after their wins.\n\nIreland holds the record for the highest number of wins, having won the contest seven times—including four times in five years in 1992, 1993, 1994 and 1996. Under the current voting system, the highest scoring winner (and only winner) is Jamala of Ukraine who won the 2016 contest in Stockholm, Sweden with 534 points. Under the previous system, in place from 1975 to 2015, the highest scoring winner is Alexander Rybak of Norway with 387 points in 2009.\n\nIn the 1950s, as a war-torn Europe rebuilt itself, the European Broadcasting Union (EBU)—based in Switzerland—set up an ad hoc committee to search for ways of bringing together the countries of the EBU around a \"light entertainment programme\". At a committee meeting held in Monaco in January 1955 with Marcel Bezençon of the Swiss television as chairman, the committee conceived the idea (initially proposed by Sergio Pugliese of the Italian television RAI) of an international song contest where countries would participate in one television programme to be transmitted simultaneously to all countries of the union. The competition was based upon the existing Sanremo Music Festival held in Italy and was seen as a technological experiment in live television, as in those days it was a very ambitious project to join many countries together in a wide-area international network. Satellite television did not exist, and the Eurovision Network comprised a terrestrial microwave network. The concept, then known as \"Eurovision Grand Prix\", was approved by the EBU General Assembly in a meeting held in Rome on 19 October 1955, and it was decided that the first contest would take place in spring 1956 in Lugano, Switzerland. The name \"Eurovision\" was first used in relation to the EBU's network by British journalist George Campey in the London \"Evening Standard\" in 1951.\n\nThe first contest was held in the town of Lugano, Switzerland, on 24 May 1956. Seven countries participated—each submitting two songs, for a total of 14. This was the only contest in which more than one song per country was performed: since 1957, all contests have allowed one entry per country. The 1956 contest was won by the host nation, Switzerland.\n\nThe programme was first known as the \"Eurovision Grand Prix\" (in English). This \"Grand Prix\" name was adopted by Denmark, Norway and the Francophone countries, with the French designation being \"\". The \"Grand Prix\" has since been dropped and replaced with\" \" (contest) in French, but not in Danish or Norwegian. The Eurovision network is used to carry many news and sports programmes internationally, among other specialised events organised by the EBU. However, in the minds of the public, the name \"Eurovision\" is most closely associated with the Song Contest.\n\nThe format of the contest has changed over the years, though the basic tenets have always been thus: participant countries submit new original songs, which are performed live in a television programme transmitted across the Eurovision Network by the EBU simultaneously to all countries. A \"country\" as a participant is represented by one television broadcaster from that country: typically, but not always, that country's national public broadcasting organisation. The programme is hosted by one of the participant countries, and the transmission is sent from the auditorium in the host city. During this programme, after all the songs have been performed, the countries then proceed to cast votes for the other countries' songs: nations are not allowed to vote for their own song. At the end of the programme, the song with the most points is declared as the winner. The winner receives, simply, the prestige of having won—although it is usual for a trophy to be awarded to the winning songwriters, and the winning country is formally invited to host the event the following year.\n\nThe programme is invariably opened by one or more presenters, welcoming viewers to the show. Between the songs and the announcement of the voting, an interval act is performed. These acts can be any form of entertainment. Interval entertainment has included such acts as the Wombles () and the first international presentation of Riverdance ().\n\nAs national broadcasters join and leave the Eurovision feed transmitted by the EBU, the EBU/Eurovision network logo ident (not to be confused with the song contest logo) is displayed. The accompanying theme music (used on other Eurovision broadcasts) is the prelude to Marc-Antoine Charpentier's \"Te Deum\". Originally, the same logo was used for both the Eurovision network and the European Broadcasting Union, however, they now have two different logos; when the ident is transmitted, it is the Eurovision network logo that appears.\n\nThe Eurovision Song Contest finals are traditionally held on a Saturday evening in May, at 19:00 UTC (15:00 EDT, 20:00 BST/IST, or 21:00 CEST). Usually one Saturday in May is chosen, although the contest has been held on a Tuesday (since the two semi-final system was introduced in 2008), on a Thursday (in 1956; and since 2005 in the semi-finals) and as early as March (in 1979).\n\nEligible participants include primarily Active Members (as opposed to Associate Members) of the EBU. Active members are those who are located in states that fall within the European Broadcasting Area, or are member states of the Council of Europe.\n\nThe European Broadcasting Area is defined by the International Telecommunication Union:\n\nThe western boundary of Region 1 is defined by a line running from the North Pole along meridian 10° West of Greenwich to its intersection with parallel 72° North; thence by great circle arc to the intersection of meridian 50° West and parallel 40° North; thence by great circle arc to the intersection of meridian 20° West and parallel 10° South; thence along meridian 20° West to the South Pole.\n\nActive members include broadcasting organisations, whose transmissions are often made available to at least 98% of households in their own country which are equipped to receive such transmissions.\n\nIf an EBU Active Member wishes to participate they must fulfil conditions as laid down by the rules of the contest. A separate copy is drafted annually. As of 2015, this includes the necessity to have broadcast the previous year's programme within their country, and the broadcaster must have paid the EBU a participation fee in advance of the deadline specified in the rules of the contest for the year in which they wish to participate.\n\nEligibility to participate is not determined by geographic inclusion within the continent of Europe, despite the \"Euro\" in \"Eurovision\" – nor does it have any relation to the European Union. Several countries geographically outside the boundaries of Europe have competed: Israel and Cyprus in Western Asia (Cyprus is a member of the Council of Europe and a member state of the European Union), since 1973 and 1981 respectively; Australia in the Australian continent, since 2015 and Morocco, in North Africa, in the 1980 competition alone. In addition, several transcontinental countries with only part of their territory in Europe have competed: Turkey, since 1975; Russia, since 1994; Armenia, since 2006; Georgia, since 2007; and Azerbaijan, which made its first appearance in the 2008 edition.\n\n52 countries have participated at least once. These are listed here alongside the year in which they made their début:\n\nMost of the expense of the contest is covered by commercial sponsors and contributions from the other participating nations. The contest is considered to be a unique opportunity for promoting the host country as a tourist destination. In the summer of 2005, Ukraine abolished its normal visa requirement for visitors from the EU to coincide with its hosting of the event.\n\nPreparations for the event start a matter of weeks after the host wins in the previous year, and confirms to the EBU that they intend to—and have the capacity to—host the event. A host city is chosen—often a national or regional capital city—and a suitable concert venue is identified. The two largest concert venues were Parken in Copenhagen (which held approximately 38,000 people when Denmark hosted in 2001) and the Esprit Arena in Düsseldorf (which held approximately 36,500 people when Germany hosted in 2011). The smallest town to have been hosts was Millstreet in County Cork, Ireland, in 1993. The village had a population of 1,500—although the Green Glens Arena venue could hold up to 8,000 people.\n\nThe hotel and press facilities in the vicinity are always a consideration when choosing a host city and venue. In Kiev 2005, hotel rooms were scarce as the contest organisers asked the Ukrainian government to put a block on bookings they did not control themselves through official delegation allocations or tour packages: this led to many people's hotel bookings being cancelled.\n\nAfter the first two contests were hosted by Switzerland and Germany, it was decided that henceforth the winning country would host the contest the next year. The winner of the 1957 Contest was the Netherlands, and Dutch television accepted the responsibility of hosting in 1958. In all but five of the years since this rule has been in place, the winning country has hosted the show the following year. The exceptions are:\n\n\nWith the invitation of Australia to participate since 2015, it was announced that due to the logistical and financial issues that would occur if Australia were to host, in the event of an Australia victory, the broadcaster SBS will co-host the next contest in a European city in collaboration with an EBU Member Broadcaster of their choice. However, this has yet to happen and since 1981, all contests have been held in the country which won the previous year.\n\nThe former generic logo was introduced for the 2004 Eurovision Song Contest in Turkey, to create a consistent visual identity. The host country's flag appears in the heart of the generic logo. Each year of the contest, the host country creates a sub-theme which is usually accompanied and expressed with a sub-logo and slogan. The theme and slogan are announced by the EBU and the host country's national broadcaster.\n\nThe generic logo was revamped in 2014, ten years after the first generic logo was created. The revamped logo was conducted by lead designer Cornelis Jacobs and his team of Cityzen Agency. The logo was used for the first time in the 2015 Eurovision Song Contest, the 60th anniversary of the contest.\n\nSince the 2002 contest, slogans (or themes) have been introduced in the show (2009 being the only exception). The slogan is decided by the host broadcaster and based on the slogan, the theme and the visual design are developed.\nThe term \"Eurovision Week\" is used to refer to the week during which the Contest takes place. As it is a live show, the Eurovision Song Contest requires the performers to have perfected their acts in rehearsals in order for the programme to run smoothly. In addition to rehearsals in their home countries, every participant is given the opportunity to rehearse on the stage in the Eurovision auditorium. These rehearsals are held during the course of several days before the Saturday show, and consequently the delegations arrive in the host city many days before the event. Journalists and fans are also present during the preceding days, and so the events of Eurovision last a lot longer than a few hours of television. A number of officially accredited hotels are selected for the delegations to stay in, and shuttle-bus services are used to transport the performers and accompanying people to and from the contest venue.\n\nEach participating broadcaster nominates a Head of Delegation, whose job it is to co-ordinate the movements of the delegate members, and who acts as that country's representative to the EBU in the host city. Members of the delegations include performers, lyricists, composers, official press officers and—in the years where songs were performed with a live orchestra—a conductor. Also present if desired is a commentator: each broadcaster may supply their own commentary for their TV and/or radio feed, to be broadcast in each country. The commentators are given dedicated commentary booths situated around the back of the arena behind the audience.\n\nSince 2004, the first rehearsals have commenced on the Sunday almost two weeks before the Grand Final. There are two rehearsal periods for each country. The countries taking part in the semi-finals have their first rehearsal over four days from the first Sunday to Wednesday. The second is from Thursday to Sunday. The countries which have already directly qualified for the Grand Final rehearse on the Saturday and Sunday.\nAfter each country has rehearsed, the delegation meets with the show's artistic director in the video viewing room. Here, they watch the footage of the rehearsal just performed. At this point the Head of Delegation may make known any special requirements needed for the performance, and request them from the host broadcaster. Following this meeting, the delegation hold a press conference where members of the accredited press may pose them questions. The rehearsals and press conferences are held in parallel; so one country holds its press conference, while the next one is in the auditorium rehearsing. A printed summary of the questions and answers which emerge from the press conferences is produced by the host press office, and distributed to journalists' pigeon-holes.\n\nBefore each of the semi-finals three dress rehearsals are held. Two rehearsals are held the day before (one in the afternoon and the other in the evening), while the third is held on the afternoon of the live event. Since tickets to the live shows are often scarce, tickets are also sold in order that the public may attend these dress rehearsals.\n\nThe same applies for the final, with two rehearsals on the Friday and the third on Saturday afternoon before the live transmission of the grand final on Saturday evening. For both semi-finals and for the final, the second dress rehearsal is also the \"Jury Final\", this is where the jury from each country casts their votes. This means that 50% of the result is already decided before the live contests have taken place.\n\nOn the Monday evening of Eurovision Week, a \"Mayor's Reception\" is traditionally held, where the city administration hosts a celebration that Eurovision has come to their city. This is usually held in a grand municipally owned location in the city centre. All delegations are invited, and the party is usually accompanied by live music, complimentary food and drink and—in recent years—fireworks.\n\nAfter the semi-final and grand final there are after-show parties, held either in a facility in the venue complex or in another suitable location within the city.\n\nA \"Euroclub\" is held every night of the week: this is a Eurovision-themed nightclub, to which all accredited personnel are invited.\n\nDuring the week many delegations have traditionally hosted their own parties in addition to the officially sponsored ones. However, in the new millennium the trend has been for the national delegations to centralise their activity and hold their celebrations in the Euroclub.\n\nNumerous detailed rules must be observed by the participating nations, and a new version is produced each year, for instance the rules specify various deadlines, including the date by which all the participating broadcasters must submit the final recorded version of their song to the EBU. The rules also cover sponsorship agreements and rights of broadcasters to re-transmit the show. The most notable rules which affect the format and presentation of the contest have changed over the years, and are highlighted here.\n\nAll vocals must be sung live; no voices are permitted on the backing tracks. In 1999, the Croatian song performed by Doris Dragović and composed by Tonči Huljić featured sounds on their backing track which sounded suspiciously like human voices. The Croatian delegation stated that there were no human voices, but only digitally synthesised sounds which replicated vocals. The EBU nevertheless decided that they had broken the spirit of the rules, and docked them 33% of their points total that year for the purpose of calculating their five-year points average for future qualification.\n\nFrom 1956 until 1998, the host country was required to provide a live orchestra. Before 1973, all music had to be played by the host orchestra. From 1973 onwards, pre-recorded, non-vocal backing tracks were permitted—although the host country was still obliged to provide a live orchestra in order to give participants a choice. If a backing track was used, then all the instruments heard on the track were required to be present on the stage. In 1997 this requirement was dropped.\n\nIn 1999 the requirement for a live orchestra was removed: it was left as an optional contribution. The host that year, Israel's IBA, decided not to use an orchestra in order to save expenses, and thus 1999 was the first year when all the songs were played as pre-recorded backing tracks (in conjunction with live vocals).\n\nEach submission must have vocals; purely instrumental music has never been allowed. In the past, competitors have been required to sing in one of their own national languages, but this rule has been changed several times over the years. From 1956 until 1965, there was no rule restricting the languages in which the songs could be sung. In 1966 a rule was imposed stating that the songs must be performed in one of the official languages of the country participating, after Sweden was the first country to not sing in their own language, with opera singer Ingvar Wixell performing Sweden's 1965 entry in English. The Swedish-language version of the song was originally selected at Melodifestivalen 1965, but it was later translated into English for the Eurovision contest.\n\nThe language restriction continued until 1973, when performers were again allowed to sing in any language they wished. Several winners in the mid-1970s took advantage of this: performers from non-English-speaking countries sang in English, including ABBA in 1974.\n\nIn 1977, the EBU decided to revert to the national language restriction. However, special dispensation was given to Germany and Belgium as their national selections had already taken place before the decision was made; both countries' entries that year were in English.\n\nIn 1999 the rule was changed again to allow the choice of language once more, which resulted in 12 out of 23 countries, including United Kingdom, singing in English that year. Belgium entered the 2003 contest with \"Sanomi\", a song sung in a constructed language, finishing in second place. In 2006 the Dutch entry, \"Amambanda\", was sung partly in English and partly in an artificial language. In 2008 the Belgian entry, \"O Julissi\", was sung in an artificial language. In 2011 the Norwegian entry, \"Haba Haba\", which was sung in English and Swahili, was the first song to be sung in an African language, apart from Arabic. In 2016 all but three out of 36 semi-finalists had songs in English, with only two (Bosnia and Herzegovina and Macedonia) performing songs in their native languages, as Austria sent a song in French. In the final, all but three out of 26 contestants had songs in English.\n\nThe voting system used in the contest has changed over the years. The current system has been in place since 2016, and is a positional voting system. Each country awards two sets of 12, 10, 8–1 points to their 10 favourite songs: one from their professional jury of votes of five music professionals and the other from televoting.\n\nHistorically, a country's votes were decided by an internal jury, but in 1997 five countries (Austria, Switzerland, Germany, Sweden and the United Kingdom) experimented with televoting, giving members of the public in those countries the opportunity to vote \"en masse\" for their favourite songs. The experiment was a success, and from 1998 onwards all countries were encouraged to use televoting wherever possible. Back-up juries are still used by each country, in the event of a televoting failure. Nowadays members of the public may also vote by SMS, in addition to televoting. From 2013, the public may also vote via a mobile app.\n\nThe current method for ranking entries, introduced in 2016, is to sum together the points calculated from the telephone vote and the jury separately. Prior to this, the jury and televoting rankings were combined 50/50 before the number of points were calculated. It was first used in the final of the 2009 edition, and extended the following year to the semi-finals.\nSince 1964 the voting has been presided over by the EBU scrutineer, who is responsible for ensuring that all points are allocated correctly and in turn. The following are the scrutineers and Executive Supervisors of the Eurovision Song Contest appointed by the EBU:\nAccording to one study of Eurovision voting patterns, certain countries tend to form \"clusters\" or \"cliques\" by frequently voting in the same way.\n\nAfter the interval act is over, when all the points have been calculated, the presenter(s) of the show call upon each voting country in turn to invite them to announce the results of their vote. Prior to 1994 the announcements were made over telephone lines; with the audio being piped into the auditorium for the audience to hear, and over the television transmission. However, since and including 1994 the announcements have been presented visually. Often the opportunity is taken by each country to show their spokesperson standing in front of a backdrop which includes a famous place in that country. For example, the French spokesperson might be seen standing in front of the Eiffel Tower or an Italian presenter might be seen with the Colosseum in the background.\n\nFrom 1957 to 1962, the participating countries were called in reverse order of the presentation of their songs, and from 1963 to 2003, they were called in the same order in which their songs had been presented (except for 1974). Since 2004, when semi-finals were introduced, the order of the countries' announcements of votes has changed; and the countries that did not make it to the final each year could also vote. In 2004, the countries were called in alphabetical order (according to their ISO codes). In 2005, the votes from the non-qualifying semi-finalists were announced first, in their running order on the Thursday night; then the finalists gave their votes in their own order of performance. Between 2006 and 2010, a separate draw was held to determine the order in which countries would present their votes. In 2011, the voting order was determined by the results of a jury the day before the final so as to create as much suspense as possible when the votes were revealed.\n\nFrom 1971 to 1973, each country sent two jurors, who were present at the contest venue (though the juries in 1972 were locked away in the Great Hall of Edinburgh Castle) and announced their votes as the camera was trained on them. In 1973 one of the Swiss jurors made a great show of presenting his votes with flamboyant gestures. This system was retired the next year.\n\nIn 1956 no public votes were presented: a closed jury simply announced that Switzerland had won. From 1957 to 1987, the points were displayed on a physical scoreboard to the side of the stage. As digital graphic technology progressed, the physical scoreboards were superseded in 1988 by an electronic representation which could be displayed on the TV screen at the will of the programme's director.\n\nIn 2006 the EBU decided to save time during the broadcast—much of which had been taken up with the announcement of every single point—because there was an ever-increasing number of countries voting. Since then, votes from 1 to 7 from each country have been displayed automatically on screen and the remaining points (8, 10 and 12) are read out in ascending order by the spokesperson, culminating with the maximum 12 points. Countries must announce the country names and points in either English or French and the scores are repeated by the contest's presenters in the other language. For this reason, the expression \"\" when the host or spokesperson states the top score in French is popularly associated with the contest throughout the continent.\n\nIn , four of the sixteen countries taking part, France, Spain, the Netherlands and the United Kingdom, all tied for first place with 18 points each. There was nothing in the rules to decide an outright winner, so all four were declared joint winners. This caused much discontent among most of the other participating countries, and mass walkouts were threatened. Finland, Norway, Sweden and Portugal did not participate in the 1970 Contest as a protest against the results of the previous year. This prompted the EBU to introduce a tie-break rule.\n\nUnder the current rules, in the event of more than one country scoring the same total number of points, a count is made of the numbers of countries who awarded points to each of the tied countries, and the one who received points from the most countries is declared the winner. If the numbers are still tied, it is counted how many sets of maximum points (12 points) each country received. If there is still a tie, the numbers of 10-point scores awarded are compared—and then the numbers of 8-point scores, all the way down the list. In the extremely unlikely event of there then \"still\" being a tie for first place, the song performed earliest in the running order is declared the winner, unless the host country performed first in the running order. Since 2008, the same tie-break rule now applies to ties for all places.\n\nAs of 2017, the only time since 1969 when two or more countries have tied for first place on total points alone was in , when France and Sweden both totalled 146 points. At that time, the rules did not include counting the numbers of countries awarding any points to these countries' songs, but began with tallying up the numbers of 12-point scores awarded. Both France and Sweden had received four sets of 12 points. However, because Sweden had received more sets of 10-point scores, they were declared the winners. Had the current rule been in play, France would have won instead.\n\nEach participating broadcaster is required to broadcast the show in its entirety: including all songs, recap, voting and reprise, skipping only the interval act for advertising breaks if they wish. From 1999 onwards, broadcasters who wished to do so were given the opportunity to take more advertising breaks as short, non-essential hiatuses were introduced into the programme. Three major interruptions or preemptions of the contest broadcast have taken place since 1999. The Dutch state broadcaster pulled their broadcast of the 2000 final to provide emergency news coverage of a major incident, the Enschede fireworks disaster. Spain's RTVE delayed their broadcast of the second semi-final in the 2009 Contest, due to the Madrid Open tennis tournament. The Albanian state broadcaster deferred their broadcast of the first semi-final in 2012 to provide emergency news coverage of the Qafa e Vishës bus accident.\n\nThe first edition ever of the Eurovision Song Contest in 1956 was broadcast live and never recorded, and only a sound recording of the radio transmission has survived from the original broadcast. The ninth edition in 1964 hosted by Danmarks Radio was recorded on tape, but a fire destroyed the recording, and it is unknown if any other TV station in Europe has another copy. Only small portions of the original broadcast and audio from the radio transmission have survived.\n\nIn late 2011, the EBU had begun archiving all the contests since the first edition in 1956 in order to be finalised before the 2015 Contest, for the 60th anniversary. It was later reported that the archive is ready and will be released on the 60th anniversary with making the content available to journalists in broadcast-ready formats while also giving public accessibility to \"selected content\" through the official Eurovision website.\n\nIn 1978, hosted in Paris only a month after the 1978 South Lebanon conflict, during the performance of the Israeli entry, the Jordanian broadcaster JRTV suspended the broadcast and showed pictures of flowers. When it became apparent during the later stages of the voting sequence that Israel's song \"A-Ba-Ni-Bi\" was going to win the contest, JRTV abruptly ended the transmission. Afterwards, the Jordanian news media refused to acknowledge that Israel had won and announced that the winner was Belgium (who had actually come in 2nd place). In 1981 JRTV did not broadcast the voting because the name of Israel appeared on the scoreboard.\n\nIn 2005, Lebanon intended to participate in the contest. However, Lebanese law does not allow recognition of Israel, and consequently Lebanese broadcaster Télé Liban did not intend to transmit the Israeli entry. The EBU informed them that such an act would breach the rules of the contest, and Lebanon was subsequently forced to withdraw from the competition. Their late withdrawal incurred a fine, since they had already confirmed their participation and the deadline had passed. However, the Eurovision Song Contest albums were still being sold in Lebanese music stores until 2009, with the word Israel erased from the back cover. As of 2010, the albums were banned completely from sale.\n\nIn 2009, the song \"We Don't Wanna Put In\" was selected to represent Georgia. However, the song text was banned by Eurovision as it was interpreted as criticism against Prime Minister of Russia Vladimir Putin after the Russo-Georgian War the previous year. When asked to change the lyrics of the song, the Georgian broadcaster GPB withdrew from the 2009 contest.\n\n\nThe number of countries participating has steadily grown over time, from seven in 1956 to over 20 in the late 1980s. In 1993, twenty-five countries participated in the competition, including, for the first time, Bosnia-Herzegovina, Croatia and Slovenia, entering independently due to the dissolution of Yugoslavia. In the most recent edition in 2016, a total of 42 countries took part, with 26 appearing in the final.\n\nBecause the contest is a live television programme, a reasonable time limit must be imposed on the duration of the show. In recent years the nominal limit has been three hours, with the broadcast occasionally over-running.\n\nSince 1993, and following the cessation of the Eastern European OIRT network and the merger with the EBU, there have been more entries than there is time to reasonably include in a single TV show. Several relegation or qualification systems have been tried in order to limit the number of countries participating in the contest at one time. Thus the 1993 Contest introduced two new features: first, a pre-selection competition was held in Ljubljana in which seven new countries fought for three places in the international competition. Bosnia-Herzegovina, Croatia, Estonia, Hungary, Romania, Slovenia and Slovakia took part in \"Kvalifikacija za Millstreet\"; and the three former Yugoslav republics, Bosnia-Herzegovina, Croatia and Slovenia, qualified for a place in the international final. Also to be introduced that year was \"relegation\": the lowest-placed countries in the 1993 score table were not invited in 1994, to allow the countries which failed the 1993 pre-selection into the 1994 Contest. The 1994 Contest included—for the first time—Estonia, Romania, Slovakia, Lithuania, Hungary, Poland and Russia.\n\nRelegation continued in 1994 and 1995; but in 1996 a different pre-selection system was used, in which nearly all the countries participated. Audio tapes of all the songs were sent to juries in each of the countries some weeks before the television show. These juries selected the songs which would be included in the international broadcast. Norway, as the host country in 1996 (having won the previous year), automatically qualified and so did not need to go through pre-selection.\n\nOne country which failed to qualify in the 1996 pre-selection was Germany. As one of the largest financial contributors to the EBU, their non-participation in the contest brought about a funding issue, which the EBU would have to consider.\n\nSince 2000, France, Germany, Spain and United Kingdom have automatically qualified for the final, regardless of their positions on the scoreboard in previous contests, as they are the four biggest financial contributors to the EBU. These countries became known as the \"Big Four\". On 31 December 2010, it was announced that Italy would compete in the Eurovision Song Contest after a fourteen-year absence and that it would also automatically qualify for the final, joining the other four qualifiers to become the \"Big Five\", considered by some to be a controversial decision. Germany became the first and, as of 2016, the only \"Big Five\" country to win the contest since the rule was made in 2000, when Lena Meyer-Landrut won the 2010 Contest. Turkey withdrew from the 2013 Contest with the status of the \"Big Five\" being one of the reasons cited. They also did not participate in the following 3 years' of contests (2014–16) for similar reasons, as well as stating their opposition to the 50/50 jury and televoting system that began being applied in the final of the 2009 Contest.\n\nFrom 1997 to 2001, countries qualified for each contest based on the average of their points totals for their entries over the previous five years. However, there was much discontent voiced over this system because a country could be excluded merely because of poor previous results, which did not take into account how good a fresh attempt might be. This led the EBU to create what was hoped would be a more permanent solution to the problem. A qualification round, known as the semi-final, was introduced for the 2004 Contest. This semi-final was held on the Wednesday during Eurovision Week, and was a programme similar in format to the grand final, whose time slot remained 19:00 UTC on the Saturday. The highest-placed songs from the semi-final qualified for the grand final, while the lower-placed songs were eliminated. From 2005 to 2007, the semi-final programme was held on the Thursday of Eurovision Week. In these two shows there was enough time to include all the countries who wished to participate.\n\nThe ten highest-placed non-Big Four countries in the \"grand final\" were guaranteed a place in the following year's grand final, without having to qualify. If, for example, Germany came in the top ten, the eleventh-placed non-Big-Four country would automatically qualify for the next year's grand final. The remaining countries—which had not automatically qualified for the grand final—had to enter the semi-final.\n\nAt the 50th annual meeting of the EBU reference group in September 2007, it was decided that, with still more nations entering, starting from the 2008 contest onwards \"two\" semi-finals would be held, from each of which one could qualify for the final. From 2008 onwards, the scoreboard position in previous years has not been relevant, and—save for the automatic qualifiers—all participating countries have had to participate in the semi-finals, regardless of their previous year's scoreboard position. The only countries which automatically qualify for the grand final are the host country and the Big Five: France, Germany, Italy, Spain and the United Kingdom, who continue to enjoy their protected status.\n\nIn each of the semi-finals the voting is conducted among those countries which participate in that semi-final. With regard to the automatic grand final qualifiers, who do not participate in the semi-finals, a draw is conducted to determine in which semi-final each of them will be allowed to vote. In contrast, \"every\" participating country in a particular year may vote in the Saturday grand final – whether their song qualified for the final or not.\n\nThe ten countries which receive the most votes in each semi-final qualify for the grand final. They are announced by the presenters in English and French, in a random order. Full voting results are withheld until after the grand final, whereupon they are published on the EBU's website. To date only five countries have always qualified to the Final since the implementation of the semi-finals system in 2004: Australia, Azerbaijan, Romania, Russia and Ukraine.\n\nThere have been a number of Eurovision artists and groups whose careers were directly launched into the spotlight following their win. Notable examples were ABBA, who won the contest for Sweden in 1974 with their song \"Waterloo\", and went on to become one of the most successful bands of all time, and the French Canadian singer Céline Dion, who won the contest for Switzerland in 1988 with the song \"Ne partez pas sans moi\", which subsequently helped launch her international career,\nand the winners of the 1981 contest, Bucks Fizz for the United Kingdom with the song \"Making Your Mind Up\", which also launched their successful international career.\n\nOther artists who have achieved varying degrees of success after winning the contest include\n\nFrance Gall (\"Poupée de cire, poupée de son\", Luxembourg 1965), Dana (\"All Kinds of Everything\", Ireland 1970),\nVicky Leandros (\"Après toi\", Luxembourg 1972),\nBrotherhood of Man (\"Save Your Kisses for Me\", United Kingdom 1976), Johnny Logan (who won twice for Ireland; with \"What's Another Year\" in 1980, and \"Hold Me Now\" in 1987).\nSeveral other winners were well-known artists who won the contest mid-career after they had already established themselves, including Katrina and the Waves, winners in 1997 with \"Love Shine a Light\", Lulu, winner in 1969 with \"Boom Bang-a-Bang\", and Sandie Shaw, winner in 1967 with \"Puppet on a String\". Women have dominated the contest since its inception, either performing solo or as a member of a group on 49 of the 64 winning entries as of 2016.\n\n holds the record for the highest number of wins, having won the contest seven times—including four times in five years in 1992, 1993, 1994 and 1996. is second with six wins as of 2016. , and the are joint third with five wins. Next comes the , with four victories. Three countries have won three times, , and . Six countries have won twice, , , , , and .\n\nThe United Kingdom holds the record for the highest number of runner-up placings, coming in second on no less than 15 occasions as of 2016. Germany, Russia, France, Spain and Ireland have four runner-up entries.\n\nThe early years of the contest saw many wins for \"traditional\" Eurovision countries: France, the Netherlands, and Luxembourg. However, the success of these countries has declined in recent decades; the Netherlands last won in 1975; France, in 1977; and Luxembourg, in 1983. Luxembourg last entered the contest in 1993.\n\nThe first years of the 21st century produced numerous first-time winners, from both \"new\" and long-serving countries who had previous entered numerous times but without victories. Every year from 2001 to 2008 inclusive, a country won for its first time. Estonia was the first post-Soviet country to win the competition in 2001. In 2005, won for the first time, 15 years after the last Southern European country won, i.e. in 1990; overall the South of Europe won the competition only four times. The 2006 winner was with a song performed by a hard rock band dressed in monster constumes, making it Finland's first win after having entered the contest for 45 years. , on the other hand, did not have to wait so long, winning with only their second entry in 2004.\n\nThe country that has participated the longest without any win is , which made its debut in 1964 and has never finished in the top five. is the most successful country without a win, achieving two-second places and two third places.\n\nIn 2009, won the contest with 387 points – Alexander Rybak held the winning title with his song \"Fairytale\". His outstanding performance meant he had the highest total in the history of the competition, becoming the first competitor to score 300 or more points, including 16 maximum scores. This feat was emulated in 2012, when Sweden won with 372 points, but with a new record of 18 maximum scores. In 2015, won the contest with 365 points, becoming the first country to ever reach 300 points or more twice while winning both times. placed second with 303 points, becoming the first country to score more than 300 points without winning. In 2016, the scoring system was changed, which meant that it was much easier to achieve over 300 points – in fact, the winner – Jamala of , achieved 534 points, and all of top 9 scored 200 or more points, and 25 of the 26 positions got their highest points ever.\n\nSince the introduction of the 50/50 voting system in 2009, the juries and the voters have disagreed on the winner on three occasions, in 2011, 2015, and in 2016. 2011 winner Azerbaijan won only the televotes (jury votes were won by Italy) and 2015 winner Sweden won only the jury votes (televotes were won by Italy). In 2016, Ukraine did not win either the jury vote or the televote, but won the contest with the highest combined vote. The Televote was won by Russia and the jury vote by Australia.\n\nIn 1981, a concert television programme was held to commemorate the contest's twenty-fifth anniversary. The event, entitled Songs of Europe, took place in Mysen, Norway, featuring nearly all the winners of the contest, from 1956 to 1981. It was hosted by Rolf Kirkvaag and Titten Tei.\n\nIn 2005, the EBU had agreed with the Danish broadcaster, DR, to produce a programme to celebrate the 50th anniversary of the contest. The show, entitled after Cliff Richard's entry for the United Kingdom, was held in Copenhagen, and featured a competition among fourteen of the most popular songs from the last 50 years of the contest. A telephone vote was held to determine the most popular Eurovision song of all-time, which was won by the ABBA song \"Waterloo\" (winner for Sweden in ). The event was hosted by the 1997 Contest winner for the United Kingdom, Katrina Leskanich, and Latvia's representative on its debut at the 2000 Contest, Renārs Kaupers.\n\nIn 2015, the EBU had decided again to commemorate the contest and agreed with the United Kingdom's broadcaster, BBC, to produce a show for the 60th anniversary of the contest, after evaluating several proposals from member broadcasters in regards to the anniversary celebration beyond the 2015 Contest in May. The event, entitled Eurovision Song Contest's Greatest Hits, took place at the Eventim Apollo in Hammersmith, London and featured fifteen acts from thirteen countries in the official line-up. Unlike the 50th anniversary show in 2005 which was broadcast live, this event didn't feature a competition and was pre-recorded to be televised across Europe and other EBU members on various dates schedule by the respective broadcasters. The event was hosted by the British commentator for Eurovision, Graham Norton, and the host of the 2013 and 2016 Contest, Petra Mede.\n\nThe contest has been the subject of criticism regarding both its musical and political content. For example, on rare occasions, certain countries have been booed when performing or receiving points, especially when being given by a neighbour country. Most recently in 2014 and 2015, Russia was heavily booed when it qualified for the final and received high points. The reason for the booing is considered to be due to the Russian military intervention in Ukraine and opposition to the country's policy on LGBT rights.\n\nBecause the songs play to such a diverse supranational audience with contrasting musical tastes, and countries want to be able to appeal to as many people as possible to gain votes, this has led to the music of the contest being characterised as a \"mishmash of power ballads, ethnic rhythms and bubblegum pop\". This well-established pattern, however, was notably broken in 2006 with Finnish glam metal band Lordi's victory. As Eurovision is a visual show, many performances attempt to attract the attention of the voters through means other than the music, notably elaborate lighting sequences and pyrotechnics; sometimes leading to bizarre on-stage theatrics, costumes, including the use of revealing dress.\n\nThe contest has long been accused by some of political bias; the perception is that judges and televoters allocate points based on their nation's relationship to the other countries, rather than the musical merits of the songs. According to one study of Eurovision voting patterns, certain countries tend to form \"clusters\" or \"cliques\" by frequently voting in the same way. Another study concludes that as of 2006, voting blocs have, on at least two occasions, crucially affected the outcome of the contest. On the other hand, others argue that certain countries allocate disproportionately high points to others because of similar musical tastes and cultures and because they speak similar languages, and are therefore more likely to appreciate each other's music.\n\nAs an example, Terry Wogan, the United Kingdom's well-known presenter of Eurovision since 1980 and one of the only three presenters mentioned by name during the contest proper stood down from the BBC One's broadcast in 2008 saying \"The voting used to be about the songs. Now it's about national prejudices. We [the United Kingdom] are on our own. We had a very good song, a very good singer, we came joint last. I don't want to be presiding over another debacle\".\n\nAnother influential factor is the high proportion of expatriates and ethnic minorities living in certain countries. Although judges and televoters cannot vote for their own country's entry, expatriates can vote for their country of origin.\n\nThe total numbers of points to be distributed by each country are equal, irrespective of the country's population. Thus voters in countries with larger populations have less power as individuals to influence the result of the contest than those voting in smaller countries. For example, San Marino holds the same voting power as Russia despite the vast geographic and population differences between them.\n\nTo try to reduce the effect of voting blocs, national juries were re-introduced alongside televoting in the final in 2009: each contributing 50% of the vote. This hybrid system was expanded in 2010 to also be implemented in the semi-finals. However, since 1994 no country has won two years in a row, and semi-finals have also been won by different countries, until 2012 when Sweden won the second semi-final in 2011 and 2012. Although many of them used to give their 12 points to the same country each year, like Cyprus and Greece, it has been noticed that factors such as the sets of other high votes received (7, 8 or 10 points) and the number of countries giving points to a specific entry, also highly affect the final positions.\n\nFrom 2013 onwards, the final and the semifinals running order of the competing performances at the semi-finals and the final has been decided by the show's producers and then approved by the EBU Executive Supervisor and the Reference Group. An \"allocation draw\" occurs for the final and the semifinals with each nation drawing to perform in the first or second half. Prior to 2013, the order was decided at random (though when the host nation performs is still decided at random, to ensure fairness). The change in procedure was aimed to make the show more exciting and ensure that all contestants had a chance to stand out, preventing entries that are too similar from cancelling each other out. The decision elicited mixed reactions from both fans and participating broadcasters. Some fans have alleged that there is a risk of corruption and that the order can be manipulated to benefit certain countries, since the running order is considered to be of importance to the result. Including 2016 edition, the only positions in the running order that have never won the contest are numbers 2 and 16, with position number 21 winning for the first time in 2016. Position 17 has the most victories, with 7. Positions 25, 26 and 27 haven't won either, but there have been very few finals with that many participants.\n\nA number of spin-offs and imitators of the Eurovision Song Contest have been produced over the years, some national and other international.\n\nSimilar competitions that are still held, include:\n\nSimilar competitions that are no longer held, include:\n\n", "id": "9954", "title": "Eurovision Song Contest"}
{"url": "https://en.wikipedia.org/wiki?curid=9955", "text": "Nitrox\n\nNitrox is used to a lesser extent in surface-supplied diving, as these advantages are reduced by the more complex logistical requirements for nitrox compared to the use of simple low-pressure compressors for breathing gas supply. Nitrox can also be used in hyperbaric treatment of decompression illness, usually at pressures where pure oxygen would be hazardous. Nitrox is not a safer gas than compressed air in all respects; although its use can reduce the risk of decompression sickness, it increases the risk of oxygen toxicity and fire.\n\nThough not generally referred to as nitrox, an oxygen-enriched air mixture is routinely provided at normal surface ambient pressure as oxygen therapy to patients with compromised respiration and circulation.\n\nEnriched Air Nitrox, nitrox with an oxygen content above 21%, is mainly used in scuba diving to reduce the proportion of nitrogen in the breathing gas mixture. Reducing the proportion of nitrogen by increasing the proportion of oxygen reduces the risk of decompression sickness for the same dive profile, or allows extended dive times without increasing the need for decompression stops for the same risk. The significant aspect of extended no-stop time when using nitrox mixtures is reduced risk in a situation where breathing gas supply is compromised, as the diver can make a direct ascent to the surface with an acceptably low risk of decompression sickness. The exact values of the extended no-stop times vary depending on the decompression model used to derive the tables, but as an approximation, it is based on the partial pressure of nitrogen at the dive depth. This principle can be used to calculate an equivalent air depth (EAD) with the same partial pressure of nitrogen as the mix to be used, and this depth is less than the actual dive depth for oxygen enriched mixtures. The equivalent air depth is used with air decompression tables to calculate decompression obligation and no-stop times. The Goldman decompression model predicts a significant risk reduction by using nitrox (more so than the PADI tables suggest).\n\nControlled tests have not shown breathing nitrox to reduce the effects of nitrogen narcosis, as oxygen seems to have similarly narcotic properties under pressure to nitrogen; thus one should not expect a reduction in narcotic effects due only to the use of nitrox. Nonetheless, there are people in the diving community who insist that they feel reduced narcotic effects at depths breathing nitrox.\nThis may be due to a dissociation of the subjective and behavioural effects of narcosis. However, because of risks associated with oxygen toxicity, divers do not usually use nitrox at greater depths where more pronounced narcosis symptoms are more likely to occur. For a reduction in narcotic effects, trimix or heliox, gases which also contain helium, are generally used by divers.\n\nThere is anecdotal evidence that the use of nitrox reduces post-dive fatigue, particularly in older and or obese divers; however a double-blind study to test this found no statistically significant reduction in reported fatigue. There was, however, some suggestion that post dive fatigue is due to sub-clinical decompression sickness (DCS) (i.e. micro bubbles in the blood insufficient to cause symptoms of DCS); the fact that the study mentioned was conducted in a dry chamber with an ideal decompression profile may have been sufficient to reduce sub-clinical DCS and prevent fatigue in both nitrox and air divers. In 2008, a study was published using wet divers at the same depth and confirmed that no statistically significant reduction in reported fatigue is seen.\n\nFurther studies with a number of different dive profiles, and also different levels of exertion, would be necessary to fully investigate this issue. For example, there is much better scientific evidence that breathing high-oxygen gases increases exercise tolerance, during aerobic exertion. Though even moderate exertion while breathing from the regulator is a relatively uncommon occurrence in scuba, as divers usually try to minimize it in order to conserve gas, episodes of exertion while regulator-breathing do occasionally occur in sport diving. Examples are surface-swimming a distance to a boat or beach after surfacing, where residual \"safety\" cylinder gas is often used freely, since the remainder will be wasted anyway when the dive is completed. It is possible that these so-far un-studied situations have contributed to some of the positive reputation of nitrox.\n\nNitrox50 is used as one of the options in the first stages of therapeutic recompression using the Comex CX 30 table for treatment of vestibular or general decompression sickness. Nitrox is breathed at 30 msw and 24 msw and the ascents from these depths to the next stop. At 18m the gas is switched to oxygen for the rest of the treatment.\n\nNitrox is known by many names: Enriched Air Nitrox, Oxygen Enriched Air, Nitrox, EANx or Safe Air. Since the word is a compound contraction or coined word and not an acronym, it should not be written in all upper case characters as \"NITROX\", but may be initially capitalized when referring to specific mixtures such as Nitrox32, which contains 68% nitrogen and 32% oxygen. When one figure is stated, it refers to the oxygen percentage, not the nitrogen percentage. The original convention, Nitrox68/32 became shortened as the first figure is redundant.\n\nThe term \"nitrox\" was originally used to refer to the breathing gas in a seafloor habitat where the oxygen has to be kept to a lower fraction than in air to avoid long term oxygen toxicity problems. It was later used by Dr Morgan Wells of NOAA for mixtures with an oxygen fraction higher than air, and has become a generic term for binary mixtures of nitrogen and oxygen with any oxygen fraction, and in the context of recreational and technical diving, now usually refers to a mixture of nitrogen and oxygen with more than 21% oxygen. \"Enriched Air Nitrox\" or \"EAN\", and \"Oxygen Enriched Air\" are used to emphasize richer than air mixtures. In \"EANx\", the \"x\" was originally the x of nitrox, but has come to indicate the percentage of oxygen in the mix and is replaced by a number when the percentage is known; for example a 40% oxygen mix is called EAN40. The two most popular blends are EAN32 and EAN36, developed by NOAA for scientific diving, and also named Nitrox I and Nitrox II, respectively, or Nitrox68/32 and Nitrox64/36. These two mixtures were first utilized to the depth and oxygen limits for scientific diving designated by NOAA at the time.\n\nThe term Oxygen Enriched Air (OEN) was accepted by the (American) scientific diving community, but although it is probably the most unambiguous and simply descriptive term yet proposed, it was resisted by the recreational diving community, sometimes in favour of less appropriate terminology.\n\nIn its early days of introduction to non-technical divers, nitrox has occasionally also been known by detractors by less complimentary terms, such as \"devil gas\" or \"voodoo gas\" (a term now sometimes used with pride).\n\nAmerican Nitrox Divers International (ANDI) uses the term \"SafeAir\", which they define as any oxygen-enriched air mixture with O concentrations between 22% and 50% that meet their gas quality and handling specifications, and specifically claim that these mixtures are safer than normally produced breathing air. Considering the complexities and hazards of mixing, handling, analyzing, and using oxygen-enriched air, this name is considered inappropriate by those who consider that it is not inherently \"safe\", but merely has decompression advantages.\n\nThe constituent gas percentages are what the gas blender aims for, but the final actual mix may vary from the specification, and so a small flow of gas from the cylinder must be measured with an oxygen analyzer, before the cylinder is used underwater.\n\nMaximum Operating Depth (MOD) is the maximum safe depth at which a given nitrox mixture can be used. MOD depends on the allowed partial pressure of oxygen, which is related to exposure time and the acceptable risk assumed for central nervous system oxygen toxicity. Acceptable maximum ppO varies depending on the application:\nHigher values are used by commercial and military divers in special circumstances, often when the diver uses surface supplied breathing apparatus, or for treatment in a chamber, where the airway is relatively secure.\n\nThe two most common recreational diving nitrox mixes contain 32% and 36% oxygen, which have maximum operating depths (MODs) of and respectively when limited to a maximum partial pressure of oxygen of . Divers may calculate an equivalent air depth to determine their decompression requirements or may use nitrox tables or a nitrox-capable dive computer.\n\nNitrox with more than 40% oxygen is uncommon within recreational diving. There are two main reasons for this: the first is that all pieces of diving equipment that come into contact with mixes containing higher proportions of oxygen, particularly at high pressure, need special cleaning and servicing to reduce the risk of fire. The second reason is that richer mixes extend the time the diver can stay underwater without needing decompression stops far further than the duration permitted by the capacity of typical diving cylinders. For example, based on the PADI nitrox recommendations, the maximum operating depth for EAN45 would be and the maximum dive time available at this depth even with EAN36 is nearly 1 hour 15 minutes: a diver with a breathing rate of 20 litres per minute using twin 10-litre, 230-bar (about double 85 cu. ft.) cylinders would have completely emptied the cylinders after 1 hour 14 minutes at this depth.\n\nUse of nitrox mixtures containing 50% to 80% oxygen is common in technical diving as decompression gas, which by virtue of its lower partial pressure of inert gases such as nitrogen and helium, allows for more efficient (faster) elimination of these gases from the tissues than leaner oxygen mixtures.\n\nIn deep open circuit technical diving, where hypoxic gases are breathed during the bottom portion of the dive, a Nitrox mix with 50% or less oxygen called a \"travel mix\" is sometimes breathed during the beginning of the descent in order to avoid hypoxia. Normally, however, the most oxygen-lean of the diver's decompression gases would be used for this purpose, since descent time spent reaching a depth where bottom mix is no longer hypoxic is normally small, and the distance between this depth and the MOD of any nitrox decompression gas is likely to be very short, if it occurs at all.\n\nThe composition of a nitrox mix can be optimized for a given planned dive profile. This is termed \"Best mix\", for the dive, and provides the maximum no-decompression time compatible with acceptable oxygen exposure. An acceptable maximum partial pressure of oxygen is selected based on depth and planned bottom time, and this value is used to calculate the oxygen content of the best mix for the dive:\n\nThere are several methods of production: \n\nAny diving cylinder containing a blend of gasses other than standard air is required by most diver training organizations, and some national governments, to be clearly marked to indicate the current gas mixture. In practice it is common to use a printed adhesive label to indicate the type of gas (in this case nitrox), and to add a temporary label to specify the analysis of the current mix.\n\nTraining standards for nitrox certification suggest the composition must be verified by the diver by using an oxygen analyzer before use.\n\nWithin the EU, valves with M26x2 outlet thread are recommended for cylinders with increased oxygen content. Regulators for use with these cylinders require compatible connectors, and are not directly connectable with cylinders for compressed air.\n\nA German standard specifies that any mixture with an oxygen content greater than atmospheric air must be treated as pure oxygen. A nitrox cylinder is specially cleaned and identified. The cylinder colour is overall white with the letter N on opposite sides of the cylinder. The fraction of oxygen in the bottle is checked after filling and marked on the cylinder.\n\nSouth African National Standard 10019:2008 specifies the colour of all scuba cylinders as Golden yellow with French gray shoulder. This applies to all underwater breathing gases except medical oxygen, which must be carried in cylinders that are Black with a White shoulder. Nitrox cylinders must be identified by a transparent, self-adhesive label with green lettering, fitted below the shoulder. In effect this is green lettering on a yellow cylinder, with a gray shoulder. The composition of the gas must also be specified on the label. In practice this is done by a small additional self-adhesive label with the oxygen fraction, which is changed when a new mix is filled.\n\nEvery nitrox cylinder should also have a sticker stating whether or not the cylinder is \"oxygen clean\" and suitable for partial pressure blending. Any oxygen-clean cylinder may have any mix up to 100% oxygen inside. If by some accident an oxygen-clean cylinder is filled at a station that does not supply gas to oxygen-clean standards it is then considered contaminated and must be re-cleaned before a gas containing more than 40% oxygen may again be added. Cylinders marked as 'not oxygen clean' may only be filled with oxygen-enriched air mixtures from membrane or stick blending systems where the gas is mixed before being added to the cylinder, and to an oxygen fraction not exceeding 40% by volume.\n\nDiving with and handling nitrox raise a number of potentially fatal dangers due to the high partial pressure of oxygen (ppO). Nitrox is not a deep-diving gas mixture owing to the increased proportion of oxygen, which becomes toxic when breathed at high pressure. For example, the maximum operating depth of nitrox with 36% oxygen, a popular recreational diving mix, is to ensure a maximum ppO of no more than . The exact value of the maximum allowed ppO and maximum operating depth varies depending on factors such as the training agency, the type of dive, the breathing equipment and the level of surface support, with professional divers sometimes being allowed to breathe higher ppO than those recommended to recreational divers.\n\nTo dive safely with nitrox, the diver must learn good buoyancy control, a vital part of scuba diving in its own right, and a disciplined approach to preparing, planning and executing a dive to ensure that the ppO is known, and the maximum operating depth is not exceeded. Many dive shops, dive operators, and gas blenders require the diver to present a nitrox certification card before selling nitrox to divers.\n\nSome training agencies, such as PADI and Technical Diving International, teach the use of two depth limits to protect against oxygen toxicity. The shallower depth is called the \"maximum operating depth\" and is reached when the partial pressure of oxygen in the breathing gas reaches . The deeper depth, called the \"contingency depth\", is reached when the partial pressure reaches . Diving at or beyond this level exposes the diver to a greater risk of central nervous system (CNS) oxygen toxicity. This can be extremely dangerous since its onset is often without warning and can lead to drowning, as the regulator may be spat out during convulsions, which occur in conjunction with sudden unconsciousness (general seizure induced by oxygen toxicity).\n\nDivers trained to use nitrox may memorise the acronym VENTID-C or sometimes ConVENTID, (which stands for Vision (blurriness), Ears (ringing sound), Nausea, Twitching, Irritability, Dizziness, and Convulsions). However, evidence from non-fatal oxygen convulsions indicates that most convulsions are not preceded by any warning symptoms at all. Further, many of the suggested warning signs are also symptoms of nitrogen narcosis, and so may lead to misdiagnosis by a diver. A solution to either is to ascend to a shallower depth.\n\nMany training agencies such as PADI, CMAS, SSI and NAUI train their divers to personally check the oxygen percentage content of each nitrox cylinder before every dive. If the oxygen percentage deviates by more than 1% from the value written on the cylinder by the gas blender, the scuba diver must either recalculate his or her bottom times with the actual mix, or else abort the dive to avoid increased risk of oxygen toxicity or decompression sickness. Under IANTD and ANDI rules for use of nitrox, which are followed by most dive resorts around the world, filled nitrox cylinders are signed out personally in a gas blender log book, which contains, for each cylinder and fill, the cylinder number, the measured oxygen percent composition, the signature of the receiving diver (who should have personally measured the oxygen partial pressure before taking delivery), and finally a calculation of the maximum operating depth for that fill/cylinder. All of these steps minimize danger but increase complexity of operations (for example, personalized cylinders for each diver must generally be kept track of on dive boats with nitrox, which is not the case with generic compressed air cylinders).\nIn South Africa, the national standard for handling and filling portable cylinders with pressurised gases (SANS 10019) requires that the cylinder be labelled with a sticker identifying the contents as nitrox, and specifying the oxygen fraction. Similar requirements may apply in other countries.\n\nPartial pressure blending using pure oxygen decanted into the cylinder before topping up with air may involve very high oxygen fractions and oxygen partial pressures during the decanting process, which constitute a relatively high fire hazard. This procedure requires care and precautions by the operator, and decanting equipment and cylinders which are clean for oxygen service, but the equipment is relatively simple and inexpensive. Partial pressure blending using pure oxygen is often used to provide nitrox on live-aboard dive boats, but it is also used in some dive shops and clubs.\n\nAny gas which contains a significantly larger percentage of oxygen than air is a fire hazard, and such gases can react with hydrocarbons or lubricants and sealing materials inside the filling system to produce toxic gases, even if a fire is not apparent. Some organisations exempt equipment from oxygen-clean standards if the oxygen fraction is limited to 40% or less.\n\nAmong training agencies, only ANDI subscribes to the guideline of requiring oxygen cleaning for equipment used with more than 23% oxygen fraction. The USCG, NOAA, U.S. Navy, OSHA, and the other nitrox training agencies accept the limit as 40% as no accident or incident has been known to occur when this guideline has been properly applied. Tens of thousands of recreational divers are trained each year and the overwhelming majority of these divers are taught the \"over 40% rule\". Most nitrox fill stations which supply pre-mixed nitrox will fill cylinders with mixtures below 40% without certification of cleanliness for oxygen service. For a history of this controversy see Luxfer cylinders.\n\nThe following references for oxygen cleaning specifically cite the \"over 40%\" guideline that has been in widespread use since the 1960s, and consensus at the 1992 Enriched Air Workshop was to accept that guideline and continue the status quo.\n\nMuch of the confusion appears to be a result of misapplying PVHO (pressure vessel for human occupancy) guidelines which prescribe a maximum ambient oxygen content of 25% when a human is sealed into a pressure vessel (chamber). The concern here is for a fire hazard to a living person who could be trapped in an oxygen-rich burning environment.\n\nOf the three commonly applied methods of producing enriched air mixes - continuous blending, partial pressure blending, and membrane separation systems - only partial pressure blending would require the valve and cylinder components to be oxygen cleaned for mixtures with less than 40% oxygen. The other two methods ensure that the equipment is never subjected to greater than 40% oxygen content.\n\nIn a fire, the pressure in a gas cylinder rises in direct proportion to its absolute temperature. If the internal pressure exceeds the mechanical limitations of the cylinder and there are no means to safely vent the pressurized gas to the atmosphere, the vessel will fail mechanically. If the vessel contents are ignitable or a contaminant is present this event may result in a \"fireball\".\n\nIn the 1920s or 1930s Draeger of Germany made a nitrox backpack independent air supply for a standard diving suit.\n\nChristian J. Lambertsen proposed calculations for nitrogen addition to prevent oxygen toxicity in divers utilizing nitrogen-oxygen rebreather diving.\n\nIn World War II or soon after, British commando frogmen and work divers started sometimes diving with oxygen rebreathers adapted for semi-closed-circuit nitrox (which they called \"mixture\") diving by fitting larger cylinders and carefully setting the gas flow rate using a flow meter. These developments were kept secret until independently duplicated by civilians in the 1960s.\n\nIn the 1950s the United States Navy (USN) documented enriched oxygen gas procedures for military use of what we today call nitrox, in the USN Diving Manual.\n\nIn 1970, Morgan Wells, who was the first director of the National Oceanographic and Atmospheric Administration (NOAA) Diving Center, began instituting diving procedures for oxygen-enriched air. He also developed a process for mixing oxygen and air which he called a continuous blending system. For many years Wells' invention was the only practical alternative to partial pressure blending. In 1979 NOAA published Wells' procedures for the scientific use of nitrox in the NOAA Diving Manual.\n\nIn 1985 Dick Rutkowski, a former NOAA diving safety officer, formed IAND (International Association of Nitrox Divers) and began teaching nitrox use for recreational diving. This was considered dangerous by some, and met with heavy skepticism by the diving community.\n\nIn 1991, in a watershed moment, the annual DEMA show (held in Houston, Texas that year) banned nitrox training providers from the show. This caused a backlash, and when DEMA relented, a number of organizations took the opportunity to present nitrox workshops outside the show.\n\nIn 1992 BSAC banned its members from using nitrox during BSAC activities.\n\nIn 1992 IAND's name was changed to the International Association of Nitrox and Technical Divers (IANTD), the T being added when the European Association of Technical Divers (EATD) merged with IAND. In the early 1990s, these agencies were teaching nitrox, but the main scuba agencies were not. Additional new organizations, including the American Nitrox Divers International (ANDI) - which invented the term \"Safe Air\" for marketing purposes - and Technical Diving International (TDI) were begun.\n\nMeanwhile, diving stores were finding a purely economic reason to offer nitrox: not only was an entire new course and certification needed to use it, but instead of cheap or free tank fills with compressed air, dive shops found they could charge premium amounts of money for custom-gas blending of nitrox to their ordinary, moderately experienced divers. With the new dive computers which could be programmed to allow for the longer bottom-times and shorter residual nitrogen times that nitrox gave, the incentive for the sport diver to use the gas increased. An intersection of economics and scientific validity had occurred.\n\nIn 1993 \"Skin Diver\" magazine, the leading recreational diving publication at the time, published a three-part series arguing that nitrox was unsafe for sport divers. Against this trend, in 1992 NAUI became the first existing major recreational diver training agency to sanction nitrox.\n\nIn 1993 DiveRite manufactured the first nitrox-compatible dive computer, called the Bridge.\n\nIn 1994 BSAC reversed its policy on Nitrox and announced BSAC nitrox training to start in 1995\n\nIn 1996, the Professional Association of Diving Instructors (PADI) announced full educational support for nitrox. While other mainline scuba organizations had announced their support of nitrox earlier, it was PADI's endorsement that established nitrox as a standard recreational diving option.\n\nAt times in the geological past the Earth's atmosphere contained much more than 20% oxygen: e.g. up to 35% in the Upper Carboniferous period. This let animals absorb oxygen more easily and influenced their evolutionary patterns.\n\n\n", "id": "9955", "title": "Nitrox"}
{"url": "https://en.wikipedia.org/wiki?curid=9956", "text": "Erik Satie\n\nÉric Alfred Leslie Satie (; 17 May 18661 July 1925), who signed his name Erik Satie after 1884, was a French composer and pianist. Satie was a colourful figure in the early 20th-century Parisian avant-garde. His work was a precursor to later artistic movements such as minimalism, Surrealism, repetitive music, and the Theatre of the Absurd.\n\nAn eccentric, Satie was introduced as a \"gymnopedist\" in 1887, shortly before writing his most famous compositions, the \"Gymnopédies\". Later, he also referred to himself as a \"phonometrician\" (meaning \"someone who measures sounds\"), preferring this designation to that of \"musician\", after having been called \"a clumsy but subtle technician\" in a book on contemporary French composers published in 1911.\n\nIn addition to his body of music, Satie was \"a thinker with a gift of eloquence\" who left a remarkable set of writings, having contributed work for a range of publications, from the dadaist \"391\" to the American culture chronicle \"Vanity Fair\". Although in later life he prided himself on publishing his work under his own name, in the late 19th century he appears to have used pseudonyms such as ' and ' in some of his published writings.\n\nSatie was the son of Alfred Satie and his wife Jane Leslie (née Anton), who was born in London to Scottish parents. Erik was born at Honfleur in Normandy; his home there is open to the public. When Satie was four years old, his family moved to Paris, his father having been offered a translator's job in the capital. After his mother's death in 1872, he was sent (at age 6), together with his younger brother, Conrad, back to Honfleur to live with his paternal grandparents. There he received his first music lessons from a local organist. In 1878, when he was 12 years old, his grandmother died, and the two brothers were reunited in Paris with their father, who remarried (a piano teacher) shortly afterwards. From the early 1880s onwards, Satie started publishing salon compositions by his step-mother and himself, among others.\n\nIn 1879, Satie entered the Paris Conservatoire, where he was soon labelled untalented by his teachers. Georges Mathias, his professor of piano at the Conservatoire, described his pupil's piano technique in flatly negative terms, \"insignificant and laborious\" and \"worthless\". Émile Decombes called him \"the laziest student in the Conservatoire\". Years later, Satie related that Mathias, with great insistence, told him that his real talent lay in composing. After being sent home for two and a half years, he was readmitted to the Conservatoire at the end of 1885 (age 19), but was unable to make a much more favourable impression on his teachers than he had before, and, as a result, resolved to take up military service a year later. However, Satie's military career did not last very long; within a few months he was discharged after deliberately infecting himself with bronchitis.\n\nSatie moved from his father's residence to lodgings in Montmartre in 1887. By this time he had started what was to be an enduring friendship with the romantic poet Patrice Contamine, and had his first compositions published by his father. He soon integrated with the artistic clientele of the Le Chat Noir Café-cabaret, and started publishing his \"Gymnopédies\". Publication of compositions in the same vein (', ', etc.) followed. In the same period he befriended Claude Debussy. He moved to a smaller room, still in Montmartre (, now a museum), in 1890. By 1891 he was the official composer and chapel-master of the Rosicrucian Order \"\", led by , which led to compositions such as ', \"Le Fils des étoiles\", and the '. Satie gave performances at the Salon de la Rose + Croix, organized by Péladan.\n\nBy mid-1892, Satie had composed the first pieces in a compositional system of his own making ('), provided incidental music to a chivalric esoteric play (two '), had his first hoax published (announcing the premiere of \"\", an anti-Wagnerian opera he probably never composed), and broken from Péladan, starting that autumn with the \"Uspud\" project, a \"Christian Ballet\", in collaboration with . While the comrades from both the and sympathised, a promotional brochure was produced for the project, which reads as a pamphlet for a new esoteric sect.\n\nIn 1893, Satie met the young Maurice Ravel for the first time, Satie's style emerging in the first compositions of the youngster. One of Satie's own compositions of that period, \"Vexations\", was to remain undisclosed until after his death. By the end of the year he had founded the (Metropolitan Art Church of Jesus the Conductor). As its only member, in the role of \"Parcier et Maître de Chapelle\", he started to compose a \"\" (later to become known as the \"Messe des pauvres\"), and wrote a flood of letters, articles and pamphlets showing off his self-assuredness in religious and artistic matters. To give an example: he applied for membership in the Académie Française twice, leaving no doubt in the application letter that the board of that organisation (presided over by Camille Saint-Saëns) as much as owed him such membership. Such proceedings without doubt rather helped to wreck his popularity in the cultural establishment. In 1895 he inherited some money, allowing him to have more of his writings printed, and to change from wearing a priest-like habit to being the \"Velvet Gentleman\".\n\nBy mid-1896, all of Satie's financial means had vanished, and he had to move to cheaper and much smaller lodgings, first at the , and two years later, after he had composed the two first sets of \"\" in 1897, to Arcueil, a suburb some five kilometres from the centre of Paris. During this period he re-established contact with his brother Conrad for numerous practical and financial matters, disclosing some of his inner feelings in the process. The letters to Conrad made it clear that he had set aside his religious ideas.\n\nFrom 1899 on, Satie started making money as a cabaret pianist, adapting over a hundred compositions of popular music for piano or piano and voice, adding some of his own. The most popular of these were ', text by Henry Pacory; ', text by Vincent Hyspa; ', a waltz; \"La Diva de l'Empire\", text by Dominique Bonnaud/Numa Blès; ', a march; ', text by Contamine de Latour lost, but the music later reappears in '; and quite a few more, many of which have been lost. In his later years, Satie would reject all his cabaret music as vile and against his nature, but for the time being, it was an income.\n\nOnly a few compositions that Satie took seriously remain from this period: \"Jack in the Box\", music to a pantomime by Jules Depaquit (called a \"\" by Satie); \"\", a short comic opera on a serious theme, text by \"Lord Cheminot\"; \"The Dreamy Fish\", piano music to accompany a lost tale by Cheminot; and a few others that were mostly incomplete, hardly any of them staged, and none of them published at the time.\n\nBoth ' and \"The Dreamy Fish\" have been analysed by Ornella Volta as containing elements of competition with Claude Debussy, of which Debussy was probably not aware, Satie not making this music public. Meanwhile, Debussy was having one of his first major successes with ' in 1902, leading a few years later to 'who-was-precursor-to-whom' debates between the two composers, in which Maurice Ravel would also get involved.\n\nIn October 1905, Satie enrolled in Vincent d'Indy's Schola Cantorum de Paris to study classical counterpoint while still continuing his cabaret work. Most of his friends were as dumbfounded as the professors at the Schola when they heard about his new plan to return to the classrooms, especially as d'Indy was an admiring pupil of Saint-Saëns, not particularly favoured by Satie. Satie would follow these courses at the Schola, as a respected pupil, for more than five years, receiving a first (intermediate) diploma in 1908. Some of his classroom counterpoint-exercises, such as the \"\", were published after his death. Another summary, of the period prior to the Schola, also appeared in 1911: the \"Trois morceaux en forme de poire\", which was a kind of compilation of the best of what he had written up to 1903.\n\nSomething that becomes clear through these published compilations is that Satie did not so much reject Romanticism and its exponents like Wagner, but that he rejected certain aspects of it. From his first composition to his last, he rejected the idea of musical development, in the strict definition of this term: the intertwining of different themes in a development section of a sonata form. As a result, his contrapuntal and other works were very short; the \"new, modern\" Fugues do not extend further than the exposition of the theme(s). Generally, he would say that he did not think it permitted that a composer take more time from his public than strictly necessary. Also Melodrama, in its historical meaning of the then popular romantic genre of \"spoken words to a background of music\", was something Satie avoided. His 1913 \"Le piège de Méduse\" could be seen as an absurdist spoof of that genre.\n\nIn the meantime, other changes had also taken place: Satie was a member of a radical socialist party (he later switched his membership to the Communist Party in that area after December 1920), and had socialised with the Arcueil community: amongst other things, he had been involved in the \"\" work for children. He also changed his appearance to that of the 'bourgeois functionary' with bowler hat, umbrella, etc. He channelled his medieval interests into a peculiar secret hobby: in a filing cabinet he maintained a collection of imaginary buildings, most of them described as being made out of some kind of metal, which he drew on little cards. Occasionally, extending the game, he would publish anonymous small announcements in local journals, offering some of these buildings, e.g., a \"castle in lead\", for sale or rent.\n\nStarting in 1912, Satie's new humorous miniatures for piano became very successful, and he wrote and published many of these over the next few years (most of them premiered by the pianist Ricardo Viñes). His habit of accompanying the scores of his compositions with all kinds of written remarks was now well established, so that a few years later he had to insist that these not be read out during performances. He wrote in the first edition of \"Heures séculaires et instantanées\", \"To whom it may concern: 'I forbid anyone to read the text aloud during the musical performance. Ignorance of my instructions will incur my righteous indignation against the presumptuous culprit. No exception will be allowed.'\" He had mostly stopped using barlines by this time. In some ways, these compositions were very reminiscent of Rossini's compositions from the final years of his life, grouped under the name Péchés de vieillesse.\n\nHowever, the acceleration in Satie's life did not come so much from the success of his new piano pieces; it was Ravel who inadvertently triggered the characteristics of Satie's remaining years and thus influenced the successive progressive artistic and cultural movements that rapidly manifested themselves in Paris over the following years. Paris was seen as the artistic capital of the world, and the beginning of the new century appeared to have set many minds on fire. In 1910 the \"\", a group of young musicians around Ravel, proclaimed their preference for Satie's earlier work from before the Schola period, reinforcing the idea that Satie had been a precursor of Debussy.\n\nAt first, Satie was pleased that at least some of his works were receiving public attention, but when he realised that this meant that his more recent work was overlooked or dismissed, he looked for other young artists who related better to his more recent ideas, so as to have better mutual support in creative activity. Thus, young artists such as Roland-Manuel, and later Georges Auric, and Jean Cocteau, started to receive more of his attention than the \"\".\n\nAs a result of his contact with Roland-Manuel, Satie again began publicising his thoughts, with far more irony than he had done before (amongst other things, the ' and ').\n\nWith Jean Cocteau, whom he had first met in 1915, Satie started work on incidental music for a production of Shakespeare's \"A Midsummer Night's Dream\", resulting in the \"Cinq grimaces pour Le songe d'une nuit d'été\". From 1916, he and Cocteau worked on the ballet \"Parade\", which was premiered in 1917 by Sergei Diaghilev's Ballets Russes, with sets and costumes by Pablo Picasso, and choreography by Léonide Massine. Through Picasso, Satie also became acquainted with other cubists, such as Georges Braque, with whom he would work on other, aborted, projects.\n\nWith Georges Auric, Louis Durey, Arthur Honegger, and Germaine Tailleferre, Satie formed the Nouveaux jeunes, shortly after writing \"Parade\". Later, the group was joined by Francis Poulenc and Darius Milhaud. In September 1918, Satie – giving little or no explanation – withdrew from the . Jean Cocteau gathered the six remaining members, forming the Groupe des six (to which Satie would later have access, but later again would fall out with most of its members).\n\nFrom 1919, Satie was in contact with Tristan Tzara, the initiator of the Dada movement. He became acquainted with other artists involved in the movement, such as Francis Picabia (later to become a Surrealist), André Derain, Marcel Duchamp, Jean Hugo and Man Ray, among others. On the day of his first meeting with Man Ray, the two fabricated the artist's first readymade: \"The Gift\" (1921). Satie contributed writing to the Dadaist publication \"391\". In the first months of 1922, he was surprised to find himself entangled in the argument between Tzara and André Breton about the true nature of avant-garde art, epitomised by the failure of the Congrès de Paris. Satie originally sided with Tzara, but managed to maintain friendly relations with most players in both camps. Meanwhile, an \"\" had formed around Satie, taking the name from the relatively remote district of Paris where Satie lived; it included young musicians such as Henri Sauguet, Maxime Jacob, Roger Désormière and Henri Cliquet-Pleyel.\n\nSatie's last compositions were two 1924 ballets. \"Mercure\" reunited him with Picasso and Massine for a mythological spoof produced by Count Étienne de Beaumont's Soirées de Paris; and he wrote the \"\" ballet (') in collaboration with Picabia, for the of Rolf de Maré. In a simultaneous project, Satie added music to the surrealist film ' by René Clair, which was given as an intermezzo for \".\n\nSatie and Suzanne Valadon (an artists' model, artist, long-time friend of Miguel Utrillo's, and mother of Maurice Utrillo) began an affair early in 1893. After their first night together, he proposed marriage. The two did not marry, but Valadon moved to a room next to Satie's at the . Satie became obsessed with her, calling her his \" and writing impassioned notes about \"her whole being, lovely eyes, gentle hands, and tiny feet\". During their relationship, Satie composed the \"Danses gothiques\" as a kind of prayer to restore peace of mind, and Valadon painted a portrait of Satie, which she gave to him. After six months she moved away, leaving Satie broken-hearted. Afterwards, he said that he was left with \"nothing but an icy loneliness that fills the head with emptiness and the heart with sadness\". It is believed this was the only intimate relationship Satie ever had.\n\nA rare autochrome photograph of Satie exists that dates from 1911. It was reproduced on the cover of Robert Orledge's second book on the composer, \"Satie Remembered\" (1995), but where this autochrome was found has not been made known.\n\nAfter years of heavy drinking (including consumption of absinthe), Satie died on 1 July 1925 from cirrhosis of the liver. He is buried in the cemetery in Arcueil. There is a tiny stone monument designating a grassy area in front of an apartment building – 'Parc Erik Satie'. Over the course of his 27 years in residence at Arcueil, where Satie lived in stark simplicity, no one had ever visited his room. After his death, Satie's friends discovered an apartment replete with squalor and chaos. Among many other unsorted papers and miscellaneous items, it contained a large number of umbrellas, and two grand pianos placed one on top of the other, the upper instrument used as storage for letters and parcels. They discovered compositions that were either thought to have been lost or totally unknown. The score to \"Jack in the Box\" was thought, by Satie, to have been left on a bus years before. These were found behind the piano, in the pockets of his velvet suits, and in other odd places, and included \"Vexations\"; \"Geneviève de Brabant\" and other unpublished or unfinished stage works; \"The Dreamy Fish\"; many Schola Cantorum exercises; a previously unseen set of \"canine\" piano pieces; and several other works for piano, many untitled. Some of these would be published later as additional ', ', \"\", and \"furniture music\". \n\n\n\n\n\n\n\n\n\n", "id": "9956", "title": "Erik Satie"}
{"url": "https://en.wikipedia.org/wiki?curid=9960", "text": "Elliptic integral\n\nIn integral calculus, elliptic integrals originally arose in connection with the problem of giving the arc length of an ellipse. They were first studied by Giulio Fagnano and Leonhard Euler (). Modern mathematics defines an \"elliptic integral\" as any function which can be expressed in the form\n\nwhere is a rational function of its two arguments, is a polynomial of degree 3 or 4 with no repeated roots, and is a constant.\n\nIn general, integrals in this form cannot be expressed in terms of elementary functions. Exceptions to this general rule are when has repeated roots, or when contains no odd powers of . However, with the appropriate reduction formula, every elliptic integral can be brought into a form that involves integrals over rational functions and the three Legendre canonical forms (i.e. the elliptic integrals of the first, second and third kind).\n\nBesides the Legendre form given below, the elliptic integrals may also be expressed in Carlson symmetric form. Additional insight into the theory of the elliptic integral may be gained through the study of the Schwarz–Christoffel mapping. Historically, elliptic functions were discovered as inverse functions of elliptic integrals.\n\n\"Incomplete elliptic integrals\" are functions of two arguments; \"complete elliptic integrals\" are functions of a single argument. These arguments are expressed in a variety of different but equivalent ways (they give the same elliptic integral). Most texts adhere to a canonical naming scheme, using the following naming conventions.\n\nFor expressing one argument:\n\nEach of the above three quantities is completely determined by any of the others (given that they are non-negative). Thus, they can be used interchangeably.\n\nThe other argument can likewise be expressed as , the \"amplitude\", or as or , where and is one of the Jacobian elliptic functions.\n\nSpecifying the value of any one of these quantities determines the others. Note that also depends on . Some additional relationships involving \"u\" include\n\nThe latter is sometimes called the \"delta amplitude\" and written as . Sometimes the literature also refers to the \"complementary parameter\", the \"complementary modulus,\" or the \"complementary modular angle\". These are further defined in the article on quarter periods.\n\nThe incomplete elliptic integral of the first kind is defined as\n\nThis is the trigonometric form of the integral; substituting and , one obtains Jacobi's form:\n\nEquivalently, in terms of the amplitude and modular angle one has:\n\nIn this notation, the use of a vertical bar as delimiter indicates that the argument following it is the \"parameter\" (as defined above), while the backslash indicates that it is the modular angle. The use of a semicolon implies that the argument preceding it is the sine of the amplitude:\n\nThis potentially confusing use of different argument delimiters is traditional in elliptic integrals and much of the notation is compatible with that used in the reference book by Abramowitz and Stegun and that used in the integral tables by Gradshteyn and Ryzhik.\n\nWith one has:\n\nthus, the Jacobian elliptic functions are inverses to the elliptic integrals.\n\nThere are still other conventions for the notation of elliptic integrals employed in the literature. The notation with interchanged arguments, , is often encountered; and similarly for the integral of the second kind. Abramowitz and Stegun substitute the integral of the first kind, , for the argument in their definition of the integrals of the second and third kinds, unless this argument is followed by a backslash: i.e. for . Moreover, their complete integrals employ the \"parameter\" as argument in place of the modulus , i.e. rather than . And the integral of the third kind defined by Gradshteyn and Ryzhik, , puts the amplitude first and not the \"characteristic\" .\n\nThus one must be careful with the notation when using these functions, because various reputable references and software packages use different conventions in the definitions of the elliptic functions. For example, some references, and Wolfram's Mathematica software and Wolfram Alpha, define the complete elliptic integral of the first kind in terms of the parameter , instead of the elliptic modulus . \n\nThe incomplete elliptic integral of the second kind in trigonometric form is\n\nSubstituting and , one obtains Jacobi's form:\n\nEquivalently, in terms of the amplitude and modular angle:\n\nRelations with the Jacobi elliptic functions include\n\nThe meridian arc length from the equator to latitude is written in terms of :\n\nwhere is the semi-major axis, and is the eccentricity.\n\nThe incomplete elliptic integral of the third kind is \n\nor\n\nThe number is called the characteristic and can take on any value, independently of the other arguments. Note though that the value is infinite, for any .\n\nA relation with the Jacobian elliptic functions is\n\nThe meridian arc length from the equator to latitude is also related to a special case of :\n\nElliptic Integrals are said to be 'complete' when the amplitude and therefore . The complete elliptic integral of the first kind may thus be defined as\n\nor more compactly in terms of the incomplete integral of the first kind as\n\nIt can be expressed as a power series\n\nwhere is the Legendre polynomials, which is equivalent to\n\nwhere denotes the double factorial. In terms of the Gauss hypergeometric function, the complete elliptic integral of the first kind can be expressed as\n\nThe complete elliptic integral of the first kind is sometimes called the quarter period. It can be computed very efficiently in terms of the arithmetic–geometric mean:\nSee for details.\n\nThe relation to Jacobi's theta function is given by \nwhere the nome is\n\nThis approximation has a relative precision better than for . Keeping only the first two terms is correct to 0.01 precision for .\n\nThe differential equation for the elliptic integral of the first kind is\n\nA second solution to this equation is . This solution satisfies the relation\n\nThe complete elliptic integral of the second kind is defined as\n\nor more compactly in terms of the incomplete integral of the second kind as\n\nFor an ellipse with semi-major axis and semi-minor axis and eccentricity , the complete elliptic integral of the second kind is equal to one quarter of the circumference of the ellipse measured in units of the semi-major axis . In other words:\n\nThe complete elliptic integral of the second kind can be expressed as a power series\n\nwhich is equivalent to\n\nIn terms of the Gauss hypergeometric function, the complete elliptic integral of the second kind can be expressed as\n\nThe complete elliptic integral of the second kind can also be computed very efficiently using the arithmetic–geometric mean .\n\nA second solution to this equation is .\n\nThe complete elliptic integral of the third kind can be defined as\n\nNote that sometimes the elliptic integral of the third kind is defined with an inverse sign for the \"characteristic\" ,\n\nJust like the complete elliptic integrals of the first and second kind, the complete elliptic integral of the third kind can be computed very efficiently using the arithmetic-geometric mean .\n\nLegendre's relation:\n\n\n\n", "id": "9960", "title": "Elliptic integral"}
{"url": "https://en.wikipedia.org/wiki?curid=9961", "text": "Epistle to the Romans\n\nThe Epistle to the Romans or Letter to the Romans, often shortened to Romans, is the sixth book in the New Testament. Biblical scholars agree that it was composed by the Apostle Paul to explain that salvation is offered through the gospel of Jesus Christ. It is the longest of the Pauline epistles and is often considered his \"most important theological legacy\" and magnum opus.\n\nIn the opinion of Jesuit scholar Joseph Fitzmyer, the book \"overwhelms the reader by the density and sublimity of the topic with which it deals, the gospel of the justification and salvation of Jew and Greek alike by the grace of God through faith in Jesus Christ, revealing the uprightness and love of God the Father.\"\n\nN. T. Wright notes that Romans is\nThe scholarly consensus is that Paul authored the Epistle to the Romans.\n\nC. E. B. Cranfield, in the introduction to his commentary on Romans, says:\nThe denial of Paul's authorship of Romans by such critics ... is now rightly relegated to a place among the curiosities of NT scholarship. Today no responsible criticism disputes its Pauline origin. The evidence of its use in the Apostolic Fathers is clear, and before the end of the second century it is listed and cited as Paul's. Every extant early list of NT books includes it among his letters. The external evidence of authenticity could indeed hardly be stronger; and it is altogether borne out by the internal evidence, linguistic, stylistic, literary, historical and theological.\n\nThe letter was most probably written while Paul was in Corinth, probably while he was staying in the house of Gaius, and transcribed by Tertius his amanuensis. There are a number of reasons why Corinth is considered most plausible. Paul was about to travel to Jerusalem on writing the letter, which matches Acts where it is reported that Paul stayed for three months in Greece. This probably implies Corinth as it was the location of Paul’s greatest missionary success in Greece. Additionally Phoebe was a deacon of the church in Cenchreae, a port to the east of Corinth, and would have been able to convey the letter to Rome after passing through Corinth and taking a ship from Corinth’s west port. Erastus, mentioned in , also lived in Corinth, being the city's commissioner for public works and city treasurer at various times, again indicating that the letter was written in Corinth.\n\nThe precise time at which it was written is not mentioned in the epistle, but it was obviously written when the collection for Jerusalem had been assembled and Paul was about to \"go unto Jerusalem to minister unto the saints\", that is, at the close of his second visit to Greece, during the winter preceding his last visit to that city. The majority of scholars writing on Romans propose the letter was written in late 55/early 56 or late 56/early 57. Early 55 and early 58 both have some support, while German New Testament scholar Gerd Lüdemann argues for a date as early as 51/52 (or 54/55), following on from Knox, who proposed 53/54. Lüdemann is the only serious challenge to the consensus of mid to late 50s.\n\nSome manuscripts have a subscription at the end of the Epistle:\n\nFor ten years before writing the letter (approx. 47–57), Paul had traveled around the territories bordering the Aegean Sea evangelizing. Churches had been planted in the Roman provinces of Galatia, Macedonia, Achaia and Asia. Paul, considering his task complete, wanted to preach the gospel in Spain, where he would not \"build upon another man’s foundation\". This allowed him to visit Rome on the way, a long-time ambition of his. The letter to the Romans, in part, prepares them and gives reasons for his visit.\n\nIn addition to Paul’s geographic location, his religious views are important. First, Paul was a Hellenistic Jew with a Pharisaic background (see Gamaliel), integral to his identity: see Paul the Apostle and Judaism for details. His concern for his people is one part of the dialogue and runs throughout the letter. Second, the other side of the dialogue is Paul’s conversion and calling to follow Christ in the early 30s.\n\nThe most probable ancient account of the beginning of Christianity in Rome is given by a 4th-century writer known as Ambrosiaster:\n\nFrom Adam Clarke:\nAt this time, the Jews made up a substantial number in Rome, and their synagogues, frequented by many, enabled the Gentiles to become acquainted with the story of Jesus of Nazareth. Consequently, churches composed of both Jews and Gentiles were formed at Rome. According to Irenaeus, a 2nd-century Church Father, the church at Rome was founded directly by the apostles Peter and Paul. However, many modern scholars disagree with Irenaeus, holding that while little is known of the circumstances of the church's founding, it was not founded by Paul:\n\nNote the large number of names in of those then in Rome, and verses 5, 15 and 16 indicate there was more than one church assembly or company of believers in Rome. Verse 5 mentions a church that met in the house of Aquila and Priscilla. Verses 14 and 15 each mention groupings of believers and saints.\n\nJews were expelled from Rome because of disturbances around AD 49 by the edict of Claudius. Fitzmyer claims that both Jews and Jewish Christians were expelled as a result of their infighting. Claudius died around the year AD 54, and his successor, Emperor Nero, allowed the Jews back into Rome, but then, after the Great Fire of Rome of 64, Christians were persecuted. Fitzmyer argues that with the return of the Jews to Rome in 54 new conflict arose between the Gentile Christians and the Jewish Christians who had formerly been expelled. Keck thinks Gentile Christians may have developed a dislike of or looked down on Jews (see also Antisemitism and Responsibility for the death of Jesus), because they theologically rationalized that Jews were no longer God's people.\n\nScholars often have difficulty assessing whether Romans is a letter or an epistle, a relevant distinction in form-critical analysis:\nA letter is something non-literary, a means of communication between persons who are separated from each other. Confidential and personal in nature, it is intended only for the person or persons to whom it is addressed, and not at all for the public or any kind of publicity...An Epistle is an artistic literary form, just like the dialogue, the oration, or the drama. It has nothing in common with the letter except its form: apart from that one might venture the paradox that the epistle is the opposite of a real letter. The contents of the epistle are intended for publicity—they aim at interesting \"the public.\"\nJoseph Fitzmyer argues, from evidence put forth by Stirewalt, that the style of Romans is an \"essay-letter.\" Philip Melanchthon, a writer during the Reformation, suggested that Romans was \"caput et summa universae doctrinae christianae\" (\"a summary of all Christian doctrine\"). While some scholars attempt to suggest, like Melanchthon, that it is a type of theological treatise, this view largely ignores chapters 14 and 15 of Romans. There are also many \"noteworthy elements\" missing from Romans that are included in other areas of the Pauline corpus. The breakdown of Romans as a treatise began with F.C. Baur in 1836 when he suggested \"this letter had to be interpreted according to the historical circumstances in which Paul wrote it.\"\n\nPaul sometimes uses a style of writing common in his time called a \"diatribe\". He appears to be responding to a \"heckler\" (probably an imaginary one based on Paul's encounters with real objections in his previous preaching), and the letter is structured as a series of arguments. In the flow of the letter, Paul shifts his arguments, sometimes addressing the Jewish members of the church, sometimes the Gentile membership and sometimes the church as a whole.\n\nTo review the current scholarly viewpoints on the purpose of Romans, along with a bibliography, see \"Dictionary of Paul and His Letters\". For a 16th-century \"Lollard\" reformer view, see the work of William Tyndale. In his prologue to his translation of the book of Romans, which was largely taken from the prologue of German Reformer Martin Luther, Tyndale writes that:\n... this epistle is the principal and most excellent part of the new testament, and most pure evangelion, that is to say glad tidings and what we call the gospel, and also a light and a way in unto the whole scripture ... The sum and whole cause of the writings of this epistle, is, to prove that a man is justified by faith only: which proposition whoso denieth, to him is not only this epistle and all that Paul writeth, but also the whole scripture, so locked up that he shall never understand it to his soul's health. And to bring a man to the understanding and feeling that faith only justifieth, Paul proveth that the whole nature of man is so poisoned and so corrupt, yea and so dead concerning godly living or godly thinking, that it is impossible for her to keep the law in the sight of God.\n\nThis essay-letter composed by Paul was written to a specific audience at a specific time; to understand it, the situations of both Paul and the recipients must be understood.\n\nThe introduction provides some general notes about Paul. He introduces here and introductory notes about the gospel he wishes to preach to the church at Rome. Jesus' human line stems from David. Paul, however, does not limit his ministry to Jews. Paul's goal is that the Gentiles would also hear the gospel.\n\nHe commends the Romans for their faith. Paul also speaks of the past obstacles that have blocked his coming to Rome earlier.\n\nPaul's announcement that he is not \"ashamed\" (\"epaiscúnomai\") of his gospel because it holds power (\"dúnamis\"). These two verses form a backdrop for the rest of the book. First, we note that Paul is unashamed of his love for this gospel that he preaches about Jesus Christ. He also notes that he is speaking to the \"Jew first.\" There is significance to this, but much of it is scholarly conjecture as the relationship of Paul the Apostle and Judaism is still debated. We are hard-pressed to find an answer to such a question without knowing more about the audience in question. Wayne Brindle argues, based on Paul's former writings against the Judaizers in Galatians and 2 Corinthians, that rumors had probably spread about Paul totally negating the Jewish existence in a Christian world (see also Antinomianism in the New Testament and Supersessionism). Paul may have used the \"Jew first\" approach to counter such a view.\n\nPaul now begins into the main thrust of his letter. He begins by suggesting that humans have taken up ungodliness and wickedness for which there will be wrath from God. People have taken God's invisible image and made him into an idol. Paul draws heavily here from the Wisdom of Solomon. He condemns unnatural sexual behavior and warns that such behavior will result in a depraved body and mind and says that people who do such things (including murder and wickedness ) are worthy of death. Paul stands firmly against the idol worship system which was common in Rome. Several scholars believe the passage is a non-Pauline interpolation.\n\nOn the traditional Protestant interpretation, Paul here calls out Jews who are condemning others for not following the law when they themselves are also not following the law. Stanley Stowers, however, has argued on rhetorical grounds that Paul is in these verses not addressing a Jew at all but rather an easily recognizable caricature of the typical boastful person (ὁ ἀλαζων). Stowers writes, \"There is absolutely no justification for reading as Paul's attack on 'the hypocrisy of the Jew.' No one in the first century would have identified \"ho alazon\" with Judaism. That popular interpretation depends upon anachronistically reading later Christian characterizations of Jews as 'hypocritical Pharisees'\". See also Anti-Judaism.\n\nPaul says that a righteousness from God has made itself known apart from the law, to which the law and prophets testify, and this righteousness from God comes through faith in Jesus to all who believe. He describes justification – legally clearing the believer of the guilt and penalty of sin – as a gift of God, and not the work of man (lest he might boast), but by faith.\n\nIn chapters five through eight, Paul argues that believers can be assured of their hope in salvation, having been freed from the bondage of sin. Paul teaches that through faith, the faithful have been joined with Jesus and freed from sin. Believers should celebrate in the assurance of salvation. This promise is open to everyone since everyone has sinned, save the one who paid for all of them.\n\nIn Paul addresses the faithfulness of God to the Israelites, where he says that God has been faithful to His promise. Paul hopes that all Israelites will come to realize the truth since he himself was also an Israelite, and had in the past been a persecutor of Early Christians. In Paul talks about how the nation of Israel has been cast away, and the conditions under which Israel will be God's chosen nation again: when Israel returns to its faith, sets aside its unbelief.\n\nIn , Paul says that humans are under the law while we live: \"Know ye not...that the law hath dominion over a man as long as he liveth?\" However, Jesus' death on the cross makes believers dead to the law (, \"Wherefore, my brethren, ye are also become dead to the law by the body of Christ\"), according to an antinomistic interpretation.\n\nFrom chapter 12 through the first part of chapter 15, Paul outlines how the Gospel transforms believers and the behaviour that results from such a transformation. This transformation is described as a \"renewing of your mind\" (12:2), a transformation that Douglas J. Moo characterizes as “the heart of the matter.” It is a transformation so radical that it amounts to a “a transfiguration of your brain,” a \"metanoia\", a “mental revolution.”\n\nPaul goes on to describe how believers should live. Christians are no longer under the law, that is, no longer bound by the law of Moses, but under the grace of God, see Law and grace. We do not need to live under the law because to the extent our minds have been renewed, we will know \"almost instinctively\" what God wants of us. The law then provides an \"objective standard\" for judging progress in the \"lifelong process\" of our mind's renewal.\n\nTo the extent they have been set free from sin by renewed minds (Romans 6:18), believers are no longer bound to sin. Believers are free to live in obedience to God and love everybody. As Paul says in Romans 13:10, \"love (ἀγάπη) worketh no ill to his neighbor: therefore love is the fulfilling of law\".\n\nThe fragment in Romans 13:1–7 dealing with obedience to earthly powers is considered by some, for example James Kallas, to be a gloss incorporated later. (See also the Great Commandment and Christianity and politics).\n\n\nThe concluding verses contain a description of his travel plans, personal greetings and salutations. One-third of the twenty-one Christians identified in the greetings are women, some of whom played an important role in the early church at Rome. Additionally, none of these Christians answer to the name Peter, although according to the Catholic storyline, he had been ruling as Pope in Rome for about 25 years. Possibly related was the Incident at Antioch between Paul and Cephas.\n\n\nCatholics accept the necessity of faith for salvation but point to for the necessity of living a virtuous life as well:\nBut by your hard and impenitent heart you are storing up wrath for yourself on the day of wrath when God's righteous judgment will be revealed. For he will render to every man according to his works: to those who by patience in well-doing seek for glory and honor and immortality, he will give eternal life; but for those who are factious and do not obey the truth, but obey wickedness, there will be wrath and fury. There will be tribulation and distress for every human being who does evil, the Jew first and also the Greek, but glory and honor and peace for every one who does good, the Jew first and also the Greek. For God shows no partiality. \nThroughout his writings, St. Augustine strongly affirms the Catholic understanding of this and other such Scriptural admonitions. In his sermons to his Catholic congregations, he is especially careful to warn them against an inordinate desire for a complete assurance of salvation. In his \"Exposition of Psalm 147\" for example, he states:\n\nThe gospel warned us, \"Be on the watch for the last day, the day when the Son of Man will come,\" because it will spell disaster for those it finds secure as they are now – secure for the wrong reasons, I mean, secure in the pleasures of this world, when they ought to be secure only when they have \"dominated this world's lusts\". The apostle certainly prepares us for that future life in words of which I also reminded you on that occasion.\n\nAgain, in his \"Exposition of Psalm 85\", Augustine is perhaps even more specific:\n\nLet us not expect security while we are on pilgrimage. When we do find ourselves wanting it, what we are looking for is bodily sluggishness rather than personal security.\n\nIn the Protestant interpretation, the New Testament epistles (including Romans), describes salvation as coming from faith and not from righteous actions. For example, Romans (underlining added):\n\nThey also point out that in Romans , Paul says that God will reward those who follow the law and then goes on to say that \"no one\" follows the law perfectly (see also Sermon on the Mount: Interpretation) Romans :\n\nMartin Luther described Paul's letter to the Romans as \"the most important piece in the New Testament. It is purest Gospel. It is well worth a Christian's while not only to memorize it word for word but also to occupy himself with it daily, as though it were the daily bread of the soul\".\n\nLuther controversially added the word \"alone\" (\"allein\" in German) to Romans so that it read: \"thus, we hold, then, that man is justified without doing the works of the law, \"alone\" through faith\". The word \"alone\" does not appear in the original Greek text, but Luther defended his translation by maintaining that the adverb \"alone\" was required both by idiomatic German and Paul's intended meaning. This is a \"\"literalist view\"\" rather than an literal view of the Bible.\n\nApologist James Swan lists numerous Catholic sources that also translated Romans 3:28 with the word \"alone,\" or testified to others doing so before Luther. A Bible commentary published in 1864 reports that:\n\nThe \"Romans Road\" refers to a set of scriptures from Romans that Christian evangelists use to present a clear and simple case for personal salvation to each person. They are: Romans , , , , , , and .\n\nRomans has been at the forefront of several major movements in Protestantism. Martin Luther's lectures on Romans in 1515–1516 probably coincided with the development of his criticism of Roman Catholicism which led to the 95 Theses of 1517. In 1738, while hearing Luther's Preface to the Epistle to the Romans read at St. Botolph Church on Aldersgate Street in London, John Wesley famously felt his heart \"strangely warmed\", a conversion experience which is often seen as the beginning of Methodism. In 1919 Karl Barth's commentary on Romans, \"The Epistle to the Romans\", was the publication which is widely seen as the beginning of neo-orthodoxy.\n\n\n\n\"Translations\"\n\n\"Other\"\n", "id": "9961", "title": "Epistle to the Romans"}
{"url": "https://en.wikipedia.org/wiki?curid=9962", "text": "Eleanor of Aquitaine\n\nEleanor of Aquitaine (, ; 1122  – 1 April 1204) was a Queen consort of France and England. As a member of the Ramnulfids (\"House of Poitiers\") rulers in southwestern France, she was one of the most powerful and wealthiest women in western Europe during the High Middle Ages. She inherited the Duchy of Aquitaine from her father, William X, in 1137, and by successive marriages became Queen of France (1137–1152) and then of England (1154–1189). She was patron of literary figures such as Wace, Benoît de Sainte-Maure, and Bernart de Ventadorn. She led armies several times in her life and was a leader of the Second Crusade.\n\nAs Duchess of Aquitaine, Eleanor was the most eligible bride in Europe. Three months after she became duchess, she married King Louis VII of France, son of her guardian, King Louis VI. As Queen of France, she participated in the unsuccessful Second Crusade. Soon afterwards, Eleanor sought an annulment of her marriage, but her request was rejected by Pope Eugene III. However, after the birth of her second daughter Alix, Louis agreed to an annulment, as fifteen years of marriage had not produced a son. The marriage was annulled on 11 March 1152 on the grounds of consanguinity within the fourth degree. Their daughters were declared legitimate and custody was awarded to Louis, while Eleanor's lands were restored to her.\n\nAs soon as the annulment was granted, Eleanor became engaged to the Duke of Normandy, who became King Henry II of England in 1154. Henry was her third cousin and eleven years younger. The couple married on Whitsun, 18 May 1152, eight weeks after the annulment of Eleanor's first marriage, in a cathedral in Poitiers, France. Over the next thirteen years, she bore Henry eight children: five sons, three of whom would become kings; and three daughters. However, Henry and Eleanor eventually became estranged. Henry imprisoned her in 1173 for supporting their son Henry's revolt against him. She was not released until 6 July 1189, when Henry died and their second son, Richard the Lionheart, ascended the throne.\n\nNow Queen dowager, Eleanor acted as regent while Richard went on the Third Crusade; on his return Richard was captured and held prisoner. Eleanor lived well into the reign of her youngest son, John. She outlived all her children except for John and Eleanor.\n\nEleanor's year of birth is not known precisely: a late 13th-century genealogy of her family listing her as 13 years old in the spring of 1137 provides the best evidence that Eleanor was perhaps born as late as 1124. On the other hand, some chronicles mention a fidelity oath of some lords of Aquitaine on the occasion of Eleanor's fourteenth birthday in 1136. This, and her known age of 82 at her death, make 1122 more likely the year of birth. Her parents almost certainly married in 1121. Her birthplace may have been Poitiers, Bordeaux, or Nieul-sur-l'Autise, where her mother and brother died when Eleanor was 6 or 8.\n\nEleanor (or Aliénor) was the oldest of three children of William X, Duke of Aquitaine, whose glittering ducal court was renowned in early 12th-century Europe, and his wife, Aenor de Châtellerault, the daughter of Aimery I, Viscount of Châtellerault, and Dangereuse de l'Isle Bouchard, who was William IX's longtime mistress as well as Eleanor's maternal grandmother. Her parents' marriage had been arranged by Dangereuse with her paternal grandfather WilliamIX.\n\nEleanor is said to have been named for her mother Aenor and called \"Aliénor\" from the Latin \"alia Aenor\", which means \"the other Aenor\". It became \"Eléanor\" in the \"langues d'oïl\" of northern France and \"Eleanor\" in English. There was, however, another prominent Eleanor before her: Eleanor of Normandy, an aunt of William the Conqueror, who lived a century earlier than Eleanor of Aquitaine. In Paris as the Queen of France she was called Helienordis, her honorific name as written in the Latin epistles.\n\nBy all accounts, Eleanor's father ensured that she had the best possible education. Eleanor came to learn arithmetic, the constellations, and history. She also learned domestic skills such as household management and the needle arts of embroidery, needlepoint, sewing, spinning, and weaving. Eleanor developed skills in conversation, dancing, games such as backgammon, checkers, and chess, playing the harp, and singing. Although her native tongue was Poitevin, she was taught to read and speak Latin, was well versed in music and literature, and schooled in riding, hawking, and hunting. Eleanor was extroverted, lively, intelligent, and strong-willed. In the spring of 1130 her four-year-old brother William Aigret and their mother died at the castle of Talmont, on Aquitaine's Atlantic coast. Eleanor became the heir presumptive to her father's domains. The Duchy of Aquitaine was the largest and richest province of France; Poitou (where Eleanor spent most of her childhood) and Aquitaine together were almost one-third the size of modern France. Eleanor had only one other legitimate sibling, a younger sister named Aelith, also called Petronilla. Her half brother Joscelin was acknowledged by WilliamX as a son, but not as his heir. That she had another half brother, William, has been discredited. Later, during the first four years of HenryII's reign, her siblings joined Eleanor's royal household.\n\nIn 1137 Duke William X left Poitiers for Bordeaux and took his daughters with him. Upon reaching Bordeaux, he left them in the charge of the Archbishop of Bordeaux, one of his few loyal vassals. The duke then set out for the Shrine of Saint James of Compostela in the company of other pilgrims. He died, however, on Good Friday of that year (9April).\n\nEleanor, aged twelve to fifteen, then became the Duchess of Aquitaine, and thus the most eligible heiress in Europe. As these were the days when kidnapping an heiress was seen as a viable option for obtaining a title, William dictated a will on the very day he died that bequeathed his domains to Eleanor and appointed King Louis VI of France as her guardian. William requested of the king that he take care of both the lands and the duchess, and find her a suitable husband. However, until a husband was found, the king had the legal right to Eleanor's lands. The duke also insisted to his companions that his death be kept a secret until Louis was informed; the men were to journey from Saint James of Compostela across the Pyrenees as quickly as possible to call at Bordeaux to notify the archbishop, then to make all speed to Paris to inform the king.\n\nThe king of France, known as Louis the Fat, was also gravely ill at that time, suffering from a bout of dysentery from which he appeared unlikely to recover. Despite his impending mortality, Louis remained clear-minded. His heir, Prince Louis, had originally been destined for the monastic life of a younger son but became the heir apparent when his older brother, Philip, died from a riding accident in 1131. \n\nThe death of William, one of the king's most powerful vassals, made available the most desirable duchy in France. While presenting a solemn and dignified face to the grieving Aquitainian messengers, Louis exulted when they departed. Rather than act as guardian to the duchess and duchy, he decided to marry the duchess to his 17-year-old heir and bring Aquitaine under the control of the French crown, thereby greatly increasing the power and prominence of France and its ruling family, the House of Capet. Within hours, the king had arranged for Prince Louis to be married to Eleanor, with Abbot Suger in charge of the wedding arrangements. Prince Louis was sent to Bordeaux with an escort of 500 knights, along with Abbot Suger, Theobald II, Count of Champagne, and Count Ralph.\n\nOn 25 July 1137 Louis VII of France and Eleanor were married in the Cathedral of Saint-André in Bordeaux by the Archbishop of Bordeaux. Immediately after the wedding, the couple were enthroned as Duke and Duchess of Aquitaine. However, there was a catch: the land would remain independent of France until Eleanor's oldest son became both King of the Franks and Duke of Aquitaine. Thus, her holdings would not be merged with France until the next generation. As a wedding present she gave Louis a rock crystal vase, currently on display at the Louvre. Louis gave the vase to the Basilica of St Denis. This vase is the only object connected with Eleanor of Aquitaine that still survives.\n\nLouis' tenure as Count of Poitou and Duke of Aquitaine and Gascony lasted only a few days. Although he had been invested as such on 8 August 1137, a messenger gave him the news that Louis VI had died of dysentery on 1 August while Prince Louis and Eleanor were making a tour of the provinces. Thus he became King Louis VII of France. He and Eleanor were anointed and crowned King and Queen of the Franks on Christmas Day of the same year.\n\nPossessing a high-spirited nature, Eleanor was not popular with the staid northerners; according to sources, Louis's mother Adelaide of Maurienne thought her flighty and a bad influence. She was not aided by memories of Constance of Arles, the Provençal wife of Robert II, tales of whose immodest dress and language were still told with horror. Eleanor's conduct was repeatedly criticized by church elders, particularly Bernard of Clairvaux and Abbot Suger, as indecorous. The king was madly in love with his beautiful and worldly bride, however, and granted her every whim, even though her behavior baffled and vexed him. Much money went into making the austere Cité Palace in Paris more comfortable for Eleanor's sake.\n\nAlthough Louis was a pious man, he soon came into a violent conflict with Pope Innocent II. In 1141, the Archbishopric of Bourges became vacant, and the king put forward as a candidate one of his chancellors, Cadurc, while vetoing the one suitable candidate, Pierre de la Chatre, who was promptly elected by the canons of Bourges and consecrated by the pope. Louis accordingly bolted the gates of Bourges against the new bishop. The pope, recalling similar attempts by William X to exile supporters of Innocent from Poitou and replace them with priests loyal to himself, blamed Eleanor, saying that Louis was only a child and should be taught manners. Outraged, Louis swore upon relics that so long as he lived Pierre should never enter Bourges. An interdict was thereupon imposed upon the king's lands, and Pierre was given refuge by Theobald II, Count of Champagne.\n\nLouis became involved in a war with Count Theobald by permitting Raoul I, Count of Vermandois and seneschal of France, to repudiate his wife Eléonore of Blois, Theobald's sister, and to marry Petronilla of Aquitaine, Eleanor's sister. Eleanor urged Louis to support her sister's marriage to Count Raoul. Theobald had also offended Louis by siding with the pope in the dispute over Bourges. The war lasted two years (1142–44) and ended with the occupation of Champagne by the royal army. Louis was personally involved in the assault and burning of the town of Vitry. More than a thousand people who sought refuge in the church there died in the flames. Horrified, and desiring an end to the war, Louis attempted to make peace with Theobald in exchange for his support in lifting the interdict on Raoul and Petronilla. This was duly lifted for long enough to allow Theobald's lands to be restored; it was then lowered once more when Raoul refused to repudiate Petronilla, prompting Louis to return to Champagne and ravage it once more.\n\nIn June 1144, the king and queen visited the newly built monastic church at Saint-Denis. While there, the queen met with Bernard of Clairvaux, demanding that he use his influence with the Pope to have the excommunication of Petronilla and Raoul lifted, in exchange for which King Louis would make concessions in Champagne and recognise Pierre de la Chatre as Archbishop of Bourges. Dismayed at her attitude, Bernard scolded Eleanor for her lack of penitence and interference in matters of state. In response, Eleanor broke down and meekly excused her behaviour, claiming to be bitter because of her lack of children. In response, Bernard became more kindly towards her: \"My child, seek those things which make for peace. Cease to stir up the King against the Church, and urge upon him a better course of action. If you will promise to do this, I in return promise to entreat the merciful Lord to grant you offspring.\" In a matter of weeks, peace had returned to France: Theobald's provinces were returned and Pierre de la Chatre was installed as Archbishop of Bourges. In April 1145, Eleanor gave birth to a daughter, Marie.\n\nLouis, however, still burned with guilt over the massacre at Vitry and wished to make a pilgrimage to the Holy Land to atone for his sins. In autumn 1145, Pope Eugene III requested that Louis lead a Crusade to the Middle East to rescue the Frankish Kingdoms there from disaster. Accordingly, Louis declared on Christmas Day 1145 at Bourges his intention of going on crusade.\n\nEleanor of Aquitaine also formally took up the cross symbolic of the Second Crusade during a sermon preached by Bernard of Clairvaux. In addition, she had been corresponding with her uncle Raymond, Prince of the Crusader kingdom of Antioch, who was seeking further protection against the \"Saracens\" from the French crown. Eleanor recruited some of her royal ladies-in-waiting for the campaign, as well as 300 non-noble Aquitainian vassals. She insisted on taking part in the Crusades as the feudal leader of the soldiers from her duchy. The story that she and her ladies dressed as Amazons is disputed by historians, sometimes confused with the account of King Conrad's train of ladies during this campaign (in Edward Gibbon's \"The History of the Decline and Fall of the Roman Empire\"). She left for the Second Crusade from Vézelay, the rumored location of Mary Magdalene's grave.\n\nThe Crusade itself achieved little. Louis was a weak and ineffectual military leader with no skill for maintaining troop discipline or morale, or of making informed and logical tactical decisions. In eastern Europe, the French army was at times hindered by Manuel I Comnenus, the Byzantine Emperor, who feared that the Crusade would jeopardize the tenuous safety of his empire. Notwithstanding, during their three-week stay at Constantinople, Louis was fêted and Eleanor was much admired. She was compared with Penthesilea, mythical queen of the Amazons, by the Greek historian Nicetas Choniates. He added that she gained the epithet \"chrysopous\" (golden-foot) from the cloth of gold that decorated and fringed her robe. Louis and Eleanor stayed in the Philopation palace just outside the city walls.\n\nFrom the moment the Crusaders entered Asia Minor, things began to go badly. The king and queen were still optimistic – the Byzantine Emperor had told them that the German King Conrad had won a great victory against a Turkish army (when in fact the German army had been massacred). However, while camping near Nicea, the remnants of the German army, including a dazed and sick King Conrad, staggered past the French camp, bringing news of their disaster. The French, with what remained of the Germans, then began to march in increasingly disorganized fashion towards Antioch. They were in high spirits on Christmas Eve, when they chose to camp in a lush valley near Ephesus. Here they were ambushed by a Turkish detachment; the French proceeded to slaughter this detachment and appropriate their camp.\n\nLouis then decided to cross the Phrygian mountains directly in the hope of reaching Eleanor's uncle Raymond in Antioch more quickly. As they ascended the mountains, however, the army and the king and queen were horrified to discover the unburied corpses of the previously slaughtered German army.\n\nOn the day set for the crossing of Mount Cadmos, Louis chose to take charge of the rear of the column, where the unarmed pilgrims and the baggage trains marched. The vanguard, with which Queen Eleanor marched, was commanded by her Aquitainian vassal Geoffrey de Rancon. Unencumbered by baggage, they reached the summit of Cadmos, where Rancon had been ordered to make camp for the night. Rancon however chose to continue on, deciding in concert with Amadeus III, Count of Savoy (Louis's uncle) that a nearby plateau would make a better campsite: such disobedience was reportedly common.\n\nAccordingly, by mid-afternoon, the rear of the column – believing the day's march to be nearly at an end – was dawdling. This resulted in the army becoming separated, with some having already crossed the summit and others still approaching it. At this point the Turks, who had been following and feinting for many days, seized their opportunity and attacked those who had not yet crossed the summit. The French (both soldiers and pilgrims), taken by surprise, were trapped; those who tried to escape were caught and killed. Many men, horses, and much of the baggage were cast into the canyon below. The chronicler William of Tyre, writing between 1170 and 1184 and thus perhaps too late to be considered historically accurate, placed the blame for this disaster firmly on the amount of baggage (much of it reputedly belonging to Eleanor and her ladies) and the presence of non-combatants.\n\nThe king, having scorned royal apparel in favour of a simple pilgrim's tunic, escaped notice (unlike his bodyguards, whose skulls were brutally smashed and limbs severed). He reportedly \"nimbly and bravely scaled a rock by making use of some tree roots which God had provided for his safety\", and managed to survive the attack. Others were not so fortunate: \"No aid came from Heaven, except that night fell.\"\n\nOfficial blame for the disaster was placed on Geoffrey de Rancon, who had made the decision to continue, and it was suggested that he be hanged (a suggestion which the king ignored). Since he was Eleanor's vassal, many believed that it was she who had been ultimately responsible for the change in plan, and thus the massacre. This did nothing for her popularity in Christendom – she was also blamed for the size of the baggage train and the fact that her Aquitainian soldiers had marched at the front, thus not involved in the fight. Continuing on, the army became split, with the commoners marching toward Antioch and the royalty traveling by sea. When most of the land army arrived, the king and queen had a profound dispute. Some, such as John of Salisbury and William of Tyre, say Eleanor's reputation was sullied by rumours of an affair with her uncle Raymond. However, this may have been a ruse, as Raymond through Eleanor tried to sway Louis forcibly to use his army to attack the actual Muslim encampment at nearby Aleppo, gateway to retaking Edessa, by papal decree the objective of the Crusade. Although this was perhaps the better military plan, Louis was not keen to fight in northern Syria. One of Louis's avowed Crusade goals was to journey in pilgrimage to Jerusalem, and he stated his intention to continue. Reputedly Eleanor then requested to stay with Raymond and brought up the matter of consanguinity – the fact that she and her husband, King Louis, were too closely related. This was grounds for annulment in the medieval period. Rather than allowing her to stay, Louis took Eleanor from Antioch against her will and continued on to Jerusalem with his dwindling army.\n\nThis episode humiliated Eleanor, and she maintained a low profile for the rest of the crusade. Louis's subsequent assault on Damascus in 1148 with his remaining army, fortified by King Conrad and Baldwin III of Jerusalem, achieved little. Damascus was a major trading centre that abounded in wealth and was under normal circumstances a potential threat, but the rulers of Jerusalem had recently entered into a truce with the city, which they then forswore. It was a gamble that did not pay off, and whether through military error or betrayal, the Damascus campaign was a failure. The French royal family retreated to Jerusalem and then sailed to Rome and made their way back to Paris.\n\nWhile in the eastern Mediterranean, Eleanor learned about maritime conventions developing there, which were the beginnings of what would become admiralty law. She introduced those conventions in her own lands on the island of Oleron in 1160 (with the \"Rolls of Oléron\") and later in England as well. She was also instrumental in developing trade agreements with Constantinople and ports of trade in the Holy Lands.\n\nEven before the Crusade, Eleanor and Louis were becoming estranged, and their differences were only exacerbated while they were abroad. Eleanor's purported relationship with her uncle Raymond, the ruler of Antioch, was a major source of discord. Eleanor supported her uncle's desire to re-capture the nearby County of Edessa, the objective of the Crusade. In addition, having been close to him in their youth, she now showed what was considered to be \"excessive affection\" toward her uncle. Raymond had plans to abduct Eleanor, to which she consented. While many historians today dismiss this as normal affection between uncle and niece (noting their early friendship and his similarity to her father and grandfather), some of Eleanor's adversaries interpreted the generous displays of affection as an incestuous affair. Louis's long march to Jerusalem and back north, which Eleanor was forced to join, debilitated his army and disheartened her knights; the divided Crusade armies could not overcome the Muslim forces, and the royal couple had to return home.\n\nHome, however, was not easily reached. Louis and Eleanor, on separate ships due to their disagreements, were first attacked in May 1149 by Byzantine ships attempting to capture both on the orders of the Byzantine Emperor. Although they escaped this attempt unharmed, stormy weather drove Eleanor's ship far to the south (to the Barbary Coast) and caused her to lose track of her husband. Neither was heard of for over two months. In mid-July, Eleanor's ship finally reached Palermo in Sicily, where she discovered that she and her husband had both been given up for dead. She was given shelter and food by servants of King Roger II of Sicily, until the king eventually reached Calabria, and she set out to meet him there. Later, at King Roger's court in Potenza, she learned of the death of her uncle Raymond, who was beheaded by Muslim forces in the Holy Land. This appears to have forced a change of plans, for instead of returning to France from Marseilles, they went to see Pope Eugene III in Tusculum, where he had been driven five months before by a revolt of the Commune of Rome.\n\nEugene did not, as Eleanor had hoped, grant an annulment. Instead, he attempted to reconcile Eleanor and Louis, confirming the legality of their marriage. He proclaimed that no word could be spoken against it, and that it might not be dissolved under any pretext. Eventually, he arranged events so that Eleanor had no choice but to sleep with Louis in a bed specially prepared by the pope. Thus was conceived their second child – not a son, but another daughter, Alix of France.\n\nThe marriage was now doomed. Still without a son and in danger of being left with no male heir, facing substantial opposition to Eleanor from many of his barons and her own desire for annulment, Louis bowed to the inevitable. On 11 March 1152, they met at the royal castle of Beaugency to dissolve the marriage. Hugues de Toucy, Archbishop of Sens, presided, and Louis and Eleanor were both present, as were the Archbishops of Bordeaux and Rouen. Archbishop Samson of Reims acted for Eleanor.\n\nOn 21 March, the four archbishops, with the approval of Pope Eugene, granted an annulment on grounds of consanguinity within the fourth degree. (Eleanor was Louis' third cousin once removed, and shared common ancestry with Robert II of France.) Their two daughters were, however, declared legitimate. (Children born to a marriage that was later annulled were not at risk of being \"bastardized,\" because \"[w]here parties married in good faith, without knowledge of an impediment, ... children of the marriage were legitimate.\" [Berman 228.] ) Custody of them was awarded to King Louis. Archbishop Samson received assurances from Louis that Eleanor's lands would be restored to her.\n\nAs Eleanor traveled to Poitiers, two lords – Theobald V, Count of Blois, and Geoffrey, Count of Nantes (brother of Henry II, Duke of Normandy) – tried to kidnap and marry her to claim her lands. As soon as she arrived in Poitiers, Eleanor sent envoys to Henry, Duke of Normandy and future king of England, asking him to come at once to marry her. On 18 May 1152 (Whit Sunday), eight weeks after her annulment, Eleanor married Henry \"without the pomp and ceremony that befitted their rank\".\n\nEleanor was related to Henry even more closely than she had been to Louis: they were cousins to the third degree through their common ancestor Ermengarde of Anjou (wife of Robert I, Duke of Burgundy and Geoffrey, Count of Gâtinais), and they were also descended from King Robert II of France. A marriage between Henry and Eleanor's daughter Marie had earlier been declared impossible due to their status as third cousins once removed. It was rumored by some that Eleanor had had an affair with Henry's own father, Geoffrey V, Count of Anjou, who had advised his son to avoid any involvement with her.\n\nOn 25 October 1154, Henry became King of England. Eleanor was crowned Queen of England by the Archbishop of Canterbury on 19 December 1154. She may not have been anointed on this occasion, however, because she had already been anointed in 1137. Over the next thirteen years, she bore Henry five sons and three daughters: William, Henry, Richard, Geoffrey, John, Matilda, Eleanor, and Joan. John Speed, in his 1611 work \"History of Great Britain\", mentions the possibility that Eleanor had a son named Philip, who died young. His sources no longer exist, and he alone mentions this birth.\n\nEleanor's marriage to Henry was reputed to be tumultuous and argumentative, although sufficiently cooperative to produce at least eight pregnancies. Henry was by no means faithful to his wife and had a reputation for philandering. Henry fathered other, illegitimate children throughout the marriage. Eleanor appears to have taken an ambivalent attitude towards these affairs: for example, Geoffrey of York, an illegitimate son of Henry, was acknowledged by Henry as his child and raised at Westminster in the care of the queen.\n\nDuring the period from Henry's accession to the birth of Eleanor's youngest son John, affairs in the kingdom were turbulent: Aquitaine, as was the norm, defied the authority of Henry as Eleanor's husband and answered only to their Duchess. Attempts were made to claim Toulouse, the rightful inheritance of Eleanor's grandmother Philippa of Toulouse, but they ended in failure. A bitter feud arose between the king and Thomas Becket, initially his Chancellor and closest adviser and later the Archbishop of Canterbury. Louis of France had remarried and been widowed; he married for the third time and finally fathered a long hoped-for son, Philip Augustus (also known as Dieudonne - God-given). \"Young Henry,\" son of Henry and Eleanor, wed Marguerite of France, daughter of Louis from his second marriage. Little is known of Eleanor's involvement in these events. It is certain that by late 1166, Henry's notorious affair with Rosamund Clifford had become known, and Eleanor's marriage to Henry appears to have become terminally strained.\n\nIn 1167, Eleanor's third daughter, Matilda, married Henry the Lion of Saxony. Eleanor remained in England with her daughter for the year prior to Matilda's departure for Normandy in September. In December, Eleanor gathered her movable possessions in England and transported them on several ships to Argentan. Christmas was celebrated at the royal court there, and she appears to have agreed to a separation from Henry. She certainly left for her own city of Poitiers immediately after Christmas. Henry did not stop her; on the contrary, he and his army personally escorted her there before attacking a castle belonging to the rebellious Lusignan family. Henry then went about his own business outside Aquitaine, leaving Earl Patrick (his regional military commander) as her protective custodian. When Patrick was killed in a skirmish, Eleanor (who proceeded to ransom his captured nephew, the young William Marshal), was left in control of her lands.\n\nOf all her influence on culture, Eleanor's time in Poitiers between 1168 and 1173 was perhaps the most critical, yet very little is known about it. Henry II was elsewhere, attending to his own affairs after escorting Eleanor there. Some believe that Eleanor's court in Poitiers was the \"Court of Love\", where Eleanor and her daughter Marie meshed and encouraged the ideas of troubadours, chivalry, and courtly love into a single court. It may have been largely to teach manners, as the French courts would be known for in later generations. The existence and reasons for this court are debated.\n\nIn \"The Art of Courtly Love\", Andreas Capellanus (Andrew the chaplain) refers to the court of Poitiers. He claims that Eleanor, her daughter Marie, Ermengarde, Viscountess of Narbonne, and Isabelle of Flanders would sit and listen to the quarrels of lovers and act as a jury to the questions of the court that revolved around acts of romantic love. He records some twenty-one cases, the most famous of them being a problem posed to the women about whether true love can exist in marriage. According to Capellanus, the women decided that it was not at all likely.\n\nSome scholars believe that the \"court of love\" probably never existed, since the only evidence for it is Andreas Capellanus' book. To strengthen their argument, they state that there is no other evidence that Marie ever stayed with her mother in Poitiers. Andreas wrote for the court of the king of France, where Eleanor was not held in esteem. Polly Schoyer Brooks (the author of a non-academic biography of Eleanor) suggests that the court did exist, but that it was not taken very seriously, and that acts of courtly love were just a \"parlor game\" made up by Eleanor and Marie in order to place some order over the young courtiers living there.\n\nThere is no claim that Eleanor invented courtly love, since it was a concept that had begun to grow before Eleanor's court arose. All that can be said is that her court at Poitiers was most likely a catalyst for the increased popularity of courtly love literature in the Western European regions. Amy Kelly, in her article, \"Eleanor of Aquitaine and her Courts of Love\", gives a very plausible description of the origins of the rules of Eleanor's court: \"in the Poitevin code, man is the property, the very thing of woman; whereas a precisely contrary state of things existed in the adjacent realms of the two kings from whom the reigning duchess of Aquitaine was estranged.\"\n\nIn March 1173, aggrieved at his lack of power and egged on by Henry's enemies, his son by the same name, the younger Henry, launched the Revolt of 1173–1174. He fled to Paris. From there, \"the younger Henry, devising evil against his father from every side by the advice of the French King, went secretly into Aquitaine where his two youthful brothers, Richard and Geoffrey, were living with their mother, and with her connivance, so it is said, he incited them to join him\". One source claimed that the Queen sent her younger sons to France \"to join with him against their father the king\". Once her sons had left for Paris, Eleanor may have encouraged the lords of the south to rise up and support them.\n\nSometime between the end of March and the beginning of May, Eleanor left Poitiers, but was arrested and sent to the king at Rouen. The king did not announce the arrest publicly; for the next year, the Queen's whereabouts were unknown. On 8 July 1174, Henry and Eleanor took ship for England from Barfleur. As soon as they disembarked at Southampton, Eleanor was taken either to Winchester Castle or Sarum Castle and held there.\n\nEleanor was imprisoned for the next sixteen years, much of the time in various locations in England. During her imprisonment, Eleanor became more and more distant with her sons, especially Richard (who had always been her favorite). She did not have the opportunity to see her sons very often during her imprisonment, though she was released for special occasions such as Christmas. About four miles from Shrewsbury and close by Haughmond Abbey is \"Queen Eleanor's Bower\", the remains of a triangular castle which is believed to have been one of her prisons.\n\nHenry lost the woman reputed to be his great love, Rosamund Clifford, in 1176. He had met her in 1166 and began his liaison in 1173, supposedly contemplating divorce from Eleanor. This notorious affair caused a monkish scribe to transcribe Rosamund's name in Latin to \"Rosa Immundi\", or \"Rose of Unchastity\". The king had many mistresses, but although he treated earlier liaisons discreetly, he flaunted Rosamund. He may have done so to provoke Eleanor into seeking an annulment but, if so, the queen disappointed him. Nevertheless, rumours persisted, perhaps assisted by Henry's camp, that Eleanor had poisoned Rosamund. It is also speculated that Eleanor placed Rosamund in a bathtub and had an old woman cut Rosamund's arms. Henry donated much money to Godstow Nunnery, where Rosamund was buried.\n\nIn 1183, the Young King Henry tried again to force his father to hand over some of his patrimony. In debt and refused control of Normandy, he tried to ambush his father at Limoges. He was joined by troops sent by his brother Geoffrey and Philip II of France. Henry II's troops besieged the town, forcing his son to flee. After wandering aimlessly through Aquitaine, Henry the Younger caught dysentery. On Saturday, 11 June 1183, the Young King realized he was dying and was overcome with remorse for his sins. When his father's ring was sent to him, he begged that his father would show mercy to his mother, and that all his companions would plead with Henry to set her free. Henry II sent Thomas of Earley, Archdeacon of Wells, to break the news to Eleanor at Sarum. Eleanor reputedly had had a dream in which she foresaw her son Henry's death. In 1193 she would tell Pope Celestine III that she was tortured by his memory.\n\nKing Philip II of France claimed that certain properties in Normandy belonged to his half-sister Marguerite, widow of the young Henry, but Henry insisted that they had once belonged to Eleanor and would revert to her upon her son's death. For this reason Henry summoned Eleanor to Normandy in the late summer of 1183. She stayed in Normandy for six months. This was the beginning of a period of greater freedom for the still-supervised Eleanor. Eleanor went back to England probably early in 1184. Over the next few years Eleanor often travelled with her husband and was sometimes associated with him in the government of the realm, but still had a custodian so that she was not free.\n\nUpon the death of her husband Henry II on 6 July 1189, Richard I was the undisputed heir. One of his first acts as king was to send William Marshal to England with orders to release Eleanor from prison; he found upon his arrival that her custodians had already released her. Eleanor rode to Westminster and received the oaths of fealty from many lords and prelates on behalf of the king. She ruled England in Richard's name, signing herself \"Eleanor, by the grace of God, Queen of England\". On 13August 1189, Richard sailed from Barfleur to Portsmouth and was received with enthusiasm. Between 1190 and 1194, Richard was absent from England, engaged in the Third Crusade from 1190 to 1192 and then held in captivity by HenryVI, Holy Roman Emperor. During Richard's absence, royal authority in England was represented by a Council of Regency in conjunction with a succession of chief justiciarsWilliam de Longchamp (1190–1191), Walter deCoutances (1191–1193) and finally Hubert Walter. Although Eleanor held no formal office in England during this period, she arrived in England in the company of Coutances in June 1191, and for the remainder of Richard's absence she exercised a considerable degree of influence over the affairs of England as well as the conduct of Prince John. Eleanor played a key role in raising the ransom demanded from England by HenryVI and in the negotiations with the Holy Roman Emperor that eventually secured Richard's release.\n\nEleanor survived Richard and lived well into the reign of her youngest son, King John. In 1199, under the terms of a truce between King PhilipII and King John, it was agreed that Philip's twelve-year-old heir-apparent Louis would be married to one of John's nieces, daughters of his sister Eleanor of Castile. John instructed his mother to travel to Castile to select one of the princesses. Now 77, Eleanor set out from Poitiers. Just outside Poitiers she was ambushed and held captive by Hugh IX of Lusignan, whose lands had been sold to HenryII by his forebears. Eleanor secured her freedom by agreeing to his demands. She continued south, crossed the Pyrenees, and travelled through the Kingdoms of Navarre and Castile, arriving in Castile before the end of January 1200.\n\nKing Alfonso VIII and Eleanor's daughter, Queen Eleanor of Castile, had two remaining unmarried daughters, Urraca and Blanche. Eleanor selected the younger daughter, Blanche. She stayed for two months at the Castilian court, then late in March journeyed with granddaughter Blanche back across the Pyrenees. She celebrated Easter in Bordeaux, where the famous warrior Mercadier came to her court. It was decided that he would escort the Queen and Princess north. \"On the second day in Easter week, he was slain in the city by a man-at-arms in the service of Brandin\", a rival mercenary captain. This tragedy was too much for the elderly queen, who was fatigued and unable to continue to Normandy. She and Blanche rode in easy stages to the valley of the Loire, and she entrusted Blanche to the Archbishop of Bordeaux, who took over as her escort. The exhausted Eleanor went to Fontevraud, where she remained. In early summer, Eleanor was ill, and John visited her at Fontevraud.\nEleanor was again unwell in early 1201. When war broke out between John and Philip, Eleanor declared her support for John and set out from Fontevraud to her capital Poitiers to prevent her grandson Arthur I, Duke of Brittany, posthumous son of Eleanor's son Geoffrey and John's rival for the English throne, from taking control. Arthur learned of her whereabouts and besieged her in the castle of Mirebeau. As soon as John heard of this, he marched south, overcame the besiegers, and captured the 15-year-old Arthur. Eleanor then returned to Fontevraud where she took the veil as a nun.\n\nEleanor died in 1204 and was entombed in Fontevraud Abbey next to her husband Henry and her son Richard. Her tomb effigy shows her reading a Bible and is decorated with magnificent jewelry. By the time of her death she had outlived all of her children except for King John of England and Queen Eleanor of Castile.\n\nContemporary sources praise Eleanor's beauty. Even in an era when ladies of the nobility were excessively praised, their praise of her was undoubtedly sincere. When she was young, she was described as \"perpulchra\"more than beautiful. When she was around 30, Bernard de Ventadour, a noted troubadour, called her \"gracious, lovely, the embodiment of charm\", extolling her \"lovely eyes and noble countenance\" and declaring that she was \"one meet to crown the state of any king\". William of Newburgh emphasized the charms of her person, and even in her old age Richard of Devizes described her as beautiful, while Matthew Paris, writing in the 13thcentury, recalled her \"admirable beauty.\"\n\nIn spite of all these words of praise, no one left a more detailed description of Eleanor; the colour of her hair and eyes, for example, are unknown. The effigy on her tomb shows a tall and large-boned woman with brown skin, though this may not be an accurate representation. Her seal of 1152 shows a woman with a slender figure, but this is likely an impersonal image.\n\nJudy Chicago's artistic installation \"The Dinner Party\" features a place setting for Eleanor.\n\nEleanor and Henry are the main characters in James Goldman's play \"The Lion in Winter\", which was made into a film starring Peter O'Toole and Katharine Hepburn in 1968 (for which Hepburn won the Academy Award for Best Actress and the BAFTA Award for Best Actress in a Leading Role and was nominated for the Golden Globe Award for Best Actress - Motion Picture Drama). The film was remade for television in 2003 with Patrick Stewart and Glenn Close (for which Close won the Golden Globe Award for Best Performance by an Actress In A Mini-series or Motion Picture Made for Television and was nominated for the Primetime Emmy Award for Outstanding Lead Actress - Miniseries or a Movie).\n\nThe depiction of Eleanor in the play \"Becket\", which was filmed in 1964 with Pamela Brown as Eleanor, contains historical inaccuracies, as acknowledged by the author, Jean Anouilh.\n\nIn 2004 Catherine Muschamp's one-woman play, \"Mother of the Pride\", toured the UK with Eileen Page in the title role. In 2005 Chapelle Jaffe played the same part in Toronto.\n\nThe character \"Queen Elinor\" appears in William Shakespeare's \"King John\", along with other members of the family. On television, she has been portrayed in this play by Una Venning in the \"BBC Sunday Night Theatre\" version (1952) and by Mary Morris in the BBC Shakespeare version (1984).\n\nShe figures prominently in Sharon Kay Penman's novels, \"When Christ And His Saints Slept\", \"Time and Chance\", and \"Devil's Brood\", and also appears briefly in \"Here Be Dragons\".\n\nPenman has written a series of historical mysteries in which Eleanor, in old age, sends a trusted servant to unravel various puzzles. The titles are \"The Queen's Man\", \"Cruel as the Grave\", \"Dragon's Lair\", and \"Prince of Darkness\".\n\nEleanor features as a more minor character in \"Lionheart \"and \"A King's Ransom\", both of which focus on the reign of her son, Richard, as king of England.\n\nEleanor is the principal character of \"Beloved Enemy\" and \"Gilded Cages\", the last two volumes of a trilogy of romance novels by Ellen Jones.\n\nE. L. Konigsburg's young adult novel \"A Proud Taste for Scarlet and Miniver\" takes place in Heaven of the late 20th century, where Eleanor of Aquitaine, Empress Matilda, and William the Marshall are waiting for King HenryII to be admitted to eternity at last. The Abbot Suger stops to chat with Eleanor and stays to wait, too. To pass the time, the four recall Eleanor's time on Earth. The flashbacks on earth are set during the Middle Ages in France and England, with a brief trip to the Holy Land. The flashbacks trace the highlights of Eleanor's life from 1137 (when she is 15 years old and about to wed Louis Capet, soon to be King LouisVII of France) to her death in 1204. Her life encompasses the rule of England by her husband Henry II and by her sons Richard and John. Originally published in 1973, the novel was put back in print by Atheneum in 2001.\n\nEleanor is associated with Nicole des Jardins in Arthur C. Clarke and Gentry Lee's follow-up series to Clarke's \"Rendezvous with Rama\".\n\nJean Plaidy has written about her in several novels. \"Courts of Love\" is written from a first person perspective in Plaidy's \"Queens of England series\", and in \"The Plantagenet Saga\", Eleanor of Aquitaine is featured in \"The Plantagenet Prelude\", \"Revolt of the Eaglets\", \"The Heart of the Lion\", and \"Prince of Darkness\".\n\nEleanor is featured in a book in The Royal Diaries series, \"Eleanor: Crown Jewel of Aquitaine\" (2002) by Kristiana Gregory. The books in the series are written as fictional diaries of royal women in their earlier years; Eleanor's is set in the year 1136.\n\nChristy English's historical novel, \"The Queen's Pawn\", published in April 2010, depicts Eleanor of Aquitaine from 1169 to 1173, during her marriage to King HenryII of England and her relationship with Princess Alais of France. In April 2011 English published a second novel, \"To Be Queen\", which is another historical novel centered on Eleanor of Aquitaine's life. This novel covers the years 1132–1152, from before she became Duchess of Aquitaine until the end of her first marriage to Louis VII of France. Also published in April 2010 was Alison Weir's novel, \"The Captive Queen\", which details Eleanor's life from when she first met Henry II of England to her death in 1204.\n\nCecelia Holland's 2010 novel, \"The Secret Eleanor: A Novel of Eleanor of Aquitaine\", set in the years 1151–1152, is centered on Eleanor's relationship with her sister Petronilla; it narrates the meeting of Eleanor and Henry Plantagenet, the beginning of their love affair, Eleanor's annulment of her marriage to LouisVII, and Petronilla's role helping her sister in these events, in a fictional secret history concordant with the known facts of their lives.\n\nIn 2013 Elizabeth Chadwick published the first of three announced books about Eleanor's life, titled \"The Summer Queen\". The second, titled \"The Winter Crown\" was published in 2014. Eleanor was a major character in Chadwick's historical novels about William Marshal especially \"The Greatest Knight\".\n\nEleanor is one of four women profiled in Helen Castor's 2011 book \"She-Wolves: The Women Who Ruled England Before Elizabeth\" (Faber & Faber), and of the BBC documentary \"She-Wolves: England's Early Queens\", presented by Castor.\n\nEleanor features in Susan Appleyard's novel of HenryII 'The First Plantagenet'.\n\nIn 2012, Mark Richard Beaulieu published the first of six books in the Eleanor Code series, titled \"Eleanor of Aquitaine: The Young Life.\"\n\nEleanor featured as a supporting character in A.C. Gaughen's \"Lady Thief\" in 2014 and \"Lion Heart\" in 2015, which are re-tellings of the Robin Hood stories.\n\nEleanor has featured in a number of screen versions of the \"Ivanhoe\" and \"Robin Hood\" stories. She has been played by Martita Hunt in \"The Story of Robin Hood and His Merrie Men\" (1952), Jill Esmond in the British TV adventure series \"The Adventures of Robin Hood\" (1955–1960), Phyllis Neilson-Terry in the British TV adventure series \"Ivanhoe\" (1958), Yvonne Mitchell in the BBC TV drama series \"The Legend of Robin Hood\" (1975), Siân Phillips in the TV series \"Ivanhoe\" (1997), and Tusse Silberg in the TV series \"The New Adventures of Robin Hood\" (1997). She was portrayed by Lynda Bellingham in the BBC series \"Robin Hood\". Most recently, she was portrayed by Eileen Atkins in \"Robin Hood\" (2010).\n\nIn the 1964 film, \"Becket\" (1964), Eleanor is briefly played by Pamela Brown to Peter O'Toole's first performance as a young Henry II.\n\nIn the 1968 film, \"The Lion in Winter,\" Eleanor is played by Katharine Hepburn, while Henry is again portrayed by O'Toole. The film is about the difficult relationship between them and the struggle of their three sons Richard, Geoffrey, and John for their father's favour and the succession. A 2003 TV film, The Lion in Winter (2003 film), starred Glenn Close as Eleanor and Patrick Stewart as Henry.\n\nShe was portrayed by Mary Clare in the silent film, \"Becket\" (1923), by Prudence Hyman in \"Richard the Lionheart\" (1962), and twice by Jane Lapotaire; in the BBC TV drama series, \"The Devil's Crown\" (1978), and again in Mike Walker's BBC Radio 4 series, \"Plantagenet\" (2010). In the 2010 film, \"Robin Hood,\" starring Russell Crowe, Eleanor is played by Eileen Atkins. In the 2014 film, \"\", Eleanor is played by Debbie Rochon.\n\nEleanor and Rosamund Clifford, as well as Henry IIand Rosamund's father appear in Gaetano Donizetti's opera \"Rosmonda d'Inghilterra\" with a libretto by Felice Romani, which was premiered in Florence, at the Teatro Pergola, on 27February 1834. A recording made by Opera Rara (1994), features Nelly Miricioiu as Eleanor and Renée Fleming as Rosamund.\n\n\n\nBeaulieu, Mark Richard (2012) Eleanor of Aquitaine : The Young Life (The Eleanor Code)ISBN 1-4801-4736-2\nBeaulieu, Mark Richard (2012) Eleanor of Aquitaine : The Journey East (The Eleanor Code Book 2) ASIN: B009U047JQ\nBeaulieu, Mark Richard (2013) Eleanor of Aquitaine : The Voyage West (The Eleanor Code Book 3 ASIN: B00CMJ8IZQ\nBeaulieu, Mark Richard (2014) Eleanor of Aquitaine : The Generation (The Eleanor Code Book 4)ASIN: B00I3SREI2\nhttp://www.eleanorofaquitaine.net/Eleanor.html Mark Richard Beaulieu a series of books about Eleanor. The fifth, Love and Rebellion is coming in 2017.\n\n", "id": "9962", "title": "Eleanor of Aquitaine"}
{"url": "https://en.wikipedia.org/wiki?curid=9963", "text": "Epistle to Philemon\n\nThe Epistle of Paul to Philemon, known simply as Philemon, is one of the books of the Christian New Testament. It is a prison letter, co-authored by Paul the Apostle with Timothy, to Philemon, a leader in the Colossian church. It deals with the themes of forgiveness and reconciliation. Paul does not identify himself as an apostle with authority, but as \"a prisoner of Jesus Christ\", calling Timothy \"our brother\", and addressing Philemon as \"fellow labourer\" and \"brother.\" Onesimus, a slave that had departed from his master Philemon, was returning with this epistle wherein Paul asked Philemon to receive him as a \"brother beloved.\"\n\nPhilemon was a wealthy Christian, possibly a bishop of the house church that met in his home () in Colosse. This letter is now generally regarded as one of the undisputed works of Paul. It is the shortest of Paul's extant letters, consisting of only 335 words in the Greek text.\n\nThe epistle of Philemon is attributed to the apostle Paul, and this attribution has rarely been questioned by scholars. Along with six others, it is numbered among the \"undisputed letters\", which are widely considered to be authentically Pauline. The main challenge to the letter's authenticity came from a group of German scholars in the nineteenth century known as the Tübingen School. Their leader, Ferdinand Christian Baur, only accepted four New Testament epistles as genuinely written by Paul: Romans, 1 and 2 Corinthians and Galatians. Commenting on Philemon, Baur described the subject matter as \"so very singular as to arouse our suspicions,\" and concluded that it is perhaps a \"Christian romance serving to convey a genuine Christian idea.\"\n\nThe opening verse of the salutation also names Timothy alongside Paul. This, however, does not mean that Timothy was the epistle's co-author. Rather, Paul regularly mentions others in the address if they have a particular connection with the recipient. In this case, Timothy may have encountered Philemon while accompanying Paul in his work in Ephesus.\n\nAccording to the majority interpretation, Paul wrote this letter on behalf of Onesimus, a runaway slave who had wronged his owner Philemon. The details of the offence are unstated, although it is often assumed that Onesimus had fled after stealing money, as Paul states in verse 18 that if Onesimus owes anything, Philemon should charge this to Paul's account. Sometime after leaving, Onesimus came into contact with Paul, although again the details are unclear. He may have been arrested and imprisoned alongside Paul. Alternatively, he may have previously heard Paul's name (as his owner was a Christian) and so travelled to him for help. After meeting Paul, Onesimus became a Christian believer. An affection grew between them, and Paul would have been glad to keep Onesimus with him. However, he considered it better to send him back to Philemon with an accompanying letter, which aimed to effect reconciliation between them as Christian brothers. The preservation of the letter suggests that Paul's request was granted.\n\nOnesimus' status as a runaway slave was challenged by Allen Dwight Callahan in an article published in the \"Harvard Theological Review\" and in a later commentary. Callahan argues that, beyond verse 16, \"nothing in the text conclusively indicates that Onesimus was ever the chattel of the letter's chief addressee. Moreover, the expectations fostered by the traditional fugitive slave hypothesis go unrealized in the letter. Modern commentators, even those committed to the prevailing interpretation, have tacitly admitted as much.\" Callahan argues that the earliest commentators on this work – the homily of Origen and the Anti-Marcion Preface – are silent about Onesimus' possible servile status, and traces the origins of this interpretation to John Chrysostom, who proposed it in his \"Homiliae in epistolam ad Philemonem\", during his ministry in Antioch, circa 386–398. In place of the traditional interpretation, Callahan suggests that Onesimus and Philemon are brothers both by blood and religion, but who have become estranged, and the intent of this letter was to reconcile the two men. Ben Witherington III has challenged Callahan's interpretation as a misreading of Paul's rhetoric. Further, Margaret M. Mitchell has demonstrated that a number of writers before Chrysostom either argue or assume that Onesimus was a runaway slave, including Athanasius, Basil of Caesarea and Ambrosiaster.\n\nThe only extant information about Onesimus apart from this letter is found in Paul's epistle to the Colossians 4:7–9, where Onesimus is called \"a faithful and beloved brother\":\n\nThe letter is addressed to Philemon, who is described as a \"fellow worker\" of Paul. It is generally assumed that Philemon lived in Colossae; in the letter to the Colossians, Onesimus (the slave who fled from Philemon) and Archippus (whom Paul greets in the letter to Philemon) are described as members of the church there. Philemon may have converted to Christianity through Paul's ministry, possibly in Ephesus. The salutation mentions two other figures beyond Philemon. The first is Apphia, who is probably Philemon's wife. Paul also mentions Archippus, a \"fellow soldier\"; some have speculated that he is the son of Philemon and Apphia.\n\nThe American John Knox proposed that Onesimus' owner was in fact Archippus, and the letter was addressed to him rather than Philemon. In this reconstruction, Philemon would receive the letter first and then encourage Archippus to release Onesimus so that he could work alongside Paul. This view, however, has not found widespread support. In particular, Knox's view has been challenged on the basis of the opening verses. According to O'Brien, the fact that Philemon's name is mentioned first, together with the use of the phrase \"in your house\" in verse 2, makes it unlikely that Archippus was the primary addressee. Knox further argued that the letter was intended to be read aloud in the Colossian church in order to put pressure on Archippus. A number of commentators, however, see this view as contradicting the tone of the letter. J. B. Lightfoot, for example, wrote: \"The tact and delicacy of the Apostle's pleading for Onesimus would be nullified at one stroke by the demand for publication.\"\n\nThe opening salutation follows a typical pattern found in other Pauline letters. Paul first introduces himself, with a self-designation as a \"prisoner of Jesus Christ,\" which in this case refers to a physical imprisonment. He also mentions his associate Timothy, as a valued colleague who was presumably known to the recipient. As well as addressing the letter to Philemon, Paul sends greetings to Apphia, Archippus and the church that meets in Philemon's house. Apphia is often presumed to be Philemon's wife and Archippus, a \"fellow labourer\", is sometimes suggested to be their son. Paul concludes his salutation with a prayerful wish for grace and peace.\n\nBefore addressing the main topic of the letter, Paul continues with a paragraph of thanksgiving and intercession. This serves to prepare the ground for Paul's central request. He gives thanks to God for Philemon's love and faith and prays for his faith to be effective. He concludes this paragraph by describing the joy and comfort he has received from knowing how Philemon has shown love towards the Christians in Colossae.\n\nAs a background to his specific plea for Onesimus, Paul clarifies his intentions and circumstances. Although he has the boldness to command Philemon to do what would be right in the circumstances, he prefers to base his appeal on his knowledge of Philemon's love and generosity. He also describes the affection he has for Onesimus and the transformation that has taken place with Onesimus's conversion to the Christian faith. Where Onesimus was \"useless\", now he is \"useful\" – a wordplay, as Onesimus means \"useful\". Paul indicates that he would have been glad to keep Onesimus with him, but recognised that it was right to send him back. Paul's specific request is for Philemon to welcome Onesimus as he would welcome Paul, namely as a Christian brother. He offers to pay for any debt created by Onesimus' departure and expresses his desire that Philemon might refresh his heart in Christ.\n\nIn the final section of the letter, Paul describes his confidence that Philemon would do even more than he had requested, perhaps indicating his desire for Onesimus to return to work alongside him. He also mentions his wish to visit and asks Philemon to prepare a guest room. Paul sends greetings from five of his co-workers and concludes the letter with a benediction.\n\nPaul uses slavery vs. freedom language often in his writings as a metaphor. At this time slavery was common, and can be seen as a theme in the book of Philemon. Slavery was most commonly found in households. This letter, seemingly, provided alleviation of suffering of some slaves due to the fact that Paul placed pastoral focus on the issue.\n\nAlthough it is a main theme, Paul does not label slavery as negative or positive. Some scholars see it as unthinkable in the times to even question ending slavery. Because slavery was so ingrained into society that the “abolitionist would have been at the same time an insurrectionist, and the political effects of such a movement would have been unthinkable. Paul doesn’t question it in this epistle. Paul may have envisioned slavery as a fixed institution. He was not questioning the “rightness or wrongness” of it. Paul did however view slavery as a human institution, and believed that all human institutions were about to fade away. This may be because Paul had the perspective that Jesus would return soon. Paul viewed his present world as something that was swiftly passing away. This is a part of Pauline Christianity and theology.\n\nWhen it comes to Onesimus and his circumstance as a slave, Paul felt that Onesimus should return to Philemon but not as a slave, but under a bond of familial love. Paul also was not suggesting that Onesimus be punished, but Roman law allowed the owner of a runaway slave nearly unlimited privileges of punishment, even execution. This is a concern of Paul and a reason he is writing to Philemon, asking that Philemon accept Onesimus back in a bond of friendship, forgiveness, and reconciliation. Paul is trying to break through the social barriers dividing people. We see this in many of Paul’s other epistles, including his letters to the Corinthians, delivering the message of unity with others and unity with Christ - a change of identity. As written in Sacra Pagina Philippians and Philemon, the move from slave to freedman has to do with a shift in “standing under the lordship of Jesus Christ”. So in short, Onesimus’ honor and obedience is not claimed by Philemon, but by Christ.\n\nVerses 13-14 suggest that Paul wants Philemon to send Onesimus back to Paul (possibly freeing him for the purpose). Marshall, Travis and Paul write, \"Paul hoped that it might be possible for [Onesimus] to spend some time with him as a missionary colleague... If that is not a request for Onesimus to join Paul’s circle, I do not know what more would need to be said\".\n\nPaul's tactful address to Philemon was labelled \"holy flattery\" by Martin Luther. He saw a parallel between Paul and Christ in their work of reconciliation, which is also contained within the concept of Christian grace. Still, Luther insisted that the letter upheld the social \"status quo\": though not explicit, the text could be interpreted to indicate that Paul did nothing to change Onesimus's legal position as a servant and that Paul was complying with Roman law in returning him to Philemon. However, the text could also be interpreted as indicating that Paul was demanding the legal freedom of Onesimus and, as an act of both trust and reconciliation, holding Philemon accountable in the higher court of God to accomplish this change himself.\n\nSarah Ruden, in her \"Paul Among the People\" (2010), argues that in the letter to Philemon, Paul created the Western conception of the individual human being, \"unconditionally precious to God and therefore entitled to the consideration of other human beings.\" Before Paul, Ruden argues, a slave was considered subhuman, and entitled to no more consideration than an animal.\n\nDiarmaid MacCulloch, in his \"A History of Christianity\", described the epistle as \"a Christian foundation document in the justification of slavery\". Due to its ambiguity, the letter was a cause of debate during the British and later American struggles over the abolition of slavery. Both sides cited Philemon for support.\n\nFor understanding the Philemon's Book, it is necessary to know, the situation of the early Christian community in the Roman Empire and the economic system of the Classical Antiquity based on the slavery. According to the Epistle to Diognetus: \"For the Christians are distinguished from other men neither by country, nor language, nor the customs which they observe... They are in the flesh, but they do not live after the flesh. They pass their days on earth, but they are citizens of heaven. They obey the prescribed laws, and at the same time surpass the laws by their lives\".\n\nPope Benedict XVI refers to this letter in the Encyclic, \"Spe salvi\", highlighting the power of Christianity as power of the transformation of society. In fact it will be decisive to virtual disappearance of slavery during the middle age:\n\n\n\n\n", "id": "9963", "title": "Epistle to Philemon"}
{"url": "https://en.wikipedia.org/wiki?curid=9966", "text": "Elliptic curve cryptography\n\nElliptic curve cryptography (ECC) is an approach to public-key cryptography based on the algebraic structure of elliptic curves over finite fields. ECC requires smaller keys compared to non-ECC cryptography (based on plain Galois fields) to provide equivalent security.\n\nElliptic curves are applicable for encryption, digital signatures, pseudo-random generators and other tasks. They are also used in several integer factorization algorithms based on elliptic curves that have applications in cryptography, such as Lenstra elliptic curve factorization.\n\nPublic-key cryptography is based on the intractability of certain mathematical problems. Early public-key systems are secure assuming that it is difficult to factor a large integer composed of two or more large prime factors. For elliptic-curve-based protocols, it is assumed that finding the discrete logarithm of a random elliptic curve element with respect to a publicly known base point is infeasible: this is the \"elliptic curve discrete logarithm problem\" (ECDLP). The security of elliptic curve cryptography depends on the ability to compute a point multiplication and the inability to compute the multiplicand given the original and product points. The size of the elliptic curve determines the difficulty of the problem.\n\nThe primary benefit promised by elliptic curve cryptography is a smaller key size, reducing storage and transmission requirements, i.e. that an elliptic curve group could provide the same level of security afforded by an RSA-based system with a large modulus and correspondingly larger key: for example, a 256-bit elliptic curve public key should provide comparable security to a 3072-bit RSA public key.\n\nThe U.S. National Institute of Standards and Technology (NIST) has endorsed elliptic curve cryptography in its Suite B set of recommended algorithms, specifically elliptic curve Diffie–Hellman (ECDH) for key exchange and Elliptic Curve Digital Signature Algorithm (ECDSA) for digital signature. The U.S. National Security Agency (NSA) allows their use for protecting information classified up to top secret with 384-bit keys. However, in August 2015, the NSA announced that it plans to replace Suite B with a new cipher suite due to concerns about quantum computing attacks on ECC.\n\nWhile the RSA patent expired in 2000, there may be patents in force covering certain aspects of ECC technology. However some argue that the US government elliptic curve digital signature standard (ECDSA; NIST FIPS 186-3) and certain practical ECC-based key exchange schemes (including ECDH) can be implemented without infringing them, including RSA Laboratories and Daniel J. Bernstein\n\nThe use of elliptic curves in cryptography was suggested independently by Neal Koblitz and Victor S. Miller in 1985. Elliptic curve cryptography algorithms entered wide use in 2004 to 2005.\n\nFor current cryptographic purposes, an \"elliptic curve\" is a plane curve over a finite field (rather than the real numbers) which consists of the points satisfying the equation\n\nalong with a distinguished point at infinity, denoted ∞. (The coordinates here are to be chosen from a fixed finite field of characteristic not equal to 2 or 3, or the curve equation will be somewhat more complicated.)\n\nThis set together with the group operation of elliptic curves is an Abelian group, with the point at infinity as identity element. The structure of the group is inherited from the divisor group of the underlying algebraic variety.\n\nSeveral discrete logarithm-based protocols have been adapted to elliptic curves, replacing the group formula_3 with an elliptic curve:\n\nAt the RSA Conference 2005, the National Security Agency (NSA) announced Suite B which exclusively uses ECC for digital signature generation and key exchange. The suite is intended to protect both classified and unclassified national security systems and information.\n\nRecently, a large number of cryptographic primitives based on bilinear mappings on various elliptic curve groups, such as the Weil and Tate pairings, have been introduced. Schemes based on these primitives provide efficient identity-based encryption as well as pairing-based signatures, signcryption, key agreement, and proxy re-encryption.\n\nSome common implementation considerations include:\n\nTo use ECC, all parties must agree on all the elements defining the elliptic curve, that is, the \"domain parameters\" of the scheme. The field is defined by \"p\" in the prime case and the pair of \"m\" and \"f\" in the binary case. The elliptic curve is defined by the constants \"a\" and \"b\" used in its defining equation. Finally, the cyclic subgroup is defined by its \"generator\" (a.k.a. \"base point\") \"G\". For cryptographic application the order of \"G\", that is the smallest positive number \"n\" such that formula_4, is normally prime. Since \"n\" is the size of a subgroup of formula_5 it follows from Lagrange's theorem that the number formula_6 is an integer. In cryptographic applications this number \"h\", called the \"cofactor\", must be small (formula_7) and, preferably, formula_8. To summarize: in the prime case, the domain parameters are formula_9; in the binary case, they are formula_10.\n\nUnless there is an assurance that domain parameters were generated by a party trusted with respect to their use, the domain parameters \"must\" be validated before use.\n\nThe generation of domain parameters is not usually done by each participant because this involves computing the number of points on a curve which is time-consuming and troublesome to implement. As a result, several standard bodies published domain parameters of elliptic curves for several common field sizes. Such domain parameters are commonly known as \"standard curves\" or \"named curves\"; a named curve can be referenced either by name or by the unique object identifier defined in the standard documents:\nSECG test vectors are also available. NIST has approved many SECG curves, so there is a significant overlap between the specifications published by NIST and SECG. EC domain parameters may be either specified by value or by name.\n\nIf one (despite the above) wants to construct one's own domain parameters, one should select the underlying field and then use one of the following strategies to find a curve with appropriate (i.e., near prime) number of points using one of the following methods:\n\nSeveral classes of curves are weak and should be avoided:\n\n\nBecause all the fastest known algorithms that allow one to solve the ECDLP (baby-step giant-step, Pollard's rho, etc.), need formula_19 steps, it follows that the size of the underlying field should be roughly twice the security parameter. For example, for 128-bit security one needs a curve over formula_18, where formula_21. This can be contrasted with finite-field cryptography (e.g., DSA) which requires 3072-bit public keys and 256-bit private keys, and integer factorization cryptography (e.g., RSA) which requires a 3072-bit value of \"n\", where the private key should be just as large. However the public key may be smaller to accommodate efficient encryption, especially when processing power is limited.\n\nThe hardest ECC scheme (publicly) broken to date had a 112-bit key for the prime field case and a 109-bit key for the binary field case. For the prime field case, this was broken in July 2009 using a cluster of over 200 PlayStation 3 game consoles and could have been finished in 3.5 months using this cluster when running continuously. The binary field case was broken in April 2004 using 2600 computers over 17 months.\n\nA current project is aiming at breaking the ECC2K-130 challenge by Certicom, by using a wide range of different hardware: CPUs, GPUs, FPGA.\n\nA close examination of the addition rules shows that in order to add two points, one needs not only several additions and multiplications in formula_18 but also an inversion operation. The inversion (for given formula_23 find formula_24 such that formula_25) is one to two orders of magnitude slower than multiplication. Fortunately, points on a curve can be represented in different coordinate systems which do not require an inversion operation to add two points. Several such systems were proposed: in the \"projective\" system each point is represented by three coordinates formula_26 using the following relation: formula_27, formula_28; in the \"Jacobian system\" a point is also represented with three coordinates formula_26, but a different relation is used: formula_30, formula_31; in the \"López–Dahab system\" the relation is formula_27, formula_33; in the \"modified Jacobian\" system the same relations are used but four coordinates are stored and used for calculations formula_34; and in the \"Chudnovsky Jacobian\" system five coordinates are used formula_35. Note that there may be different naming conventions, for example, IEEE P1363-2000 standard uses \"projective coordinates\" to refer to what is commonly called Jacobian coordinates. An additional speed-up is possible if mixed coordinates are used.\n\nReduction modulo \"p\" (which is needed for addition and multiplication) can be executed much faster if the prime \"p\" is a pseudo-Mersenne prime, that is formula_36; for example, formula_37 or formula_38 Compared to Barrett reduction, there can be an order of magnitude speed-up. The speed-up here is a practical rather than theoretical one, and derives from the fact that the moduli of numbers against numbers near powers of two can be performed efficiently by computers operating on binary numbers with bitwise operations.\n\nThe curves over formula_14 with pseudo-Mersenne \"p\" are recommended by NIST. Yet another advantage of the NIST curves is that they use \"a\" = −3, which improves addition in Jacobian coordinates.\n\nAccording to Bernstein and Lange, many of the efficiency-related decisions in NIST FIPS 186-2 are sub-optimal. Other curves are more secure and run just as fast.\n\nElliptic curves are applicable for encryption, digital signatures, pseudo-random generators and other tasks. They are also used in several integer factorization algorithms that have applications in cryptography, such as Lenstra elliptic curve factorization.\n\nIn 1999, NIST recommended 15 elliptic curves. Specifically, FIPS 186-3 has 10 recommended finite fields:\n\nThe NIST recommendation thus contains a total of 5 prime curves and 10 binary curves. The curves were ostensibly chosen for optimal security and implementation efficiency.\n\nIn 2013, the \"New York Times\" stated that Dual Elliptic Curve Deterministic Random Bit Generation (or Dual_EC_DRBG) had been included as a NIST national standard due to the influence of NSA, which had included a deliberate weakness in the algorithm and the recommended elliptic curve. RSA Security in September 2013 issued an advisory recommending that its customers discontinue using any software based on Dual_EC_DRBG. In the wake of the exposure of Dual_EC_DRBG as \"an NSA undercover operation\", cryptography experts have also expressed concern over the security of the NIST recommended elliptic curves, suggesting a return to encryption based on non-elliptic-curve groups.\n\nUnlike most other DLP systems (where it is possible to use the same procedure for squaring and multiplication), the EC addition is significantly different for doubling (\"P\" = \"Q\") and general addition (\"P\" ≠ \"Q\") depending on the coordinate system used. Consequently, it is important to counteract side channel attacks (e.g., timing or simple/differential power analysis attacks) using, for example, fixed pattern window (a.k.a. comb) methods (note that this does not increase computation time). Alternatively one can use an Edwards curve; this is a special family of elliptic curves for which doubling and addition can be done with the same operation. Another concern for ECC-systems is the danger of fault attacks, especially when running on smart cards.\n\nCryptographic experts have expressed concerns that the National Security Agency has inserted a kleptographic backdoor into at least one elliptic curve-based pseudo random generator. Internal memos leaked by former NSA contractor, Edward Snowden, suggest that the NSA put a backdoor in the Dual_EC_DRBG standard. One analysis of the possible backdoor concluded that an adversary in possession of the algorithm's secret key could obtain encryption keys given only 32 bytes of ciphertext.\n\nThe SafeCurves project has been launched in order to catalog curves that are easy to securely implement and are designed in a fully publicly verifiable way to minimize the chance of a backdoor.\n\nIn contrast with its current standing over RSA, elliptic curve cryptography is expected to be more vulnerable to an attack based on Shor's algorithm. In theory, making a practical attack feasible many years before an attack on an equivalently secure RSA scheme is possible. This is because smaller elliptic curve keys are needed to match the classical security of RSA. The work of Proos and Zalka show how a quantum computer for breaking 2048-bit RSA requires roughly 4096 qubits, while a quantum computer to break the equivalently secure 224-bit Elliptic Curve Cryptography requires between 1300 and 1600 qubits.\n\nTo avoid quantum computing concerns, an elliptic curve-based alternative to Elliptic Curve Diffie Hellman which is not susceptible to Shor's attack is the Supersingular Isogeny Diffie–Hellman Key Exchange of De Feo, Jao and Plut. It uses elliptic curve isogenies to create a drop-in replacement for the quantum attackable Diffie–Hellman and Elliptic curve Diffie–Hellman key exchanges. This key exchange uses the same elliptic curve computational primitives of existing elliptic curve cryptography and requires computational and transmission overhead similar to many currently used public key systems.\n\nIn August, 2015, NSA announced that it planned to transition \"in the not distant future\" to a new cipher suite that is resistant to quantum attacks. \"Unfortunately, the growth of elliptic curve use has bumped up against the fact of continued progress in the research on quantum computing, necessitating a re-evaluation of our cryptographic strategy.\"\n\nAt least one ECC scheme (ECMQV) and some implementation techniques are covered by patents.\n\nAlternative representations of elliptic curves include:\n\n\n\n\n", "id": "9966", "title": "Elliptic curve cryptography"}
{"url": "https://en.wikipedia.org/wiki?curid=9967", "text": "EDM\n\nEDM or E-DM may refer to:\n\n\n\n\n\n", "id": "9967", "title": "EDM"}
{"url": "https://en.wikipedia.org/wiki?curid=9970", "text": "Eightfold Path (policy analysis)\n\nThe Eightfold Path is a method of policy analysis assembled by Eugene Bardach, a professor at the Goldman School of Public Policy at the University of California, Berkeley. It is outlined in his book \"A Practical Guide for Policy Analysis: The Eightfold Path to More Effective Problem Solving\", which is now in its fourth edition. The book is commonly referenced in public policy and public administration scholarship.\n\nBardach's procedure is as follows:\n\nA possible ninth-step, based on Bardach's own writing, might be \"Repeat Steps 1 - 8 as Necessary.\"\n\nThe New York taxi driver test is a technique for evaluating the effectiveness of communication between policy makers and analysts. Bardach contends that policy explanations must be clear and down-to-earth enough for a taxi driver to be able to understand the premise during a trip through city streets. The New York taxi driver is presumed to be both a non-specialist and a tough customer.\n\n\n", "id": "9970", "title": "Eightfold Path (policy analysis)"}
{"url": "https://en.wikipedia.org/wiki?curid=9971", "text": "Eden Project\n\nThe Eden Project () is a popular visitor attraction in Cornwall, England. Inside the two biomes are plants that are collected from many diverse climates and environments. The project is located in a reclaimed Kaolinite pit, located from the town of St Blazey and from the larger town of St Austell, Cornwall.\n\nThe complex is dominated by two huge enclosures consisting of adjoining domes that house thousands of plant species, and each enclosure emulates a natural biome. The biomes consist of hundreds of hexagonal and pentagonal, inflated, plastic cells supported by steel frames. The largest of the two biomes simulates a Rainforest environment and the second, a Mediterranean environment. The attraction also has an outside botanical garden which is home to many plants and wildlife native to Cornwall and the UK in general; it also has many plants that provide an important and interesting backstory, for example, those with a prehistoric heritage.\n\nThe project was conceived by Tim Smit and designed by architect Nicholas Grimshaw and engineering firm Anthony Hunt and Associates (now part of Sinclair Knight Merz). Davis Langdon carried out the project management, Sir Robert McAlpine and Alfred McAlpine did the construction, MERO designed and built the biomes, and Arup was the services engineer, economic consultant, environmental engineer and transportation engineer. Land use consultants led the masterplan and landscape design. The project took 2½ years to construct and opened to the public on 17 March 2001.\n\nOnce into the attraction, there is a meandering path with views of the two biomes, planted landscapes, including vegetable gardens, and sculptures that include a giant bee and The WEEE Man, a towering figure made from old electrical appliances and is meant to represent the average electrical waste used by one person in a lifetime.\n\nAt the bottom of the pit are two covered biomes:\n\nThe Tropical Biome, covers and measures high, wide, and long. It is used for tropical plants, such as fruiting banana plants, coffee, rubber and giant bamboo, and is kept at a tropical temperature and moisture level.\nThe Mediterranean Biome covers and measures high, wide, and long. It houses familiar warm temperate and arid plants such as olives and grape vines and various sculptures.\n\nThe Outdoor Gardens represent the temperate regions of the world with plants such as tea, lavender, hops, hemp and sunflowers, as well as local plant species.\n\nThe covered biomes are constructed from a tubular steel (hex-tri-hex) with mostly hexagonal external cladding panels made from the thermoplastic ETFE. Glass was avoided due to its weight and potential dangers. The cladding panels themselves are created from several layers of thin UV-transparent ETFE film, which are sealed around their perimeter and inflated to create a large cushion. The resulting cushion acts as a thermal blanket to the structure. The ETFE material is resistant to most stains, which simply wash off in the rain. If required, cleaning can be performed by abseilers. Although the ETFE is susceptible to punctures, these can be easily fixed with ETFE tape. The structure is completely self-supporting, with no internal supports, and takes the form of a geodesic structure. The panels vary in size up to across, with the largest at the top of the structure.\n\nThe ETFE technology was supplied and installed by the firm Vector Foiltec, which is also responsible for ongoing maintenance of the cladding. The steel spaceframe and cladding package (with Vector Foiltec as ETFE subcontractor) was designed, supplied and installed by MERO (UK) PLC, who also jointly developed the overall scheme geometry with the architect, Nicholas Grimshaw & Partners.\n\nThe entire build project was managed by McAlpine Joint Venture.\n\nThe Core is the latest addition to the site and opened in September 2005. It provides the Eden Project with an education facility, incorporating classrooms and exhibition spaces designed to help communicate Eden's central message about the relationship between people and plants. Accordingly, the building has taken its inspiration from plants, most noticeable in the form of the soaring timber roof, which gives the building its distinctive shape.\n\nGrimshaw developed the geometry of the copper-clad roof in collaboration with a sculptor, Peter Randall-Page, and Mike Purvis of structural engineers SKM Anthony Hunts. It is derived from phyllotaxis, which is the mathematical basis for nearly all plant growth; the \"opposing spirals\" found in many plants such as the seeds in a sunflower's head, pine cones and pineapples. The copper was obtained from traceable sources, and the Eden Project is working with Rio Tinto Group to explore the possibility of encouraging further traceable supply routes for metals, which would enable users to avoid metals mined unethically. The services and acoustic, mechanical and electrical engineering design was carried out by Buro Happold.\n\nThe Core is also home to art exhibitions throughout the year. A permanent installation entitled \"Seed\", by Peter Randall-Page, occupies the anteroom. \"Seed\" is a large, egg-shaped stone installation displaying a complex pattern of protrusions that are based upon the geometric and mathematical principles that underlie plant growth.\n\nThe domes provide diverse growing conditions, and many plants are on display.\n\nThe Eden Project includes environmental education focusing on the interdependence of plants and people; plants are labelled with their medicinal uses. The massive amounts of water required to create the humid conditions of the Tropical Biome, and to serve the toilet facilities, are all sanitised rain water that would otherwise collect at the bottom of the quarry. The only mains water used is for hand washing and for cooking. The complex also uses Green Tariff Electricity – the energy comes from one of the many wind turbines in Cornwall, which were among the first in Europe.\n\nControversially, one of the companies the Eden Project currently partners with is the British mining company Rio Tinto Group.\n\nIn December 2010 the Eden Project received permission to build a geothermal electricity plant which will generate approx 4MWe, enough to supply Eden and about 5000 households.\n\nThe clay pit in which the project is sited was in use for over 160 years. In 1981, the pit was used by the BBC as the planet surface of Magrathea in the 1981 TV series of \"the Hitchhiker's Guide to the Galaxy\". By the mid-1990s the pit was all but exhausted.\n\nThe initial idea for the project dates back to 1996, with construction beginning in 1998. The work was hampered by torrential rain in the first few months of the project, and parts of the pit flooded as it sits below the water table.\n\nThe first part of the Eden Project, the visitor centre, opened to the public in May 2000. The first plants began arriving in September of that year, and the full site opened on 17 March 2001.\n\nThe Eden Project was used as a filming location for the 2002 James Bond film, \"Die Another Day\" (starring Pierce Brosnan). On 2 July 2005 The Eden Project hosted the \"Africa Calling\" concert of the Live 8 concert series. It has also provided some plants for the British Museum's Africa garden.\n\nIn 2005, the Project launched \"A Time of Gifts\" for the winter months, November to February. This features an ice rink covering the lake, with a small café/bar attached, as well as a Christmas market. Cornish choirs regularly perform in the biomes.\n\nOn 6 December 2007, the Eden Project invited people all over Cornwall to try to break the world record for the biggest ever pub quiz as part of its campaign to bring £50 million of lottery funds to Cornwall.\n\nIn December 2007, the project failed in its bid for £50 million of funding, after the Big Lottery Fund popular vote,\nwhen it received just 12.07% of the votes, the lowest for the four projects being considered. Eden wanted the money for Edge, a proposed desert biome that was going to look at people and plants living on the edge today and the solutions that they have come up with to the challenge of living within limits.\n\nIn December 2009, much of the project, including both greenhouses, became available to navigate through Google Street View.\n\nThe Eden Trust revealed a trading loss of £1.3 million for 2012-13,on a turnover of £25.4 million. The Eden Project had posted a surplus of £136,000 for the previous year. In 2014 Eden accounts showed a surplus of £2 million.\n\nThe World Pasty Championships have been held at the Eden Project since 2012, an international competition to find the best Cornish pasties and other pasty-type savoury snacks.\nThe Eden Project is said to have contributed over £1 billion into the Cornish economy.\n\nSince 2002, the Project has hosted a series of musical performances, called the Eden Sessions. Artists have included Amy Winehouse, James Morrison, Muse, Lily Allen, Snow Patrol, Pulp, Brian Wilson and The Magic Numbers. 2008's summer headliners were: The Verve, Kaiser Chiefs, and KT Tunstall. Oasis were also set to play in the summer of 2008, but the concert was postponed because Noel Gallagher was unable to perform after breaking three ribs in a stage invasion incident several weeks before. The concert was instead played in the summer of 2009. 2010 saw performances from artists including Mika, Jack Johnson, Mojave 3, Doves, Paolo Nutini, Mumford & Sons, and Martha Wainwright.\n\nThe 2011 sessions were headlined by The Flaming Lips, Primal Scream, Pendulum, Fleet Foxes and Brandon Flowers with support from The Horrors, The Go! Team, OK Go, Villagers, and The Bees.\n\nThe 2012 Eden sessions were headlined by: Tim Minchin, Example, Frank Turner, Chase & Status, Plan B, Blink-182, Noah and the Whale, and The Vaccines.\n\nThe 2013 Eden Sessions were headlined by: Kaiser Chiefs, Jessie J, Eddie Izzard, Sigur Rós, and The xx.\n\nThe 2014 Eden Sessions were headlined by: Dizzee Rascal, Skrillex, Pixar in Concert, Ellie Goulding and Elbow.\n\nThe 2015 Eden Sessions were headlined by: Paolo Nutini, Elton John, Paloma Faith, Motörhead, The Stranglers, Spandau Ballet and Ben Howard.\n\nThe 2016 Eden Sessions were headlined by: Lionel Richie, Tom Jones, PJ Harvey, Manic Street Preachers and Jess Glynne.\n\n\n\n\n", "id": "9971", "title": "Eden Project"}
{"url": "https://en.wikipedia.org/wiki?curid=9974", "text": "European Commission\n\nThe European Commission (EC) is an institution of the European Union, responsible for proposing legislation, implementing decisions, upholding the EU treaties and managing the day-to-day business of the EU. Commissioners swear an oath at the European Court of Justice in Luxembourg, pledging to respect the treaties and to be completely independent in carrying out their duties during their mandate.\n\nThe Commission operates as a cabinet government, with 28 members of the Commission (informally known as \"commissioners\"). There is one member per member state, but members are bound by their oath of office to represent the general interest of the EU as a whole rather than their home state. One of the 28 is the Commission President (currently Jean-Claude Juncker) proposed by the European Council and elected by the European Parliament. The Council of the European Union then nominates the other 27 members of the Commission in agreement with the nominated President, and the 28 members as a single body are then subject to a vote of approval by the European Parliament. The current Commission is the Juncker Commission, which took office in late 2014.\n\nThe term \"Commission\" is used either in the narrow sense of the 28-member \"College of Commissioners\" (or \"College\") or to also include the administrative body of about 23,000 European civil servants who are split into departments called directorates-general and services. The procedural languages of the Commission are English, French and German.\nThe Members of the Commission and their \"cabinets\" (immediate teams) are based in the Berlaymont building in Brussels.\n\nThe European Commission derives from one of the five key institutions created in the supranational European Community system, following the proposal of Robert Schuman, French Foreign Minister, on 9 May 1950. Originating in 1951 as the High Authority in the European Coal and Steel Community, the Commission has undergone numerous changes in power and composition under various presidents, involving three Communities.\n\nThe first Commission originated in 1951 as the nine-member \"High Authority\" under President Jean Monnet (see Monnet Authority). The High Authority was the supranational administrative executive of the new European Coal and Steel Community (ECSC). It took office first on 10 August 1952 in Luxembourg. In 1958 the Treaties of Rome had established two new communities alongside the ECSC: the European Economic Community (EEC) and the European Atomic Energy Community (Euratom). However their executives were called \"Commissions\" rather than \"High Authorities\". The reason for the change in name was the new relationship between the executives and the Council. Some states such as France expressed reservations over the power of the High Authority and wished to limit it giving more power to the Council rather than the new executives.\n\nLouis Armand led the first Commission of Euratom. Walter Hallstein led the first Commission of the EEC, holding the first formal meeting on 16 January 1958 at the Château of Val-Duchesse. It achieved agreement on a contentious cereal price accord as well as making a positive impression upon third countries when it made its international debut at the Kennedy Round of General Agreement on Tariffs and Trade (GATT) negotiations. Hallstein notably began the consolidation of European law and started to have a notable impact on national legislation. Little heed was taken of his administration at first but, with help from the European Court of Justice, his Commission stamped its authority solidly enough to allow future Commissions to be taken more seriously. However, in 1965 accumulating differences between the French government of Charles de Gaulle and the other member states (over British entry, direct elections to Parliament, the Fouchet Plan and the budget) triggered the \"empty chair\" crisis ostensibly over proposals for the common agricultural policy. Although the institutional crisis was solved the following year, it cost Etienne Hirsch his presidency of Euratom and later Walter Hallstein the EEC presidency despite otherwise being viewed as the most 'dynamic' leader until Jacques Delors.\n\nThe three bodies, collectively named the European Executives, co-existed until 1 July 1967 when, under the Merger Treaty, they were combined into a single administration under President Jean Rey. Due to the merger the Rey Commission saw a temporary increase to 14 members, although subsequent Commissions were reduced back down to nine, following the formula of one member for small states and two for larger states. The Rey Commission completed the Community's customs union in 1968 and campaigned for a more powerful, elected, European Parliament. Despite Rey being the first President of the combined communities, Hallstein is seen as the first President of the modern Commission.\n\nThe Malfatti and Mansholt Commissions followed with work on monetary co-operation and the first enlargement to the north in 1973. With that enlargement the Commission's membership increased to thirteen under the Ortoli Commission (the United Kingdom as a large member was granted two Commissioners), which dealt with the enlarged community during economic and international instability at that time. The external representation of the Community took a step forward when President Roy Jenkins, recruited to the presidency in January 1977 from his role as Home Secretary of the United Kingdom's Labour government, became the first President to attend a G8 summit on behalf of the Community. Following the Jenkins Commission, Gaston Thorn's Commission oversaw the Community's enlargement to the south, in addition to beginning work on the Single European Act.\n\nThe Commission headed by Jacques Delors was seen as giving the Community a sense of direction and dynamism. Delors and his team are also considered as the \"founding fathers of the euro\". The International Herald Tribune noted the work of Delors at the end of his second term in 1992: \"Mr. Delors rescued the European Community from the doldrums. He arrived when Europessimism was at its worst. Although he was a little-known former French finance minister, he breathed life and hope into the EC and into the dispirited Brussels Commission. In his first term, from 1985 to 1988, he rallied Europe to the call of the single market, and when appointed to a second term he began urging Europeans toward the far more ambitious goals of economic, monetary and political union\".\n\nThe successor to Delors was Jacques Santer. The entire Santer Commission was forced to resign in 1999 by the Parliament as result of a fraud and corruption scandal, with a central role played by Édith Cresson. These frauds were revealed by an internal auditor Paul van Buitenen.\n\nThat was the first time a Commission had been forced to resign \"en masse\" and represented a shift of power towards the Parliament. However the Santer Commission did carry out work on the Amsterdam Treaty and the euro. In response to the scandal the European Anti-Fraud Office (OLAF) was created.\n\nFollowing Santer, Romano Prodi took office. The Amsterdam Treaty had increased the Commission's powers and Prodi was dubbed by the press as something akin to a Prime Minister. Powers were strengthened again with the Nice Treaty in 2001 giving the Presidents more power over the composition of their Commissions.\n\nIn 2004 José Manuel Barroso became President: the Parliament once again asserted itself in objecting to the proposed membership of the Barroso Commission. Due to the opposition Barroso was forced to reshuffle his team before taking office. The Barroso Commission was also the first full Commission since the enlargement in 2004 to 25 members and hence the number of Commissioners at the end of the Prodi Commission had reached 30. As a result of the increase in the number of states, the Amsterdam Treaty triggered a reduction in the number of Commissioners to one per state, rather than two for the larger states.\n\nAllegations of fraud and corruption were again raised in 2004 by former chief auditor Jules Muis. A Commission officer Guido Strack reported alleged fraud and abuses in his department in years 2002–2004 to OLAF and was fired as result. In 2008 Paul van Buitenen (the former auditor known from Santer Commission scandal) accused the European Anti-Fraud Office (OLAF) of a lack of independence and effectiveness.\n\nBarroso's first Commission term expired on 31 October 2009. Under the Treaty of Nice, the first Commission to be appointed after the number of member states reached 27 would have to be reduced to \"less than the number of Member States\". The exact number of Commissioners was to be decided by a unanimous vote of the European Council and membership would rotate equally between member states. Following the accession of Romania and Bulgaria in January 2007, this clause took effect for the next Commission. The Treaty of Lisbon, which came into force on 1 December 2009, mandated a reduction of the number of commissioners to two-thirds of member-states from 2014 unless the Council decided otherwise. Membership would rotate equally and no member state would have more than one Commissioner. However, the treaty was rejected by voters in Ireland in 2008 with one main concern being the loss of their Commissioner. Hence a guarantee given for a rerun of the vote was that the Council would use its power to amend the number of Commissioners upwards. However, according to the treaties it still has to be fewer than the total number of members, thus it was proposed that the member state that does not get a Commissioner would get the post of High Representative – the so-called 26+1 formula. This guarantee (which may find its way into the next treaty amendment, probably in an accession treaty) contributed to the Irish approving the treaty in a second referendum in 2009.\n\nLisbon also combined the posts of European Commissioner for External Relations with the Council's High Representative for the Common Foreign and Security Policy. This post, also a Vice-President of the Commission, would chair the Council of the European Union's foreign affairs meetings as well as the Commission's external relations duties. The treaty further provides that the most recent European elections should be \"\"taken into account\"\" when appointing the Commission, although the President is still proposed by the European Council; the European Parliament \"\"elects\"\" the Commission rather than \"\"approves\"\" it as under the Treaty of Nice.\n\nIn 2014, Jean-Claude Juncker became President of the European Commission.\n\nThe Commission was set up from the start to act as an independent supranational authority separate from governments; it has been described as \"the only body paid to think European\". The members are proposed by their member state governments, one from each. However, they are bound to act independently – neutral from other influences such as those governments which appointed them. This is in contrast to the Council, which represents governments, the Parliament, which represents citizens, the Economic and Social Committee, which represents organised civil society, and the Committee of the Regions, which represents local and regional authorities.\n\nThrough the Commission has several responsibilities: to develop medium-term strategies; to draft legislation and arbitrate in the legislative process; to represent the EU in trade negotiations; to make rules and regulations, for example in competition policy; to draw up the budget of the European Union; and to scrutinise the implementation of the treaties and legislation. The rules of procedure of the European Commission set out the Commission's operation and organisation.\n\nBefore the Treaty of Lisbon came into force, the executive power of the EU was held by the Council: it conferred on the Commission such powers for it to exercise. However, the Council was allowed to withdraw these powers, exercise them directly, or impose conditions on their use. This aspect has been changed by the Treaty of Lisbon, after which the Commission exercises its powers just by virtue of the treaties. Powers are more restricted than most national executives, in part due to the Commission's lack of power over areas like foreign policy – that power is held by the European Council, which some analysts have described as another executive.\n\nConsidering that under the Lisbon Treaty the European Council has become a formal institution with the power of appointing the Commission, it could be said that the two bodies hold the executive power of the EU (the European Council also holds individual national executive powers). However, it is the Commission that currently holds executive powers over the European Union. The governmental powers of the Commission have been such that some such as former Belgian Prime Minister Guy Verhofstadt have suggested changing its name to the \"European Government\", calling the present name of the Commission \"ridiculous\".\n\nThe Commission differs from the other institutions in that it alone has legislative initiative in the EU. Only the Commission can make formal proposals for legislation: they cannot originate in the legislative branches. Under the Treaty of Lisbon, no legislative act is allowed in the field of the Common Foreign and Security Policy. In the other fields the Council and Parliament are able to request legislation; in most cases the Commission initiates the basis of these proposals. This monopoly is designed to ensure coordinated and coherent drafting of EU law. This monopoly has been challenged by some who claim the Parliament should also have the right, with most national parliaments holding the right in some respects. However, the Council and Parliament may request the Commission to draft legislation, though the Commission does have the power to refuse to do so as it did in 2008 over transnational collective conventions. Under the Lisbon Treaty, EU citizens are also able to request the Commission to legislate in an area via a petition carrying one million signatures, but this is not binding.\n\nThe Commission's powers in proposing law have usually centred on economic regulation. It has put forward a large number of regulations based on a \"precautionary principle\". This means that pre-emptive regulation takes place if there is a credible hazard to the environment or human health: for example on tackling climate change and restricting genetically modified organisms. This is opposed to weighting regulations for their effect on the economy. Thus, the Commission often proposes stricter legislation than other countries. Due to the size of the European market this has made EU legislation an important influence in the global market.\n\nRecently the Commission has moved into creating European criminal law. In 2006, a toxic waste spill off the coast of Côte d'Ivoire, from a European ship, prompted the Commission to look into legislation against toxic waste. Some EU states at that time did not even have a crime against shipping toxic waste leading to the Commissioners Franco Frattini and Stavros Dimas to put forward the idea of \"ecological crimes\". Their right to propose criminal law was challenged in the European Court of Justice but upheld. As of 2007, the only other criminal law proposals which have been brought forward are on the intellectual property rights directive, and on an amendment to the 2002 counter-terrorism framework decision, outlawing terrorism‑related incitement, recruitment (especially via the internet) and training.\n\nOnce legislation is passed by the Council and Parliament, it is the Commission's responsibility to ensure it is implemented. It does this through the member states or through its agencies. In adopting the necessary technical measures, the Commission is assisted by committees made up of representatives of member states and of the public and private lobbies (a process known in jargon as \"comitology\"). Furthermore, the Commission is responsible for the implementation of the EU budget, ensuring, along with the Court of Auditors, that EU funds are correctly spent.\n\nIn particular the Commission has a duty to ensure the treaties and law are upheld, potentially by taking member states or other institutions to the Court of Justice in a dispute. In this role it is known informally as the \"guardian of the treaties\". Finally, the Commission provides some external representation for the Union, alongside the member states and the Common Foreign and Security Policy, representing the Union in bodies such as the World Trade Organisation. It is also usual for the President to attend meetings of the G8.\n\nThe Commission is composed of a college of \"Commissioners\" of 28 members, including the President and vice-presidents. Even though each member is appointed by a national government, one per state, they do not represent their state in the Commission. In practice, however, they do occasionally press for their national interest. Once proposed, the President delegates portfolios among each of the members. The power of a Commissioner largely depends upon their portfolio, and can vary over time. For example, the Education Commissioner has been growing in importance, in line with the rise in the importance of education and culture in European policy-making. Another example is the Competition Commissioner, who holds a highly visible position with global reach. Before the Commission can assume office, the college as a whole must be approved by the Parliament. Commissioners are supported by their personal cabinet who give them political guidance, while the Civil Service (the DGs, see below) deal with technical preparation.\n\nThe President of the Commission is first proposed by the European Council taking into account the latest Parliamentary elections; that candidate can then be elected by the European Parliament or not. If not, the European Council shall propose another candidate within one month.\nThe candidate has often been a leading national politician, but this is not a requirement. In 2009 (as with 2004), the Lisbon Treaty was not in force and Barroso was not \"elected\" by the Parliament, but rather nominated by the European Council; in any case, the centre-right parties of the EU pressured for a candidate from their own ranks. In the end, a centre-right candidate was chosen: José Manuel Barroso of the European People's Party.\n\nThere are further criteria influencing the choice of the candidate, including: which area of Europe the candidate comes from, favoured as Southern Europe in 2004; the candidate's political influence, credible yet not overpowering members; language, proficiency in French considered necessary by France; and degree of integration, their state being a member of both the eurozone and the Schengen Agreement. In 2004, this system produced a number of candidates and was thus criticised by some MEPs: following the drawn-out selection, the ALDE group leader Graham Watson described the procedure as a \"Justus Lipsius carpet market\" producing only the \"lowest common denominator\"; while Green-EFA co-leader Daniel Cohn-Bendit asked Barroso after his first speech \"If you are the best candidate, why were you not the first?\"\n\nFollowing the election of the President, and the appointment of the High Representative by the European Council, each Commissioner is nominated by their member state (except for those states who provided the President and High Representative) in consultation with the Commission President, although he holds no hard power to force a change in candidate. However the more capable the candidate is, the more likely the Commission President will assign them a powerful portfolio, the distribution of which is entirely at his discretion. The President's team is then subject to hearings at the European Parliament which will question them and then vote on their suitability as a whole. If members of the team are found to be too inappropriate, the President must then reshuffle the team or request a new candidate from the member state or risk the whole Commission being voted down. As Parliament cannot vote against individual Commissioners there is usually a compromise whereby the worst candidates are removed but minor objections are put aside so the Commission can take office. Once the team is approved by parliament, it is formally put into office by the European Council ().\n\nFollowing their appointment, the President appoints a number of Vice-Presidents (the High Representative is mandated to be one of them) from among the commissioners. For the most part, the position grants little extra power to Vice-Presidents, except the first Vice-President who stands in for the President when he is away.\n\nThe European Parliament can dissolve the Commission as a whole following a vote of no-confidence but only the President can request the resignation of an individual Commissioner. However, individual Commissioners, by request of the Council or Commission, can be compelled to retire on account of a breach of obligation(s) and if so ruled by the European Court of Justice (Art. 245 and 247, Treaty on the Functioning of the European Union).\n\nThe Barroso Commission took office in late 2004 after being delayed by objections from the Parliament, which forced a reshuffle. In 2007 the Commission increased from 25 to 27 members with the accession of Romania and Bulgaria who each appointed their own Commissioners. With the increasing size of the Commission, Barroso adopted a more presidential style of control over the college, which earned him some criticism.\n\nHowever, under Barroso, the Commission began to lose ground to the larger member states as countries such as France, the UK and Germany sought to sideline its role. This has increased with the creation of the President of the European Council under the Treaty of Lisbon. There has also been a greater degree of politicisation within the Commission.\n\nThe Commission is divided into departments known as Directorates-General (DGs) that can be likened to departments or ministries. Each covers a specific policy area such as Agriculture or Justice and citizens' rights or internal services such as Human Resources and Translation and is headed by Director-General who is responsible to a Commissioner. A Commissioner's portfolio can be supported by numerous DGs, they prepare proposals for them and if approved by a majority of Commissioners it goes forward to Parliament and Council for consideration. The Commission's civil service is headed by a Secretary General, currently Alexander Italianer. The rules of procedure of the European Commission set out the Commission's operation and organisation.\n\nThere has been criticism from a number of people that the highly fragmented DG structure wastes a considerable amount of time in turf wars as the different departments and Commissioners compete with each other. Furthermore, the DGs can exercise considerable control over a Commissioner with the Commissioner having little time to learn to assert control over their staff.\n\nAccording to figures published by the Commission, 23,803 persons were employed by the Commission as officials and temporary agents in September 2012. In addition to these, 9230 \"external staff\" (e.g. Contractual agents, detached national experts, young experts, trainees etc.) were employed. The single largest DG is the Directorate-General for Translation, with a 2309-strong staff, while the largest group by nationality is Belgian (18.7%), probably due to a majority (17,664) of staff being based in the country.\n\nCommunication with the press is handled by the Directorate-General Communication. The Commission's chief spokesperson is Pia Ahrenkilde Hansen who takes the midday press briefings, commonly known as the \"Midday Presser\". It takes place every weekday in the Commission's press room at the Berlaymont where journalists may ask questions of Commission officials on any topic and legitimately expect to get an \"on the record\" answer for live TV. Such a situation is unique in the world.\n\nIt has been noted by one researcher that the press releases issued by the Commission are uniquely political. A release often goes through several stages of drafting which emphasises the role of the Commission and is used \"for justifying the EU and the commission\" increasing their length and complexity. Where there are multiple departments involved a press release can also be a source of competition between areas of the Commission and Commissioners themselves. This also leads to an unusually high number of press releases, 1907 for 2006, and is seen as a unique product of the EU's political set-up. The number of Commission press releases shows a decreasing trend. 1768 press releases were published in 2010 and 1589 in 2011.\n\nThere is a larger press corps in Brussels than Washington D.C.; in 2007 media outlets in every Union member-state had a Brussels correspondent. However, since the global downturn by 2010 the press corps in Brussels shrunk by a third. There is one journalist covering EU news for Latvia and none for Lithuania. Although there has been a worldwide cut in journalists, the considerable press releases and operations such as Europe by Satellite and EuroparlTV leads many news organisations to believe they can cover the EU from these source and news agencies. In the face of high-level criticism, the Commission is also due to shut down Presseurop on 20 December 2013.\n\nWhile the Commission is the executive branch, the candidates are chosen individually by the 28 national governments, which means it is not possible for a Commission Member or its President to be removed by a direct election. Rather, the legitimacy of the Commission is mainly drawn from the vote of approval that is required from the European Parliament, along with Parliament's power to dismiss the body, which, in turn, raises the concern of the relatively low turnout (less than 50%) in elections for the European Parliament since 1999. While that figure may be higher than that of some national elections, including the off-year elections of the United States Congress, the fact that there are no elections for the position of Commission President calls the position's legitimacy into question in the eyes of some. The fact that the Commission can directly decide (albeit with oversight from specially formed 'comitology committees') on the shape and character of implementing legislation further raises concerns about democratic legitimacy.\n\nEven though democratic structures and methods are developing there is not such a mirror in creating a European civil society. The Treaty of Lisbon may go some way to resolving the deficit in creating greater democratic controls on the Commission, including enshrining the procedure of linking elections to the selection of the Commission president. An alternative viewpoint is that electoral pressures undermine the Commission's role as an independent regulator, considering it akin with institutions such as independent central banks which deal with technical areas of policy. In addition some defenders of the Commission point out that legislation must be approved by the Council in all areas (the ministers of member states) and the European Parliament in some areas before it can be adopted, thus the amount of legislation which is adopted in any one country without the approval of its government is limited.\n\nIn 2009 the European ombudsman published statistics of citizens' complaints against EU institutions, with most of them filed against the Commission (66%) and concerning lack of transparency (36%). In 2010 the Commission was sued for blocking access to documents on EU biofuel policy. This happened after media accused the Commission of blocking scientific evidence against biofuel subsidies. Lack of transparency, unclear lobbyist relations, conflicts of interests and excessive spending of the Commission was highlighted in a number of reports by internal and independent auditing organisations. It has also been criticised on IT-related issues, particularly with regard to Microsoft.\n\nThe Commission is primarily based in Brussels, with the President's office and the Commission's meeting room on the 13th floor of the Berlaymont building. The Commission also operates out of numerous other buildings in Brussels and Luxembourg. When the Parliament is meeting in Strasbourg, the Commissioners also meet there in the Winston Churchill building to attend the Parliament's debates.\n\n", "id": "9974", "title": "European Commission"}
{"url": "https://en.wikipedia.org/wiki?curid=9975", "text": "Linear filter\n\nLinear filters process time-varying input signals to produce output signals, subject to the constraint of linearity. This results from systems composed solely of components (or digital algorithms) classified as having a linear response. Most filters implemented in analog electronics, in digital signal processing, or in mechanical systems are classified as causal, time invariant, and linear signal processing filters.\n\nThe general concept of linear filtering is also used in statistics, data analysis, and mechanical engineering among other fields and technologies. This includes non-causal filters and filters in more than one dimension such as those used in image processing; those filters are subject to different constraints leading to different design methods.\n\nA linear time-invariant (LTI) filter can be uniquely specified by its impulse response \"h\", and the output of any filter is mathematically expressed as the convolution of the input with that impulse response. The frequency response, given by the filter's transfer function formula_1, is an alternative characterization of the filter. Typical filter design goals are to realize a particular frequency response, that is, the magnitude of the transfer function formula_2; the importance of the phase of the transfer function varies according to the application, inasmuch as the shape of a waveform can be distorted to a greater or lesser extent in the process of achieving a desired (amplitude) response in the frequency domain. The frequency response may be tailored to, for instance, eliminate unwanted frequency components from an input signal, or to limit an amplifier to signals within a particular band of frequencies.\n\nThe impulse response \"h\" of a linear time-invariant causal filter specifies the output that the filter would produce if it were to receive an input consisting of a single impulse at time 0. An \"impulse\" in a continuous time filter means a Dirac delta function; in a discrete time filter the Kronecker delta function would apply. The impulse response completely characterizes the response of any such filter, inasmuch as any possible input signal can be expressed as a (possibly infinite) combination of weighted delta functions. Multiplying the impulse response shifted in time according to the arrival of each of these delta functions by the amplitude of each delta function, and summing these responses together (according to the superposition principle, applicable to all linear systems) yields the output waveform.\n\nMathematically this is described as the convolution of a time-varying input signal \"x(t)\" with the filter's impulse response \"h\", defined as:\n\nThe first form is the continuous-time form, which describes mechanical and analog electronic systems, for instance. The second equation is a discrete-time version used, for example, by digital filters implemented in software, so-called \"digital signal processing\". The impulse response \"h\" completely characterizes any linear time-invariant (or shift-invariant in the discrete-time case) filter. The input \"x\" is said to be \"convolved\" with the impulse response \"h\" having a (possibly infinite) duration of time \"T\" (or of \"N\" sampling periods).\n\nFilter design consists of finding a possible transfer function that can be implemented within certain practical constraints dictated by the technology or desired complexity of the system, followed by a practical design that realizes that transfer function using the chosen technology. The complexity of a filter may be specified according to the order of the filter.\n\nAmong the time-domain filters we here consider, there are two general classes of filter transfer functions that can approximate a desired frequency response. Very different mathematical treatments apply to the design of filters termed infinite impulse response (IIR) filters, characteristic of mechanical and analog electronics systems, and finite impulse response (FIR) filters, which can be implemented by discrete time systems such as computers (then termed \"digital signal processing\").\n\nConsider a physical system that acts as a linear filter, such as a system of springs and masses, or an analog electronic circuit that includes capacitors and/or inductors (along with other linear components such as resistors and amplifiers). When such a system is subject to an impulse (or any signal of finite duration) it responds with an output waveform that lasts past the duration of the input, eventually decaying exponentially in one or another manner, but never completely settling to zero (mathematically speaking). Such a system is said to have an infinite impulse response (IIR). The convolution integral (or summation) above extends over all time: T (or N) must be set to infinity.\n\nFor instance, consider a damped harmonic oscillator such as a pendulum, or a resonant L-C tank circuit. If the pendulum has been at rest and we were to strike it with a hammer (the \"impulse\"), setting it in motion, it would swing back and forth (\"resonate\"), say, with an amplitude of 10 cm. After 10 minutes, say, the pendulum would still be swinging but the amplitude would have decreased to 5 cm, half of its original amplitude. After another 10 minutes its amplitude would be only 2.5 cm, then 1.25 cm, etc. However it would never come to a complete rest, and we therefore call that response to the impulse (striking it with a hammer) \"infinite\" in duration.\n\nThe complexity of such a system is specified by its order \"N\". N is often a constraint on the design of a transfer function since it specifies the number of reactive components in an analog circuit; in a digital IIR filter the number of computations required is proportional to N.\n\nA filter implemented in a computer program (or a so-called digital signal processor) is a discrete-time system; a different (but parallel) set of mathematical concepts defines the behavior of such systems. Although a digital filter can be an IIR filter if the algorithm implementing it includes feedback, it is also possible to easily implement a filter whose impulse truly goes to zero after N time steps; this is called a finite impulse response (FIR) filter.\n\nFor instance, suppose one has a filter that, when presented with an impulse in a time series:\noutputs a series that responds to that impulse at time 0 until time 4, and has no further response, such as:\nAlthough the impulse response has lasted 4 time steps after the input, starting at time 5 it has truly gone to zero. The extent of the impulse response is \"finite\", and this would be classified as a fourth-order FIR filter.\nThe convolution integral (or summation) above need only extend to the full duration of the impulse response T, or the order N in a discrete time filter.\n\nClassical analog filters are IIR filters, and classical filter theory centers on the determination of transfer functions given by low order rational functions, which can be synthesized using the same small number of reactive components. Using digital computers, on the other hand, both FIR and IIR filters are straightforward to implement in software.\n\nA digital IIR filter can generally approximate a desired filter response using less computing power than a FIR filter, however this advantage is more often unneeded given the increasing power of digital processors. The ease of designing and characterizing FIR filters makes them preferable to the filter designer (programmer) when ample computing power is available. Another advantage of FIR filters is that their impulse response can be made symmetric, which implies a response in the frequency domain that has zero phase at all frequencies (not considering a finite delay), which is absolutely impossible with any IIR filter.\n\nThe frequency response or transfer function formula_2 of a filter can be obtained if the impulse response is known, or directly through analysis using Laplace transforms, or in discrete-time systems the Z-transform. The frequency response also includes the phase as a function of frequency, however in many cases the phase response is of little or no interest. FIR filters can be made to have zero phase, but with IIR filters that is generally impossible. With most IIR transfer functions there are related transfer functions having a frequency response with the same magnitude but a different phase; in most cases the so-called minimum phase transfer function is preferred.\n\nFilters in the time domain are most often requested to follow a specified frequency response. Then, a mathematical procedure finds a filter transfer function that can be realized (within some constraints), and approximates the desired response to within some criterion. Common filter response specifications are described as follows:\n\n\nMeeting a frequency response requirement with an FIR filter uses relatively straightforward procedures. In the most basic form, the desired frequency response itself can be sampled with a resolution of formula_6 and Fourier transformed to the time domain. This obtains the filter coefficients \"h\", which implements a zero phase FIR filter that matches the frequency response at the sampled frequencies used. To better match a desired response, formula_6 must be reduced. However the duration of the filter's impulse response, and the number of terms that must be summed for each output value (according to the above discrete time convolution) is given by formula_8 where \"T\" is the sampling period of the discrete time system (N-1 is also termed the \"order\" of an FIR filter). Thus the complexity of a digital filter and the computing time involved, grows inversely with formula_6, placing a higher cost on filter functions that better approximate the desired behavior. For the same reason, filter functions whose critical response is at lower frequencies (compared to the sampling frequency \"1/T\") require a higher order, more computationally intensive FIR filter. An IIR filter can thus be much more efficient in such cases.\n\nElsewhere the reader may find further discussion of design methods for practical FIR filter design.\n\nSince classical analog filters are IIR filters, there has been a long history of studying the range of possible transfer functions implementing various of the above desired filter responses in continuous time systems. Using transforms it is possible to convert these continuous time frequency responses to ones that are implemented in discrete time, for use in digital IIR filters. The complexity of any such filter is given by the \"order\" N, which describes the order of the rational function describing the frequency response. The order N is of particular importance in analog filters, because an N order electronic filter requires N reactive elements (capacitors and/or inductors) to implement. If a filter is implemented using, for instance, biquad stages using op-amps, N/2 stages are needed. In a digital implementation, the number of computations performed per sample is proportional to N. Thus the mathematical problem is to obtain the best approximation (in some sense) to the desired response using a smaller N, as we shall now illustrate.\n\nBelow are the frequency responses of several standard filter functions that approximate a desired response, optimized according to some criterion. These are all fifth-order low-pass filters, designed for a cutoff frequency of .5 in normalized units. Frequency responses are shown for the Butterworth, Chebyshev, inverse Chebyshev, and elliptic filters.\n\nAs is clear from the image, the elliptic filter is sharper than the others, but at the expense of ripples in both its passband and stopband. The Butterworth filter has the poorest transition but has a more even response, avoiding ripples in either the passband or stopband. A Bessel filter (not shown) has an even poorer transition in the frequency domain, but maintains the best phase fidelity of a waveform. Different applications emphasize different design requirements, leading to different choices among these (and other) optimizations, or requiring a filter of a higher order.\n\nA popular circuit implementing a second order active R-C filter is the Sallen-Key design, whose schematic diagram is shown here. This topology can be adapted to produce low-pass, band-pass, and high pass filters.\n\nAn N order FIR filter can be implemented in a discrete time system using a computer program or specialized hardware in which the input signal is subject to N delay stages. The output of the filter is formed as the weighted sum of those delayed signals, as is depicted in the accompanying signal flow diagram. The response of the filter depends on the weighting coefficients denoted \"b\", \"b\", ... \"b\". For instance, if all of the coefficients were equal to unity, a so-called boxcar function, then it would implement a low-pass filter with a low frequency gain of N+1 and a frequency response given by the sinc function. Superior shapes for the frequency response can be obtained using coefficients derived from a more sophisticated design procedure.\n\nLTI system theory describes linear \"time-invariant\" (LTI) filters of all types. LTI filters can be completely described by their frequency response and phase response, the specification of which uniquely defines their impulse response, and \"vice versa\". From a mathematical viewpoint, continuous-time IIR LTI filters may be described in terms of linear differential equations, and their impulse responses considered as Green's functions of the equation. Continuous-time LTI filters may also be described in terms of the Laplace transform of their impulse response, which allows all of the characteristics of the filter to be analyzed by considering the pattern of poles and zeros of their Laplace transform in the complex plane. Similarly, discrete-time LTI filters may be analyzed via the Z-transform of their impulse response.\n\nBefore the advent of computer filter synthesis tools, graphical tools such as Bode plots and Nyquist plots were extensively used as design tools. Even today, they are invaluable tools to understanding filter behavior. Reference books had extensive plots of frequency response, phase response, group delay, and impulse response for various types of filters, of various orders. They also contained tables of values showing how to implement such filters as RLC ladders - very useful when amplifying elements were expensive compared to passive components. Such a ladder can also be designed to have minimal sensitivity to component variation a property hard to evaluate without computer tools.\n\nMany different analog filter designs have been developed, each trying to optimise some feature of the system response. For practical filters, a custom design is sometimes desirable, that can offer the best tradeoff between different design criteria, which may include component count and cost, as well as filter response characteristics.\n\nThese descriptions refer to the \"mathematical\" properties of the filter (that is, the frequency and phase response). These can be \"implemented\" as analog circuits (for instance, using a Sallen Key filter topology, a type of active filter), or as algorithms in digital signal processing systems.\n\nDigital filters are much more flexible to synthesize and use than analog filters, where the constraints of the design permits their use. Notably, there is no need to consider component tolerances, and very high Q levels may be obtained.\n\nFIR digital filters may be implemented by the direct convolution of the desired impulse response with the input signal.\nThey can easily be designed to give a matched filter for any arbitrary pulse shape.\n\nIIR digital filters are often more difficult to design, due to problems including dynamic range issues, quantization noise and instability.\nTypically digital IIR filters are designed as a series of digital biquad filters.\n\nAll low-pass second-order continuous-time filters have a transfer function given by\n\nAll band-pass second-order continuous-time have a transfer function given by\n\nwhere\n\n\n", "id": "9975", "title": "Linear filter"}
{"url": "https://en.wikipedia.org/wiki?curid=9976", "text": "Ergative case\n\nThe ergative case (abbreviated ) is the grammatical case that identifies the noun as a subject of a transitive verb in ergative–absolutive languages.\n\nIn such languages, the ergative case is typically marked (most salient), while the absolutive case is unmarked. New work in case theory has vigorously supported the idea that the ergative case identifies the agent (the intentful performer of an action) of a verb (Woolford 2004).\n\nIn Kalaallisut (Greenlandic) for example, the ergative case is used to mark subjects of transitive verbs and possessors of nouns.\n\nNez Perce has a three-way nominal case system with both ergative (\"-nim\") and accusative (\"-ne\") plus an absolute (unmarked) case for intransitive subjects: \"hipáayna qíiwn\" ‘the old man arrived’; \"hipáayna wewúkiye\" ‘the elk arrived’; \"wewúkiyene péexne qíiwnim\" ‘the old man saw an elk’.\n\nSahaptin has an ergative noun case (with suffix \"-nɨm\") that is limited to transitive constructions only when the direct object is 1st or 2nd person: \"iwapáatayaaš łmámanɨm\" ‘the old woman helped me’; \"paanáy iwapáataya łmáma\" ‘the old woman helped him/her’ (direct); \"páwapaataya łmámayin\" ‘the old woman helped him/her’ (inverse).\n\nOther languages that use the ergative case are Georgian, Chechen, and other Caucasian languages, Mayan languages, Mixe–Zoque languages, Wagiman and other Australian Aboriginal languages as well as Basque, Burushaski, Yaghnobi and Tibetan. Among all Indo-European languages only Zazaki, Kurdish language varieties (including Kurmanji and Sorani), and Hindi/Urdu along with most Indo-Aryan languages, are ergative.\n\nThe ergative case is also a feature of some constructed languages such as Na'vi, Ithkuil and Black Speech.\n\n\n", "id": "9976", "title": "Ergative case"}
{"url": "https://en.wikipedia.org/wiki?curid=9977", "text": "Ewe\n\n\n\n\n\n", "id": "9977", "title": "Ewe"}
{"url": "https://en.wikipedia.org/wiki?curid=9978", "text": "Essenes\n\nThe Essenes (in Modern Hebrew: , \"Isiyim\"; Greek: Ἐσσηνοί, Ἐσσαῖοι, or Ὀσσαῖοι, \"Essenoi, Essaioi, Ossaioi\") were a sect of Second Temple Judaism that flourished from the 2nd century BCE to the 1st century CE which some scholars claim seceded from the Zadokite priests. Being much fewer in number than the Pharisees and the Sadducees (the other two major sects at the time), the Essenes lived in various cities but congregated in communal life dedicated to asceticism (some groups practised celibacy), voluntary poverty, and daily immersion. Many separate but related religious groups of that era shared similar mystic, eschatological, messianic, and ascetic beliefs. These groups are collectively referred to by various scholars as the \"Essenes.\" Josephus records that Essenes existed in large numbers, and thousands lived throughout Roman Judaea.\n\nThe Essenes have gained fame in modern times as a result of the discovery of an extensive group of religious documents known as the Dead Sea Scrolls, which are commonly believed to be the Essenes' library—although not conclusive. These documents preserve multiple copies of parts of the Hebrew Bible untouched from possibly as early as 300 BCE until their discovery in 1946. Some scholars dispute the notion that the Essenes wrote the Dead Sea Scrolls. Rachel Elior questions even the existence of the Essenes.\n\nThe first reference is by the Roman writer Pliny the Elder (died ) in his \"Natural History\". Pliny relates in a few lines that the Essenes do not marry, possess no money, and had existed for thousands of generations. Unlike Philo, who did not mention any particular geographical location of the Essenes other than the whole land of Israel, Pliny places them in Ein Gedi, next to the Dead Sea.\n\nA little later, Josephus gave a detailed account of the Essenes in \"The Jewish War\" (), with a shorter description in \"Antiquities of the Jews\" () and \"The Life of Flavius Josephus\" (). Claiming first hand knowledge, he lists the \"Essenoi\" as one of the three sects of Jewish philosophy alongside the Pharisees and the Sadducees. He relates the same information concerning piety, celibacy, the absence of personal property and of money, the belief in communality, and commitment to a strict observance of Sabbath. He further adds that the Essenes ritually immersed in water every morning, ate together after prayer, devoted themselves to charity and benevolence, forbade the expression of anger, studied the books of the elders, preserved secrets, and were very mindful of the names of the angels kept in their sacred writings.\n\nPliny, also a geographer, located them in the desert near the northwestern shore of the Dead Sea, where the Dead Sea Scrolls were discovered.\nJosephus uses the name \"Essenes\" in his two main accounts but some manuscripts read here \"Essaion\"; \"holding the Essenes in honour\"; \"a certain Essene named Manaemus\"; \"to hold all Essenes in honor\"; \"the Essenes\").\n\nIn several places, however, Josephus has \"Essaios\", which is usually assumed to mean \"Essene\" (\"Judas of the \"Essaios\" race\"; \"Simon of the \"Essaios\" race\"; \"John the \"Essaios\"\"; \"those who are called by us \"Essaioi\"\"; \"Simon a man of the \"Essaios\" race\"). Josephus identified the Essenes as one of the three major Jewish sects of that period.\n\nPhilo's usage is \"Essaioi\", although he admits this Greek form of the original name that according to his etymology signifies \"holiness\" to be inexact. Pliny's Latin text has \"Esseni\".\n\nGabriele Boccaccini implies that a convincing etymology for the name Essene has not been found, but that the term applies to a larger group within Palestine that also included the Qumran community.\n\nIt was proposed before the Dead Sea Scrolls were discovered that the name came into several Greek spellings from a Hebrew self-designation later found in some Dead Sea Scrolls, \"'osey hatorah\", \"observers of torah\". Although dozens of etymology suggestions have been published, this is the only etymology published before 1947 that was confirmed by Qumran text self-designation references, and it is gaining acceptance among scholars. It is recognized as the etymology of the form \"Ossaioi\" (and note that Philo also offered an O spelling) and \"Essaioi\" and \"Esseni\" spelling variations have been discussed by VanderKam, Goranson, and others. In medieval Hebrew (e.g. Sefer Yosippon) \"Hassidim\" (\"the pious ones\") replaces \"Essenes\". While this Hebrew name is not the etymology of \"Essaioi\"/\"Esseni\", the Aramaic equivalent \"Hesi'im\" known from Eastern Aramaic texts has been suggested. Others suggest that Essene is a transliteration of the Hebrew word \"chitzonim\" (chitzon=outside), which the Mishna (e.g. Megila 4:8) uses to describe various sectarian groups. Another theory is that the name was borrowed from a cult of devotees to Artemis in Asia Minor, whose demeanor and dress somewhat resembled those of the group in Judaea.\n\nFlavius Josephus in Chapter 8 of \"The Jewish War\" states:\n\nAccording to Josephus, the Essenes had settled \"not in one city\" but \"in large numbers in every town\". Philo speaks of \"more than four thousand\" \"Essaioi\" living in \"Palestine and Syria\", more precisely, \"in many cities of Judaea and in many villages and grouped in great societies of many members\".\n\nPliny locates them \"on the west side of the Dead Sea, away from the coast... [above] the town of Engeda\".\n\nSome modern scholars and archaeologists have argued that Essenes inhabited the settlement at Qumran, a plateau in the Judean Desert along the Dead Sea, citing Pliny the Elder in support, and giving credence that the Dead Sea Scrolls are the product of the Essenes. This theory, though not yet conclusively proven, has come to dominate the scholarly discussion and public perception of the Essenes.\n\nThe accounts by Josephus and Philo show that the Essenes led a strictly communal lifeoften compared to later Christian monasticism. Many of the Essene groups appear to have been celibate, but Josephus speaks also of another \"\"order\" of Essenes\" that observed the practice of being engaged for three years and then becoming married. According to Josephus, they had customs and observances such as collective ownership, electing a leader to attend to the interests of the group, and obedience to the orders from their leader. Also, they were forbidden from swearing oaths and from sacrificing animals. They controlled their tempers and served as channels of peace, carrying weapons only for protection against robbers. The Essenes chose not to possess slaves but served each other and, as a result of communal ownership, did not engage in trading. Josephus and Philo provide lengthy accounts of their communal meetings, meals and religious celebrations.\n\nAfter a total of three years' probation, newly joining members would take an oath that included the commitment to practice piety towards \"the Deity\" (το θειον) and righteousness towards humanity, to maintain a pure lifestyle, to abstain from criminal and immoral activities, to transmit their rules uncorrupted and to preserve the books of the Essenes and the names of the Angels. Their theology included belief in the immortality of the soul and that they would receive their souls back after death. Part of their activities included purification by water rituals, which was supported by rainwater catchment and storage.\n\nRitual purification was a common practice among the peoples of Judea during this period and was thus not specific to the Essenes. Ritual baths are found near many Synagogues of the period. Purity and cleanliness was considered so important to the Essenes that they would refrain from defecation on the Sabbath.\n\nAccording to Joseph Lightfoot the Church Father Epiphanius (writing in the 4th century CE) seems to make a distinction between two main groups within the Essenes: \"Of those that came before his [Elxai, an Ossaean prophet] time and during it, the Ossaeans and the Nazarean.\" Epiphanius describes each group as following:\n\nIf it is correct to identify the community at Qumran with the Essenes (and claim that the community at Qumran are the authors of the Dead Sea Scrolls), then according to the Dead Sea Scrolls the Essenes' community school was called \"Yahad\" (meaning \"community\") in order to differentiate themselves from the rest of the Jews who are repeatedly labeled \"The Breakers of the Covenant\".\n\nJosephus and Philo discuss the Essenes in detail. Most scholars believe that the community at Qumran that allegedly produced the Dead Sea Scrolls was an offshoot of the Essenes; however, this theory has been disputed by some, for example, Norman Golb argues that the primary research on the Qumran documents and ruins (by Father Roland de Vaux, from the \"École Biblique et Archéologique de Jérusalem\") lacked scientific method, and drew wrong conclusions that comfortably entered the academic canon. For Golb, the amount of documents is too extensive and includes many different writing styles and calligraphies; the ruins seem to have been a fortress, used as a military base for a very long period of timeincluding the 1st centuryso they could not have been inhabited by the Essenes; and the large graveyard excavated in 1870, just 50 metres east of the Qumran ruins was made of over 1200 tombs that included many women and childrenPliny clearly wrote that the Essenes who lived near the Dead Sea \"had not one woman, had renounced all pleasure ... and no one was born in their race\". Golb's book presents observations about de Vaux's premature conclusions and their uncontroverted acceptance by the general academic community. He states that the documents probably stemmed from various libraries in Jerusalem, kept safe in the desert from the Roman invasions. Other scholars refute these arguments—particularly since Josephus describes some Essenes as allowing marriage.\n\nAnother issue is the relationship between the \"Essaioi\" and Philo's \"Therapeutae\" and \"Therapeutrides\". He regarded the \"Therapeutae\" as a contemplative branch of the \"Essaioi\" who, he said, pursued an active life.\n\nOne theory on the formation of the Essenes suggests that the movement was founded by a Jewish high priest, dubbed by the Essenes the Teacher of Righteousness, whose office had been usurped by Jonathan (of priestly but not of Zadokite lineage), labeled the \"man of lies\" or \"false priest\". Others follow this line and a few argue that the Teacher of Righteousness was not only the leader of the Essenes at Qumran, but was also identical to the original Jesus [Essa] about 150 years before the time of the Gospels. Fred Gladstone Bratton notes that\n\nThe Saint Thomas Christians (\"Nasrani\") of southwestern India may have connections with the Essenes, according to the Manimekalai, one of the great Tamil epic poems, which refers to a people called \"Issani\".\n\nAccording to a Jewish legend, one of the Essenes, named Menachem, had passed at least some of his mystical knowledge to the Talmudic mystic Nehunya ben HaKanah, to whom the Kabbalistic tradition attributes Sefer HaBahir and, by some opinions, Sefer HaKanah, Sefer HaPeliah, and Sefer HaTemunah. Some Essene rituals, such as daily immersion in the mikveh, coincide with contemporary Hasidic practices; some historians have also suggested that the name \"Essene\" is a Hellenized form of the word \"Hasidim\" or \"Hasid\" (\"pious ones\"). However, the legendary connections between Essene and Kabbalistic tradition are not verified by modern historians.\n\n\n\n", "id": "9978", "title": "Essenes"}
{"url": "https://en.wikipedia.org/wiki?curid=9979", "text": "Eyes Wide Shut\n\nEyes Wide Shut is a 1999 erotic drama film directed, produced, and co-written by Stanley Kubrick. Based on Arthur Schnitzler's 1926 novella \"Traumnovelle\" (\"Dream Story\"), the story is transferred from early 20th century Vienna to 1990s New York City. The film follows the sexually charged adventures of Dr. Bill Harford, who is shocked when his wife, Alice, reveals that she had contemplated an affair a year earlier. He embarks on a night-long adventure, during which he infiltrates a massive masked orgy of an unnamed secret society.\n\nKubrick obtained the filming rights for \"Dream Story\" in the 1960s, considering it a perfect text for a film adaptation about sexual relations. The project was only revived in the 1990s, when the director hired writer Frederic Raphael to help him with the adaptation. The film was mostly shot in the United Kingdom (aside from some exterior establishing shots), and included a detailed recreation of some exterior Greenwich Village street scenes at Pinewood Studios. The film spent a long time in production, and holds the Guinness World Record for the longest continuous film shoot at 400 days.\n\n\"Eyes Wide Shut\" was Kubrick's last film, as he died four days after showing his final cut to Warner Bros. To ensure a theatrical R rating in the United States, Warner Bros. digitally altered several sexually explicit scenes during post-production. This version was released on July 16, 1999 to moderately positive reactions from critics; worldwide takings at the box office amounted to $162 million. The uncut version has since been released in DVD, HD DVD, and Blu-ray Disc formats.\n\nAs with many of Kubrick's works, reception to \"Eyes Wide Shut\" has become more favorable in the years since the film's initial release.\n\nDr. Bill Harford and his wife, Alice, are a young couple living in New York who attend a Christmas party thrown by a wealthy patient, Victor Ziegler. Bill is reunited with Nick Nightingale, a medical school drop-out who now plays piano professionally. While a Hungarian man named Sandor Szavost tries to seduce Alice, two young models try to take Bill off. He is interrupted by a call from his host upstairs, who had been having sex with Mandy, a young woman who overdosed on a speedball. Mandy recovers with Bill's aid.\n\nThe next evening at home, while smoking cannabis, Alice asks him if he had sex with the two girls. After Bill reassures her, she asks if he is ever jealous of men who are attracted to her. As the discussion gets heated, he states that he thinks women are more faithful than men. She rebuts him, telling him of a recent fantasy she had about a naval officer they had encountered on a vacation. Disturbed by Alice's revelation, Bill is then called by the daughter of a patient who has just died. After visiting the home, he meets a prostitute named Domino and goes to her apartment. Alice phones as Domino begins to kiss Bill, after which he calls off the awkward encounter. Meeting Nick at the jazz club Bill learns that Nick has an engagement where he must play piano blindfolded. Bill learns that to gain admittance, one needs a costume, a mask, and the password, which Nick had written down. Bill goes to a costume shop and offers the owner, Mr. Milich, a generous amount of money to rent a costume. In the shop, Milich catches his teenage daughter with two Japanese men and expresses outrage at their lack of a sense of decency.\n\nBill takes a taxi to the country mansion mentioned by Nick. He gives the password and discovers a sexual ritual is taking place. A woman warns him he is in terrible danger. A porter then takes him to the ritual room, where a disguised red-cloaked Master of Ceremonies confronts Bill with a question about a second password. Bill says he has forgotten it. The masked woman who had tried to warn Bill intervenes and insists that she will redeem him. Bill is ushered from the mansion and warned not to tell anyone about what happened there.\n\nJust before dawn, Bill arrives home guilty and confused. He finds Alice laughing loudly in her sleep and awakens her. While crying, she tells him of a troubling dream in which she was having sex with the naval officer and many other men, and laughing at the idea of Bill seeing her with them. The next morning, Bill goes to Nick's hotel, where the desk clerk tells Bill that a bruised and frightened Nick checked out a few hours earlier after returning with two large, dangerous-looking men. Bill goes to return the costume, but not the mask, which he has misplaced, and learns Milich has sold his daughter into prostitution. \n\nBill returns to the country mansion in his own car and is met at the gate by a man with a note warning him to cease and desist his inquiries. After reading a newspaper story about a beauty queen who died of a drug overdose, Bill views the body at the morgue and identifies it as Mandy. Bill is summoned to Ziegler's house, where he is confronted with the events of the past night and day. Ziegler was one of those involved with the ritual orgy, and identified Bill and his connection with Nick. Ziegler claims that he had Bill followed for his own protection, and that the warnings made against Bill by the society are only intended to scare him from speaking about the orgy. However, he implies the society is capable of acting on their threats. Bill asks about the death of Mandy, whom Ziegler has identified as the masked woman at the party who'd \"sacrificed\" herself to prevent Bill's punishment, and about the disappearance of Nick, the piano player. Ziegler insists that Nick is safely back at his home in Seattle. Ziegler also says the \"punishment\" was a charade by the secret society to further frighten Bill, and it had nothing to do with Mandy's death; she was a hooker and addict and had died from another accidental drug overdose. Bill does not know if Ziegler is telling him the truth about Nick's disappearance or Mandy's death, but he says nothing further. When he returns home, Bill finds the rented mask on his pillow next to his sleeping wife. He breaks down in tears and decides to tell Alice the whole truth of the past two days. The next morning, they go Christmas shopping with their daughter. Alice muses that they should be grateful they have survived, that she loves him, and there is something they must do as soon as possible. When Bill asks what it is, she simply says: \"Fuck\".\n\n\nWhile Stanley Kubrick was interested in making a film about sexual relations as early as 1962, during production of \"Dr. Strangelove\", the project only took off after he read Arthur Schnitzler's \"Dream Story\" in 1968, when he was seeking a work to follow on \"\". Kubrick got interested in adapting the story, and with the help of then-journalist Jay Cocks, bought the filming rights to the novel. In the 1970s, Kubrick had thought of Woody Allen as the Jewish protagonist. For the following decade, Kubrick even considered making his \"Dream Story\" adaptation a sex comedy \"with a wild and somber streak running through it\", starring Steve Martin in the main role. The project was only revived in 1994, when Kubrick hired Frederic Raphael to work on the script, updating the setting from early 20th century Vienna to late 20th century New York City. Kubrick invited Michael Herr, a personal friend who helped write \"Full Metal Jacket\", for revisions, but Herr declined for fear that he would both be underpaid and would commit to an overlong production.\n\nArthur Schnitzler's 1926 novella \"Dream Story\" is set around Vienna shortly after the turn of the century. The main characters are a couple named Fridolin and Albertina; their home is a typical suburban middle-class home, not the film's posh urban apartment. Schnitzler himself, like the protagonist of this novel, lived in Vienna, was Jewish, and a medical doctor, though Schnitzler eventually abandoned medicine for writing.\n\nWhile Fridolin and Albertina, the protagonist couple of \"Dream Story\", are sometimes implied to be Jewish, there is nothing in the novella which justifies this assumption, and neither Fridolin nor Albertina are typical Jewish names; whereas Nachtigall (Nightingale) is overtly identified as Jewish. Kubrick (himself of Jewish descent) frequently removed references to the Jewishness of characters in the novels he adapted. In the case of \"Eyes Wide Shut\", Frederic Raphael (who is also Jewish) wanted to keep the Jewish background of the protagonists, but Kubrick insisted that they should be \"vanilla\" Americans, without any details that would arouse any presumptions. The director added that Bill should be a \"Harrison Ford-ish goy\" (though Ford's mother was Jewish), and created the surname of Harford as an allusion to the actor. This is reflected in the way the film's Bill Harford is taunted by college students when going home in the morning. In the film, Bill is taunted with homophobic slurs. In the novella, these boys are recognized to be members of an anti-Semitic college fraternity. Kubrick's co-screenwriter, Frederic Raphael, in an introduction to a Penguin Classics edition of \"Dream Story\", writes \"Fridolin is not declared to be a Jew, but his feelings of cowardice, for failing to challenge his aggressor, echo the uneasiness of Austrian Jews in the face of Gentile provocation.\"\n\nThe novella is set during the Carnival, when people often wear masks to parties. The party that both husband and wife attend at the opening of the story is a Carnival Masquerade ball, whereas the film's story begins at Christmas time.\n\nCritic Randy Rasmussen suggests that the character of Bill is fundamentally more naïve, strait-laced, less disclosing and more unconscious of his vindictive motives than his counterpart, Fridolin. For Rasmussen and others, the film's Bill Harford is essentially sleep-walking through life with no deeper awareness of his surroundings. In the novella, when his wife discloses a private sexual fantasy, he in turn admits one of his own (of a girl in her mid to late teens), while in the film he is simply shocked. The film's argument over whether he has fantasies over female patients and whether women have sexual fantasies is simply absent from the novella, where both husband and wife assume the other has fantasies. In the film, Bill's estrangement from Alice revolves around her confessing a recent fantasy to him; in the novella, both exchange fantasies, after which she declares that in her youth she could have easily married someone else, which is what precipitates their sense of estrangement.\n\nIn the novella, the husband long suspected that his patient (Marion) was infatuated with him, while in the film it is a complete surprise and he seems shocked. He is also more overwhelmed by the orgy in the film than in the novella. Fridolin is socially bolder but less sexual with the prostitute (Mizzi in the novella, Domino in the film). Fridolin is also conscious of looking old in the novella, though he hardly does in the film.\n\nIn the novella, the party (which is sparsely attended) uses \"Denmark\" as the password for entrance; that is significant in that Albertina had her infatuation with her soldier in Denmark. The film's password is \"Fidelio\", from the Latin word for \"faithful\", and which is the title of Beethoven's only opera (\"Fidelio, or Married Love\"). In early drafts of the screenplay, the password was \"Fidelio Rainbow\". Jonathan Rosenbaum noted that both passwords echo elements of one member of the couple's behaviour, though in opposite ways. The party in the novella consists mostly of nude ballroom dancing.\n\nIn the novella, the woman who \"redeems\" Fridolin at the party, saving him from punishment, is costumed as a nun, and most of the characters at the party are dressed as nuns or monks; Fridolin himself used a monk costume. This aspect was retained in the film's original screenplay, but was deleted in the filmed version.\n\nIn the novella, when the husband returns home, the wife's dream is an elaborate drama that concludes with him getting crucified in a village square after Fridolin refuses to separate from Albertina and become the paramour of the village princess, even though Albertina is now occupied with copulating with other men, and watches him \"without pity\". By being faithful, Fridolin thus fails to save himself from execution in Albertina's dream, although he was apparently spared by the woman's \"sacrifice\" at the masked sex party. In both the novella and film, the wife states that the laugh in her sleep just before she woke was a laugh of scornful contempt for her husband; although awake, she states this matter-of-factly. The novella makes it clear that Fridolin at this point hates Albertina more than ever, thinking they are now lying together \"like mortal enemies\". It has been argued that the dramatic climax of the novella is actually Albertina's dream, and the film has shifted the focus to Bill's visit to the secret society's orgy, whose content is more shocking in the film.\n\nThe adaptation created a character with no counterpart in the novella: Ziegler, who represents both the high wealth and prestige to which Bill Harford aspires, and a connection between Bill's two worlds (his regular life, and the secret society organizing the ball). Critic Randy Rasmussen interprets Ziegler as representing Bill's worst self, much as in other Kubrick films; the title character in \"Dr. Strangelove\" represents the worst of the American national security establishment, Charles Grady represents the worst of Jack Torrance in \"The Shining\", and Clare Quilty represents the worst of Humbert Humbert in \"Lolita\".\n\nZiegler's presence allows Kubrick to change the mechanics of the story in a few ways. In the film, Bill first meets his piano-playing friend at Ziegler's party, and then while wandering around town, seeks him out at the Sonata Café. In the novella, the café encounter with Nightingale is a delightful coincidence. Similarly, the dead woman whom Bill suspects of being the woman at the party who saved him is a baroness that he was acquainted with earlier, not a hooker at Ziegler's party.\n\nMore significantly, in the film, Ziegler gives a commentary on the whole story to Bill, including an explanation that the party incident, where Bill is apprehended, threatened, and ultimately redeemed by the woman's sacrifice, was staged. Whether this is to be believed or not, it is an exposition of Ziegler's view of the ways of the world as a member of the power elite.\n\nThe novella explains why the husband's mask is on the pillow next to his sleeping wife, she having discovered it when it slipped out of his suitcase, and placing it there as a statement of understanding. This is left unexplained in the film and left to the viewer's interpretation.\n\nWhen Warner Bros. president Terry Semel approved production, he asked Kubrick to cast a movie star, as \"you haven't done that since Jack Nicholson [in \"The Shining\"].\" Cruise was in England because his wife Nicole Kidman was there shooting \"The Portrait of a Lady\", and eventually Cruise decided to visit Kubrick's estate with Kidman. After that meeting, the director awarded them the roles. Jennifer Jason Leigh and Harvey Keitel each were cast and filmed by Kubrick. Due to scheduling conflicts, both had to drop out – first Keitel with \"Finding Graceland\", then Leigh with \"eXistenZ\" – and they were replaced by Marie Richardson and Sydney Pollack in the final cut.\n\nPrincipal photography began in November 1996. Kubrick's perfectionism led to script pages being rewritten on the set, and most scenes requiring numerous takes. The shoot went much longer than expected, with Vinessa Shaw—playing the HIV-positive prostitute—being initially contracted for two weeks, but ending up working for two months. The crew got exhausted. Filming finally wrapped in June 1998. The \"Guinness World Records\" recognized \"Eyes Wide Shut\" as the longest constant movie shoot, \"for over 15 months, a period that included an unbroken shoot of 46 weeks\".\n\nGiven Kubrick's fear of flying, the entire film was shot in England. Sound-stage works were done at London's Pinewood Studios, which included a detailed recreation of Greenwich Village. Kubrick's perfectionism went as far as sending workmen to Manhattan to measure street widths and note newspaper vending machine locations. Real New York footage was also shot to be rear projected behind Cruise. Production was followed by a strong campaign of secrecy, helped by Kubrick always working with a short team on set. Outdoor locations included Hatton Garden for a Greenwich Village street, Hamleys for the toy store from the film's ending, and Mentmore Towers and Elveden Hall in Elveden, Suffolk, England for the mansion. Larry Smith, who had first served as a gaffer on both \"Barry Lyndon\" and \"The Shining\", was chosen by Kubrick to be the film's cinematographer. Kubrick refused to use studio lighting, forcing Smith to use the available light sources visible in the shot, such as lamps and Christmas tree lights. When this was not adequate, Smith used Chinese paper ball lamps to softly brighten the scene. The color was enhanced by push processing the film reels, which helped bring out the intensity of color.\n\nKubrick's perfectionism led him to oversee every visual element that would appear in a given frame, from props and furniture to the color of walls and other objects. One such element were the masks used in the orgy, which were inspired by the masked Carnival balls visited by the protagonists of the novel. Costume designer Marit Allen explained that Kubrick felt they fit in that scene for being part of the imaginary world, and ended up \"creat[ing] the impression of menace, but without exaggeration\". Many masks as used in the Venetian carnival were sent to London, and Kubrick separated who would wear each piece. The paintings of Kubrick's wife Christiane are featured on decoration.\n\nAfter shooting completed, Kubrick entered a prolonged post-production process. On March 1, 1999, Kubrick showed a cut to Cruise, Kidman, and the Warner Bros. executives. The director died six days later.\n\nJocelyn Pook wrote the original music for \"Eyes Wide Shut\", but like other Kubrick movies, the film was noted for its usage of classical music. The opening title music is Shostakovich's \"Suite for Variety Stage Orchestra\", misidentified as \"Waltz 2 from Jazz Suite\". One recurring piece is the second movement of György Ligeti's piano cycle \"Musica ricercata\". Kubrick originally intended to feature \"Im Treibhaus\" from Wagner's \"Wesendonck Lieder\", but the director eventually replaced it with Ligeti's tune feeling Wagner's song was \"too beautiful\". In the morgue scene, Franz Liszt's late solo piano piece, \"Nuages Gris\" (\"Grey Clouds\") (1881), is heard. \"Rex tremendae\" from Mozart's \"Requiem\" plays as Bill walks into the cafe and reads of Mandy's death.\n\nPook was hired after choreographer Yolande Snaith rehearsed the masked ball orgy scene using Pook's composition \"Backwards Priests\" – which features a Romanian Orthodox Divine Liturgy recorded in a church in Baia Mare, played backwards – as a reference track. Kubrick then called the composer and asked if she had anything else \"weird\" like that song, which was reworked for the final cut of the scene, with the title \"Masked Ball\". Pook ended up composing and recording four pieces of music, many times based on her previous work, totaling 24 minutes. The composer's work ended up having mostly string instruments – including a viola played by Pook herself – with no brass or woodwinds as Pook \"just couldn't justify these other textures\", particularly as she wanted the tracks played on dialogue-heavy scenes to be \"subliminal\" and felt such instruments would be intrusive.\n\nAnother track in the orgy, \"Migrations\", features a Tamil song sung by Manickam Yogeswaran, a Carnatic singer. The original cut had a scriptural recitation of the Bhagavad Gita, which Pook took from a previous Yogeswaran recording. As a result of Hindus protesting against their most sacred scripture being used in such a context, Warner Bros. issued a public apology, and hired the singer to record a similar track to replace the chant.\n\nThe party at Ziegler's house features rearrangements of love songs such as \"When I Fall in Love\" and \"It Had to Be You\", used in increasingly ironic ways considering how Alice and Bill flirt with other people in the scene. As Kidman was nervous about doing nude scenes, Kubrick stated she could bring music to liven up. When Kidman brought a Chris Isaak CD, Kubrick approved it, and incorporated Isaak's song \"Baby Did a Bad, Bad Thing\" to both an early romantic embrace of Bill and Alice and the film's trailer.\n\nThe film was described by some reviewers, and partially marketed, as an erotic thriller, a categorization disputed by others. It is classified as such in the book \"The Erotic Thriller in Contemporary Cinema\", by Linda Ruth Williams, and was described as such in news articles about Cruise and Kidman's lawsuit over assertions they saw a sex therapist during filming. The positive review in \"Combustible Celluloid\" describes it as an erotic thriller upon first viewing, but actually a \"complex story about marriage and sexuality\". High-Def Digest also called it an erotic thriller.\n\nHowever, reviewing the film at AboutFilm.com, Carlo Cavagna regards this as a misleading classification, as does Leo Goldsmith, writing at notcoming.com, and the review on Blu-ray.com. Writing in \"TV Guide\", Maitland McDonagh writes \"No one familiar with the cold precision of Kubrick's work will be surprised that this isn't the steamy erotic thriller a synopsis (or the ads) might suggest.\" Writing in general about the genre of 'erotic thriller' for CineAction in 2001, Douglas Keesey states that \"whatever [Eyes Wide Shut's] actual type, [it] was at least marketed as an erotic thriller\". Michael Koresky, writing in the 2006 issue of film journal \"Reverse Shot\", writes \"this director, who defies expectations at every turn and brings genre to his feet, was ... setting out to make neither the 'erotic thriller' that the press maintained nor an easily identifiable 'Kubrick film'\". \"DVD Talk\" similarly dissociates the film from this genre.\n\nIn addition to relocating the story from Vienna in the 1900s to New York City in the 1990s, Kubrick changed the time-frame of Schnitzler's story from Mardi Gras to Christmas. One critic believed Kubrick did this because of the rejuvenating symbolism of Christmas. Others have noted that Christmas lights allow Kubrick to employ some of his distinct methods of shooting including using source location lighting, as he did in \"Barry Lyndon\". The \"New York Times\" noted that the film \"gives an otherworldly radiance and personality to Christmas lights\", and critic Randy Rasmussen noted that \"colorful Christmas lights ... illuminate almost every location in the film.\" \"Harper\"'s film critic, Lee Siegel, believes the film's recurring motif is the Christmas tree, because it symbolizes the way that \"Compared with the everyday reality of sex and emotion, our fantasies of gratification are, yes, pompous and solemn in the extreme ... For desire is like Christmas: it always promises more than it delivers.\" Author Tim Kreider noted that the \"Satanic\" mansion-party at Somerton is the only set in the film without a Christmas tree, stating \"Almost every set is suffused with the dreamlike, hazy glow of colored lights and tinsel ... \"Eyes Wide Shut\", though it was released in summer, was \"the\" Christmas movie of 1999.\" Noting that Kubrick has shown viewers the dark side of Christmas consumerism, Louise Kaplan stated that the film illustrates ways that the \"material reality of money\" is shown replacing the spiritual values of Christmas, charity and compassion. While virtually every scene has a Christmas tree, there is \"no Christmas music or cheery Christmas spirit.\" Critic Alonso Duralde, in his book \"Have Yourself a Movie Little Christmas\", categorized the film as a \"Christmas movie for grownups\" (as he also did with Bergman's \"Fanny and Alexander\" and \"The Lion in Winter\"), arguing that \"Christmas weaves its way through the film from start to finish\".\n\nHistorians, travel guide authors, novelists, and merchants of Venetian masks have noted that these have a long history of being worn during promiscuous activities. Authors Tim Kreider and Thomas Nelson have linked the film's usage of these to Venice's reputation as a center of both eroticism and mercantilism. Nelson notes that the sex ritual combines elements of Venetian Carnival and Catholic rites. (In particular, the character of \"Red Cloak\" simultaneously serves as Grand Inquisitor and King of Carnival.) As such, Nelson argues the sex ritual is a symbolic mirror of the darker truth behind the façade of Victor Ziegler's earlier Christmas party. Writing in her 2007 book \"Symbols in Stanley Kubrick's Movie 'Eyes Wide Shut\"', Carolin Ruwe argues that the mask is the prime symbol of the film, the masks at Somerton mansion reflecting the masks that all wear in society, a point reinforced by Tim Kreider, who noted the many masks in the prostitute's apartment and her having been renamed in the film \"Domino\", which is a style of Venetian mask.\n\nThe statements of Todd Field's character, Nick Nightingale, that he had dropped out of medical school ten years earlier and now plays the piano, are a reference to the film \"Gross Anatomy\" in which Feld's character \"David Schreiner\" portrays a medical student who quits his education to pursue playing the piano. \"Gross Anatomy\" was released ten years before \"Eyes Wide Shut\" and featured Matthew Modine who had played \"Joker\" in Stanley Kubrick's 1987 film \"Full Metal Jacket\".\n\nWarner Bros. heavily promoted \"Eyes Wide Shut\", while following Kubrick's secrecy campaign – to the point the film's press kits contained no production notes – and also the director's suggestions to Semel regarding the marketing campaign, given one week prior to Kubrick's death. The first footage was shown to theater owners attending the 1999 edition of the ShoWest convention in Las Vegas. TV spots featured both Isaak and Ligeti's songs from the soundtrack, while revealing little about the movie's plot. The film also appeared on the cover of \"Time\" magazine, and on show business programs such as \"Entertainment Tonight\" and \"Access Hollywood\".\n\n\"Eyes Wide Shut\" opened on July 16, 1999, in the United States. The film topped the weekend box office, with $21.7 million from 2,411 screens. These numbers surpassed the studio's expectations of $20 million, and became both Cruise's sixth consecutive chart topper and Kubrick's highest opening weekend. Audiences attendance dropped from Friday to Saturday, which analysts attributed to the news coverage of John F. Kennedy, Jr.'s disappearance. \"Eyes Wide Shut\" ended up grossing a total of $55,691,208 in the US. The numbers put it as Kubrick's second most successful film in the country, behind \"2001: A Space Odyssey\", but were considered a box office disappointment.\n\nShortly after its screening at the Venice Film Festival, \"Eyes Wide Shut\" had a British premiere on September 3, 1999, at the Warner Village cinema in Leicester Square. The film's wide opening occurred the following weekend, and topped the UK charts with £1,189,672. It remained atop the charts the following weekend, and finished its box office run with £5,065,520.\n\nThe international performances for \"Eyes Wide Shut\" were more positive, with Kubrick's long-time assistant and brother-in-law Jan Harlan stating that \"It was badly received in the Anglo-Saxon world, but it was very well received in the Latin world and Japan. In Italy, it was a huge hit.\" Overseas earnings of over $105 million led to a $162,091,208 box office run worldwide, turning it into the highest-grossing Kubrick film.\n\n\"Eyes Wide Shut\" received positive reviews from critics, and currently has a \"Certified Fresh\" score of 74% on Rotten Tomatoes, based on 146 reviews with an average rating of 7.5 out of 10. The critical consensus states \"Kubrick's intense study of the human psyche yields an impressive cinematic work.\" The film also has a score of 68 out of 100 on Metacritic, based on 33 critics, indicating \"Generally favorable reviews\". Over 50 critics listed the film among the best of 1999.\n\nIn the \"Chicago Tribune\", Michael Wilmington declared the film a masterpiece, lauding it as \"provocatively conceived, gorgeously shot and masterfully executed [...] Kubrick's brilliantly choreographed one-take scenes create a near-hypnotic atmosphere of commingled desire and dread.\" Nathan Rabin of \"The A.V. Club\" was also highly positive, arguing that \"the film's primal, almost religious intensity and power is primarily derived from its multifaceted realization that disobeying the dictates of society and your conscience can be both terrifying and exhilarating. [...] The film's depiction of sexual depravity and amorality could easily venture into the realm of camp in the hands of a lesser filmmaker, but Kubrick depicts primal evil in a way that somehow makes it seem both new and deeply terrifying.\"\n\nReviewer James Berardinelli stated that it was arguably one of Kubrick's best films. Along with considering Kidman \"consistently excellent\", he wrote that Kubrick \"has something to say about the causes and effects of depersonalized sex\", and praised the work as \"thought-provoking and unsettling.\" Writing for \"The New York Times\", reviewer Janet Maslin commented, \"This is a dead-serious film about sexual yearnings, one that flirts with ridicule yet sustains its fundamental eeriness and gravity throughout. The dreamlike intensity of previous Kubrick visions is in full force here.\" Writing about erotic mystery thrillers, writer Leigh Lundin comments that watching the dissolving marriage was painful and the backdrop of Christmas against the dark topic was disturbing, but \"the oblique, well-told plot rewards an attentive viewer\".\n\nSome reviewers were unfavorable. One complaint was that the movie's pacing was too slow; while this may have been intended to convey a dream state, critics objected that it made actions and decisions seem labored. Another complaint was that since Kubrick shot the scenes set in New York City on a sound stage, the result was that they felt inauthentic. In \"The Washington Post\", Stephen Hunter wrote, \"Kubrick is annoyingly offhand while at the same time grindingly pedantic; plot points are made over and over again, things are explained till the dawn threatens to break in the east, and the movie stumbles along at a glacial pace.\" David Edelstein of \"Slate\" dismissed it as \"estranged from any period I recognize. Who are these people played by Cruise and Kidman, who act as if no one has ever made a pass at them and are so deeply traumatized by their newfound knowledge of sexual fantasies--the kind that mainstream culture absorbed at least half a century ago? [...] Who are these aristocrats whose limos take them to secret masked orgies in Long Island mansions? Even dream plays need some grounding in the real world.\" J. Hoberman wrote that the film \"feels like a rough draft at best.\"\n\nLee Siegel from \"Harper's\" felt that most critics responded mainly to the marketing campaign and did not address the film on its own terms. Others felt that American censorship took an esoteric film and made it even harder to understand.\n\nOn the television show \"Roger Ebert & the Movies\", director Martin Scorsese named \"Eyes Wide Shut\" his fourth-favorite film of the 1990s. For the introduction to Michel Ciment's \"Kubrick: The Definitive Edition\", Scorsese wrote: \"When \"Eyes Wide Shut\" came out a few months after Stanley Kubrick's death in 1999, it was severely misunderstood, which came as no surprise. If you go back and look at the contemporary reactions to any Kubrick picture (except the earliest ones), you'll see that all his films were initially misunderstood. Then, after five or ten years came the realization that \"\" or \"Barry Lyndon\" or \"The Shining\" was like nothing else before or since.\" Mystery writer and commentator Jon Breen agreed. In 2012, \"Slant Magazine\" ranked the film as the second greatest of the 1990s. The BBC listed it number 61 in its list of the 100 greatest American films of all time.\n\nThe film is recognized by American Film Institute in these lists:\n\n\"Eyes Wide Shut\" was first released in VHS and DVD on March 7, 2000. The original DVD release corrects technical gaffes, including a reflected crew member, and altering a piece of Alice Harford's dialogue. Most home videos remove the verse that was claimed to be cited from the sacred Hindu scripture Bhagavad Gita (although it was Pook's reworking of \"Backwards Priests\" as stated above.)\n\nOn October 23, 2007, Warner Home Video released \"Eyes Wide Shut\" in a special edition DVD, plus the HD DVD and Blu-ray Disc formats. This is the first home video release that presents the film in anamorphic 1.78:1 (16:9) format (note that the film was shown theatrically as soft matted 1.66:1 in Europe and 1.85:1 in the USA and Japan). The previous DVD release used a 1.33:1 (4:3) aspect ratio. It is also the first American home video release to feature the uncut version. Although the earliest American DVD of the uncut version states on the cover that it includes both the R-rated and unrated editions, in actuality only the unrated edition is on the DVD.\n\nThough Warner Bros. insisted that Kubrick had turned in his final cut before his death, the film was still in the final stages of post-production, which was therefore completed by the studio in collaboration with Kubrick's estate. Some have argued that the work which remained was minor and exclusively technical in nature, allowing the estate to faithfully complete the film based on the director's notes. However, decisions regarding sound mixing, scoring and color-correction would have necessarily been made without Kubrick's input. Furthermore, Kubrick had a history of continuing to edit his films up until the last minute, and in some cases even after initial public screenings, as had been the case with \"2001: A Space Odyssey\" and \"The Shining\".\n\nWriting for Vanity Fair, Kubrick collaborator Michael Herr recalled a phone call from the director regarding the cut that would be screened for the Warner Bros. executives four days before his death:\n\nGarrett Brown, inventor of the Steadicam, has expressed that he considers \"Eyes Wide Shut\" to be an unfinished film:\n\nJan Harlan, Kubrick's brother-in-law and executive producer, reported that Kubrick was \"very happy\" with the film and considered it to be his \"greatest contribution to the art of cinema\".\n\nR. Lee Ermey, an actor in Kubrick's film \"Full Metal Jacket\", claimed that Kubrick phoned him two weeks before his death to express his despondency over \"Eyes Wide Shut\". \"He told me it was a piece of shit\", Ermey said in \"Radar\" magazine, \"and that he was disgusted with it and that the critics were going to 'have him for lunch'. He said Cruise and Kidman had their way with him – exactly the words he used.\"\n\nAccording to Todd Field, Kubrick's friend and an actor in \"Eyes Wide Shut\", Ermey's claims do not accurately reflect Kubrick's essential attitude. Field's response appeared in a 26 October 2006 interview with Slashfilm.com:\n\nThe polite thing would be to say 'No comment'. But the truth is that ... let's put it this way, you've never seen two actors more completely subservient and prostrate themselves at the feet of a director. Stanley was absolutely thrilled with the film. He was still working on the film when he died. And he probably died because he finally relaxed. It was one of the happiest weekends of his life, right before he died, after he had shown the first cut to Terry, Tom and Nicole. He would have kept working on it, like he did on all of his films. But I know that from people around him personally, my partner who was his assistant for thirty years. And I thought about R. Lee Ermey for \"In the Bedroom\". And I talked to Stanley a lot about that film, and all I can say is Stanley was adamant that I shouldn't work with him for all kinds of reasons that I won't get into because there is no reason to do that to anyone, even if they are saying slanderous things that I know are completely untrue.\n\nIn a reddit \"Ask Me Anything\" session, Stanley Kubrick's daughter, Katharina Kubrick, claimed that her father was very proud of the film. She also discredited Ermey's claims, saying to a user who asked about Kubrick's alleged comments, \"[not to] believe that for a second.\"\n\nCiting contractual obligations to deliver an R rating, Warner Bros. digitally altered the orgy for the American release, blocking out graphic sexuality by inserting additional figures to obscure the view, avoiding an adults-only NC-17 rating that limited distribution, as some large American theaters and video store operators disallow films with that rating. This alteration antagonized film critics and cinephiles, as they argued that Kubrick had never been shy about ratings (\"A Clockwork Orange\" was originally given an X-rating). The unrated version of \"Eyes Wide Shut\" was released in the United States on October 23, 2007, in DVD, HD DVD, and Blu-ray Disc formats.\n\nThe version in South America, Europe and Australia featured the orgy scene intact (theatrical and DVD release) with ratings mostly for people of 18+. In New Zealand and in Europe, the uncensored version has been shown on television with some controversy. In Australia, it was broadcast on Network Ten with the alterations in the American version for an MA rating, blurring and cutting explicit sexuality.\n\nRoger Ebert objected to the technique of using digital images to mask the action. He said it \"should not have been done at all\" and it is \"symbolic of the moral hypocrisy of the rating system that it would force a great director to compromise his vision, while by the same process making his adult film more accessible to young viewers.\" Although Ebert has been frequently cited as calling the standard North American R-rated version the \"Austin Powers\" version of \"Eyes Wide Shut\" – referencing two scenes in \"\" in which, through camera angles and coincidences, sexual body parts are blocked from view in a comical way – his review stated that this joke referred to an early rough draft of the altered scene, never publicly released.\n\nNotes\nBibliography\n\n", "id": "9979", "title": "Eyes Wide Shut"}
{"url": "https://en.wikipedia.org/wiki?curid=9986", "text": "Outline of education\n\nThe following outline is provided as an overview of and topical guide to education:\n\nEducation – in the general sense is any act or experience that has a formative effect on the mind, character, or physical ability of an individual. In its technical sense, education is the process by which society deliberately transmits its accumulated knowledge, skills, and values from one generation to another. Education can also be defined as the process of becoming an educated person.\n\nEducation\n\n\n\n\n\n\n\n\n\nHistory of education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "9986", "title": "Outline of education"}
{"url": "https://en.wikipedia.org/wiki?curid=9987", "text": "Outline of engineering\n\nThe following outline is provided as an overview of and topical guide to engineering:\n\nEngineering – discipline, art, skill and profession of acquiring and applying scientific, mathematical, economic, social, and practical knowledge, in order to design and build structures, machines, devices, systems, materials and processes that safely realize improvements to the lives of people.\n\n\n\n\n\n\n\n\n\n\n", "id": "9987", "title": "Outline of engineering"}
{"url": "https://en.wikipedia.org/wiki?curid=9988", "text": "Outline of entertainment\n\nThe following outline provides an overview of and topical guide to entertainment and the entertainment industry:\n\nEntertainment is any activity which provides a diversion or permits people to amuse themselves in their leisure time, and may also provide fun, enjoyment and laughter. People may create their own entertainment, such as when they spontaneously invent a game; participate actively in an activity they find entertaining, such as when they play sport as a hobby; or consume an entertainment product passively, such as when they attend a performance. \n\nThe entertainment industry (informally known as show business or show biz) is part of the tertiary sector of the economy and includes a large number of sub-industries devoted to entertainment. However, the term is often used in the mass media to describe the mass media companies that control the distribution and manufacture of mass media entertainment. In the popular parlance, the term \"show biz\" in particular connotes the commercially popular performing arts, especially musical theatre, vaudeville, comedy, film, and music. It applies to every aspect of entertainment including cinema, television, radio, theatre and music.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "9988", "title": "Outline of entertainment"}
{"url": "https://en.wikipedia.org/wiki?curid=9992", "text": "List of contemporary ethnic groups\n\nThe following is a list of contemporary ethnic groups.\nThere has been constant debate over the classification of ethnic groups. Membership of an ethnic group tends to be associated with shared cultural heritage, ancestry, history, homeland, language or dialect, the term culture specifically including aspects such as religion, mythology and ritual, cuisine, dressing style, etc. \nBy the nature of the concept, ethnic groups tend to be divided into ethnic subgroups, which may themselves be or not be identified as independent ethnic groups depending on the source consulted.\n\nThe groups commonly identified as \"ethnic groups\" (as opposed to ethno-linguistic phyla, national groups, racial groups or similar). Smaller groups are often indigenous peoples.\n\n\n\n\n[[Category:Ethnic groups| ]]\n[[Category:Ethnic groups by region|*]]\n[[Category:Society-related lists|Ethnic groups]]\n[[Category:Lists of ethnic groups]]", "id": "9992", "title": "List of contemporary ethnic groups"}
{"url": "https://en.wikipedia.org/wiki?curid=9993", "text": "Edda\n\n\"Edda\" (; Old Norse \"Edda\", plural \"Eddur\") is an Old Norse term that has been attributed by modern scholars to the collective of two Medieval Icelandic literary works: what is now known as the \"Prose Edda\" and an older collection of poems without an original title now known as the \"Poetic Edda\". The term historically referred only to the \"Prose Edda\", but this sense has fallen out of use because of the confusion with the other work. Both works were written down in Iceland during the 13th century in Icelandic, although they contain material from earlier traditional sources, reaching into the Viking Age. The books are the main sources of medieval skaldic tradition in Iceland and Norse mythology.\n\nThere are several theories concerning the origins of the word \"edda\". One theory holds that it is identical to a word that means \"great-grandmother\" appearing in the Eddic poem \"Rígsþula.\" Another theory holds that \"edda\" derives from Old Norse \"óðr\", \"poetry\". A third, proposed in 1895 by Eiríkr Magnússon, is that it derives from the Icelandic place name \"Oddi\", site of the church and school where students, including Snorri Sturluson, were educated. A fourth hypothesis—the derivation of the word \"Edda\" as the name of Snorri Sturluson’s treatise on poetry from the Latin \"edo\", \"I compose (poetry)\", by analogy with \"kredda\", \"superstition\", from Latin \"credo\", \"creed\"—is now widely accepted, though this acceptance may stem from its agreement with modern usage rather than historical accuracy.\n\nThe \"Poetic Edda\", also known as \"Sæmundar Edda\" or the \"Elder Edda\", is a collection of Old Norse poems from the Icelandic medieval manuscript Codex Regius (\"Royal Book\"). Along with the \"Prose Edda\", the \"Poetic Edda\" is the most expansive source on Norse mythology. The first part of the Codex Regius preserves poems that narrate the creation and foretold destruction and rebirth of the Old Norse mythological world as well as individual myths about gods concerning Norse deities. The poems in the second part narrate legends about Norse heroes and heroines, such as Sigurd, Brynhildr and Gunnar.\n\nThe Codex Regius was written in the 13th century, but nothing is known of its whereabouts until 1643, when it came into the possession of Brynjólfur Sveinsson, then the Church of Iceland's Bishop of Skálholt. At that time, versions of the \"Prose Edda\" were well known in Iceland, but scholars speculated that there once was another \"Edda\"—an \"Elder Edda\"—which contained the pagan poems Snorri quotes in his book. When the Codex Regius was discovered, it seemed that this speculation had proven correct. Brynjólfur attributed the manuscript to Sæmundr the Learned, a larger-than-life 12th century Icelandic priest. While this attribution is rejected by modern scholars, the name \"Sæmundar Edda\" is still sometimes encountered.\n\nBishop Brynjólfur sent the Codex Regius as a present to King Christian IV of Denmark, hence the name \"Codex Regius\". For centuries it was stored in the Royal Library in Copenhagen but in 1971 it was returned to Iceland.\n\nThe \"Prose Edda\", sometimes referred to as the \"Younger Edda\" or \"Snorri's Edda\", is an Icelandic manual of poetics which also contains many mythological stories. Its purpose was to enable Icelandic poets and readers to understand the subtleties of alliterative verse, and to grasp the mythological allusions behind the many \"kennings\" that were used in skaldic poetry.\n\nIt was written by the Icelandic scholar and historian Snorri Sturluson around 1220. It survives in four known manuscripts and three fragments, written down from about 1300 to about 1600.\n\nThe \"Prose Edda\" consists of a Prologue and three separate books: \"Gylfaginning\", concerning the creation and foretold destruction and rebirth of the Norse mythical world; \"Skáldskaparmál\", a dialogue between Ægir, a Norse god connected with the sea, and Bragi, the skaldic god of poetry; and \"Háttatal\", a demonstration of verse forms used in Norse mythology.\n\n", "id": "9993", "title": "Edda"}
{"url": "https://en.wikipedia.org/wiki?curid=9994", "text": "Ephemeris time\n\nThe term ephemeris time (often abbreviated ET) can in principle refer to time in connection with any astronomical ephemeris. In practice it has been used more specifically to refer to:\n\n\nMost of the following sections relate to the ephemeris time of the 1952 standard.\n\nAn impression has sometimes arisen that ephemeris time was in use from 1900: this probably arose because ET, though proposed and adopted in the period 1948–1952, was defined in detail using formulae that made retrospective use of the epoch date of 1900 January 0 and of Newcomb's Tables of the Sun.\n\nThe ephemeris time of the 1952 standard leaves a continuing legacy, through its ephemeris second which became closely duplicated in the length of the current standard SI second (see below: Redefinition of the second).\n\nEphemeris time (ET), adopted as standard in 1952, was originally designed as an approach to a uniform time scale, to be freed from the effects of irregularity in the rotation of the earth, \"for the convenience of astronomers and other scientists\", for example for use in ephemerides of the Sun (as observed from the Earth), the Moon, and the planets. It was proposed in 1948 by G M Clemence.\n\nFrom the time of John Flamsteed (1646–1719) it had been believed that the Earth's daily rotation was uniform. But in the later nineteenth and early twentieth centuries, with increasing precision of astronomical measurements, it began to be suspected, and was eventually established, that the rotation of the Earth (\"i.e.\" the length of the day) showed irregularities on short time scales, and was slowing down on longer time scales. The evidence was compiled by W de Sitter (1927) who wrote \"If we accept this hypothesis, then the 'astronomical time', given by the earth's rotation, and used in all practical astronomical computations, differs from the 'uniform' or 'Newtonian' time, which is defined as the independent variable of the equations of celestial mechanics\". De Sitter offered a correction to be applied to the mean solar time given by the Earth's rotation to get uniform time.\n\nOther astronomers of the period also made suggestions for obtaining uniform time, including A Danjon (1929), who suggested in effect that observed positions of the Moon, Sun and planets, when compared with their well-established gravitational ephemerides, could better and more uniformly define and determine time.\n\nThus the aim developed, to provide a new time scale for astronomical and scientific purposes, to avoid the unpredictable irregularities of the mean solar time scale, and to replace for these purposes Universal Time (UT) and any other time scale based on the rotation of the Earth around its axis, such as sidereal time.\n\nG M Clemence (1948) made a detailed proposal of this type based on the results of H Spencer Jones (1939). Clemence (1948) made it clear that his proposal was intended \"for the convenience of astronomers and other scientists only\" and that it was \"logical to continue the use of mean solar time for civil purposes\".\n\nDe Sitter and Clemence both referred to the proposal as 'Newtonian' or 'uniform' time. D Brouwer suggested the name 'ephemeris time'.\n\nFollowing this, an astronomical conference held in Paris in 1950 recommended \"that in all cases where the mean solar second is unsatisfactory as a unit of time by reason of its variability, the unit adopted should be the sidereal year at 1900.0, that the time reckoned in this unit be designated \"ephemeris time\"\", and gave Clemence's formula (see Definition of ephemeris time (1952)) for translating mean solar time to ephemeris time.\n\nThe International Astronomical Union approved this recommendation at its 1952 general assembly. Practical introduction took some time (see Use of ephemeris time in official almanacs and ephemerides); ephemeris time (ET) remained a standard until superseded in the 1970s by further time scales (see Revision).\nDuring the currency of ephemeris time as a standard, the details were revised a little. The unit was redefined in terms of the tropical year at 1900.0 instead of the sidereal year; and the standard second was defined first as 1/31556925.975 of the tropical year at 1900.0, and then as the slightly modified fraction 1/31556925.9747 instead, finally being redefined in 1967/8 in terms of the cesium atomic clock standard (see below).\n\nAlthough ET is no longer directly in use, it leaves a continuing legacy. Its successor time scales, such as TDT, as well as the atomic time scale IAT (TAI), were designed with a relationship that \"provides continuity with ephemeris time\". ET was used for the calibration of atomic clocks in the 1950s. Close equality between the ET second with the later SI second (as defined with reference to the cesium atomic clock) has been verified to within 1 part in 10.\n\nIn this way, decisions made by the original designers of ephemeris time influenced the length of today's standard SI second, and in turn, this has a continuing influence on the number of leap seconds which have been needed for insertion into current broadcast time scales, to keep them approximately in step with mean solar time.\n\nEphemeris time was defined in principle by the orbital motion of the Earth around the Sun, (but its practical implementation was usually achieved in another way, see below).\n\nIts detailed definition depended on Simon Newcomb's Tables of the Sun (1895), interpreted in a new way to accommodate certain observed discrepancies:\n\nIn the introduction to Newcomb's Tables of the Sun (1895) the basis of the tables (p. 9) includes a formula for the Sun's mean longitude, at a time indicated by interval T (in Julian centuries of 36525 mean solar days) reckoned from Greenwich Mean Noon on 0 January 1900:\n\nSpencer Jones' work of 1939 showed that the positions of the Sun actually observed, when compared with those obtained from Newcomb's formula, show the need for the following correction to the formula to represent the observations:\n\n(where \"the times of observation are in Universal time, not corrected to Newtonian time\", and 0.0748B represents an irregular fluctuation calculated from lunar observations).\n\nThus a conventionally corrected form of Newcomb's formula, to incorporate the corrections on the basis of mean solar time, would be the sum of the two preceding expressions:\n\nClemence's 1948 proposal did not adopt a correction of this kind in terms of mean solar time: instead, the same numbers were used as in Newcomb's original uncorrected formula (1), but now in a reverse sense, to define the time and time scale implicitly, based on the real position of the Sun:\n\nwhere the time variable, here represented as E, now represents time in ephemeris centuries of 36525 ephemeris days of 86400 ephemeris seconds. The 1961 official reference put it this way: \"The origin and rate of ephemeris time are defined to make the Sun's mean longitude agree with Newcomb's expression\"\n\nFrom the comparison of formulae (2) and (3), both of which express the same real solar motion in the same real time but on different time scales, Clemence arrived at an explicit expression, estimating the difference in seconds of time between ephemeris time and mean solar time, in the sense (ET-UT):\n\nClemence's formula, now superseded by more modern estimations, was included in the original conference decision on ephemeris time. In view of the fluctuation term, practical determination of the difference between ephemeris time and UT depended on observation. Inspection of the formulae above shows that the (ideally constant) unit of ephemeris time such as the ephemeris second has been for the whole of the twentieth century very slightly shorter than the corresponding (but not precisely constant) unit of mean solar time (which besides its irregular fluctuations tends gradually to increase), consistently also with the modern results of Morrison and Stephenson (see article ΔT).\n\nAlthough ephemeris time was defined in principle by the orbital motion of the Earth around the Sun, it was usually measured in practice by the orbital motion of the Moon around the Earth. These measurements can be considered as secondary realizations (in a metrological sense) of the primary definition of ET in terms of the solar motion, after a calibration of the mean motion of the Moon with respect to the mean motion of the Sun.\n\nReasons for the use of lunar measurements were practically based: the Moon moves against the background of stars about 13 times as fast as the Sun's corresponding rate of motion, and the accuracy of time determinations from lunar measurements is correspondingly greater.\n\nWhen ephemeris time was first adopted, time scales were still based on astronomical observation, as they always had been. The accuracy was limited by the accuracy of optical observation, and corrections of clocks and time signals were published in arrear.\n\nA few years later, with the invention of the cesium atomic clock, an alternative offered itself. Increasingly, after the calibration in 1958 of the cesium atomic clock by reference to ephemeris time, cesium atomic clocks running on the basis of ephemeris seconds began to be used and kept in step with ephemeris time. The atomic clocks offered a further secondary realization of ET, on a quasi-real time basis that soon proved to be more useful than the primary ET standard: not only more convenient, but also more precisely uniform than the primary standard itself. Such secondary realizations were used and described as 'ET', with an awareness that the time scales based on the atomic clocks were not identical to that defined by the primary ephemeris time standard, but rather, an improvement over it on account of their closer approximation to uniformity. The atomic clocks gave rise to the atomic time scale, and to what was first called Terrestrial Dynamical Time and is now Terrestrial Time, defined to provide continuity with ET.\n\nThe availability of atomic clocks, together with the increasing accuracy of astronomical observations (which meant that relativistic corrections were at least in the foreseeable future no longer going to be small enough to be neglected), led to the eventual replacement of the ephemeris time standard by more refined time scales including terrestrial time and barycentric dynamical time, to which ET can be seen as an approximation.\n\nIn 1976 the IAU resolved that the theoretical basis for its current (1952) standard of Ephemeris Time was non-relativistic, and that therefore, beginning in 1984, Ephemeris Time would be replaced by two relativistic timescales intended to constitute dynamical timescales: Terrestrial Dynamical Time (TDT) and Barycentric Dynamical Time (TDB). Difficulties were recognized, which led to these being in turn superseded in the 1990s by time scales Terrestrial Time (TT), Geocentric Coordinate Time GCT(TCG) and Barycentric Coordinate Time BCT(TCB).\n\nHigh-precision ephemerides of sun, moon and planets were developed and calculated at the Jet Propulsion Laboratory (JPL) over a long period, and the latest available were adopted for the ephemerides in the Astronomical Almanac starting in 1984. Although not an IAU standard, the ephemeris time argument T has been in use at that institution since the 1960s. The time scale represented by T has been characterized as a relativistic coordinate time that differs from Terrestrial Time only by small periodic terms with an amplitude not exceeding 2 milliseconds of time: it is linearly related to, but distinct (by an offset and constant rate which is of the order of 0.5 s/a) from the TCB time scale adopted in 1991 as a standard by the IAU. Thus for clocks on or near the geoid, T (within 2 milliseconds), but not so closely TCB, can be used as approximations to Terrestrial Time, and via the standard ephemerides T is in widespread use.\n\nPartly in acknowledgement of the widespread use of T via the JPL ephemerides, IAU resolution 3 of 2006 (re-)defined Barycentric Dynamical Time (TDB) as a current standard. As re-defined in 2006, TDB is a linear transformation of TCB. The same IAU resolution also stated (in note 4) that the \"independent time argument of the JPL ephemeris DE405, which is called T\" (here the IAU source cites ), \"is for practical purposes the same as TDB defined in this Resolution\". Thus the new TDB, like T, is essentially a more refined continuation of the older ephemeris time ET and (apart from the periodic fluctuations) has the same mean rate as that established for ET in the 1950s.\n\nEphemeris time based on the standard adopted in 1952 was introduced into the Astronomical Ephemeris (UK) and the American Ephemeris and Nautical Almanac, replacing UT in the main ephemerides in the issues for 1960 and after. (But the ephemerides in the Nautical Almanac, by then a separate publication for the use of navigators, continued to be expressed in terms of UT.) The ephemerides continued on this basis through 1983 (with some changes due to adoption of improved values of astronomical constants), after which, for 1984 onwards, they adopted the JPL ephemerides.\n\nPrevious to the 1960 change, the 'Improved Lunar Ephemeris' had already been made available in terms of ephemeris time for the years 1952-1959 (computed by W J Eckert from Brown's theory with modifications recommended by Clemence (1948)).\n\nSuccessive definitions of the unit of ephemeris time are mentioned above (History). The value adopted for the 1956/1960 standard second:\n\nwas obtained from the linear time-coefficient in Newcomb's expression for the solar mean longitude (above), taken and applied with the same meaning for the time as in formula (3) above. The relation with Newcomb's coefficient can be seen from:\n\nCaesium atomic clocks became operational in 1955, and quickly confirmed the evidence that the rotation of the earth fluctuated randomly. This confirmed the unsuitability of the mean solar second of Universal Time as a measure of time interval for the most precise purposes. After three years of comparisons with lunar observations, Markowitz et al. (1958) determined that the ephemeris second corresponded to 9 192 631 770 ± 20 cycles of the chosen cesium resonance.\n\nFollowing this, in 1967/68, the General Conference on Weights and Measures (CGPM) replaced the definition of the SI second by the following:\nThe second is the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom.\n\nAlthough this is an independent definition that does not refer to the older basis of ephemeris time, it uses the same quantity as the value of the ephemeris second measured by the cesium clock in 1958. This SI second referred to atomic time was later verified by Markowitz (1988) to be in agreement, within 1 part in 10, with the second of ephemeris time as determined from lunar observations.\n\nFor practical purposes the length of the ephemeris second can be taken as equal to the length of the second of Barycentric Dynamical Time (TDB) or Terrestrial Time (TT) or its predecessor TDT.\n\nThe difference between ET and UT is called ΔT; it changes irregularly, but the long-term trend is parabolic, decreasing from ancient times until the nineteenth century, and increasing since then at a rate corresponding to an increase in the solar day length of 1.7 ms per century (see leap seconds).\n\nInternational Atomic Time (TAI) was set equal to UT2 at 1 January 1958 0:00:00 . At that time, ΔT was already about 32.18 seconds. The difference between Terrestrial Time (TT) (the successor to ephemeris time) and atomic time was later defined as follows:\n\nThis difference may be assumed constant—the rates of TT and TAI are designed to be identical.\n\n", "id": "9994", "title": "Ephemeris time"}
{"url": "https://en.wikipedia.org/wiki?curid=9995", "text": "EastEnders\n\nEastEnders is a long-running British soap opera created by Julia Smith and Tony Holland which has been broadcast on BBC One since 1985. Set in the East End of London in the fictional Borough of Walford, the programme follows the stories of local residents and their families as they go about their daily lives in Albert Square. The series was initially screened as two 30-minute episodes per week, however, since 2001, episodes have been broadcast every day apart from Wednesdays and weekends. Same-day repeats of the series were previously shown on BBC Three and omnibus editions were shown on BBC Two, however, since 2016 these have been broadcast on UK-based TV channel W. \n\nWithin eight months of the show's launch, it reached the number-one spot in BARB's TV ratings and has consistently remained among the top-rated TV programmes in Britain. In 2013, the average audience share for an episode was around 30 per cent. Today, \"Eastenders\" remains a significant programme in terms of the BBC's success and audience share, and also in the history of British television drama, tackling many dilemmas that are considered to be controversial and taboo issues in British culture and social life previously unseen on United Kingdom mainstream television. \n\n, \"EastEnders\" has won nine BAFTA Awards and the Inside Soap Award for Best Soap for 14 years running (from 1997 to 2012), as well as twelve National Television Awards for Most Popular Serial Drama and 11 awards for Best Soap at the British Soap Awards. It has also won 13 TV Quick and TV Choice Awards for Best Soap, six TRIC Awards for Soap of The Year, four Royal Television Society Awards for Best Continuing Drama and has been inducted into the Rose d'Or Hall of Fame.\n\nIn March 1983, under two years before \"EastEnders\" first episode was broadcast, the show was a vague idea in the mind of a handful of BBC executives, who decided that what BBC1 needed was a popular bi-weekly drama series that would attract the kind of mass audiences that ITV was getting with \"Coronation Street\". The first people to whom David Reid, then head of series and serials, turned were Julia Smith and Tony Holland, a well established producer/script editor team who had first worked together on \"Z-Cars\". The outline that Reid presented was vague: two episodes a week, 52 weeks a year. After the concept was put to them on 14 March 1983, Smith and Holland then went about putting their ideas down on paper; they decided it would be set in the East End of London. Granada Television gave Smith unrestricted access to the \"Coronation Street\" production for a month so that she could get a sense how a continuing drama was produced.\n\nThere was anxiety at first that the viewing public would not accept a new soap set in the south of England, though research commissioned by lead figures in the BBC revealed that southerners would accept a northern soap, northerners would accept a southern soap and those from the Midlands, as Julia Smith herself pointed out, did not mind where it was set as long as it was somewhere else. This was the beginning of a close and continuing association between \"EastEnders\" and audience research, which, though commonplace today, was something of a revolution in practice.\n\nThe show's creators were both Londoners, but when they researched Victorian squares, they found massive changes in areas they thought they knew well. However, delving further into the East End of London, they found exactly what they had been searching for: a real East End spirit—an inward looking quality, a distrust of strangers and authority figures, a sense of territory and community that the creators summed up as \"Hurt one of us and you hurt us all\". These themes that were found for the setting can still be found in a present-day episode of \"EastEnders\".\n\nWhen developing \"EastEnders\", both Smith and Holland looked at influential models like \"Coronation Street\", but they found that it offered a rather outdated and nostalgic view of working-class life. Only after \"EastEnders\" began, and featured the characters of Tony Carpenter and Kelvin Carpenter, did \"Coronation Street\" start to feature black characters, for example. They came to the conclusion that \"Coronation Street\" had grown old with its audience, and that \"EastEnders\" would have to attract a younger, more socially extensive audience, ensuring that it had the longevity to retain it for many years thereafter. They also looked at \"Brookside\" but found there was a lack of central meeting points for the characters, making it difficult for the writers to intertwine different storylines, so \"EastEnders\" was set in Albert Square.\n\nA previous UK soap set in an East End market was ATV's \"Market in Honey Lane\" between 1967 and 1969. However this show, which graduated from one showing a week to two in three separate series (the latter series being shown in different time slots across the ITV network) was very different in style and approach to \"EastEnders\". The British Film Institute described \"Market In Honey Lane\" thus: \"It was not an earth-shaking programme, and certainly not pioneering in any revolutionary ideas in technique and production, but simply proposed itself to the casual viewer as a mildly pleasant affair.\" \"EastEnders\", while also featuring an East End street market, would be very different in its approach and impact.\n\nThe target launch date was originally January 1985. Smith and Holland had eleven months in which to write, cast and shoot the whole thing. However, in February 1984, they did not even have a title or a place to film. Both Smith and Holland were unhappy about the January 1985 launch date, favouring November or even September 1984 when seasonal audiences would be higher, but the BBC stayed firm, and Smith and Holland had to concede that, with the massive task of getting the Elstree Studios operational, January was the most realistic date. However, this was later to be changed to February.\n\nThe project had a number of working titles—\"Square Dance\", \"Round the Square\", \"Round the Houses\", \"London Pride\" and \"East 8\". It was the latter that stuck (E8 is the postcode for Hackney) in the early months of creative process. However, the show was renamed after many casting agents mistakenly thought the show was to be called \"Estate\", and the fictional postcode E20 was created, instead of using E8. Julia Smith came up with the name \"Eastenders\" after she and Holland had spent months telephoning theatrical agents and asking \"Do you have any real East Enders on your books?\". However, Smith thought \"\"Eastenders\"\" \"looked ugly written down\" and was \"hard to say\", so decided to capitalise the second 'e'.\n\nAfter they decided on the filming location of BBC Elstree Studios in Hertfordshire) Smith and Holland set about creating the twenty-three characters needed, in just fourteen days. They took a holiday in Playa de los Pocillos, Lanzarote, and started to create the characters. Holland created the Beale and Fowler family, drawing on his own background. His mother, Ethel Holland, was one of four sisters raised in Walthamstow. Her eldest sister, Lou, had married a man named Albert Beale and had two children, named Peter and Pauline. These family members were the basis for Lou Beale, Pete Beale and Pauline Fowler. Holland also created Pauline's unemployed husband Arthur Fowler, their children Mark Fowler and Michelle Fowler, Pete's wife Kathy Beale and their son Ian Beale. Smith used her personal memories of East End residents she met when researching Victorian squares. Ethel Skinner was based on an old woman she met in a pub, with ill-fitting false teeth, and a \"face to rival a neon sign\", holding a Yorkshire Terrier in one hand and a pint of Guinness in the other. Other characters created included Jewish doctor Harold Legg, the Anglo-Cypriot Osman family, Ali Osman, Sue Osman and baby Hassan Osman, black father and son, Tony Carpenter and Kelvin Carpenter, single mother Mary Smith and Bangladeshi couple Saeed Jeffery and Naima Jeffery. Jack, Pearl and Tracey Watts were created to bring \"flash, trash, and melodrama\" to the Square (they were later renamed Den Watts, Angie Watts and Sharon Watts). The characters of Andy O'Brien and Debbie Wilkins were created to show a modern couple with outwardly mobile pretensions, and Lofty Holloway to show an outsider; someone who did not fit in with other residents. It was decided that he would be a former soldier, as Holland's personal experiences of ex-soldiers were that they had trouble fitting into society after being in the army. When they compared the characters they had created, Smith and Holland realised they had created a cross-section of East End residents. The Beale and Fowler family represented the old families of the East End, who had always been there. The Osmans, Jefferys and Carpenters represented the more modern diverse ethnic community of the East End. Debbie, Andy and Mary represented more modern day individuals.\n\nOnce they had decided on their twenty-three characters, they returned to London for a meeting with the BBC. Everyone agreed that \"EastEnders\" would be tough, violent on occasion, funny and sharp—set in Margaret Thatcher's Britain—and it would start with a bang (namely the death of Reg Cox). They decided that none of their existing characters were wicked enough to have killed Reg, so a twenty-fourth character, Nick Cotton was added to the line-up. He was a racist thug, who often tried to lead other young characters astray. When all the characters had been created, Smith and Holland set about casting the actors for the show.\n\nThrough the next few months, the set was growing rapidly at Elstree, and a composer and designer had been commissioned to create the title sequence. Simon May wrote the theme music and Alan Jeapes created the visuals. The visual images were taken from an aircraft flying over the East End of London at 1000 feet. Approximately 800 photographs were taken and pieced together to create one big image. The credits were later updated when the Millennium Dome was built.\n\nThe launch was delayed until February 1985 due to a delay in the chat show \"Wogan\", that was to be a part of the major revamp in BBC1's schedules. Smith was uneasy about the late start as \"EastEnders\" no longer had the winter months to build up a loyal following before the summer ratings lull. The press were invited to Elstree to meet the cast and see the lot, and stories immediately started circulating about the show, about a rivalry with ITV (who were launching their own market-based soap, \"Albion Market\") and about the private lives of the cast. Anticipation and rumour grew in equal measure until the first transmission at 7p.m. on 19 February 1985. Both Holland and Smith could not watch; they both instead returned to the place where it all began, Albertine's Wine Bar on Wood Lane. The next day, viewing figures were confirmed at 17 million. The reviews were largely favourable, although after three weeks on air, BBC1's early evening share had returned to the pre-\"EastEnders\" figure of 7 million, though \"EastEnders\" then climbed to highs of up to 23 million later on in the year. Following the launch, both group discussions and telephone surveys were conducted to test audience reaction to early episodes. Detailed reactions were taken after six months and since then regular monitoring was conducted.\n\nPress coverage of \"EastEnders\", which was already intense, went into overdrive once the show was broadcast. With public interest so high, the media began investigating the private lives of the show's popular stars. Within days, the scandalous headline the producers had all dreaded appeared—\"EASTENDERS STAR IS A KILLER\". This referred to Leslie Grantham, and his prison sentence for the murder of a taxi driver in an attempted robbery nearly 20 years earlier. This shocking tell-all style set the tone for relations between Albert Square and the press for the next 20 years.\n\nThe show's first episode attracted some 17 million viewers, and it continued to attract high viewing figures from then on. By Christmas 1985, the tabloids couldn't get enough of the show. 'Exclusives' about \"EastEnders\" storylines and the actors on the show became a staple of tabloid buyers daily reading.\n\nWriter Colin Brake suggested that 1989 was a year of big change for \"EastEnders\", both behind the cameras and in front of them. Original production designer, Keith Harris, left the show, and Holland and Smith both decided that the time had come to move on too; their final contribution coinciding with the exit of one of \"EastEnders\" most successful characters, Den Watts (Leslie Grantham). Producer Mike Gibbon was given the task of running the show and he enlisted the most experienced writers to take over the storylining of the programme, including Charlie Humphreys, Jane Hollowood and Tony McHale.\n\nAccording to Brake, the departure of two of the soap's most popular characters, Den and Angie Watts (Anita Dobson), left a void in the programme, which needed to be filled. In addition, several other long-running characters left the show that year including Sue and Ali Osman (Sandy Ratcliff and Nejdet Salih) and their family; Donna Ludlow (Matilda Ziegler); Carmel Jackson (Judith Jacob) and Colin Russell (Michael Cashman). Brake indicated that the production team decided that 1989 was to be a year of change in Walford, commenting, \"it was almost as if Walford itself was making a fresh start\".\n\nBy the end of 1989 \"EastEnders\" had acquired a new executive producer, Michael Ferguson, who had previously been a successful producer on ITV's \"The Bill\". Brake suggested that Ferguson was responsible for bringing in a new sense of vitality and creating a programme that was more in touch with the real world than it had been over the previous year.\n\nA new era began in 1990 with the introduction of Phil Mitchell (Steve McFadden) and Grant Mitchell (Ross Kemp)—the Mitchell brothers—successful characters who would go on to dominate the soap thereafter. As the new production team cleared the way for new characters and a new direction, all of the characters introduced under Gibbon were axed from the show at the start of the year. Ferguson introduced other characters and was responsible for storylines including HIV, Alzheimer's disease and murder. After a successful revamp of the soap, Ferguson decided to leave \"EastEnders\" in July 1991. Furguson was succeeded by both Leonard Lewis and Helen Greaves who initially shared the role as Executive Producer for \"EastEnders\". Lewis and Greaves formulated a new regime for \"EastEnders\", giving the writers of the serial more authority in storyline progression, with the script department providing \"guidance rather than prescriptive episode storylines\". By the end of 1992, Greaves left and Lewis became executive and series producer. He left \"EastEnders\" in 1994 after the BBC controllers demanded an extra episode a week, taking its weekly airtime from 60 to 90 minutes. Lewis felt that producing an hour of \"reasonable quality drama\" a week was the maximum that any broadcasting system could generate without loss of integrity. Having set up the transition to the new schedule, the first trio of episodes—dubbed The Vic siege—marked Lewis's departure from the programme. Barbara Emile then became the Executive Producer of \"EastEnders\", remaining with \"EastEnders\" until early 1995. She was succeeded by Corinne Hollingworth.\n\nHollingworth's contributions to the soap were awarded in 1997 when \"EastEnders\" won the BAFTA for Best Drama Series. Hollingworth shared the award with the next Executive Producer, Jane Harris. Harris was responsible for the critically panned Ireland episodes and Cindy Beale's attempted assassination of Ian Beale, which brought in an audience of 23 million in 1996, roughly 4 million more than \"Coronation Street\". In 1998 Matthew Robinson was appointed as the Executive Producer of \"EastEnders\". During his reign, \"EastEnders\" won the BAFTA for \"Best Soap\" in consecutive years 1999 and 2000 and many other awards. Robinson also earned tabloid soubriquet \"Axeman of Albert Square\" after sacking a large number of characters in one hit, and several more thereafter. In their place Robinson introduced new long-running characters including Melanie Healy, Jamie Mitchell, Lisa Shaw, Steve Owen and Billy Mitchell.\n\nJohn Yorke became the Executive Producer of \"EastEnders\" in 2000. Yorke was given the task of introducing the soap's fourth weekly episode. He axed the majority of the Di Marco family and helped introduce popular characters such as the Slater family. As what Mal Young described as \"two of \"EastEnders\" most successful years\", Yorke was responsible for highly-rated storlines such as \"Who Shot Phil?\", Ethel Skinner's death, Jim Branning and Dot Cotton's marriage, Trevor Morgan's domestic abuse of his wife Little Mo Morgan, and Kat Slater's revelation to her daughter Zoe Slater that she was her mother.\n\nIn 2002, Louise Berridge succeeded Yorke as the Executive Producer. During her time at \"EastEnders\", Berridge introduced popular characters such as Alfie Moon, Dennis Rickman, and the critically panned Indian Ferreira family.\n\nBerridge was responsible for some ratings success stories, such as Alfie and Kat Slater's relationship, Janine Butcher getting her comeuppance, Trevor Morgan and Jamie Mitchell's death storylines and the return of one of the greatest soap icons, Den Watts, who had been presumed dead for fourteen years. His return in late 2003 was watched by over 16 million viewers, putting \"EastEnders\" back at number one in the rating war with the \"Coronation Street\". However, other storylines, such as one about a kidney transplant involving the Ferreiras, were not well received, and although Den Watts's return proved to be a ratings success, the British press branded the plot unrealistic and felt that it questioned the show's credibility. A severe press backlash followed after Den's actor, Leslie Grantham, was outed in an internet sex scandal, which coincided with a swift decline in viewer ratings. The scandal led to Grantham's departure from the soap, but the occasion was used to mark the 20th anniversary of \"EastEnders\", with an episode showing Den's murder at the Queen Vic pub.\n\nOn 21 September 2004, Berridge quit as executive producer of \"EastEnders\" following continued criticism of the show. Kathleen Hutchison was swiftly appointed as the Executive Producer of \"EastEnders\", and was tasked with quickly turning the fortunes of the soap. During her time at the soap Hutchison axed multiple characters, and reportedly ordered the rewriting of numerous scripts. Newspapers reported on employee dissatisfaction with Hutchison's tenure at \"EastEnders\". In January 2005, Hutchison left the soap and John Yorke (who by this time, was the BBC Controller of Continuing Drama Series) took total control of the show himself and became acting Executive Producer for a short period, before appointing Kate Harwood to the role. Harwood stayed at \"EastEnders\" for 20 months before being promoted by the BBC. On Friday 11 November 2005, \"EastEnders\" was the first British drama to feature a two-minute silence. This episode later went on to win the British Soap Award for 'Best Single Episode'. In October 2006, Diederick Santer took over as Executive Producer of \"EastEnders\". He introduced several characters to the show, including ethnic minority and homosexual characters to make the show 'feel more 21st Century'. Santer also reintroduced past and popular characters to the programme. \n\nOn 2 March 2007, BBC signed a deal with Google to put videos on YouTube. A behind the scenes video of \"EastEnders\", hosted by Matt Di Angelo, who played Deano Wicks on the show, was put on the site the same day, and was followed by another on 6 March 2007. In April 2007, \"EastEnders\" became available to view on mobile phones, via 3G technology, for 3, Vodafone and Orange customers. On 21 April 2007, the BBC launched a new advertising campaign using the slogan \"There's more to \"EastEnders\"\". The first television advert showed Dot Branning with a refugee baby, Tomas, whom she took in under the pretence of being her grandson. The second and third featured Stacey Slater and Dawn Swann, respectively. There have also been adverts in magazines and on radio.\n\nIn 2009, producers introduced a limit on the number of speaking parts in each episode due to budget cuts, with an average of 16 characters per episode. The decision was criticised by Martin McGrath of Equity, who said \"Trying to produce quality TV on the cheap is doomed to fail.\" The BBC responded by saying they had been working that way for some time and it had not affected the quality of the show.\n\nFrom 4 February 2010, CGI was used in the show for the first time, with the addition of computer-generated trains.\n\n\"EastEnders\" celebrated its 25th anniversary on 19 February 2010. Santer came up with several plans to mark the occasion, including the show's first episode to be broadcast live, the second wedding between Ricky Butcher and Bianca Jackson and the return of Bianca's relatives, mother Carol Jackson, and siblings Robbie Jackson, Sonia Fowler and Billie Jackson. He told entertainment website Digital Spy, \"It's really important that the feel of the week is active and exciting and not too reflective. There'll be those moments for some of our longer-serving characters that briefly reflect on themselves and how they've changed. The characters don't know that it's the 25th anniversary of anything, so it'd be absurd to contrive too many situations in which they're reflective on the past. The main engine of that week is great stories that'll get people talking.\" The live episode featured the death of Bradley Branning and the conclusion of the \"Who Killed Archie?\" storyline, when Stacey Branning revealed she was the murderer. Viewing figures peaked at 16.6 million, which was the highest viewed episode in seven years. Other events to mark the anniversary were a spin-off DVD, \"EastEnders: Last Tango in Walford\", and an Internet spin-off, \"\".\n\nSanter officially left \"EastEnders\" in March 2010, and was replaced by Bryan Kirkwood. Kirkwood's first signing was the reintroduction of characters Alfie Moon (Shane Richie) and Kat Moon (Jessie Wallace), and his first new character was Vanessa Gold, played by Zöe Lucker. In April and May 2010, Kirkwood axed eight characters from the show, Barbara Windsor left her role of Peggy Mitchell, which left a hole in the show, which Kirkwood decided to fill by bringing back Kat and Alfie, which he said would \"herald the new era of \"EastEnders\".\" \"EastEnders\" started broadcasting in high definition on 25 December 2010. Old sets had to be rebuilt, so The Queen Victoria set was burnt down in a storyline (and in reality) to facilitate this.\n\nIn November 2011, a storyline showed character Billy Mitchell, played by Perry Fenwick, selected to be a torch bearer for the 2012 Summer Olympics. In reality, Fenwick carried the torch through the setting of Albert Square, with live footage shown in the episode on 23 July 2012. This was the second live broadcast of \"EastEnders\". In 2012, Kirkwood chose to leave his role as executive producer and was replaced by Lorraine Newman. The show lost many of its significant characters during this period. Newman stepped down as executive producer after sixteen months in the job in 2013 after the soap was criticised for its boring storylines and its lowest-ever figures pointing at around 4.8 million. Dominic Treadwell-Collins was appointed as the new executive producer on 19 August 2013 and was credited on 9 December. He axed multiple characters from the show and introduced the extended Carter family. He introduced a long-running storyline, \"Who Killed Lucy Beale?\" which peaked during the show's 30th anniversary in 2015 with a week of live episodes. Treadwell-Collins announced his departure from \"EastEnders\" on 18 February 2016.\n\nSean O'Connor, former \"EastEnders\" series story producer and then-editor on radio soap opera \"The Archers\", was announced to be taking over the role. Treadwell-Collins left on 6 May and O'Connor's first credited episode was broadcast on 11 July Although O'Connor's first credited episode aired in July, his own creative work was not seen onscreen until late September.\n\nThe central focus of \"EastEnders\" is the fictional Victorian square Albert Square in the fictional London Borough of Walford. In the show's narrative, Albert Square is a 19th-century street, named after Prince Albert (1819–61), the husband of Queen Victoria (1819–1901, reigned 1837–1901). Thus, central to Albert Square is The Queen Victoria Public House (also known as The Queen Vic or The Vic). The show's producers based the square's design on Fassett Square in Dalston. There is also a market close to Fassett Square at Ridley Road. The postcode for the area, E8, was one of the working titles for the series. The name \"Walford\" is both a street in Dalston where Tony Holland lived and a blend of Walthamstow and Stratford—the areas of Greater London where the creators were born. Other parts of the Square and set interiors are based on other locations. The bridge is based upon one near the BBC Television Centre, the Queen Vic on the old pub at the end of Scrubs Lane/Harrow Road NW10, and the interior to the Fowlers' is based on a house in Manor Road, Colchester, close to where the supervising art director lived. The fictional local newspaper, the \"Walford Gazette\", in which local news events such as the arrests or murders of characters appear, mirrors the real \"Hackney Gazette\".\n\nWalford East is a fictional tube station for Walford, and a tube map that was first seen on air in 1996 showed Walford East between Bow Road and West Ham, in the actual location of Bromley-by-Bow on the District and Hammersmith & City lines.\n\nWalford has the postal district of E20. The postcode district was selected as if it were part of the actual E postcode area which covers much of east London although the next unused postcode district in the area was, and still is (), E19. The \"E\" stands for \"Eastern\". In 1917 the current postal districts in London were assigned alphabetically according to the name of the main sorting office for each district. If Walford had been assigned in this scheme it would have been given E17, which is the current postcode district for Walthamstow. In March 2011, Royal Mail allocated the E20 postal district to the 2012 Olympic Park. The postal district in \"EastEnders\" was entirely fictional up to that point, as London East postal districts stopped at E18 at that time. The show's creators opted for E20 instead of E19 as it was thought to sound better. In September 2011 the postal code for Albert Square was revealed in an episode as E20 6PQ.\n\n\"EastEnders\" is built around the idea of relationships and strong families, with each character having a place in the community. This theme encompasses the whole Square, making the entire community a family of sorts, prey to upsets and conflict, but pulling together in times of trouble. Co-creator Tony Holland was from a large East End family, and such families have typified \"EastEnders\". The first central family was the combination of the Fowler family, consisting of Pauline Fowler, her husband Arthur Fowler, and teenage children Mark Fowler and Michelle Fowler and the Beale family, consisting of Pete Beale (Pauline's twin brother), his wife Kathy Beale and their teenage son Ian Beale. Pauline and Pete's mother was the domineering Lou Beale, who lived with Pauline and her family. Holland drew on the names of his own family for the characters.\n\nThe Watts and Mitchell families have been central to many notable \"EastEnders\" storylines, the show having been dominated by the Watts in the 1980s, with the 1990s focusing on the Mitchells. The early 2000s saw a shift in attention towards the newly introduced female Slater clan, before a renewal of emphasis upon the restored Watts family beginning in 2003. Since 2006, \"EastEnders\" has largely been dominated by the Mitchell and Branning families, though the early 2010s also saw a renewed focus on the Moon family, and from 2013 onwards, on the Carters. The Beales are the show's longest running family, having been in \"EastEnders\" since it began in 1985. Key people involved in the production of \"EastEnders\" have stressed how important the idea of strong families is to the programme. Peggy Mitchell, in particular, is notorious for her ceaseless repetition of such statements as \"You're a Mitchell!\" and \"It's all about family!\". Pauline Fowler is also known for her insistence on family and mentioning her brother and husband to instil loyalty from family members. Her mother Lou Beale is renowned for her family meetings and traditional approach to family. More recently, Derek Branning regularly expresses the importance of a strong family unit. As the eldest sibling, he is constantly asserting his position as head of his family and reminding everyone to pull together in times of trouble. Additionally, Derek commonly refers to himself, Max Branning and Jack Branning as \"the Branning brothers.\"\n\n\"EastEnders\" has an emphasis on stong family matriarchs, with examples including Pauline Fowler and Peggy Mitchell, helping to attract a female audience. John Yorke, then the BBC's head of drama production, put this down to Tony Holland's \"gay sensibility, which showed a love for strong woman\". The matriarchal role is one that has been seen in various reincarnations since the programme's inception, often depicted as the centre of the family unit. The original matriarch was Lou Beale, though later examples include Mo Harris, Pat Butcher,Zainab Masood and Cora Cross. These characters are seen as being loud and interfering but most importantly, responsible for the well-being of the family and usually stressing the importance of family, reflecting on the past.\n\nThe show often includes strong, brassy, long-suffering women who exhibit diva-like behaviour and stoically battle through an array of tragedy and misfortune. Such characters include Angie Watts, Kathy Beale, Sharon Watts, Pat Butcher, Denise Fox and Tanya Branning. Conversely there are female characters who handle tragedy less well, depicted as eternal victims and endless sufferers, who include Sue Osman, Little Mo Mitchell, Laura Beale, Lisa Fowler, Ronnie Mitchell and Linda Carter. The 'tart with a heart' is another recurring character, often popular with viewers. Often their promiscuity masks a hidden vulnerability and a desire to be loved. Such characters have included Pat Butcher (though in her latter years, this changed), Tiffany Mitchell, Kat Slater, Stacey Slater, Dawn Swann and Roxy Mitchell.\n\nA gender balance in the show is maintained via the inclusion of various \"macho\" male personalities such as Mick Carter, Phil Mitchell, Grant Mitchell, Jack Branning and Max Branning, \"bad boys\" such as Den Watts, Michael Moon and Vincent Hubbard, and \"heartthrobs\" such as Simon Wicks, Jamie Mitchell, Dennis Rickman and Joey Branning. Another recurring male character type is the smartly dressed businessman, often involved in gang culture and crime and seen as a local authority figure. Examples include Steve Owen, Jack Dalton, Andy Hunter, Johnny Allen and Derek Branning. Following criticism aimed at the show's over-emphasis on 'gangsters' in 2005, such characters have been significantly reduced. Another recurring male character seen in \"EastEnders\" is the 'loser' or 'soft touch', males often comically under the thumb of their female counterparts, which have included Arthur Fowler, Ricky Butcher, Lofty Holloway and Billy Mitchell. Other recurring character types that have appeared throughout the serial are \"cheeky-chappies\" Pete Beale, Alfie Moon, Garry Hobbs and Kush Kazemi, \"lost girls\" such as Mary Smith, Donna Ludlow and Mandy Salter, delinquents such as Stacey Slater, Jay Brown and Lola Pearce, \"villains\" such as Nick Cotton, Trevor Morgan, May Wright, Yusef Khan, Archie Mitchell and Dean Wicks, \"bitches\" such as Cindy Beale, Janine Butcher, Lucy Beale, Abi Branning and Babe Smith and cockney \"wide boys\" or \"wheeler dealers\" such as Frank Butcher, Alfie Moon, Kevin Wicks, Darren Miller and Fatboy.\nOver the years \"EastEnders\" has typically featured a number of elderly residents, who are used to show vulnerability, nostalgia, stalwart-like attributes and are sometimes used for comedic purposes. The original elderly residents included Lou Beale, Ethel Skinner and Dot Cotton. Over the years they have been joined by the likes of Mo Butcher, Jules Tavernier, Marge Green, Nellie Ellis, Jim Branning, Charlie Slater, Mo Harris, Patrick Trueman, Cora Cross, Les Coker, Rose Cotton, Pam Coker, Stan Carter, Babe Smith and Claudette Hubbard. Focus on elderly characters has decreased since the show's inception. The programme has more recently included a higher number of teenagers and successful young adults in a bid to capture the younger television audience. This has spurred criticism, most notably from the actress Anna Wing, who played Lou Beale in the show. She commented, \"I don't want to be disloyal, but I think you need a few mature people in a soap because they give it backbone and body... if all the main people are young it gets a bit thin and inexperienced. It gets too lightweight.\"\n\n\"EastEnders\" has been known to feature a 'comedy double-act', originally demonstrated with the characters of Dot and Ethel, whose friendship was one of the serial's most enduring. Other examples include Paul Priestly and Trevor Short, Huw Edwards and Lenny Wallace, Shirley Carter and Heather Trott, Garry Hobbs and Minty Peterson, Denise Fox and Zainab Masood, Poppy Meadow and Jodie Gold and Peggy Mitchell and Pat Evans. In 1989 especially, characters were brought in who were deliberately conceived as comic or light-hearted. Such characters included Julie Cooper—a brassy maneater; Marge Green—a batty older lady played by veteran comedy actress, Pat Coombs; Trevor Short (Phil McDermott)—the \"village idiot\"; his friend, northern heartbreaker Paul Priestly (Mark Thrippleton); wheeler-dealer Vince Johnson (Hepburn Graham) and Laurie Bates (Gary Powell), who became Pete Beale's (Peter Dean) sparring partner. The majority of \"EastEnders\"' characters are working-class. Middle-class characters do occasionally become regulars, but have been less successful and rarely become long-term characters. In the main, middle-class characters exist as villains, such as James Wilmott-Brown, May Wright, Stella Crawford and Yusef Khan, or are used to promote positive liberal influences, such as Colin Russell or Rachel Kominski.\n\n\"EastEnders\" has always featured a culturally diverse cast which has included black, Asian, Turkish, Polish and Latvian characters. \"The expansion of minority representation signals a move away from the traditional soap opera format, providing more opportunities for audience identification with the characters and hence a wider appeal\". Despite this, the programme has been criticised by the Commission for Racial Equality, who argued in 2002 that \"EastEnders\" was not giving a realistic representation of the East End's \"ethnic make-up\". They suggested that the average proportion of visible minority faces on \"EastEnders\" was substantially lower than the actual ethnic minority population in East London boroughs, and it therefore reflected the East End in the 1960s, not the East End of the 2000s. Furthermore, it was suggested that an element of \"tokenism\" and stereotyping surrounded many of these minority characters. The programme has since attempted to address these issues. A sari shop was opened and various characters of differing ethnicities were introduced throughout 2006 and 2007, including the Fox family, the Masoods, and various background artists. This was part of producer Diederick Santer's plan to \"diversify\", to make \"EastEnders\" \"feel more 21st century\". \"EastEnders\" has had varying success with ethnic minority characters. Possibly the least successful were the Indian Ferreira family, who were not well received by critics or viewers and were dismissed as unrealistic by the Asian community in the UK.\n\n\"EastEnders\" has been praised for its portrayal of characters with disabilities, including Adam Best (spina bifida), Noah Chambers (deaf), Jean Slater and her daughter Stacey (bipolar disorder), Janet Mitchell (Down's syndrome) and Jim Branning (stroke).\n\n\"EastEnders\" has a high cast turnover and characters are regularly changed to facilitate storylines or refresh the format. The show has also become known for the return of characters after they have left the show. Sharon Rickman returned in August 2012 for her third stint on the show. Den Watts returned 14 years after he was believed to have died, a feat repeated by Kathy Beale in 2015. Speaking extras, including Tracey the barmaid (who has been in the show since the first episode in 1985), have made appearances throughout the show's duration, without being the focus of any major storylines. The character of Nick Cotton gained a reputation for making constant exits and returns since the programme's first year, until the character's death in 2015.\n\n, Adam Woodyatt, Gillian Taylforth and Letitia Dean are the only members of the original cast remaining in the show, in their roles of Ian Beale, Kathy Beale and Sharon Watts respectively. Original character Michelle Fowler also appears in the show although recast. Ian Beale is the only character to have appeared continuously from the first episode without officially leaving, and is the longest serving character in \"EastEnders\". Dot Cotton is the longest serving female character in the show having served since 1985, whilst Pat Butcher is the longest-serving former character, appearing from 1986 until 2012.\n\n\"EastEnders\" programme makers took the decision that the show was to be about \"everyday life\" in the inner city \"today\" and regarded it as a \"slice of life\". Creator/producer Julia Smith declared that \"We don't make life, we reflect it\". She also said, \"We decided to go for a realistic, fairly outspoken type of drama which could encompass stories about homosexuality, rape, unemployment, racial prejudice, etc., in a believable context. Above all, we wanted realism\". In 2011, the head of BBC drama, John Yorke, said that the real East End had changed significantly since \"EastEnders\" started, and the show no longer truly reflected real life, but that it had an \"emotional truthfulness\" and was partly \"true to the original vision\" and partly \"adapt[ing] to a changing world\", adding that \"If it was a show where every house cost a fortune and everyone drove a Lexus, it wouldn't be \"EastEnders\". You have to show shades of that change, but certain things are immutable, I would argue, like The Vic and the market.\"\n\nIn the 1980s, \"EastEnders\" featured \"gritty\" storylines involving drugs and crime, representing the issues faced by working-class Thatcherite Britain. Storylines included the cot death of 14-month-old Hassan Osman, Nick Cotton's homophobia, racism and murder of Reg Cox, Arthur Fowler's unemployment reflecting the recession of the 1980s, the rape of Kathy Beale in 1988 by James Willmott-Brown and Michelle Fowler's teenage pregnancy. The show also dealt with prostitution, mixed-race relationships, shoplifting, sexism, divorce, domestic violence and mugging. In 1989, the programme came under criticism in the British media for being too depressing, and according to writer Colin Brake, the programme makers were determined to change this. In 1989 there was a deliberate attempt to increase the lighter, more comic aspects of life in Albert Square. This led to the introduction of some characters who were deliberately conceived as comic or light-hearted. Brake suggested that humour was an important element in \"EastEnders\"' storylines during 1989, with a greater amount of slapstick and light comedy than before. He classed 1989's changes as a brave experiment, and suggested that while some found this period of \"EastEnders\" entertaining, many other viewers felt that the comedy stretched the programme's credibility. Although the programme still covered many issues in 1989, such as domestic violence, drugs, rape and racism, Brake reflected that the new emphasis on a more balanced mix between \"light and heavy storylines\" gave the illusion that the show had lost a \"certain edge\".\n\nAs the show progressed into the 1990s, \"EastEnders\" still featured hard-hitting issues such as Mark Fowler discovering he was HIV positive in 1991, the death of his wife Gill from an AIDS-related illness in 1992, murder, adoption, abortion, Peggy Mitchell's battle with breast cancer, and Phil Mitchell's alcoholism and violence towards wife Kathy. Mental health issues were confronted in 1996 when 16-year-old Joe Wicks developed schizophrenia following the off-screen death of his sister in a car crash. The long-running storyline of Mark Fowler's HIV was so successful in raising awareness that in 1999, a survey by the National Aids Trust found teenagers got most of their information about HIV from the soap, though one campaigner noted that in some ways the storyline was not reflective of what was happening at the time as the condition was more common among the gay community. Still, heterosexual Mark struggled with various issues connected to his HIV status, including public fears of contamination, a marriage breakdown connected to his inability to have children and the side effects of combination therapies.\n\nIn the early 2000s, \"EastEnders\" covered the issue of euthanasia (Ethel Skinner's death in a pact with her friend Dot Cotton), the unveiling of Kat Slater's abuse by her uncle Harry as a child (which led to the birth of her daughter Zoe, who had been brought up to believe that Kat was her sister), the domestic abuse of Little Mo Morgan by husband Trevor (which involved rape and culminated in Trevor's death after he tried to kill Little Mo in a fire), Sonia Jackson giving birth at the age of 15 and then putting her baby up for adoption, and Janine Butcher's prostitution, agoraphobia and drug addiction. The soap also tackled the issue of mental illness and carers of people who have mental conditions, illustrated with mother and daughter Jean and Stacey Slater; Jean suffers from bipolar disorder, and teenage daughter Stacey was her carer (this storyline won a Mental Health Media Award in September 2006). Stacey went on to struggle with the disorder herself. The issue of illiteracy was highlighted by the characters of middle-aged Keith and his young son Darren. \"EastEnders\" has also covered the issue of Down's syndrome, as Billy and Honey Mitchell's baby, Janet Mitchell, was born with the condition in 2006. \"EastEnders\" covered child abuse with its storyline involving Phil Mitchell's 11-year-old son Ben and lawyer girlfriend Stella Crawford, and child grooming involving the characters Tony King and Whitney Dean.\n\nAside from this, soap opera staples of youthful romance, jealousy, domestic rivalry, gossip and extramarital affairs are regularly featured, with high-profile storylines occurring several times a year. Whodunits also feature regularly, including the \"Who Shot Phil?\" storyline in 2001 that attracted over 19 million viewers and was one of the biggest successes in British soap television, the \"Who Killed Archie?\" story, which was revealed in a special live episode of the show that drew a peak of 17 million viewers, and \"Who Killed Lucy Beale?\".\n\nThe exterior set for the fictional Albert Square is located in the permanent backlot of the BBC Elstree Centre, Borehamwood, Hertfordshire, at , and is outdoors and open to the weather. The \"EastEnders\" lot was designed by Keith Harris, who was a senior designer within the production team together with supervising art directors Peter Findley and Gina Parr. The main buildings on the square consisted originally of hollow shells, constructed from marine plywood facades mounted onto steel frames. The lower walls, pavements, etc., were constructed of real brick and tarmac. The set had to be made to look as if it had been standing for years. This was done by a number of means, including chipping at the buildings with pickaxes. The walls were intentionally built crooked to give them an aged appearance. The drains around the set are real so rain water can naturally flow from the streets. The square was built in two phases with only three sides being built, plus Bridge Street, to begin with in 1984, in time to be used for the show's first episode. Then in 1986, Harris added an extension to the set, building the fourth side of Albert Square, and in 1987, Turpin Road was added, which included buildings such as The Dagmar. \n\nIn 1993, George Street was added, and soon after Walford East tube station was built, to create further locations when \"EastEnders\" went from two to three episodes per week. The set was constructed by the BBC in house construction department under construction manager Mike Hagan. The initial build took 6 months to complete. Most of the buildings on Albert Square have no interior filming space, with a few exceptions, and most do not have rears or gardens. Most areas by the front (and sometimes back) doors are decorated and dressed to match the interior set to allow shots of doors being opened. The grocery shop was originally open fronted, it was turned into a closed front shop, with removable interior walls to allow for filming inside the shop when the set was expanded in 1987. Some interior shots are filmed in the actual buildings, and the café also has some interior decoration so some limited filming can take place by the door. The newer exterior sets including fish & chip shop, video shop and beauty salon had some interior filming space to create a greater sense of realism. As the show is filmed up to six weeks in advance, the trees need to have extra leaves stuck on them during the spring to make them look like they would in summer.\n\nIn February 2008 it was reported that Albert Square would transfer to Pinewood Studios in Buckinghamshire, where a new set would be built as the current set was looking \"shabby\", with its flaws showing up on high-definition television broadcasts. However, by April 2010 a follow-up report confirmed that Albert Square would remain at Elstree Studios for at least another four years, taking the set through its 25th anniversary. The set was consequently rebuilt for high definition on the same site, using mostly real brick with some areas using a new improved plastic brick. Throughout rebuilding filming would still take place, and so scaffolding was often seen on screen during the process, with some storylines written to accommodate the rebuilding, such as the Queen Vic fire.\n\nIn 2014, then executive producer Dominic Treadwell-Collins said that he wanted Albert Square to look like a real-life east London neighbourhood so that the soap would \"better reflect the more fashionable areas of east London beloved of young professionals\" giving a flavour of the \"creeping gentrification\" of east London. He added, \"It should feel more like London. It's been frozen in aspic for too long.\" The BBC announced that they would rebuild the \"EastEnders\" set, to secure the long-term future of the show, with completion expected to be in 2018. The set will provide a modern, upgraded exterior filming resource for \"EastEnders\", and will copy the appearance of the existing buildings. However, it will be 20% bigger than the current set, in order to enable greater editorial ambition and improve working conditions for staff. A temporary set will be created on site to enable filming to continue while the permanent structure is rebuilt. As of May 2016, the rebuild has been delayed until 2020 and will cost in excess of £15 million, although the main part of the set is scheduled to be able to start filming in May 2019.\n\nThe majority of \"EastEnders\" episodes are filmed at the BBC Elstree Centre in Borehamwood, Hertfordshire. When the number of episodes was increased to four per week, more studio space was needed, so \"Top of the Pops\" was moved from its studio at Elstree to BBC Television Centre in April 2001. Episodes are produced in \"quartets\" of four episodes, each of which starts filming on a Tuesday and takes nine days to record. Each day, between 25 and 30 scenes are recorded. During the filming week, actors can film for as many as eight to twelve episodes. Exterior scenes are filmed on a specially constructed film lot, and interior scenes take place in four studios. The episodes are usually filmed about six to eight weeks in advance of broadcast. During the winter period, filming can take place up to twelve weeks in advance, due to less daylight for outdoor filming sessions. This time difference has been known to cause problems when filming outdoor scenes. On 8 February 2007, heavy snow fell on the set and filming had to be cancelled as the scenes due to be filmed on the day were to be transmitted in April. \"EastEnders\" is normally recorded using four cameras. When a quartet is completed, it is edited by the director, videotape editor and script supervisor. The producer then reviews the edits and decides if anything needs to be re-edited, which the director will do. A week later, sound is added to the episodes and they are technically reviewed, and are ready for transmission if they are deemed of acceptable quality.\n\nAlthough episodes are predominantly recorded weeks before they are broadcast, occasionally, \"EastEnders\" includes current events in their episodes. In 1987, \"EastEnders\" covered the general election. Using a plan devised by co-creators Smith and Holland, five minutes of material was cut from four of the pre-recorded episodes preceding the election. These were replaced by specially recorded election material, including representatives from each major party, and a scene recorded on the day after the election reflecting the result, which was broadcast the following Tuesday. The result of the 2010 general election was referenced in the 7 May 2010 episode. During the 2006 FIFA World Cup, actors filmed short scenes following the tournament's events that were edited into the programme in the following episode. Last-minute scenes have also been recorded to reference the fiftieth anniversary of the end of the Second World War in 1995, the two-minute silence on Remembrance Day 2005 (2005 also being the year for the sixtieth anniversary of the end of the Second World War and the 200th anniversary of the Battle of Trafalgar), Barack Obama's election victory in 2008, the death of Michael Jackson in 2009, the 2010 Comprehensive Spending Review, Andy Murray winning the Men's Singles at the 2013 Wimbledon Championships, the wedding of Prince William and Kate Middleton, the birth of Prince George of Cambridge., Scotland voting no against independence in 2014, and the 100th anniversary of the beginning of the Great War.\n\n\"EastEnders\" is often filmed on location, away from the studios in Borehamwood. Sometimes an entire quartet is filmed on location, which has a practical function and are the result of \"EastEnders\" making a \"double bank\", when an extra week's worth of episodes are recorded at the same time as the regular schedule, enabling the production of the programme to stop for a two-week break at Christmas. These episodes often air in late June or early July and again in late October or early November. The first time this happened was in December 1985 when Pauline (Wendy Richard) and Arthur Fowler (Bill Treacher) travelled to the Southend-on-Sea to find their son Mark, who had run away from home. In 1986, \"EastEnders\" filmed overseas for the first time, in Venice, and this was also the first time it was not filmed on videotape, as a union rule at the time prevented producers taking a video crew abroad and a film crew had to be used instead.\n\nIf scenes during a normal week are to be filmed on location, this is done during the normal recording week. Off-set locations that have been used for filming include Clacton (1989), Devon (September 1990), Hertfordshire (used for scenes set in Gretna Green in July 1991), Portsmouth (November 1991), Milan (1997), Ireland (1997), Amsterdam (December 1999), Brighton (2001) and Portugal (2003). In 2003, filming took place at Loch Fyne Hotel and Leisure Club in Inveraray, The Arkinglass Estate in Cairndow and Grims Dyke Hotel, Harrow Weald, north London, for a week of episodes set in Scotland. 9 April 2007 episode featured scenes filmed at St Giles Church and The Blacksmiths Arms public house in Wormshill, the Ringlestone Inn, two miles away and Court Lodge Farm in Stansted, Kent. Other locations have included the court house, a disused office block, Evershed House, and St Peter's Church, all in St Albans, an abandoned mental facility in Worthing, Carnaby Street in London, and a wedding dress shop in Muswell Hill, north London. A week of episodes in 2011 saw filming take place on a beach in Thorpe Bay and a pier in Southend-on-Sea—during which a stuntman was injured when a gust of wind threw him off balance and he fell onto rocks— with other scenes filmed on the Essex coast. In 2012, filming took place in Keynsham, Somerset. In January 2013, on-location filming at Grahame Park in Colindale, north London, was interrupted by at least seven youths who threw a firework at the set and threatened to cut members of the crew. In October 2013, scenes were filmed on a road near London Southend Airport in Essex.\n\nThe two-handers (when only two actors appear in an episode) were originally done for speed; while a two-hander is being filmed, the rest of the cast can be making another episode.\n\n\"EastEnders\" has featured seven live broadcasts. For its 25th anniversary in February 2010, a live episode was broadcast in which Stacey Slater (Lacey Turner) was revealed as Archie Mitchell's (Larry Lamb) killer. Turner was told only 30 minutes before the live episode and to maintain suspense, she whispers this revelation to former lover and current father-in-law, Max Branning, in the very final moments of the live show. Many other cast members only found out at the same time as the public, when the episode was broadcast. On 23 July 2012, a segment of that evening's episode was screened live as Billy Mitchell (Perry Fenwick) carried the Olympic Flame around Walford in preparation for the 2012 Summer Olympics. In February 2015, for the soap's 30th anniversary, five episodes in a week featured live inserts throughout them. Episodes airing on Tuesday 17, Wednesday 18 and Thursday 19 (which featured an hour long episode and a second episode) all featured at least one live insert. The show revealed that the killer of Lucy Beale (Hetti Bywater) was her younger brother, Bobby (Eliot Carrington), during the second episode on Thursday, after a ten month mystery regarding who killed her. In a flashback episode which revisited the night of the murder, Bobby was revealed to have killed his sister. The aftermath episode, which aired on Friday 20, was completely live and explained in detail Lucy's death. Carrington was told he was Lucy's killer on Monday 16, while Laurie Brett (who plays Bobby's adoptive mother, Jane) was informed in November, due to the character playing a huge role in the cover-up of Lucy's murder. Bywater only discovered Bobby was responsible for Lucy's death on the morning of Thursday 19, several hours before they filmed the scenes revealing Bobby as Lucy's killer.\n\nEach episode should run for 27 minutes and 15 seconds, however, if any episode runs over or under then it is the job of post-production to cut or add scenes where appropriate. As noted in the 1994 behind-the-scenes book, \"EastEnders: The First 10 Years\", After filming, tapes were sent to the videotape editor, who then edited the scenes together into an episode. The videotape editor used the director's notes so they knew which scenes the director wanted to appear in a particular episode. The producer might have asked for further changes to be made. The episode was then copied onto D3 video. The final process was to add the audio which included background noise such as a train or a jukebox music and to check it met the BBC's technical standard for broadcasting.\n\nSince 2010, \"EastEnders\" no longer uses tapes in the recording or editing process. After footage is recorded, the material is sent digitally to the post production team. The editors then assemble all the scenes recorded for the director to view and note any changes that are needed. The sound team also have the capability to access the edited episode, enabling them to dub the sound and create the final version.\n\nAccording to the book \"How to Study Television\", in 1995 \"EastEnders\" cost the BBC £40,000 per episode on average. A 2012 agreement between the BBC, the Writers' Guild of Great Britain and the Personal Managers' Association set out the pay rate for \"EastEnders\" scripts as £137.70 per minute of transmission time (£4,131 for 30 minutes), which is 85% of the rate for scripts for other BBC television series. The writers would be paid 75% of that fee for any repeats of the episode. In 2011, it was reported that actors receive a per-episode fee of between £400 and £1,200, and are guaranteed a certain number of episodes per year, perhaps as few as 30 or as many as 100, therefore annual salaries could range from £12,000 to £200,000 depending on the popularity of a character. Some actors' salaries were leaked in 2006, revealing that Natalie Cassidy (Sonia Fowler) was paid £150,000, Cliff Parisi (Minty Peterson) received £220,000, Barbara Windsor (Peggy Mitchell) and Steve McFadden (Phil Mitchell) were both on £360,000 and Wendy Richard (Pauline Fowler) had a salary of £370,000.\n\nA 2011 report from the National Audit Office showed that \"EastEnders\" had an annual budget of £29.9 million. Of that, £2.9 million was spent on scripts and £6.9 million went towards paying actors, extras and chaperones for child actors. According to National Audit Office, BBC executives approved £500,000 of additional funding for the 25th anniversary live episode (19 February 2010). With a total cost of £696,000, the difference was covered from the 2009–2010 series budget for \"EastEnders\".\n\nIn 2014, two new studios were built and they were equipped with low energy lighting which has saved approximately 90,000 kwh per year. A carbon literacy course was ran with Heads of Departments of \"EastEnders\" attending and as a result, representatives from each department agreed to meet quarterly to share new sustainability ideas. The paper usage was reduced by 50% across script distribution and other weekly documents and 20% across all other paper usage. The production team now use recycled paper and recycled stationery.\n\nAlso changes to working online has also saved transportation cost of distribution 2,500 DVDs per year. Sets, costumes, paste pots and paint are all recycled by the design department. Cars used by the studio are low emission vehicles and the production team take more efficient energy efficient generators out on location. Caterers no longer use polystyrene cups and recycling on location must be provided. \n\nAs a result of \"EastEnders\" sustainability, it was awarded albert+, an award that recognises the production's commitment to becoming a more eco-friendly television production. The albert+ logo was first shown at the end of the \"EastEnders\" titles for episode 5281 on 9 May 2016.\n\nSince 1985, \"EastEnders\" has remained at the centre of BBC One's primetime schedule. It is currently broadcast at 7.30pm on Tuesday and Thursday, and 8pm on Monday and Friday. \"EastEnders\" was originally broadcast twice weekly at 7 pm on Tuesdays and Thursdays from 19 February 1985, with an omnibus at 3:30 pm on Sundays; however, in September 1985 the two episodes were moved to 7:30 pm as Michael Grade did not want the soap running in direct competition with \"Emmerdale Farm\", and this remained the same until 7 April 1994. The BBC had originally planned to take advantage of the 'summer break' that \"Emmerdale Farm\" usually took to capitalise on ratings, but ITV1 added extra episodes and repeats so that \"Emmerdale Farm\" was not taken off the air over the summer. Realising the futility of the situation, Grade decided to move the show to the later 7:30 pm slot, but to avoid tabloid speculation that it was a 'panic move' on the BBC's behalf, they had to \"dress up the presentation of that move in such a way as to protect the show\" giving \"all kinds of reasons\" for the move.\n\n\"EastEnders\" output then increased to three times a week on Mondays, Tuesday and Thursdays from 11 April 1994 until 2 August 2001.\nFrom 10 August 2001, \"EastEnders\" then added its fourth episode (shown on Fridays). This caused some controversy as it clashed with \"Coronation Street\", which at the time was moved to 8pm to make way for an hour-long episode of rural soap \"Emmerdale\" at 7pm. The move immediately provoked an angry response from ITV insiders, who argued that the BBC's last-minute move—only revealed at 3.30pm on the day—broke an unwritten scheduling rule that the two flagship soaps would not be put directly against each other. In this first head-to-head battle, \"EastEnders\" claimed victory over its rival.\n\nIn early 2003, viewers could watch episodes of \"EastEnders\" on digital channel BBC Three before they were broadcast on BBC One. This was to coincide with the relaunch of the channel and helped BBC Three break the one million viewers mark for the first time with 1.03 million who watched to see Mark Fowler's departure. EastEnders was repeated each evening at 22.30 on BBC Three – having previously been shown on that channel at 22.00. When BBC Three moved online in February 2016, the W channel took over the repeat broadcastings. As of 2006 according to the EastEnders website there are on average around 208 episodes outputted each year.\n\n, episodes of \"EastEnders\" are repeated on BBC Three and are available on-demand through BBC iPlayer for thirty days after their original screening. \"EastEnders\" was regularly repeated at 10pm on BBC Choice since the channel's launch in 1998, a practice continued by BBC Three for many years until mid-2012 with the repeat moving to 10.30pm. From 25 December 2010 to 29 April 2011 the show was repeated on BBC HD in a Simulcast with BBC Three. In 2015, the BBC Three repeat moved back to 10pm. On 24 April 2015 the last omnibus episode was broadcast and it was not until February 2016, the repeat moved to W, the rebranded Watch, after BBC Three became an online-only channel.\n\nThe omnibus edition, a compilation of the week's episodes in a continuous sequence, originally aired on BBC One on Sunday afternoons, until 1 April 2012 when it was changed to a late Friday night or early Saturday morning slot, commencing 6 April 2012, though the exact time differed. It reverted to a weekend daytime slot as from January 2013 on BBC Two. In 2014, the omnibus moved back to around midnight on Friday nights, and in April 2015, the omnibus was axed, following detailed audience research and the introduction of 30-day catch up on BBC iPlayer and the planning of BBC One +1. When W takes over the same-day repeat of \"EastEnders\", the will also show a weekend omnibus.\n\nFrom 20 February to 26 May 1995, as part of the programme's 10th Anniversary celebrations, episodes from 1985 were repeated each morning at 10am, starting from episode one. Four specially selected episodes from 1985 and 1986 were also repeated on BBC1 on Friday evenings at 8.00pm under the banner \"The Unforgettable EastEnders\". These included The wedding of Michelle Fowler and Lofty Holloway, The revelation of the father of Michelle's baby, a two-hander between Dot Cotton and Ethel Skinner and the 1986 Christmas episode featuring Den Watts presenting Angie Watts with divorce papers.\n\n\"EastEnders\" reruns began on UKTV Gold when the channel launched in 1992. The series ran from the first episode and followed its original broadcast order until August 1996 when the channel looped back to the first episode. In October 2008 UKTV Gold ceased showing \"EastEnders\". The last episode shown was from January 2006. Watch launched in October 2008, and they briefly revived the \"EastEnders\" reruns from 5 January 2009 to 24 April 2009, finishing with episodes originally broadcast in June 2006.\n\nOn 1 December 2012, the BBC uploaded the first 54 episodes of \"EastEnders\" to YouTube, and on 23 July 2013 they uploaded a further 14 episodes bringing the total to 68.\n\n\"EastEnders\" is broadcast around the world in many English-speaking countries. New Zealand became the first to broadcast EastEnders overseas, the first episode went 27 September 1985. Followed by Holland 8 December 1986, Australia 5 January 1987, Norway 27 April, Barcelona 30 June with Catalan dub. On 9 July 1987 it was announced that the show would be aired in the United States and would be broadcast on PBS. BBC Worldwide licensed 200 hours of EastEnders to for broadcast in Serbia on RTS (dubbed into Serbian), it began airing the first episode December 1997. \n\nIt is shown on BBC Entertainment (formerly BBC Prime) in Europe and in Africa, where it is approximately six episodes behind the UK. It was also shown on BBC Prime in Asia, but when the channel was replaced by BBC Entertainment, it ceased showing the series. In Canada, \"EastEnders\" was shown on BBC Canada until 2010, at which point it was picked up by VisionTV.\n\nIn Ireland, \"EastEnders\" was shown on TV3 from September 1998 until March 2001, when it moved over to RTÉ One, after RTÉ lost the rights to air rival soap \"Coronation Street\" to TV3. The series is simulcast with BBC One, which is widely available in the Republic, but carries advertising since its 1998 debut on Irish TV.\n\nHM Forces and their families stationed overseas can watch \"EastEnders\" on BBC One, carried by the British Forces Broadcasting Service, which is also available to civilians in the Falkland Islands and Tristan da Cunha. It was previously shown on BFBS1.\n\nEastEnders is currently shown on BBC Entertainment in Central Europe on weekdays at 17:30 CET, having previously been shown on BBC Prime, BBC World Service Television and BBC TV Europe since the 1980s. In Sweden, Denmark, Finland and Norway, the show is now cancelled but was for decades shown on BBC Nordic channels with local subtitles.\n\nThe series was broadcast in the United States until BBC America ceased broadcasts of the serial in 2003, amidst fan protests. In June 2004, the Dish Network satellite television provider picked up \"EastEnders\", broadcasting episodes starting at the point where BBC America had ceased broadcasting them, offering the serial as a pay-per-view item. Episodes air two months behind the UK schedule. Episodes from prior years are still shown on various PBS stations in the US.\n\nThe series was screened in Australia by ABC TV from 1987 until 1991. Currently the series is aired in Australia on Foxtel Pay TV channel BBC UKTV, from Mondays to Thursdays at 6.15 pm EST. It is around 2 weeks behind the UK airings. In New Zealand, it was shown by TVNZ on TVNZ 1 for several years, and then on Prime each weekday afternoon. It is currently shown by BBC UKTV Mondays to Thursdays at 8.00pm. Episodes are about 2 weeks behind the UK.\n\nEastEnders in the United States is available on-demand, 24 hours after it has aired in the United Kingdom via BritBox (a joint venture between BBC and ITV) which launch on 7 March 2017.\n\nIn 1991 the BBC sold the programme's format rights to a Dutch production company IDTV, the programme was renamed \"Het Oude Noorden\" (Translation: Old North). The Dutch version was re-written from already existing EastEnders scripts. The schedule remained the same as EastEnders twice weekly episodes, however some notable changes included the programme is now set in Rotterdam rathan than London, characters are given Dutch names Den and Angie became Ger and Ankie and The Queen Victoria pub is re-named \"Cade Faas\". \n\nAccording to Barbara Jurgen who re-wrote the scripts for a Dutch audience he said \"The power of the show is undeniable. The Scripts are full of hard, sharp drama, plus great one-liners which will translate well to Holland.\" The Dutch version began broadcasting on VARA 13 March 1993 and ran for 20 episodes but was canceled after twenty episodes.\n\nThe 1993 2-part story, entitled Dimensions in Time, was made in collaboration with the cast of the BBC Doctor Who and was filmed partly on the EastEnders set.\n\nIn 1998, \"EastEnders Revealed\" was launched on BBC Choice (now BBC Three). The show takes a look behind the scenes of the \"EastEnders\" and investigates particular places, characters or families within \"EastEnders\". An episode of \"EastEnders Revealed\" that was commissioned for BBC Three attracted 611,000 viewers.\n\nAs part of the BBC's digital push, \"EastEnders Xtra\" was introduced in 2005. The show was presented by Angellica Bell and was available to digital viewers at 8.30pm on Monday nights. It was also shown after the Sunday omnibus. The series went behind the scenes of the show and spoke to some of the cast members. A new breed of behind-the-scenes programmes have been broadcast on BBC Three since 1 December 2006. These are all documentaries related to current storylines in \"EastEnders\", in a similar format to \"EastEnders Revealed\", though not using the \"EastEnders Revealed\" name.\n\nIn October 2009, a 12-part Internet spin-off series entitled \"\" was announced. The series was conceived by executive producer Diederick Santer \"as a way of nurturing new, young talent, both on- and off-screen, and exploring the stories of the soaps' anonymous bystanders.\" \"E20\" features a group of sixth-form characters and targets the \"\"Hollyoaks\" demographic\". It was written by a team of young writers and was shown three times a week on the \"EastEnders\" website from 8 January 2010. A second ten-part series started in September 2010, with twice-weekly episodes available online and an omnibus on BBC Three. A third series of 15 episodes started in September 2011.\n\n\"EastEnders\" and rival soap opera \"Coronation Street\" took part in a crossover episode for Children in Need on 19 November 2010 called \"East Street\".\n\nOn 4 April 2015, \"EastEnders\" confirmed plans for a BBC One series featuring Kat and Alfie Moon. The six-part drama, \"Redwater\", was created by executive producer Dominic Treadwell-Collins and his team. In the spin-off, the Moons will leave Walford and visit Ireland where they \"search for answers to some very big questions.\"\n\n, BBC Store has released 553 \"EastEnders\" episodes from various years, including the spin-off episode \"CivvyStreet\", available to buy as digital downloads.\n\nAn example of \"EastEnders\" popularity is that after episodes, electricity use in the United Kingdom rises significantly as viewers who have waited for the show to end begin boiling water for tea, a phenomenon known as TV pickup. Over five minutes, power demand rises by three GW, the equivalent of 1.5 to 1.75 million teakettles. National Grid personnel watch the show to know when closing credits begin so they can prepare for the surge, asking for additional power from France if necessary.\n\n\"EastEnders\" is the BBC's most consistent programme in terms of ratings. It has proved highly popular and Appreciation Indexes reflected this, rising from 55–60 at the launch to 85–95 later on, a figure which was nearly ten points higher than the average for a British soap opera. Research suggested that people found the characters true to life, the plots believable and, importantly in the face of criticism of the content, people watched as a family and regarded it as viewing for all the family. Based on market research by BBC commissioning in 2003, \"EastEnders\" is most watched by 60- to 74-year-olds, closely followed by 45- to 59-year-olds. An average \"EastEnders\" episode attracts a total audience share between 35 and 40%. The same-day repeat showing on BBC Three attracted an average of 500,000 viewers, whilst the Sunday omnibus generally attracted 3 million. \"EastEnders\" is one of the more popular programmes on British television and while the show's ratings have fallen since its initial surge in popularity and the advent of multichannel digital television, the programme continues to be successful for the BBC. \"EastEnders\" two main rivals are ITV soaps \"Coronation Street\" and \"Emmerdale\".\nThe launch show in 1985 attracted 17.35 million viewers. 25 July 1985 was the first time the show's viewership rose to first position in the weekly top ten shows for BBC One. The highest rated episode of \"EastEnders\" is the Christmas Day 1986 episode, which attracted a combined 30.15 million viewers who tuned into either the original transmission or the omnibus to see Den Watts hand over divorce papers to his wife Angie. This remains the highest rated episode of a soap in British television history.\n\nIn 2001, \"EastEnders\" clashed with \"Coronation Street\" for the first time. \"EastEnders\" won the battle with 8.4 million viewers (41% share) whilst \"Coronation Street\" lagged behind with 7.3 million viewers (34% share). On 21 September 2004, Louise Berridge, the then executive producer, quit following criticism of the show. The following day the show received its lowest ever ratings at that time (6.2 million) when ITV scheduled an hour-long episode of \"Emmerdale\" against it. \"Emmerdale\" was watched by 8.1 million people. The poor ratings motivated the press into reporting viewers were bored with implausible and ill-thought-out storylines. Following this, the ratings gradually rose under new executive producers. However, \"EastEnders\" and \"Emmerdale\" continued to clash at times, and \"Emmerdale\" tended to come out on top, giving \"EastEnders\" lower than average ratings. In 2006, \"EastEnders\" regularly attracted between 8 and 12 million viewers in official ratings. \"EastEnders\" received its second lowest ratings on 17 May 2007, when 4.0 million viewers tuned in. This was also the lowest ever audience share, with just 19.6%. This was attributed to a conflicting one hour special episode of \"Emmerdale\" on ITV1. However, ratings for the 10pm \"EastEnders\" repeat on BBC Three reached an all-time high of 1.4 million. However, there have been times when \"EastEnders\" had higher ratings than \"Emmerdale\" despite the two going head-to-head.\n\n\"EastEnders\" ratings remained consistent until the mid-2000s decade, with the emergence of reality TV, with overnight ratings in September 2005 falling to 6.6 million, though it since recovered. The ratings increased in 2010, thanks to the \"Who Killed Archie?\" storyline and second wedding of Ricky Butcher and Bianca Jackson, and the show's first live episode on 19 February 2010. The live-episode averaged 15.6 million viewers, peaking at 16.6 million in the final five minutes of broadcast. In January 2010, the average audience was higher than that of \"Coronation Street\" for the first time in three years. During the 30th anniversary week in which there were live elements and the climax of the Who Killed Lucy Beale? storyline, 10.84 million viewers tuned in for the 30th anniversary episode itself in an hour long special on 19 February 2015 (peaking with 11.9 million). Later on in the same evening, a special flashback episode averaged 10.3 million viewers, and peaked with 11.2 million. The following day, the anniversary week was rounded off with another fully live episode (the second after 2010) with 9.97 million viewers watching the aftermath of the reveal, the Beale family finding out the truth of Lucy's killer and deciding to keep it a secret.\n\n\"EastEnders\" is the most complained about programme on the BBC. It has received both praise and criticism for most of its storylines, which have dealt with difficult themes, such as violence, rape, murder and child abuse.\n\nMary Whitehouse, social critic, argued at the time that \"EastEnders\" represented a violation of \"family viewing time\" and that it undermined the watershed policy. She regarded \"EastEnders\" as a fundamental assault on the family and morality itself. She made reference to representation of family life and emphasis on psychological and emotional violence within the show. She was also critical of language such as \"bleeding\", \"bloody hell\", \"bastard\" and \"for Christ's sake\". However, Whitehouse also praised the programme, describing Michelle Fowler's decision not to have an abortion as a \"very positive storyline\". She also felt that \"EastEnders\" had been cleaned up as a result of her protests, though she later commented that \"EastEnders\" had returned to its old ways. Her criticisms were widely reported in the tabloid press as ammunition in its existing hostility towards the BBC. The stars of \"Coronation Street\" in particular aligned themselves with Mary Whitehouse, gaining headlines such as \"STREETS AHEAD! RIVALS LASH SEEDY EASTENDERS\" and \"CLEAN UP SOAP! Street Star Bill Lashes 'Steamy' EastEnders\".\n\n\"EastEnders\" has been criticised for being too violent, most notably during a domestic violence storyline between Little Mo Morgan and her husband Trevor Morgan. As \"EastEnders\" is shown pre-watershed, there were worries that some scenes in this storyline were too graphic for its audience. Complaints against a scene in which Little Mo's face was pushed in gravy on Christmas Day were upheld by the Broadcasting Standards Council. However, a helpline after this episode attracted over 2000 calls. Erin Pizzey, who became internationally famous for having started one of the first women's refuges, said that \"EastEnders\" had done more to raise the issue of violence against women in one story than she had done in 25 years. The character of Phil Mitchell (played by Steve McFadden since early 1990) has been criticised on several occasions for glorifying violence and proving a bad role model to children. On one occasion following a scene in an episode broadcast in October 2002, where Phil brutally beat his godson, Jamie Mitchell (Jack Ryder), 31 complaints came from viewers who watched the scenes.\n\nIn 2003, cast member Shaun Williamson, who was in the final months of his role of Barry Evans, said that the programme had become much grittier over the past ten to fifteen years, and found it \"frightening\" that parents let their young children watch.\n\nIn 2005, the BBC was accused of anti-religious bias by a House of Lords committee, who cited \"EastEnders\" as an example. Dr. Indarjit Singh, editor of the Sikh Messenger and patron of the World Congress of Faiths, said: \"\"EastEnders\" Dot Cotton is an example. She quotes endlessly from the Bible and it ridicules religion to some extent.\" In July 2010, complaints were received following the storyline of Christian minister Lucas Johnson committing a number of murders that he believed was his duty to God, claiming that the storyline was offensive to Christians.\n\nIn 2008, \"EastEnders\", along with \"Coronation Street\", was criticised by Martin McGuinness, then Northern Ireland's deputy first minster, for \"the level of concentration around the pub\" and the \"antics portrayed in the [...] Queen Vic\".\n\nIn 1997 several episodes were shot and set in Ireland, resulting in criticisms for portraying the Irish in a negatively stereotypical way. Ted Barrington, the Irish Ambassador to the UK at the time, described the portrayal of Ireland as an \"unrepresentative caricature\", stating he was worried by the negative stereotypes and the images of drunkenness, backwardness and isolation. Jana Bennett, the BBC's then director of production, later apologised for the episodes, stating on BBC1's news bulletin: \"It is clear that a significant number of viewers have been upset by the recent episodes of \"EastEnders\", and we are very sorry, because the production team and programme makers did not mean to cause any offence.\" A year later BBC chairman Christopher Bland admitted that as result of the Irish-set EastEnders episodes, the station failed in its pledge to represent all groups accurately and avoid reinforcing prejudice.\n\nIn 2008, the show was criticised for stereotyping their Asian and Black characters, by having a black single mum, Denise Wicks, and an Asian shopkeeper, Zainab Masood. There has been criticism that the programme does not authentically portray the ethnic diversity of the population of East London, with the programme being 'twice as white' as the real East End.\n\nSome storylines have provoked high levels of viewer complaints. In August 2006, a scene involving Carly Wicks (Kellie Shirley) and Jake Moon (Joel Beckett) having sex on the floor of Scarlet nightclub, and another scene involving Owen Turner violently attacking Denise Fox, prompted 129 and 128 complaints, respectively. Carly and Jake's sex scenes were later removed from the Sunday omnibus edition. The showdown of Rob, Dawn and May's storyline where May stated to Dawn she could give her an elective caesarean (Dawn being handcuffed to the bed) prompted 200 complaints.The 2007 domestic abuse storyline involving Ben Mitchell and Stella Crawford attracted sixty complaints from viewers, who found scenes where Ben was attacked by bullies as Stella looked on \"upsetting\". \n\nIn March 2008, scenes showing Tanya Branning (Jo Joyner) and boyfriend, Sean Slater (Rob Kazinsky), burying Tanya's husband Max (Jake Wood) alive, attracted many complaints. The UK communications regulator Ofcom later found that the episodes depicting the storyline were in breach of the 2005 Broadcasting Code. They contravened the rules regarding protection of children by appropriate scheduling, appropriate depiction of violence before the 9 p.m. watershed and appropriate depiction of potentially offensive content. In September 2008, \"EastEnders\" began a grooming and paedophilia storyline involving characters Tony King (Chris Coghill), Whitney Dean (Shona McGarty), Bianca Jackson (Patsy Palmer), Lauren Branning (Madeline Duggan) and Peter Beale (Thomas Law). The storyline attracted over 200 complaints . \n\nIn December 2010, Ronnie swapped her newborn baby, who died in cot, with Kat Moon's living baby. Around 3,400 complaints were received, with viewers branding the storyline \"insensitive\", \"irresponsible\" and \"desperate\". Roz Laws from the \"Sunday Mercury\" called the plot \"shocking and ridiculous\" and asked \"are we really supposed to believe that Kat won't recognise that the baby looks different?\" The Foundation for the Study of Infant Deaths (FSID) praised the storyline, and its director Joyce Epstein explained, \"We are very grateful to \"EastEnders\" for their accurate depiction of the devastating effect that the sudden death of an infant can have on a family. We hope that this story will help raise the public's awareness of cot death, which claims 300 babies' lives each year.\" By 7 January, that storyline had generated the most complaints in show history: the BBC received about 8,500 complaints, and media regulator Ofcom received 374. Despite the controversy however, \"EastEnders\" pulled in rating highs of 9–10 million throughout the duration of the storyline. \n\nIn October 2014, the BBC defended a storyline, after receiving 278 complaints about the 6 October 2014 episode where pub landlady Linda Carter was raped. On 17 November 2014 it was announced that Ofcom will investigate over the storyline. On 5 January 2015, the investigation was cleared by Ofcom. A spokesman of Ofcom said: \"After carefully investigating complaints about this scene, Ofcom found the BBC took appropriate steps to limit offence to viewers. This included a warning before the episode and implying the assault, rather than depicting it. Ofcom also took into account the programme's role in presenting sometimes challenging or distressing social issues.\"\n\nIn 2010, \"EastEnders\" came under criticism from the police for the way that they were portrayed during the \"Who Killed Archie?\" storyline. During the storyline, DCI Jill Marsden and DC Wayne Hughes talk to locals about the case and Hughes accepts a bribe. The police claimed that such scenes were \"damaging\" to their reputation and added that the character DC Deanne Cunningham was \"irritatingly inaccurate\". In response to the criticism, \"EastEnders\" apologised for offending real-life detectives and confirmed that they use a police consultant for such storylines. \n\nIn October 2012, a storyline involving Lola Pearce, forced to hand over her baby Lexi Pearce, was criticised by the charity The Who Cares? Trust, who called the storyline an \"unhelpful portrayal\" and said it had already received calls from members of the public who were \"distressed about the \"EastEnders\" scene where a social worker snatches a baby from its mother's arms\". The scenes were also condemned by the British Association of Social Workers (BASW), calling the BBC \"too lazy and arrogant\" to correctly portray the child protection process, and saying that the baby was taken \"without sufficient grounds to do so\". Bridget Robb, acting chief of the BASW, said the storyline provoked \"real anger among a profession well used to a less than accurate public and media perception of their jobs .. \"EastEnders\" shabby portrayal of an entire profession has made a tough job even tougher.\"\n\nSince its premiere in 1985, \"EastEnders\" has had a large impact on British popular culture. It has frequently been referred to in many different media, including songs and television programmes.\n\nMany books have been written about \"EastEnders\". Notably, from 1985 to 1988, author and television writer Hugh Miller wrote seventeen novels, detailing the lives of many of the show's original characters before 1985, when events on screen took place.\n\nKate Lock also wrote four novels centred on more recent characters; Steve Owen, Grant Mitchell, Bianca Jackson and Tiffany Mitchell. Lock also wrote a character guide entitled \"Who's Who in EastEnders\" (ISBN 978-0-563-55178-2) in 2000, examining main characters from the first fifteen years of the show.\n\nShow creators Julia Smith and Tony Holland also wrote a book about the show in 1987, entitled \"EastEnders: The Inside Story\" (ISBN 978-0-563-20601-9), telling the story of how the show made it to screen. Two special anniversary books have been written about the show; \"EastEnders: The First 10 Years: A Celebration\" (ISBN 978-0-563-37057-4) by Colin Brake in 1995 and \"EastEnders: 20 Years in Albert Square\" (ISBN 978-0-563-52165-5) by Rupert Smith in 2005.\n\n\n\n", "id": "9995", "title": "EastEnders"}
{"url": "https://en.wikipedia.org/wiki?curid=9996", "text": "Embroidery\n\nEmbroidery is the handicraft of decorating fabric or other materials with needle and thread or yarn. Embroidery may also incorporate other materials such as, pearls, beads, quills, and sequins. Today, embroidery is most often seen on caps, hats, coats, blankets, dress shirts, denim, stockings, and golf shirts. Embroidery is available with a wide variety of thread or yarn color.\n\nThe basic techniques or stitches on surviving examples of the earliest embroidery—chain stitch, buttonhole or blanket stitch, running stitch, satin stitch, cross stitch—remain the fundamental techniques of hand embroidery today.\n\nThe process used to tailor, patch, mend and reinforce cloth fostered the development of sewing techniques, and the decorative possibilities of sewing led to the art of embroidery.\nIndeed, the remarkable stability of basic embroidery stitches has been noted:\n\nThe art of embroidery has been found world-wide and several early examples have been found. Works in China have been dated to the Warring States period (5th–3rd century BC). In a garment from Migration period Sweden, roughly 300–700 AD, the edges of bands of trimming are reinforced with running stitch, back stitch, stem stitch, tailor's buttonhole stitch, and whip-stitching, but it is uncertain whether this work simply reinforced the seams or should be interpreted as decorative embroidery.\n\nDepending on time, location and materials available, embroidery could be the domain of a few experts or a wide-spread, popular technique. This flexibility led to a variety of works, from the royal to the mundane.\n\nElaborately embroidered clothing, religious objects, and household items often were seen as a mark of wealth and status, as in the case of Opus Anglicanum, a technique used by professional workshops and guilds in medieval England. In 18th century England and its colonies, samplers employing fine silks were produced by the daughters of wealthy families. Embroidery was a skill marking a girl's path into womanhood as well as conveying rank and social standing.\n\nConversely, embroidery is also a folk art, using materials that were accessible to nonprofessionals. Examples include Hardanger from Norway, Merezhka from Ukraine, Mountmellick embroidery from Ireland, Nakshi kantha from Bangladesh and West Bengal, and Brazilian embroidery. Many techniques had a practical use such as Sashiko from Japan, which was used as a way to reinforce clothing.\n\nEmbroidery was an important art in the Medieval Islamic world. The 17th century Turkish traveler Evliya Çelebi called it the \"craft of the two hands\". Because embroidery was a sign of high social status in Muslim societies, it became widely popular. In cities such as Damascus, Cairo and Istanbul, embroidery was visible on handkerchiefs, uniforms, flags, calligraphy, shoes, robes, tunics, horse trappings, slippers, sheaths, pouches, covers, and even on leather belts. Craftsmen embroidered items with gold and silver thread. Embroidery cottage industries, some employing over 800 people, grew to supply these items.\n\nIn the 16th century, in the reign of the Mughal Emperor Akbar, his chronicler Abu al-Fazl ibn Mubarak wrote in the famous Ain-i-Akbari:\n\"His majesty (Akbar) pays much attention to various stuffs; hence Irani, Ottoman, and Mongolian articles of wear are in much abundance especially textiles embroidered in the patterns of \"Nakshi\", \"Saadi\", \"Chikhan\", \"Ari\", \"Zardozi\", \"Wastli\", \"Gota\" and \"Kohra\". The imperial workshops in the towns of Lahore, Agra, Fatehpur and Ahmedabad turn out many masterpieces of workmanship in fabrics, and the figures and patterns, knots and variety of fashions which now prevail astonish even the most experienced travelers. Taste for fine material has since become general, and the drapery of embroidered fabrics used at feasts surpasses every description.\"\n\nThe development of machine embroidery and its mass production came about in stages in the Industrial Revolution. The earliest machine embroidery used a combination of machine looms and teams of women embroidering the textiles by hand. This was done in France by the mid-1800s. The manufacture of machine-made embroideries in St. Gallen in eastern Switzerland flourished in the latter half of the 19th century.\nEmbroidery can be classified according to what degree the design takes into account the nature of the base material and by the relationship of stitch placement to the fabric. The main categories are free or surface embroidery, counted embroidery, and needlepoint or canvas work.\n\nIn free or surface embroidery, designs are applied without regard to the weave of the underlying fabric. Examples include crewel and traditional Chinese and Japanese embroidery.\n\nCounted-thread embroidery patterns are created by making stitches over a predetermined number of threads in the foundation fabric. Counted-thread embroidery is more easily worked on an even-weave foundation fabric such as embroidery canvas, aida cloth, or specially woven cotton and linen fabrics . Examples include cross-stitch and some forms of blackwork embroidery.\nWhile similar to counted thread in regards to technique, in canvas work or needlepoint threads are stitched through a fabric mesh to create a dense pattern that completely covers the foundation fabric. Examples of canvas work include bargello and Berlin wool work.\n\nEmbroidery can also be classified by the similarity of appearance. In drawn thread work and cutwork, the foundation fabric is deformed or cut away to create holes that are then embellished with embroidery, often with thread in the same color as the foundation fabric. When created with white thread on white linen or cotton, this work is collectively referred to as whitework. However, whitework can either be counted or free. Hardanger embroidery is a counted embroidery and the designs are often geometric. Conversely, styles such as Broderie anglaise are similar to free embroidery, with floral or abstract designs that are not dependent on the weave of the fabric.\n\nThe fabrics and yarns used in traditional embroidery vary from place to place. Wool, linen, and silk have been in use for thousands of years for both fabric and yarn. Today, embroidery thread is manufactured in cotton, rayon, and novelty yarns as well as in traditional wool, linen, and silk. Ribbon embroidery uses narrow ribbon in silk or silk/organza blend ribbon, most commonly to create floral motifs.\n\nSurface embroidery techniques such as chain stitch and couching or laid-work are the most economical of expensive yarns; couching is generally used for goldwork. Canvas work techniques, in which large amounts of yarn are buried on the back of the work, use more materials but provide a sturdier and more substantial finished textile.\n\nIn both canvas work and surface embroidery an embroidery hoop or frame can be used to stretch the material and ensure even stitching tension that prevents pattern distortion. Modern canvas work tends to follow symmetrical counted stitching patterns with designs emerging from the repetition of one or just a few similar stitches in a variety of hues. In contrast, many forms of surface embroidery make use of a wide range of stitching patterns in a single piece of work.\n\nContemporary embroidery is stitched with a computerized embroidery machine using patterns digitized with embroidery software. In machine embroidery, different types of \"fills\" add texture and design to the finished work. Machine embroidery is used to add logos and monograms to business shirts or jackets, gifts, and team apparel as well as to decorate household linens, draperies, and decorator fabrics that mimic the elaborate hand embroidery of the past.\n\nThere has also been a development in free hand machine embroidery, new machines have been designed that allow for the user to create free-motion embroidery which has its place in textile arts, quilting, dressmaking, home furnishings and more.\n\nCity and Guilds qualification in Embroidery allows embroiderers to become recognized for their skill. This qualification also gives them the credibility to teach. For example, the notable textiles artist, Kathleen Laurel Sage- Textiles Artist, began her teaching career by getting the City and Guilds Embroidery 1 and 2 qualifications. She has now gone on to write a book on the subject.\n\n", "id": "9996", "title": "Embroidery"}
{"url": "https://en.wikipedia.org/wiki?curid=9997", "text": "Edward Mitchell Bannister\n\nEdward Mitchell Bannister (ca. 1828 – January 9, 1901) was a Black Canadian-American Tonalist painter. Like other Tonalists, his style and predominantly pastoral subject matter were drawn from his admiration for Millet and the French Barbizon School.\n\nBannister was born in St. Andrews, New Brunswick and moved to New England in the late 1840s, where he remained for the rest of his life. While Bannister was well known in the artistic community of his adopted home of Providence, Rhode Island and admired within the wider East Coast art world (he won a bronze medal for his large oil \"Under the Oaks\" at the 1876 Philadelphia Centennial), he was largely forgotten for almost a century for a complexity of reasons, principally connected with racial prejudice.\n\nWith the ascendency of the Civil Rights Movement in the 1970s, his work was again celebrated and collected. In 1978, Rhode Island College dedicated its Art Gallery in Bannister's name with the exhibition: \"Four From Providence ~ Alston, Bannister, Jennings & Prophet\". This event was attended and commented on by numerous notable political figures of the time, and supported by the Rhode Island Committee for Humanities and the Rhode Island Historical Society. Events like this, across the entire cultural landscape, have ensured that his artwork and life will not be again forgotten.\n\nAlthough primarily known for his idealised landscapes and seascapes, Bannister also executed portraits, biblical and mythological scenes, and genre scenes. An intellectual autodidact, his tastes in literature were typical of an educated Victorian painter, including Spenser, Virgil, Ruskin and Tennyson, from whose works much of his iconography can be traced.\n\nBannister died of a heart attack in 1901 while attending a prayer meeting at his church, Elmwood Avenue Free Baptist Church. He is buried in the North Burial Ground in Providence.\n\nThe historian Anne Louise Avery is currently compiling the first Catalogue Raisonné and major biography of Bannister's work. See the International Foundation for Art Research for further details.\n\nE. M. Bannister Gallery at Rhode Island College is named after Bannister.\n\nThe house at 93 Benevolent Street in Providence, where Bannister lived between 1884 and 1899, was owned by Brown University until 2016. Brown renovated the property and restored it to its original appearance, and it was sold to Professor Jeff Huang as part of the Brown to Brown Home Ownership Program.\n\n\n\n", "id": "9997", "title": "Edward Mitchell Bannister"}
{"url": "https://en.wikipedia.org/wiki?curid=10000", "text": "Eiffel\n\nEiffel may refer to:\n\n\n\n", "id": "10000", "title": "Eiffel"}
{"url": "https://en.wikipedia.org/wiki?curid=10002", "text": "Emil Kraepelin\n\nEmil Kraepelin (; 15 February 1856 – 7 October 1926) was a German psychiatrist. H. J. Eysenck's \"Encyclopedia of Psychology\" identifies him as the founder of modern scientific psychiatry, psychopharmacology and psychiatric genetics.\n\nKraepelin was a strong and influential proponent of eugenics and racial hygiene, and believed the chief origin of psychiatric disease to be biological and genetic malfunction. His theories dominated psychiatry at the start of the 20th century and, despite the later psychodynamic influence of Sigmund Freud and his disciples, enjoyed a revival at century's end. While he proclaimed his own high clinical standards of gathering information \"by means of expert analysis of individual cases\", he also drew on reported observations of officials not trained in psychiatry. His textbooks do not contain detailed case histories of individuals but mosaic-like compilations of typical statements and behaviors from patients with a specific diagnosis. He has been described as a \"scientific manager\" and political operator, who developed a large-scale, clinically oriented, epidemiological research programme.\n\nKraepelin, whose father, Karl Wilhelm, was a former opera singer, music teacher, and later successful story teller, was born in 1856 in Neustrelitz, in the Duchy of Mecklenburg-Strelitz in Germany. He was first introduced to biology by his brother Karl, 10 years older and, later, the director of the Zoological Museum of Hamburg.\n\nKraepelin began his medical studies in 1874 at the University of Leipzig and completed them at the University of Würzburg (1877–78). At Leipzig, he studied neuropathology under Paul Flechsig and experimental psychology with Wilhelm Wundt. Kraepelin would be a disciple of Wundt and had a lifelong interest in experimental psychology based on his theories. While there, Kraepelin wrote a prize-winning essay, \"The Influence of Acute Illness in the Causation of Mental Disorders\". At Würzburg he completed his \"Rigorosum\" (roughly equivalent to an MBBS viva-voce examination) in March 1878, his \"Staatsexamen\" (licensing examination) in July 1878, and his \"Approbation\" (his license to practice medicine; roughly equivalent to an MBBS) on 9 August 1878. From August 1878 to 1882, he worked with Bernhard von Gudden at the University of Munich. Returning to the University of Leipzig in February 1882, he worked in Wilhelm Heinrich Erb's neurology clinic and in Wundt's psychopharmacology laboratory. He completed his \"Habilitation\" thesis at Leipzig; it was entitled \"The Place of Psychology in Psychiatry\". On 3 December 1883 he completed his \"\" (habilitation recognition procedure) at Munich.\n\nKraepelin's major work, \"Compendium der Psychiatrie: Zum Gebrauche für Studirende und Aertze\" (\"Compendium of Psychiatry: For the Use of Students and Physicians\"), was first published in 1883 and was expanded in subsequent multivolume editions to \"Ein Lehrbuch der Psychiatrie\" (\"A Textbook: Foundations of Psychiatry and Neuroscience\"). In it, he argued that psychiatry was a branch of medical science and should be investigated by observation and experimentation like the other natural sciences. He called for research into the physical causes of mental illness, and started to establish the foundations of the modern classification system for mental disorders. Kraepelin proposed that by studying case histories and identifying specific disorders, the progression of mental illness could be predicted, after taking into account individual differences in personality and patient age at the onset of disease.\n\nIn 1884 he became senior physician in the Prussian provincial town of Leubus, Silesia Province, and the following year he was appointed director of the Treatment and Nursing Institute in Dresden. On 1 July 1886, at the age of 30, Kraepelin was named Professor of Psychiatry at the University of Dorpat (today the University of Tartu) in what is today Estonia (see Burgmair et al., vol. IV). Four years later, on 5 December 1890, he became department head at the University of Heidelberg, where he remained until 1904. While at Dorpat he became the director of the 80-bed University Clinic. There he began to study and record many clinical histories in detail and \"was led to consider the importance of the course of the illness with regard to the classification of mental disorders\".\n\nIn 1903 Kraepelin moved to Munich to become Professor of Clinical Psychiatry at the University of Munich.\n\nHe was elected a member of the Royal Swedish Academy of Sciences in 1908.\n\nIn 1912 at the request of the German Society of Psychiatry, he began plans to establish a centre for research. Following a large donation from the Jewish German-American banker James Loeb, who had at one time been a patient, and promises of support from \"patrons of science\", the German Institute for Psychiatric Research was founded in 1917 in Munich. Initially housed in existing hospital buildings, it was maintained by further donations from Loeb and his relatives. In 1924 it came under the auspices of the Kaiser Wilhelm Society for the Advancement of Science. The German American Rockefeller family's Rockefeller Foundation made a large donation enabling the development of a new dedicated building for the institute, along Kraepelin's guidelines, which was officially opened in 1928.\n\nKraepelin spoke out against the barbarous treatment that was prevalent in the psychiatric asylums of the time, and crusaded against alcohol, capital punishment and the imprisonment rather than treatment of the insane. He rejected psychoanalytical theories that posited innate or early sexuality as the cause of mental illness, and he rejected philosophical speculation as unscientific. He focused on collecting clinical data and was particularly interested in neuropathology (e.g., diseased tissue).\n\nIn the later period of his career, as a convinced champion of social Darwinism, he actively promoted a policy and research agenda in racial hygiene and eugenics.\n\nKraepelin retired from teaching at the age of 66, spending his remaining years establishing the Institute. The ninth and final edition of his \"Textbook\" was published in 1927, shortly after his death. It comprised four volumes and was ten times larger than the first edition of 1883.\n\nIn the last years of his life he Kraepelin was preoccupied with Buddhist teachings and was planning to visit Buddhist shrines at the time of his death, according to his daughter, Antonie Schmidt-Kraepelin.\n\nKraepelin announced that he had found a new way of looking at mental illness, referring to the traditional view as \"symptomatic\" and to his view as \"clinical\". This turned out to be his paradigm-setting synthesis of the hundreds of mental disorders classified by the 19th century, grouping diseases together based on classification of syndrome—common \"patterns\" of symptoms over time—rather than by simple similarity of major symptoms in the manner of his predecessors. Kraepelin described his work in the 5th edition of his textbook as a \"decisive step from a symptomatic to a clinical view of insanity. . . . The importance of external clinical signs has . . . been subordinated to consideration of the conditions of origin, the course, and the terminus which result from individual disorders. Thus, all purely symptomatic categories have disappeared from the nosology\".\n\nKraepelin is specifically credited with the classification of what was previously considered to be a unitary concept of psychosis, into two distinct forms (known as the Kraepelinian dichotomy):\n\n\nDrawing on his long-term research, and using the criteria of course, outcome and prognosis, he developed the concept of dementia praecox, which he defined as the \"sub-acute development of a peculiar simple condition of mental weakness occurring at a youthful age\". When he first introduced this concept as a diagnostic entity in the fourth German edition of his \"Lehrbuch der Psychiatrie\" in 1893, it was placed among the degenerative disorders alongside, but separate from, catatonia and dementia paranoides. At that time, the concept corresponded by and large with Ewald Hecker's hebephrenia. In the sixth edition of the \"Lehrbuch\" in 1899 all three of these clinical types are treated as different expressions of one disease, dementia praecox.\n\nOne of the cardinal principles of his method was the recognition that any given symptom may appear in virtually any one of these disorders; e.g., there is almost no single symptom occurring in dementia praecox which cannot sometimes be found in manic depression. What distinguishes each disease symptomatically (as opposed to the underlying pathology) is not any particular (pathognomonic) symptom or symptoms, but a specific pattern of symptoms. In the absence of a direct physiological or genetic test or marker for each disease, it is only possible to distinguish them by their specific pattern of symptoms. Thus, Kraepelin's system is a method for pattern recognition, not grouping by common symptoms.\n\nKraepelin also demonstrated specific patterns in the genetics of these disorders and specific and characteristic patterns in their course and outcome. Generally speaking, there tend to be more schizophrenics among the relatives of schizophrenic patients than in the general population, while manic depression is more frequent in the relatives of manic depressives. Though, of course, this does not demonstrate genetic linkage, as this might be a socio-environmental factor as well.\n\nHe also reported a pattern to the course and outcome of these conditions. Kraepelin believed that schizophrenia had a deteriorating course in which mental function continuously (although perhaps erratically) declines, while manic-depressive patients experienced a course of illness which was intermittent, where patients were relatively symptom-free during the intervals which separate acute episodes. This led Kraepelin to name what we now know as schizophrenia, dementia praecox (the dementia part signifying the irreversible mental decline). It later became clear that dementia praecox did not necessarily lead to mental decline and was thus renamed schizophrenia by Eugen Bleuler to correct Kraepelin's misnomer.\n\nIn addition, as Kraepelin accepted in 1920, \"It is becoming increasingly obvious that we cannot satisfactorily distinguish these two diseases\"; however, he maintained that \"On the one hand we find those patients with irreversible dementia and severe cortical lesions. On the other are those patients whose personality remains intact\". Nevertheless, overlap between the diagnoses and neurological abnormalities (when found) have continued, and in fact a diagnostic category of schizoaffective disorder would be brought in to cover the intermediate cases.\n\nKraepelin devoted very few pages to his speculations about the etiology of his two major insanities, dementia praecox and manic-depressive insanity. However, from 1896 to his death in 1926 he held to the speculation that these insanities (particularly dementia praecox) would one day probably be found to be caused by a gradual systemic or \"whole body\" disease process, probably metabolic, which affected many of the organs and nerves in the body but affected the brain in a final, decisive cascade.\n\nIn the first through sixth edition of Kraepelin's influential psychiatry textbook, there was a section on moral insanity, which meant then a disorder of the emotions or moral sense without apparent delusions or hallucinations, and which Kraepelin defined as \"lack or weakness of those sentiments which counter the ruthless satisfaction of egotism\". He attributed this mainly to degeneration. This has been described as a psychiatric redefinition of Cesare Lombroso's theories of the \"born criminal\", conceptualised as a \"moral defect\", though Kraepelin stressed it was not yet possible to recognise them by physical characteristics.\n\nIn fact from 1904 Kraepelin changed the section heading to \"The born criminal\", moving it from under \"Congenital feeble-mindedness\" to a new chapter on \"Psychopathic personalities\". They were treated under a theory of degeneration. Four types were distinguished: born criminals (inborn delinquents), pathological liars, querulous persons, and Triebmenschen (persons driven by a basic compulsion, including vagabonds, spendthrifts, and dipsomaniacs). The concept of \"psychopathic inferiorities\" had been recently popularised in Germany by Julius Ludwig August Koch, who proposed congenital and acquired types. Kraepelin had no evidence or explanation suggesting a congenital cause, and his assumption therefore appears to have been simple \"biologism\". Others, such as Gustav Aschaffenburg, argued for a varying combination of causes. Kraepelin's assumption of a moral defect rather than a positive drive towards crime has also been questioned, as it implies that the moral sense is somehow inborn and unvarying, yet it was known to vary by time and place, and Kraepelin never considered that the moral sense might just be different. Kurt Schneider criticized Kraepelin's nosology for appearing to be a list of behaviors that he considered undesirable, rather than medical conditions, though Schneider's alternative version has also been criticised on the same basis. Nevertheless, many essentials of these diagnostic systems were introduced into the diagnostic systems, and remarkable similarities remain in the DSM-IV and ICD-10. The issues would today mainly be considered under the category of personality disorders, or in terms of Kraepelin's focus on psychopathy.\n\nKraepelin had referred to psychopathic conditions (or \"states\") in his 1896 edition, including compulsive insanity, impulsive insanity, homosexuality, and mood disturbances. From 1904, however, he instead termed those \"original disease conditions, and introduced the new alternative category of psychopathic personalities. In the eighth edition from 1909 that category would include, in addition to a separate \"dissocial\" type, the excitable, the unstable, the Triebmenschen driven persons, eccentrics, the liars and swindlers, and the quarrelsome. It has been described as remarkable that Kraepelin now considered mood disturbances to be not part of the same category, but only attenuated (more mild) phases of manic depressive illness; this corresponds to current classification schemes.\n\nKraepelin postulated that there is a specific brain or other biological pathology underlying each of the major psychiatric disorders. As a colleague of Alois Alzheimer, he was a co-discoverer of Alzheimer's disease, and his laboratory discovered its pathological basis. Kraepelin was confident that it would someday be possible to identify the pathological basis of each of the major psychiatric disorders.\n\nUpon moving to become Professor of Clinical Psychiatry at the University of Munich in 1903, Kraepelin increasingly wrote on social policy issues. He was a strong and influential proponent of eugenics and racial hygiene. His publications included a focus on alcoholism, crime, degeneration and hysteria. Kraepelin was convinced that such institutions as the education system and the welfare state, because of their trend to break the processes of natural selection, undermined the Germans’ biological \"struggle for survival\". He was concerned to preserve and enhance the German people, the Volk, in the sense of nation or race. He appears to have held Lamarckian concepts of evolution, such that cultural deterioration could be inherited. He was a strong ally and promoter of the work of fellow psychiatrist (and pupil and later successor as director of the clinic) Ernst Rudin to clarify the mechanisms of genetic inheritance as to make a so-called \"empirical genetic prognosis\".\n\nMartin Brune has pointed out that Kraepelin and Rudin also appear to have been ardent advocates of a self-domestication theory, a version of social Darwinism which held that modern culture was not allowing people to be weeded out, resulting in more mental disorder and deterioration of the gene pool. Kraepelin saw a number of \"symptoms\" of this, such as \"weakening of viability and resistance, decreasing fertility, proletarianisation, and moral damage due to \"penning up people\" [\"Zusammenpferchung\"]. He also wrote that \"the number of idiots, epileptics, psychopaths, criminals, prostitutes, and tramps who descend from alcoholic and syphilitic parents, and who transfer their inferiority to their offspring, is incalculable\". He felt that \"the well-known example of the Jews, with their strong disposition towards nervous and mental disorders, teaches us that their extraordinarily advanced domestication may eventually imprint clear marks on the race\". Brune states that Kraepelin's nosological system was \"to a great deal, built on the degeneration paradigm\".\n\nKraepelin's great contribution in classifying schizophrenia and manic depression remains relatively unknown to the general public, and his work, which had neither the literary quality nor paradigmatic power of Freud's, is little read outside scholarly circles. Kraepelin's contributions were also to a large extent marginalized throughout a good part of the 20th century during the success of Freudian etiological theories. However, his views now dominate many quarters of psychiatric research and academic psychiatry. His fundamental theories on the diagnosis of psychiatric disorders form the basis of the major diagnostic systems in use today, especially the American Psychiatric Association's DSM-IV and the World Health Organization's ICD system, based on the Research Diagnostic Criteria and earlier Feighner Criteria developed by espoused \"neo-Kraepelinians\", though Robert Spitzer and others in the DSM committees were keen not to include assumptions about causation as Kraepelin had.\n\nKraepelin has been described as a \"scientific manager\" and political operator, who developed a large-scale, clinically oriented, epidemiological research programme. In this role he took in clinical information from a wide range of sources and networks. Despite proclaiming high clinical standards for himself to gather information \"by means of expert analysis of individual cases\", he would also draw on the reported observations of officials not trained in psychiatry. The various editions of his textbooks do not contain detailed case histories of individuals, however, but mosaiclike compilations of typical statements and behaviors from patients with a specific diagnosis. In broader terms, he has been described as a bourgeois or reactionary citizen.\n\nKraepelin wrote in a \"knapp und klar\" (concise and clear) style that made his books useful tools for physicians. Abridged and clumsy English translations of the sixth and seventh editions of his textbook in 1902 and 1907 (respectively) by Allan Ross Diefendorf (1871–1943), an assistant physician at the Connecticut Hospital for the Insane at Middletown, inadequately conveyed the literary quality of his writings that made them so valuable to practitioners.\n\nIn the Heidelberg and early Munich years he edited \"Psychologische Arbeiten\", a journal on experimental psychology. One of his own famous contributions to this journal also appeared in the form of a monograph (105 pp.) entitled \"Über Sprachstörungen im Traume\" (\"On Language Disturbances in Dreams\"). Kraepelin, on the basis on the dream-psychosis analogy, studied for more than 20 years language disorder in dreams in order to study indirectly schizophasia. The dreams Kraepelin collected are mainly his own. They lack extensive comment by the dreamer. In order to study them the full range of biographical knowledge available today on Kraepelin is necessary (see, e.g., Burgmair et al., I-VII).\n\n\n\n\n\nFor biographies of Kraepelin see:\n\nFor English translations of Kraepelin's work see:\n", "id": "10002", "title": "Emil Kraepelin"}
{"url": "https://en.wikipedia.org/wiki?curid=10003", "text": "Evoluon\n\nThe Evoluon is a conference centre and former science museum erected by the electronics and electrical company Philips at Eindhoven in the Netherlands in 1966. Since its construction, it has become a landmark and a symbol for the city.\n\nThe building is unique due to its very futuristic design, resembling a landed flying saucer. It was designed by architects Leo de Bever and Louis Christiaan Kalff, while the exhibition it housed was conceived by James Gardner. De Bever and Kalff only got two demands for the design of the building, it had to be \"spectacular\" and it had to be possible to hold exhibitions in the building.\n\nIts concrete dome is in diameter and is held in place by of reinforcing steel bars.\n\nIn the 1960s and 1970s the Evoluon attracted large visitor numbers, since its interactive exhibitions were a new and unique concept in the Netherlands at that time. But when competing science museums opened in other cities, the number of visitors began to decline. After several years of losing money, the original museum closed down in 1989 and the Evoluon was converted into a conference centre, opening in 1998.\n\nIn the UK the Evoluon is chiefly remembered from Bert Haanstra's wordless short film entitled simply \"Evoluon\", commissioned by Philips to publicise the museum, and shown as a trade test colour film on BBC television from 1968 to 1972.\n\nIn October 2013 the Evoluon was used to stage four 3D-concerts by the German electronic band Kraftwerk, each before an audience of 1,200 spectators. Key band member Ralf Hütter handpicked the venue for its retro-futuristic look. Bespoke 3D-visuals of the saucer section of the building descending from space were used in the live rendition of their track \"Spacelab\".\n\n", "id": "10003", "title": "Evoluon"}
{"url": "https://en.wikipedia.org/wiki?curid=10004", "text": "Educational essentialism\n\nEducational essentialism is an educational philosophy whose adherents believe that children should learn the traditional basic subjects thoroughly. In this philosophical school of thought, the aim is to instill students with the \"essentials\" of academic knowledge, enacting a back-to-basics approach. Essentialism ensures that the accumulated wisdom of our civilization as taught in the traditional academic disciplines is passed on from teacher to student. Such disciplines might include Reading, Writing, Literature, Foreign Languages, History, Mathematics, Science, Art, and Music. Moreover, this traditional approach is meant to train the mind, promote reasoning, and ensure a common culture.\n\nEssentialism is a relatively conservative stance to education that strives to teach students the knowledge of a society and civilization through a core curriculum. This core curriculum involves such areas that include the study of the surrounding environment, basic natural laws, and the disciplines that promote a happier, more educated living. Other non-traditional areas are also integrated as well in moderation to balance the education. Essentialists' goals are to instill students with the \"essentials\" of academic knowledge, patriotism, and character development through traditional (or back-to-basic) approaches. This is to promote reasoning, train the mind, and ensure a common culture for all citizens.\n\nEssentialism is the most typically enacted philosophy in American classrooms today. Traces of this can be found in the organized learning centered on teachers and textbooks, in addition to the regular assignments and evaluations.\n\nThe role of the teacher as the leader of the classroom is a very important tenet of Educational essentialism. The teacher is the center of the classroom, so they should be rigid and disciplinary. Establishing order in the classroom is crucial for student learning; effective teaching cannot take place in a loud and disorganized environment. It is the teacher's responsibility to keep order in the classroom. The teacher must interpret essentials of the learning process, take the leadership position and set the tone of the classroom. These needs require an educator who is academically well-qualified with an appreciation for learning and development. The teacher must control the students with distributions of rewards and penalties.\n\nThe Essentialist movement first began in the United States in the year 1938. In Atlantic City, New Jersey, a group met for the first time called \"The Essentialist's Committee for the Advancement of Education.\" Their emphasis was to reform the educational system to a rationality-based system.\n\nThe term essentialist first appeared in the book \"An Introduction to the Philosophy of Education\" which was written by Michael John Demiashkevich. In his book, Demiashkevich labels some specific educators (including William C. Bagley) as “essentialists.\" Demiashkevich compared the essentialists to the different viewpoints of the Progressive Education Association. He described how the Progressives preached a “hedonistic doctrine of change” whereas the essentialists stressed the moral responsibility of man for his actions and looked toward permanent principles of behavior (Demiashkevich likened the arguments to those between the Socratics and the Sophists in Greek philosophy). In 1938 Bagley and other educators met together where Bagley gave a speech detailing the main points of the essentialism movement and attacking the public education in the United States. One point that Bagley noted was that students in the U.S. were not getting an education on the same levels as students in Europe who were the same age.\n\nA recent branch has emerged within the essentialist school of thought called \"neoessentialism.\" Emerging in the eighties as a response to the essentialist ideals of the thirties as well as to the criticism of the fifties and the advocates for education in the seventies, neoessentialism was created to try to appease the problems facing the United States at the time. The most notable change within this school of thought is that it called for the creation of a new discipline, computer science.\n\nWilliam Bagley (1874–1946) was an important historical essentialist. William C. Bagley completed his undergraduate degree at Michigan Agricultural College in 1895. It wasn’t until after finishing his undergraduate studies that he truly wanted to be a teacher. Bagley did his Graduate studies at the University of Chicago and at Cornell University. He acquired his Ph.D. in 1900, after which he took his first school job as a Principal in a St. Louis, Missouri Elementary School. Bagley’s devotion increased during his work at Montana State Normal School in Dillon, Montana. It was here where he decided to dedicate his time to the education of teachers and where he published \"The Educative Process\", launching his name across the nation. Throughout his career Bagley argued against the conservative position that teachers were not in need of special training for their work. He believed that liberal arts material was important in teacher education. Bagley also believed the dominant theories of education of the time were weak and lacking.\n\nIn April 1938, he published the \"Essentialist's Platform\", in which he outlined three major points of essentialism. He described the right of students to a well-educated and culturally knowledgeable teacher. Secondly, he discussed the importance of teaching the ideals of community to each group of students. Lastly, Bagley wrote of the importance of accuracy, thoroughness and effort on the part of the student in the classroom.\n\nAnother important essentialist is E. D. Hirsch (1928-). Hirsch was Founder and Chairman of the Core Knowledge Foundation and authored several books concerning fact-based approaches to education. Now retired, he spent many years teaching at the University of Virginia while also being an advocate for the \"back to basics\" movement. In his most popular book, \"Cultural Literacy — What Every American Needs To Know\", he offers lists, quotations, and information regarding what he believes is essential knowledge.\n\nSee also Arthur Bestor.\n\nThe Core Knowledge Schools were founded on the philosophy of essentialist E.D. Hirsch. Although it is difficult to maintain a pure and strict essentialist-only curriculum, these schools have the central aim of establishing a common knowledge base for all citizens. To do so, they follow a nationwide, content-specific, and teacher-centered curriculum. The Core Knowledge curriculum also allows for local variance above and beyond the core curriculum. Central curricular aims are academic excellence and the learning of knowledge, and teachers who are masters of their knowledge areas serve this aim.\n\nBecause Essentialism is largely teacher-centered, the role of the student is often called into question. Presumably, in an essentialist classroom, the teacher is the one designing the curriculum for the students based upon the core disciplines. Moreover, he or she is enacting the curriculum and setting the standards which the students must meet. The teacher's evaluative role may undermine students' interest in study. As a result, the students begin to take on more of a passive role in their education as they are forced to meet and learn such standards and information.\n\nFurthermore, there is also speculation that an essentialist education helps in promoting the cultural lag. This philosophy of education is very traditional in the mindset of passing on the knowledge of the culture via the academic disciplines. Thus, students are forced to think in the mindset of the larger culture, and individual creativity and subversive investigation are often not emphasized, or even outright discouraged.\n\n", "id": "10004", "title": "Educational essentialism"}
{"url": "https://en.wikipedia.org/wiki?curid=10005", "text": "Progressive education\n\nProgressive education is a pedagogical movement that began in the late nineteenth century; it has persisted in various forms to the present. The term \"progressive\" was engaged to distinguish this education from the traditional Euro-American curricula of the 19th century, which was rooted in classical preparation for the university and strongly differentiated by social class. By contrast, progressive education finds its roots in present experience. Most progressive education programs have these qualities in common: \n\n\nProgressive education can be traced back to the works of John Locke and Jean-Jacques Rousseau, both of whom are known as forerunners of ideas that would be developed by theorists such as Dewey. Locke believed that \"truth and knowledge… arise out of observation and experience rather than manipulation of accepted or given ideas\". He further discussed the need for children to have concrete experiences in order to learn. Rousseau deepened this line of thinking in Emile, or On Education, where he argued that subordination of students to teachers and memorization of facts would not lead to an education.\n\nIn Germany, Johann Bernhard Basedow (1724–1790) established the Philanthropinum at Dessau in 1774. He developed new teaching methods based on conversation and play with the child, and a program of physical development. Such was his success that he wrote a treatise on his methods, \"On the best and hitherto unknown method of teaching children of noblemen\".\n\nChristian Gotthilf Salzmann (1744–1811) was the founder of the Schnepfenthal institution, a school dedicated to new modes of education (derived heavily from the ideas of Jean-Jacques Rousseau). He wrote \"Elements of Morality, for the Use of Children\", one of the first books translated into English by Mary Wollstonecraft.\n\nJohann Heinrich Pestalozzi (1746–1827) was a Swiss pedagogue and educational reformer who exemplified Romanticism in his approach. He founded several educational institutions both in German- and French-speaking regions of Switzerland and wrote many works explaining his revolutionary modern principles of education. His motto was \"Learning by head, hand and heart\". His research and theories closely resemble those outlined by Rousseau in Emile. He is further considered by many to be the \"father of modern educational science\" His psychological theories pertain to education as they focus on the development of object teaching, that is, he felt that individuals best learned through experiences and through a direct manipulation and experience of objects. He further speculated that children learn through their own internal motivation rather than through compulsion. (See Intrinsic vs. Extrinsic motivation). A teacher's task will be to help guide their students as individuals through their learning and allow it to unfold naturally.\n\nFriedrich Wilhelm August Fröbel (1782–1852) was a student of Pestalozzi who laid the foundation for modern education based on the recognition that children have unique needs and capabilities. He believed in \"self-activity\" and play as essential factors in child education. The teacher's role was not to indoctrinate but to encourage self-expression through play, both individually and in group activities. He created the concept of kindergarten.\n\nJohann Friedrich Herbart (1776–1841) emphasized the connection between individual development and the resulting societal contribution. The five key ideas which composed his concept of individual maturation were Inner Freedom, Perfection, Benevolence, Justice, and Equity or Recompense. According to Herbart, abilities were not innate but could be instilled, so a thorough education could provide the framework for moral and intellectual development. In order to develop a child to lead to a consciousness of social responsibility, Herbart advocated that teachers utilize a methodology with five formal steps: \"Using this structure a teacher prepared a topic of interest to the children, presented that topic, and questioned them inductively, so that they reached new knowledge based on what they had already known, looked back, and deductively summed up the lesson's achievements, then related them to moral precepts for daily living\".\n\nJohn Melchior Bosco (1815–1888) was concerned about the education of street children who had left their villages to find work in the rapidly industrialized city of Turin, Italy. Exploited as cheap labor or imprisoned for unruly behavior, Bosco saw the need of creating a space where they would feel at home. He called it an 'Oratory' where they could play, learn, share friendships, express themselves, develop their creative talents and pick up skills for gainful self-employment. With those who had found work, he set up a mutual-fund society (an early version of the Grameen Bank) to teach them the benefits of saving and self-reliance. The principles underlying his educational method that won over the hearts and minds of thousands of youth who flocked to his oratory were: 'be reasonable', 'be kind', 'believe' and 'be generous in service'. Today his method of education is practiced in nearly 3000 institutions set up around the world by the members of the Salesian Society he founded in 1873.\n\nWhile studying for his doctorate in Göttingen in 1882–1883, Cecil Reddie was greatly impressed by the progressive educational theories being applied there. Reddie founded Abbotsholme School in Derbyshire, England in 1889. Its curriculum enacted the ideas of progressive education. Reddie rejected rote learning, classical languages and corporal punishment. He combined studies in modern languages and the sciences and arts with a program of physical exercise, manual labour, recreation, crafts and arts. Abbotsholme was imitated throughout Europe and was particularly influential in Germany. He often engaged foreign teachers, who learned its practices, before returning home to start their own schools. Hermann Lietz an Abbotsholme teacher founded five schools (Landerziehungsheime für Jungen) on Abbotsholme's principles. Other people he influenced included Kurt Hahn, Adolphe Ferrière and Edmond Demolins. His ideas also reached Japan, where it turned into \"Taisho-era Free Education Movement\" (Taisho Jiyu Kyoiku Undo)\n\nIn the United States the \"Progressive Education Movement\", starting in the 1880s and lasting for sixty years, helped boost American public schools from a budding idea to the regular norm. John Dewey, a principal figure in this movement from the 1880s to 1904, set the tone for educational philosophy as well as concrete school reforms. His thinking had been influenced by the ideas of Fröbel and Herbart. His reactions to the prevailing theories and practices in education, corrections made to these philosophies, and recommendations to teachers and administrators to embrace \"the new education,\" provide a vital account of the history of the development of educational thinking in the late nineteenth and early twentieth centuries. Dewey placed so-called pragmatism above moral absolutes and helped give rise to situational ethics. Beginning in 1897 John Dewey published a summary of his theory on progressive education in School Journal. His theoretical standpoints are divided into five sections outlined below.\n\nEducation according to Dewey is the \"participation of the individual in the social consciousness of the race\" (Dewey, 1897, para. 1). As such, education should take into account that the student is a social being. The process begins at birth with the child unconsciously gaining knowledge and gradually developing their knowledge to share and partake in society.\n\nThe educational process has two sides, the psychological and the sociological, with the psychological forming the basis. (Dewey, 1897). A child's own instincts will help develop the material that is presented to them. These instincts also form the basis of their knowledge with everything building upon it. This forms the basis of Dewey's assumption that one cannot learn without motivation.\n\nKnowledge is a social condition and it is important to help students construct their own learning, as stated: \"Hence it is impossible to prepare the child for any precise set of conditions. To prepare him for the future life means to give him command of himself; it means so to train him that he will have the full and ready use of all his capacities; that his eye and ear and hand may be tools ready to command, that his judgment may be capable of grasping the conditions under which it has to work, and the executive forces be trained to act economically and efficiently\" (Dewey, 1897, Para. 7).\n\nInstruction must focus on the child as a whole for you can never be sure as to where society may end or where that student will be needed or will take them.\n\n\"Education fails because it neglects this fundamental principle of the school as a form of community life. It conceives the school as a place where certain information is to be given, where certain lessons are to be learned, or where certain habits are to be formed\" (Dewey, 1897, para. 17) Dewey felt that as education is a social construct, it is, therefore, a part of society and should reflect the community.\n\n\"Education is the process of living and is not meant to be the preparation of future living,\" (Dewey, 1897), so the school must represent the present life. As such, parts of the student's home life (such as moral and ethical education) should take part in the schooling process. The teacher is a part of this, not as an authoritative figure, but as a member of the community who is there to assist the student.\n\nAccording to Dewey, the curriculum in the schools should reflect that of society. The center of the school curriculum should reflect the development of humans in society. The study of the core subjects (language, science, history) should be coupled with the study of cooking, sewing, and manual training. Furthermore, he feels that \"progress is not in the succession of studies but in the development of new attitudes towards, and new interests in, experience\" (Dewey, 1897, para. 38)\n\nThe method is focused on the child's powers and interests. If the child is thrown into a passive role as a student, absorbing information, the result is a waste of the child's education. (Dewey, 1897). Information presented to the student will be transformed into new forms, images, and symbols by the student so that they fit with their development and interests. The development of this is natural. To repress this process and attempt to \"substitute the adult for the child\" (Dewey, 1897, para. 52) would weaken the intellectual curiosity of the child.\n\nEducation is the most fundamental method of social reconstruction for progress and reform. Dewey believes that \"education is a regulation of the process of coming to share in the social consciousness; and that the adjustment of individual activity on the basis of this social consciousness is the only sure method of social reconstruction\" (Dewey, 1897, para. 60). As such, Dewey gives way to Social Reconstruction and schools as means to reconstruct society (See Social Reconstruction in Education). Finally, as schools become a means for social reconstruction, our educations must be given the proper equipment to help perform this task and guide their students.\n\nLaborschule Bielefeld was founded in 1974 in Germany. The Laborschule explicitly uses democratic concepts as suggested by Dewey. Studies in the last years have proven the successful implementation of these concepts into a living community.\n\nThe American teacher Helen Parkhurst (1886–1973) developed at the beginning of the twentieth century the Dalton Plan to reform the then current pedagogics and the then usual manner of classroom management. She wanted to break the teacher-centered lockstep teaching. During her first experiment, which she implemented in a small elementary school as a young teacher in 1904, she noticed that when students are given freedom for self-direction and self-pacing and to help one another, their motivation increases considerably and they learn a lot more. In a later experiment in 1911 and 1912 Parkhurst re-organized the education in a large school for nine to fourteen year-olds. Instead of each grade, each subject was appointed its own teacher and its own classroom. The subject teachers made assignments: they converted the subject matter for each grade into learning assignments. In this way, learning became the students’ own work; they could carry out their work independently, work at their own pace and plan their work themselves. The classroom turned into a laboratory, a place where students are working, furnished and equipped as work spaces, tailored to meet the requirements of specific subjects. Useful and attractive learning materials, instruments and reference books were put within the students’ reach. The benches were replaced by large tables to facilitate co-operation and group instruction. This second experiment formed the basis for the next experiments, those in Dalton and New York, from 1919 onwards. The only addition was the use of graphs, charts enabling students to keep track of their own progress in each subject. From now on it was called the Dalton Plan.\n\nIn the nineteen-twenties and nineteen-thirties, Dalton education was spread throughout the world. There is no certainty regarding the exact numbers of Dalton schools, but there was Dalton education in America, Australia, England, Germany, the Netherlands, the Soviet Union, India, China and Japan. Particularly in the Netherlands, China and Japan, Dalton education has remained in existence. In recent years there has been a revival of international interest. It crops up again, for instance, in England, Germany, the Czech Republic and Slovakia. The Netherlands is the country with the highest density of Dalton schools. At the moment (2013) there are five hundred; most of them elementary schools. Comprising five percent of all elementary schools, Dalton education is by far the largest educational reform movement in the Netherlands. And, contrary to Montessori, Jena Plan and Waldorf education, it is steadily on the increase. The only Dalton school in the USA, is the school that Helen Parkhurst founded herself in 1919, and which she was subsequently to direct for more than twenty years: the Dalton School in New York. It is a renowned school. But today its fame is not due to its origins as an experiment in progressive education: the Dalton School is one of the most expensive private schools in New York.\n\nRudolf Steiner (1869–1925) first described the principles of what was to become Waldorf education in 1907. He established a series of schools based on these principles beginning in 1919. The focus of the education is on creating a developmentally-appropriate curriculum that holistically integrates practical, artistic, social, and academic experiences. There are more than a thousand schools and many more early childhood centers worldwide; it has also become a popular form of homeschooling.\n\nMaria Montessori (1870–1952) began to develop her philosophy and methods in 1897. She based her work on her observations of children and experimentation with the environment, materials, and lessons available to them. She frequently referred to her work as \"scientific pedagogy\". Although Montessori education spread to the United States in 1911 there were conflicts with the American educational establishment and was opposed by William Heard Kilpatrick. However Montessori education returned to the United States in 1960 and has since spread to thousands of schools there.\n\nIn July 1906, Ernest Thompson Seton sent Robert Baden-Powell a copy of his book \"The Birchbark Roll of the Woodcraft Indians\". Seton was a British-born Canadian-American living in the United States. They shared ideas about youth training programs. In 1907 Baden-Powell wrote a draft called \"Boy Patrols\". In the same year, to test his ideas, he gathered 21 boys of mixed social backgrounds and held a week-long camp in August on Brownsea Island in England. His organizational method, now known as the Patrol System and a key part of Scouting training, allowed the boys to organize themselves into small groups with an elected patrol leader. Baden Powell then wrote \"Scouting for Boys\" (London, 1908). The Brownsea camp and the publication of \"Scouting for Boys\" are generally regarded as the start of the Scout movement which spread throughout the world. Baden-Powell and his sister Agnes Baden-Powell introduced the Girl Guides in 1910.\n\nTraditional education uses extrinsic motivation, such as grades and prizes. Progressive education is more likely to use intrinsic motivation, basing activities on the interests of the child. Praise may be discouraged as a motivator.\n\n21st century skills are a series of higher-order skills, abilities, and learning dispositions that have been identified as being required for success in the rapidly changing, digital society and workplaces. Many of these skills are also defining qualities of \"progressive education\" as well as being associated with deeper learning, which is based on mastering skills such as analytic reasoning, complex problem solving, and teamwork. These skills differ from traditional academic skills in that they are not primarily content knowledge-based.\n\nEdmond Demolins was inspired by Abbotsholme and Bedales to found the École des Roches in Verneuil-sur-Avre in 1899. Paul Robin implemented progressive principles at the orphanage at Cempuis from 1880 to 1894. This was the first French mixed school, and a scandal at that time. Sebastien Faure in 1904 created a libertarian school 'La Ruche' (the Hive).\n\nHermann Lietz founded three Landerziehungsheime (country boarding schools) in 1904 based on Reddie's model for boys of different ages. Lietz eventually succeeded in establishing five more Landerziehungsheime. Edith and Paul Geheeb founded Odenwaldschule in Heppenheim in the Odenwald in 1910 using their concept of progressive education, which integrated the work of the head and hand.\n\nJanusz Korczak was one notable follower and developer of Pestalozzi's ideas. He wrote\n\"The names of Pestalozzi, Froebel and Spencer shine with no less brilliance than the names of the greatest inventors of the twentieth century. For they discovered more than the unknown forces of nature; they discovered the unknown half of humanity: children.\" His Orphan's Home in Warsaw became a model institution and exerted influence on the educational process in other orphanages of the same type.\n\nIn Spain, the Escuela Moderna was founded in 1901 by Francisco Ferrer, a Catalan educator and anarchist. He had been influenced by Cecil Reddie. The Modern Schools, also called 'Ferrer Schools', that were founded in the United States, were based on Escuela Moderna. As in Spain the schools were intended to educate the working-classes from a secular, class-conscious perspective. The Modern Schools imparted day-time academic classes for children, and night-time continuing-education lectures for adults.\n\nIn Sweden, an early proponent of progressive education was Alva Myrdal, who with her husband Gunnar co-wrote \"Kris i befolkningsfrågan\" (1934), a most influential program for the social-democratic hegemony (1932–1976) popularly known as \"Folkhemmet\". School reforms went through government reports in the 1940s and trials in the 1950s, resulting in the introduction in 1962 of public comprehensive schools (\"grundskola\") instead of the previously separated parallel schools for theoretical and non-theoretical education.\n\nThe ideas from Reddie's Abbotsholme spread to schools such as Bedales School (1893), King Alfred School, London (1898) and St Christopher School, Letchworth (1915), as well as all the Friends' schools, Steiner Waldorf schools and those belonging to the Round Square Conference. The King Alfred School was radical for its time in that it provided a secular education and that boys and girls were educated together. Alexander Sutherland Neill believed children should achieve self-determination and should be encouraged to think critically rather than blindly obeying. He implemented his ideas with the founding of Summerhill School in 1921. Neill believed that children learn better when they are not compelled to attend lessons. The school was also managed democratically, with regular meetings to determine school rules. Pupils had equal voting rights with school staff.\n\nFröbel's student Margarethe Schurz founded the first kindergarten in the United States at Watertown, Wisconsin in 1856, and she also inspired Elizabeth Peabody, who went on to found the first English-speaking kindergarten in the United States – the language at Schurz's kindergarten had been German, to serve an immigrant community – in Boston in 1860. This paved the way for the concept's spread in the USA. The German émigré Adolph Douai had also founded a kindergarten in Boston in 1859, but was obliged to close it after only a year. By 1866, however, he was founding others in New York City.\n\nWilliam Heard Kilpatrick (1871–1965) was a pupil of Dewey and one of the most effective practitioners of the concept as well as the more adept at proliferating the progressive education movement and spreading word of the works of Dewey. He is especially well known for his \"project method of teaching\". This developed the progressive education notion that students were to be engaged and taught so that their knowledge may be directed to society for a socially useful need. Like Dewey he also felt that students should be actively engaged in their learning rather than actively disengaged with the simple reading and regurgitation of material.\n\nThe most famous early practitioner of progressive education was Francis Parker; its best-known spokesperson was the philosopher John Dewey. In 1875 Francis Parker became superintendent of schools in Quincy, Massachusetts after spending two years in Germany studying emerging educational trends on the continent. Parker was opposed to rote learning, believing that there was no value in knowledge without understanding. He argued instead schools should encourage and respect the child's creativity. Parker's Quincy System called for child-centered and experience-based learning. He replaced the traditional curriculum with integrated learning units based on core themes related to the knowledge of different disciplines. He replaced traditional readers, spellers and grammar books with children's own writing, literature, and teacher prepared materials. In 1883 Parker left Massachusetts to become Principal of the Cook County Normal School in Chicago, a school that also served to train teachers in Parker's methods. In 1894 Parker's Talks on Pedagogics, which drew heavily on the thinking of Fröbel, Pestalozzi and Herbart, became one of the first American writings on education to gain international fame.\n\nThat same year, philosopher John Dewey moved from the University of Michigan to the newly established University of Chicago where he became chair of the department of philosophy, psychology and education. He and his wife enrolled their children in Parker's school before founding their own school two years later.\n\nWhereas Parker started with practice and then moved to theory, Dewey began with hypotheses and then devised methods and curricula to test them. By the time Dewey moved to Chicago at the age of thirty-five, he had already published two books on psychology and applied psychology. He had become dissatisfied with philosophy as pure speculation and was seeking ways to make philosophy directly relevant to practical issues. Moving away from an early interest in Hegel, Dewey proceeded to reject all forms of dualism and dichotomy in favor of a philosophy of experience as a series of unified wholes in which everything can be ultimately related.\n\nIn 1896, John Dewey opened what he called the laboratory school to test his theories and their sociological implications. With Dewey as the director and his wife as principal, the University of Chicago Laboratory school, was dedicated \"to discover in administration, selection of subject-matter, methods of learning, teaching, and discipline, how a school could become a cooperative community while developing in individuals their own capacities and satisfy their own needs.\" (Cremin, 136) For Dewey the two key goals of developing a cooperative community and developing individuals' own capacities were not at odds; they were necessary to each other. This unity of purpose lies at the heart of the progressive education philosophy. In 1912, Dewey sent out students of his philosophy to found The Park School of Buffalo and The Park School of Baltimore to put it into practice. These schools operate to this day within a similar progressive approach.\n\nAt Columbia, Dewey worked with other educators such as Charles Eliot and Abraham Flexner to help bring progressivism into the mainstream of American education. In 1917 Columbia established the Lincoln School of Teachers College \"as a laboratory for the working out of an elementary and secondary curriculum which shall eliminate obsolete material and endeavor to work up in usable form material adapted to the needs of modern living.\" (Cremin, 282) Based on Flexner's demand that the modern curriculum \"include nothing for which an affirmative case can not be made out\" (Cremin, 281) the new school organized its activities around four fundamental fields: science, industry, aesthetics and civics. The Lincoln School built its curriculum around \"units of work\" that reorganized traditional subject matter into forms embracing the development of children and the changing needs of adult life. The first and second grades carried on a study of community life in which they actually built a city. A third grade project growing out of the day-to-day life of the nearby Hudson River became one of the most celebrated units of the school, a unit on boats, which under the guidance of its legendary teacher Miss Curtis, became an entrée into history, geography, reading, writing, arithmetic, science, art and literature. Each of the units was broadly enough conceived so that different children could concentrate on different aspects depending on their own interests and needs. Each of the units called for widely diverse student activities, and each sought to deal in depth with some critical aspect of contemporary civilization. Finally each unit engaged children working together cooperatively and also provided opportunities for individual research and exploration.\n\nIn 1924, Agnes de Lima, the lead writer on education for The New Republic and Nation, published a collection of her articles on progressive education as a book, titled \"Our Enemy the Child\".\n\nIn 1918 The National Education Association, representing superintendents and administrators in smaller districts across the country, issued its report \"Cardinal Principles of Secondary Education.\" It emphasized the education of students in terms of health, a command of fundamental processes, worthy home membership, vocation, citizenship, worthy use of leisure, and ethical character. They Emphasized life adjustment and reflected the social efficiency model of progressive education.\n\nFrom 1919 to 1955 the Progressive Education Association founded by Stanwood Cobb and others worked to promote a more student-centered approach to education. During the Great Depression the organization conducted an Eight Year study evaluating the effects of progressive programs. More than 1500 students over four years were compared to an equal number of carefully matched students at conventional schools. When they reached college, the experimental students were found to equal or surpass traditionally educated students on all outcomes: grades, extracurricular participation, dropout rates, intellectual curiosity, and resourcefulness. Moreover, the study found that the more the school departed from the traditional college preparatory program, the better was the record of the graduates. (Kohn, Schools, 232)\n\nBy mid-century many public school programs had also adopted elements of progressive curriculum. At mid-century Dewey believed that progressive education had \"not really penetrated and permeated the foundations of the educational institution.\"(Kohn, Schools, 6,7) As the influence of progressive pedagogy grew broader and more diffuse, practitioners began to vary their application of progressive principles. As varying interpretations and practices made evaluation of progressive reforms more difficult to assess, critics began to propose alternative approaches.\n\nThe seeds of the debate over progressive education can be seen in the differences of Parker and Dewey. These have to do with how much and by whom curriculum should be worked out from grade to grade, how much the child's emerging interests should determine classroom activities, the importance of child-centered vs. societal–centered learning, the relationship of community building to individual growth, and especially the relationship between emotion, thought and experience.\n\nIn 1955 the publication of Rudolf Flesch's \"Why Johnny Can't Read\" leveled criticism of reading programs at the progressive emphasis on reading in context. The conservative McCarthy era raised questions about the liberal ideas at the roots of the progressive reforms. The launching of Sputnik in 1957 at the height of the cold war gave rise to a number of intellectually competitive approaches to disciplinary knowledge, such as BSCS biology PSSC physics, led by university professors such as Jerome Bruner and Jerrold Zacharias.\n\nInterestingly, some of the cold war reforms incorporated elements of progressivism. For example, the work of Zacharias and Bruner was based in the developmental psychology of Jean Piaget and incorporated many of Dewey's ideas of experiential education. Bruner's analysis of developmental psychology became the core of a pedagogical movement known as constructivism, which argues that the child is an active participant in making meaning and must be engaged in the progress of education for learning to be effective. This psychological approach has deep connections to the work of both Parker and Dewey and led to a resurgence of their ideas in second half of the century.\n\nIn 1965, President Johnson inaugurated the Great Society and the Elementary and Secondary Education Act suffused public school programs with funds for sweeping education reforms. At the same time the influx of federal funding also gave rise to demands for accountability and the behavioral objectives approach of Robert F. Mager and others foreshadowed the No Child Left Behind Act passed in 2002. Against these critics eloquent spokespersons stepped forward in defense of the progressive tradition. The Open Classroom movement, led by Herb Kohl and George Dennison, recalled many of Parker's child centered reforms.\n\nThe late 1960s and early 1970s saw a rise and decline in the number of progressive schools. There were several reasons for the decline:\n\n\nProgressive education has been viewed as an alternative to the test-oriented instruction legislated by the No Child Left Behind educational funding act. Alfie Kohn has been an outspoken critic of the No Child Left Behind Act and a passionate defender of the progressive tradition.\n\nTaxpayer revolts, leading to cuts in funding for public education in many states, have led to the founding of an unprecedented number of independent schools, many of which have progressive philosophies. The charter school movement has also spawned an increase in progressive programs. Most recently, public outcry against No Child Left Behind testing and teaching to the test has brought progressive education again into the limelight. Despite the variations that still exist among the progressive programs throughout the country, most progressive schools today are vitalized by these common practices:\n\n\nRabindranath Tagore (1861-1941) was one of the most effective practitioners of the concept of progressive education. He established Santiniketan is a small town near Bolpur in the Birbhum district of West Bengal, India, approximately 160 km north of Kolkata. He de-emphasis on textbook learning in favor of varied learning resources from nature. The emphasis here was on self-motivation rather than on discipline, and on fostering intellectual curiosity rather than competitive excellence. There were courses on a great variety of cultures, and study programs devoted to China, Japan, and the Middle East. He was of the view that education should be a “joyous exercise of our inventive and constructive energies that help us to build up character.”\n\nSeikatsu tsuzurikata is a grassroots movement in Japan that has many parallels to the progressive education movement, but it developed completely independently, beginning in\nthe late 1920s. Japanese progressive educational movement that was one of the stepping stones to the modernization of Japan and which has resonated down to the present.\n\n\n\n", "id": "10005", "title": "Progressive education"}
{"url": "https://en.wikipedia.org/wiki?curid=10006", "text": "Electronic musical instrument\n\nAn electronic musical instrument is a musical instrument that produces sound using electronics. Such an instrument sounds by outputting an electrical audio signal that ultimately drives a loudspeaker.\n\nAn electronic instrument might include a user interface for controlling its sound, often by adjusting the pitch, frequency, or duration of each note. However, it is increasingly common to separate user interface and sound-generating functions into a music controller (input device) and a music synthesizer, respectively, with the two devices communicating through a musical performance description language such as MIDI or Open Sound Control.\n\nAll electronic musical instruments can be viewed as a subset of audio signal processing applications. Simple electronic musical instruments are sometimes called sound effects; the border between sound effects and actual musical instruments is often hazy.\n\nElectronic musical instruments are now widely used in most styles of music. Development of new electronic musical instruments, controllers, and synthesizers continues to be a highly active and interdisciplinary field of research. Specialized conferences, notably the International Conference on New Interfaces for Musical Expression, have organized to report cutting-edge work, as well as to provide a showcase for artists who perform or create music with new electronic music instruments, controllers, and synthesizers.\n\nIn the 18th-century, musicians and composers adapted a number of acoustic instruments to exploit the novelty of electricity. Thus, in the broadest sense, the first electrified musical instrument was the Denis d'or, dating from 1753, followed shortly by the clavecin électrique by the Frenchman Jean-Baptiste de Laborde in 1761. The former instrument consisted of a keyboard instrument of over 700 strings, electrified temporarily to enhance sonic qualities. The latter was a keyboard instrument with plectra (picks) activated electrically. However, neither instrument used electricity as a sound-source.\n\nThe first electric synthesizer was invented in 1876 by Elisha Gray. The \"Musical Telegraph\" was a chance by-product of his telephone technology when Gray accidentally discovered that he could control sound from a self-vibrating electromagnetic circuit and so invented a basic oscillator. The Musical Telegraph used steel reeds oscillated by electromagnets and transmitted over a telephone line. Gray also built a simple loudspeaker device into later models, which consisted of a diaphragm vibrating in a magnetic field.\n\nA significant invention, which later had a profound effect on electronic music, was the audion in 1906. This was the first thermionic valve, or vacuum tube and which led to the generation and amplification of electrical signals, radio broadcasting, and electronic computation, among other things.\n\nOther early synthesizers included the Telharmonium (1897), the Theremin (1919), Jörg Mager's Spharophon (1924) and Partiturophone, Taubmann's similar Electronde (1933), Maurice Martenot's ondes Martenot (\"Martenot waves\", 1928), Trautwein's Trautonium (1930). The Mellertion (1933) used a non-standard scale, Bertrand's Dynaphone could produce octaves and perfect fifths, while the Emicon was an American, keyboard-controlled instrument constructed in 1930 and the German Hellertion combined four instruments to produce chords. Three Russian instruments also appeared, Oubouhof's Croix Sonore (1934), Ivor Darreg's microtonal 'Electronic Keyboard Oboe' (1937) and the ANS synthesizer, constructed by the Russian scientist Evgeny Murzin from 1937 to 1958. Only two models of this latter were built and the only surviving example is currently stored at the Lomonosov University in Moscow. It has been used in many Russian movies—like \"Solaris\"—to produce unusual, \"cosmic\" sounds.\n\nHugh Le Caine, John Hanert, Raymond Scott, composer Percy Grainger (with Burnett Cross), and others built a variety of automated electronic-music controllers during the late 1940s and 1950s. In 1959 Daphne Oram produced a novel method of synthesis, her \"Oramics\" technique, driven by drawings on a 35 mm film strip; it was used for a number of years at the BBC Radiophonic Workshop. This workshop was also responsible for the theme to the TV series Doctor Who, a piece, largely created by Delia Derbyshire, that more than any other ensured the popularity of electronic music in the UK.\n\nIn 1897 Thaddeus Cahill patented an instrument called the Telharmonium (or Teleharmonium, also known as the Dynamaphone). Using tonewheels to generate musical sounds as electrical signals by additive synthesis, it was capable of producing any combination of notes and overtones, at any dynamic level. This technology was later used to design the Hammond organ. Between 1901 and 1910 Cahill had three progressively larger and more complex versions made, the first weighing seven tons, the last in excess of 200 tons. Portability was managed only by rail and with the use of thirty boxcars. By 1912, public interest had waned, and Cahill's enterprise was bankrupt.\n\nAnother development, which aroused the interest of many composers, occurred in 1919-1920. In Leningrad, Leon Theremin (actually Lev Termen) built and demonstrated his Etherophone, which was later renamed the Theremin. This led to the first compositions for electronic instruments, as opposed to noisemakers and re-purposed machines. The Theremin was notable for being the first musical instrument played without touching it.\n\nIn 1929, Joseph Schillinger composed \"First Airphonic Suite for Theremin and Orchestra\", premièred with the Cleveland Orchestra with Leon Theremin as soloist. The next year Henry Cowell commissioned Theremin to create the first electronic rhythm machine, called the Rhythmicon. Cowell wrote some compositions for it, and he and Schillinger premiered it in 1932.\n\nThe 1920s have been called the apex of the Mechanical Age and the dawning of the Electrical Age. In 1922, in Paris, Darius Milhaud began experiments with \"vocal transformation by phonograph speed change.\" These continued until 1927.\n\nThis decade brought a wealth of early electronic instruments—along with the Theremin, there is the presentation of the Ondes Martenot, which was designed to reproduce the microtonal sounds found in Hindu music, and the Trautonium.\nMaurice Martenot invented the Ondes Martenot in 1928, and soon demonstrated it in Paris. Composers using the instrument ultimately include Boulez, Honegger, Jolivet, Koechlin, Messiaen, Milhaud, Tremblay, and Varèse. Radiohead guitarist and multi-instrumentalist Jonny Greenwood also uses it in his compositions and a plethora of Radiohead songs. In 1937, Messiaen wrote \"Fête des belles eaux\" for 6 ondes Martenot, and wrote solo parts for it in \"Trois petites Liturgies de la Présence Divine\" (1943–44) and the \"Turangalîla-Symphonie\" (1946–48/90).\n\nThe Trautonium was invented in 1928. It was based on the subharmonic scale, and the resulting sounds were often used to emulate bell or gong sounds, as in the 1950s Bayreuth productions of \"Parsifal\". In 1942, Richard Strauss used it for the bell- and gong-part in the Dresden première of his \"Japanese Festival Music\". This new class of instruments, microtonal by nature, was only adopted slowly by composers at first, but by the early 1930s there was a burst of new works incorporating these and other electronic instruments.\n\nIn 1929 Laurens Hammond established his company for the manufacture of electronic instruments. He went on to produce the Hammond organ, which was based on the principles of the Telharmonium, along with other developments including early reverberation units.\n\nThe first commercially manufactured synthesizer was the Novachord, built by the Hammond Organ Company from 1938 to 1942, which offered 72-note polyphony using 12 oscillators driving monostable-based divide-down circuits, basic envelope control and resonant low-pass filters. The instrument featured 163 vacuum tubes and weighed 500 pounds. The instrument's use of envelope control is significant, since this is perhaps the most significant distinction between the modern synthesizer and other electronic instruments.\n\nThe most commonly used electronic instruments are synthesizers, so-called because they artificially generate sound using a variety of techniques. All early circuit-based synthesis involved the use of analogue circuitry, particularly voltage controlled amplifiers, oscillators and filters. An important technological development was the invention of the Clavivox synthesizer in 1956 by Raymond Scott with subassembly by Robert Moog. French composer and engineer Edgard Varèse created a variety of compositions using electronic horns, whistles, and tape. Most notably, he wrote \"Poème électronique\" for the Phillips pavilion at the Brussels World Fair in 1958.\n\nRCA produced experimental devices to synthesize voice and music in the 1950s. The Mark II Music Synthesizer, housed at the Columbia-Princeton Electronic Music Center in New York City. Designed by Herbert Belar and Harry Olson at RCA, with contributions from Vladimir Ussachevsky and Peter Mauzey, it was installed at Columbia University in 1957. Consisting of a room-sized array of interconnected sound synthesis components, it was only capable of producing music by programming, using a paper tape sequencer punched with holes to control pitch sources and filters, similar to a mechanical player piano but capable of generating a wide variety of sounds. The vacuum tube system had to be patched to create timbres.\nIn the 1960s synthesizers were still usually confined to studios due to their size. They were usually modular in design, their stand-alone signal sources and processors connected with patch cords or by other means and controlled by a common controlling device. Harald Bode, Don Buchla, Hugh Le Caine, Raymond Scott and Paul Ketoff were among the first to build such instruments, in the late 1950s and early 1960s. Buchla later produced a commercial modular synthesizer, the Buchla Music Easel. Robert Moog, who had been a student of Peter Mauzey and one of the RCA Mark II engineers, created a synthesizer that could reasonably be used by musicians, designing the circuits while he was at Columbia-Princeton. The Moog synthesizer was first displayed at the Audio Engineering Society convention in 1964. It required experience to set up sounds but was smaller and more intuitive than what had come before, less like a machine and more like a musical instrument. Moog established standards for control interfacing, using a logarithmic 1-volt-per-octave for pitch control and a separate triggering signal. This standardization allowed synthesizers from different manufacturers to operate simultaneously. Pitch control was usually performed either with an organ-style keyboard or a music sequencer producing a timed series of control voltages. During the late 1960s hundreds of popular recordings used Moog synthesizers. Other early commercial synthesizer manufacturers included ARP, who also started with modular synthesizers before producing all-in-one instruments, and British firm EMS.\n\nIn 1970, Moog designed the Minimoog, a non-modular synthesizer with a built-in keyboard. The analogue circuits were interconnected with switches in a simplified arrangement called \"normalization.\" Though less flexible than a modular design, normalization made the instrument more portable and easier to use. The Minimoog sold 12,000 units. further standardized the design of subsequent synthesizers with its integrated keyboard, pitch and modulation wheels and VCO->VCF->VCA signal flow. It has become celebrated for its \"fat\" sound—and its tuning problems. Miniaturized solid-state components allowed synthesizers to become self-contained, portable instruments that soon appeared in live performance and quickly became widely used in popular music and electronic art music.\n\nMany early analog synthesizers were monophonic, producing only one tone at a time. Popular monophonic synthesizers include the Moog Minimoog. A few, such as the Moog Sonic Six, ARP Odyssey and EML 101, could produce two different pitches at a time when two keys were pressed. Polyphony (multiple simultaneous tones, which enables chords) was only obtainable with electronic organ designs at first. Popular electronic keyboards combining organ circuits with synthesizer processing included the ARP Omni and Moog's Polymoog and Opus 3.\n\nBy 1976 affordable polyphonic synthesizers began to appear, notably the Yamaha CS-50, CS-60 and CS-80, the Sequential Circuits Prophet-5 and the Oberheim Four-Voice. These remained complex, heavy and relatively costly. The recording of settings in digital memory allowed storage and recall of sounds. The first practical polyphonic synth, and the first to use a microprocessor as a controller, was the Sequential Circuits Prophet-5 introduced in late 1977. For the first time, musicians had a practical polyphonic synthesizer that could save all knob settings in computer memory and recall them at the touch of a button. The Prophet-5's design paradigm became a new standard, slowly pushing out more complex and recondite modular designs.\nIn 1935, another significant development was made in Germany. Allgemeine Elektrizitäts Gesellschaft (AEG) demonstrated the first commercially produced magnetic tape recorder, called the \"Magnetophon\". Audio tape, which had the advantage of being fairly light as well as having good audio fidelity, ultimately replaced the bulkier wire recorders.\n\nThe term \"electronic music\" (which first came into use during the 1930s) came to include the tape recorder as an essential element: \"electronically produced sounds recorded on tape and arranged by the composer to form a musical composition\" It was also indispensable to Musique concrète.\n\nTape also gave rise to the first, analogue, sample-playback keyboards, the Chamberlin and its more famous successor the Mellotron, an electro-mechanical, polyphonic keyboard originally developed and built in Birmingham, England in the early 1960s.\n\nDuring the 1940s–1960s, Raymond Scott, an American composer of electronic music, invented various kind of music sequencers for his electric compositions. Step sequencers played rigid patterns of notes using a grid of (usually) 16 buttons, or steps, each step being 1/16 of a measure. These patterns of notes were then chained together to form longer compositions. Software sequencers were continuously utilized since the 1950s in the context of computer music, including computer-\"played\" music (software sequencer), computer-\"composed\" music (music synthesis), and computer \"sound generation\" (sound synthesis).\n\nThe first digital synthesizers were academic experiments in sound synthesis using digital computers. FM synthesis was developed for this purpose; as a way of generating complex sounds digitally with the smallest number of computational operations per sound sample. In 1983 Yamaha introduced the first stand-alone digital synthesizer, the DX-7. It used frequency modulation synthesis (FM synthesis), first developed by John Chowning at Stanford University during the late sixties. Chowning exclusively licensed his FM synthesis patent to Yamaha in 1975. Yamaha subsequently released their first FM synthesizers, the GS-1 and GS-2, which were costly and heavy. There followed a pair of smaller, preset versions, the CE20 and CE25 Combo Ensembles, targeted primarily at the home organ market and featuring four-octave keyboards. Yamaha's third generation of digital synthesizers was a commercial success; it consisted of the DX7 and DX9 (1983). Both models were compact, reasonably priced, and dependent on custom digital integrated circuits to produce FM tonalities. The DX7 was the first mass market all-digital synthesizer. It became indispensable to many music artists of the 1980s, and demand soon exceeded supply. The DX7 sold over 200,000 units within three years.\n\nThe DX series was not easy to program but offered a detailed, percussive sound that led to the demise of the electro-mechanical Rhodes piano. Following the success of FM synthesis Yamaha signed a contract with Stanford University in 1989 to develop digital waveguide synthesis, leading to the first commercial physical modeling synthesizer, Yamaha's VL-1, in 1994.\n\nThe Fairlight CMI (Computer Musical Instrument), the first polyphonic digital sampler, was the harbinger of sample-based synthesizers. Designed in 1978 by Peter Vogel and Kim Ryrie and based on a dual microprocessor computer designed by Tony Furse in Sydney, Australia, the Fairlight CMI gave musicians the ability to modify volume, attack, decay, and use special effects like vibrato. Sample waveforms could be displayed on-screen and modified using a light pen. The Synclavier from New England Digital was a similar system. Jon Appleton (with Jones and Alonso) invented the Dartmouth Digital Synthesizer, later to become the New England Digital Corp's Synclavier. The Kurzweil K250, first produced in 1983, was also a successful polyphonic digital music synthesizer, noted for its ability to reproduce several instruments synchronously and having a velocity-sensitive keyboard.\n\nAn important new development was the advent of computers for the purpose of composing music, as opposed to manipulating or creating sounds. Iannis Xenakis began what is called \"musique stochastique,\" or \"stochastic music\", which is a method of composing that employs mathematical probability systems. Different probability algorithms were used to create a piece under a set of parameters. Xenakis used graph paper and a ruler to aid in calculating the velocity trajectories of glissandi for his orchestral composition \"Metastasis\" (1953–54), but later turned to the use of computers to compose pieces like \"ST/4\" for string quartet and \"ST/48\" for orchestra (both 1962).\n\nThe impact of computers continued in 1956. Lejaren Hiller and Leonard Issacson composed \"Illiac Suite\" for string quartet, the first complete work of computer-assisted composition using algorithmic composition.\n\nIn 1957, Max Mathews at Bell Lab wrote MUSIC-N series, a first computer program family for generating digital audio waveforms through direct synthesis. Then Barry Vercoe wrote MUSIC 11 based on MUSIC IV-BF, a next-generation music synthesis program (later evolving into csound, which is still widely used).\n\nIn mid 80s, Miller Puckette at IRCAM developed graphic signal-processing software for 4X called Max (after Max Mathews), and later ported it to Macintosh (with Dave Zicarelli extending it for Opcode ) for real-time MIDI control, bringing algorithmic composition availability to most composers with modest computer programming background.\n\nIn 1980, a group of musicians and music merchants met to standardize an interface by which new instruments could communicate control instructions with other instruments and the prevalent microcomputer. This standard was dubbed MIDI (Musical Instrument Digital Interface). A paper was authored by Dave Smith of Sequential Circuits and proposed to the Audio Engineering Society in 1981. Then, in August 1983, the MIDI Specification 1.0 was finalized.\n\nThe advent of MIDI technology allows a single keystroke, control wheel motion, pedal movement, or command from a microcomputer to activate every device in the studio remotely and in synchrony, with each device responding according to conditions predetermined by the composer.\n\nMIDI instruments and software made powerful control of sophisticated instruments easily affordable by many studios and individuals. Acoustic sounds became reintegrated into studios via sampling and sampled-ROM-based instruments.\n\nThe increasing power and decreasing cost of sound-generating electronics (and especially of the personal computer), combined with the standardization of the MIDI and Open Sound Control musical performance description languages, has facilitated the separation of musical instruments into music controllers and music synthesizers.\n\nBy far the most common musical controller is the musical keyboard. Other controllers include the radiodrum, Akai's EWI, the guitar-like SynthAxe, the BodySynth, the Buchla Thunder, the Continuum Fingerboard, the Roland Octapad, various isomorphic keyboards including the Thummer, and Kaossilator Pro, and kits like I-CubeX.\n\nThe Reactable is a round translucent table with a backlit interactive display. By placing and manipulating blocks called \"tangibles\" on the table surface, while interacting with the visual display via finger gestures, a virtual modular synthesizer is operated, creating music or sound effects.\n\nAudioCubes are autonomous wireless cubes powered by an internal computer system and rechargeable battery. They have internal RGB lighting, and are capable of detecting each other's location, orientation and distance. The cubes can also detect distances to the user's hands and fingers. Through interaction with the cubes, a variety of music and sound software can be operated. AudioCubes have applications in sound design, music production, DJing and live performance.\n\nThe Kaossilator and Kaossilator Pro are compact instruments where the position of a finger on the touch pad controls two note-characteristics; usually the pitch is changed with a left-right motion and the tonal property, filter or other parameter changes with an up-down motion. The touch pad can be set to different musical scales and keys. The instrument can record a repeating loop of adjustable length, set to any tempo, and new loops of sound can be layered on top of existing ones. This lends itself to electronic dance-music but is more limited for controlled sequences of notes, as the pad on a regular Kaossilator is featureless.\n\nThe Eigenharp is a large instrument resembling a bassoon, which can be interacted with through touch-sensitive buttons, a drum sequencer and a mouthpiece. The sound processing is done on a separate computer.\n\nThe XTH Sense is a wearable instrument that uses muscle sounds from the human body (known as mechanomyogram) to make music and sound effects. As a performer moves, the body produces muscle sounds that are captured by a chip microphone worn on arm or legs. The muscle sounds are then live sampled using a dedicated software program and a library of modular audio effects. The performer controls the live sampling parameters by weighing force, speed and articulation of the movement.\n\nThe AlphaSphere is a spherical instrument that consists of 48 tactile pads that respond to pressure as well as touch. Custom software allows the pads to be indefinitely programmed individually or by groups in terms of function, note, and pressure parameter among many other settings. The primary concept of the AlphaSphere is to increase the level of expression available to electronic musicians, by allowing for the playing style of a musical instrument.\n\nChiptune, chipmusic, or chip music is music written in sound formats where many of the sound textures are synthesized or sequenced in real time by a computer or video game console sound chip, sometimes including sample-based synthesis and low bit sample playback. Many chip music devices featured synthesizers in tandem with low rate sample playback.\nDuring the late 1970s and early 1980s, DIY (Do it yourself) designs were published in hobby electronics magazines (notably the Formant modular synth, a DIY clone of the Moog system, published by Elektor) and kits were supplied by companies such as Paia in the US, and Maplin Electronics in the UK.\n\nIn 1966–67, Reed Ghazala discovered and began to teach \"circuit bending\"—the application of the creative short circuit, a process of chance short-circuiting, creating experimental electronic instruments, exploring sonic elements mainly of timbre and with less regard to pitch or rhythm, and influenced by John Cage’s aleatoric music concept.\n\nMuch of this manipulation of circuits directly, especially to the point of destruction, was pioneered by Louis and Bebe Barron in the early 1950s, such as their work with John Cage on the \"Williams Mix\" and especially in the soundtrack to Forbidden Planet.\n\nModern circuit bending is the creative customization of the circuits within electronic devices such as low voltage, battery-powered guitar effects, children's toys and small digital synthesizers to create new musical or visual instruments and sound generators. Emphasizing spontaneity and randomness, the techniques of circuit bending have been commonly associated with noise music, though many more conventional contemporary musicians and musical groups have been known to experiment with \"bent\" instruments. Circuit bending usually involves dismantling the machine and adding components such as switches and potentiometers that alter the circuit. With the revived interest for analogue synthesizer circuit bending became a cheap solution for many experimental musicians to create their own individual analogue sound generators. Nowadays many schematics can be found to build noise generators such as the Atari Punk Console or the Dub Siren as well as simple modifications for children toys such as the famous Speak & Spells that are often modified by circuit benders.\n\nThe modular synthesizer is a type of synthesizer consisting of separate interchangeable modules. These are also available as kits for hobbyist DIY constructors. Many hobbyist designers also make available bare PCB boards and front panels for sale to other hobbyists.\n\nAccording to a forum post in December 2010, Sixense Entertainment is working on musical control with the Sixense TrueMotion motion controller.\n\nImmersive virtual musical instruments, or immersive virtual instruments for music and sound aim to represent musical events and sound parameters in a virtual reality so that they can be perceived not only through auditory feedback but also visually in 3D and possibly through tactile as well as haptic feedback, allowing the development of novel interaction metaphors beyond manipulation such as prehension.\n\n\n", "id": "10006", "title": "Electronic musical instrument"}
{"url": "https://en.wikipedia.org/wiki?curid=10008", "text": "Electrode\n\nAn electrode is an electrical conductor used to make contact with a nonmetallic part of a circuit (e.g. a semiconductor, an electrolyte, a vacuum or air). The word was coined by William Whewell at the request of the scientist Michael Faraday from the Greek words \"elektron\", meaning amber (from which the word electricity is derived), and \"hodos\", a way.\n\nAn electrode in an electrochemical cell is referred to as either an \"anode\" or a \"cathode\" (words that were coined by William Whewell at Faraday's request). The anode is now defined as the electrode at which electrons leave the cell and oxidation occurs (indicated by a minus symbol, \"−\"), and the cathode as the electrode at which electrons enter the cell and reduction occurs (indicated by a plus symbol, \"+\"). Each electrode may become either the anode or the cathode depending on the direction of current through the cell. A bipolar electrode is an electrode that functions as the anode of one cell and the cathode of another cell.\n\nA primary cell is a special type of electrochemical cell in which the reaction cannot be reversed, and the identities of the anode and cathode are therefore fixed. The anode is always the negative electrode. The cell can be discharged but not recharged.\n\nA secondary cell, for example a rechargeable battery, is a cell in which the chemical reactions are reversible. When the cell is being charged, the anode becomes the positive (+) and the cathode the negative (−) electrode. This is also the case in an electrolytic cell. When the cell is being discharged, it behaves like a primary cell, with the anode as the negative and the cathode as the positive electrode.\n\nIn a vacuum tube or a semiconductor having polarity (diodes, electrolytic capacitors) the anode is the positive (+) electrode and the cathode the negative (−). The electrons enter the device through the cathode and exit the device through the anode. Many devices have other electrodes to control operation, e.g., base, gate, control grid.\n\nIn a three-electrode cell, a counter electrode, also called an auxiliary electrode, is used only to make a connection to the electrolyte so that a current can be applied to the working electrode. The counter electrode is usually made of an inert material, such as a noble metal or graphite, to keep it from dissolving.\n\nIn arc welding an electrode is used to conduct current through a workpiece to fuse two pieces together. Depending upon the process, the electrode is either consumable, in the case of gas metal arc welding or shielded metal arc welding, or non-consumable, such as in gas tungsten arc welding. For a direct current system the weld rod or stick may be a cathode for a filling type weld or an anode for other welding processes. For an alternating current arc welder the welding electrode would not be considered an anode or cathode.\n\nFor electrical systems which use alternating current the electrodes are the connections from the circuitry to the object to be acted upon by the electric current but are not designated anode or cathode because the direction of flow of the electrons changes periodically, usually many times per second.\n\nElectrodes are used to provide current through nonmetal objects to alter them in numerous ways and to measure conductivity for numerous purposes. Examples include:\n\nChemically modified electrodes are electrodes that have their surfaces chemically modified to change the electrode's physical, chemical, electrochemical, optical, electrical, and transportive properties. These electrodes are used for advanced purposes in research and investigation.\n", "id": "10008", "title": "Electrode"}
{"url": "https://en.wikipedia.org/wiki?curid=10011", "text": "Epistolary novel\n\nAn epistolary novel is a novel written as a series of documents. The usual form is letters, although diary entries, newspaper clippings and other documents are sometimes used. Recently, electronic \"documents\" such as recordings and radio, blogs, and e-mails have also come into use. The word \"epistolary\" is derived from Latin from the Greek word ἐπιστολή \"epistolē\", meaning a letter (see epistle).\n\nThe epistolary form can add greater realism to a story, because it mimics the workings of real life. It is thus able to demonstrate differing points of view without recourse to the device of an omniscient narrator.\n\nThere are two theories on the genesis of the epistolary novel. The first claims that the genre originated from novels with inserted letters, in which the portion containing the third person narrative in between the letters was gradually reduced. The other theory claims that the epistolary novel arose from miscellanies of letters and poetry: some of the letters were tied together into a (mostly amorous) plot. Both claims have some validity. The first truly epistolary novel, the Spanish \"Prison of Love\" (\"Cárcel de amor\") (c.1485) by Diego de San Pedro, belongs to a tradition of novels in which a large number of inserted letters already dominated the narrative. Other well-known examples of early epistolary novels are closely related to the tradition of letter-books and miscellanies of letters. Within the successive editions of Edmé Boursault's \"Letters of Respect, Gratitude and Love\" (\"Lettres de respect, d'obligation et d'amour\") (1669), a group of letters written to a girl named Babet were expanded and became more and more distinct from the other letters, until it formed a small epistolary novel entitled \"Letters to Babet\" (\"Lettres à Babet\"). The immensely famous \"Letters of a Portuguese Nun\" (\"Lettres portugaises\") (1669) generally attributed to Gabriel-Joseph de La Vergne, comte de Guilleragues, though a small minority still regard Marianna Alcoforado as the author, is claimed to be intended to be part of a miscellany of Guilleragues prose and poetry.\nThe founder of the epistolary novel in English is said by many to be James Howell (1594–1666) with \"Familiar Letters\" (1645–50), who writes of prison, foreign adventure, and the love of women.\n\nThe first novel to expose the complex play that the genre allows was Aphra Behn's \"Love-Letters Between a Nobleman and His Sister\", which appeared in three volumes in 1684, 1685, and 1687. The novel shows the genre's results of changing perspectives: individual points were presented by the individual characters, and the central voice of the author and moral evaluation disappeared (at least in the first volume; her further volumes introduced a narrator). Behn furthermore explored a realm of intrigue with letters that fall into the wrong hands, faked letters, letters withheld by protagonists, and even more complex interaction.\n\nThe epistolary novel as a genre became popular in the 18th century in the works of such authors as Samuel Richardson, with his immensely successful novels \"Pamela\" (1740) and \"Clarissa\" (1749). In France, there was \"Lettres persanes\" (1721) by Montesquieu, followed by \"Julie, ou la nouvelle Héloïse\" (1761) by Jean-Jacques Rousseau, and Laclos' \"Les Liaisons dangereuses\" (1782), which used the epistolary form to great dramatic effect, because the sequence of events was not always related directly or explicitly. In Germany, there was Johann Wolfgang von Goethe's \"Die Leiden des jungen Werthers\" (1774) (\"The Sorrows of Young Werther\") and Friedrich Hölderlin's \"Hyperion\". The first North American novel, \"The History of Emily Montague\" (1769) by Frances Brooke was written in epistolary form.\n\nStarting in the 18th century, the epistolary form was subject to much ridicule, resulting in a number of savage burlesques. The most notable example of these was Henry Fielding's \"Shamela\" (1741), written as a parody of \"Pamela\". In it, the female narrator can be found wielding a pen and scribbling her diary entries under the most dramatic and unlikely of circumstances. Oliver Goldsmith used the form to satirical effect in \"The Citizen of the World\", subtitled \"Letters from a Chinese Philosopher Residing in London to his Friends in the East\" (1760–61). So did the diarist Fanny Burney in a successful comic first novel, \"Evelina\" (1788).\n\nThe epistolary novel slowly fell out of use in the late 18th century. Although Jane Austen tried her hand at the epistolary in juvenile writings and her novella \"Lady Susan\" (1794), she abandoned this structure for her later work. It is thought that her lost novel \"First Impressions\", which was redrafted to become \"Pride and Prejudice\", may have been epistolary: \"Pride and Prejudice\" contains an unusual number of letters quoted in full and some play a critical role in the plot.\n\nThe epistolary form nonetheless saw continued use, surviving in exceptions or in fragments in nineteenth-century novels. In Honoré de Balzac's novel \"Letters of Two Brides\", two women who became friends during their education at a convent correspond over a 17-year period, exchanging letters describing their lives. Mary Shelley employs the epistolary form in her novel \"Frankenstein\" (1818). Shelley uses the letters as one of a variety of framing devices, as the story is presented through the letters of a sea captain and scientific explorer attempting to reach the north pole who encounters Victor Frankenstein and records the dying man's narrative and confessions. Published in 1848, Anne Brontë's novel \"The Tenant of Wildfell Hall\" is framed as a retrospective letter from one of the main heroes to his friend and brother-in-law with the diary of the eponymous tenant inside it. In the late 19th century, Bram Stoker released one of the most widely recognized and successful novels in the epistolary form to date, \"Dracula\". Printed in 1897, the novel is compiled entirely of letters, diary entries, newspaper clippings, telegrams, doctor's notes, ship's logs, and the like, which Stoker adroitly employs to balance believability and dramatic tension.\n\nThere are three types of epistolary novels: monologic (giving the letters of only one character, like \"Letters of a Portuguese Nun\" and \"The Sorrows of Young Werther\"), dialogic (giving the letters of two characters, like Mme Marie Jeanne Riccoboni's \"Letters of Fanni Butlerd\" (1757), and polylogic (with three or more letter-writing characters, such as in Bram Stoker's \"Dracula\"). In addition, a crucial element in polylogic epistolary novels like \"Clarissa\", and \"Dangerous Liaisons\" is the dramatic device of 'discrepant awareness': the simultaneous but separate correspondences of the heroines and the villains creating dramatic tension.\n\nEpistolary novels have made several memorable appearances in more recent literature:\n\nA stage play by Tony Broadwick, \"Used Hearts\" (2014) is constructed using notes, letters, emails and such. The play is available from offthewallplays.com ; a Spanish translation is available from espanol.free-ebooks.net\n\n\n\n", "id": "10011", "title": "Epistolary novel"}
{"url": "https://en.wikipedia.org/wiki?curid=10013", "text": "Evidence-based medicine\n\nEvidence-based medicine (EBM) is an approach to medical practice intended to optimize decision-making by emphasizing the use of evidence from well-designed and well-conducted research. Although all medicine based on science has some degree of empirical support, EBM goes further, classifying evidence by its epistemologic strength and requiring that only the strongest types (coming from meta-analyses, systematic reviews, and randomized controlled trials) can yield strong recommendations; weaker types (such as from case-control studies) can yield only weak recommendations. The term was originally used to describe an approach to teaching the practice of medicine and improving decisions by individual physicians about individual patients. Use of the term rapidly expanded to include a previously described approach that emphasized the use of evidence in the design of guidelines and policies that apply to groups of patients and populations (\"evidence-based practice policies\"). It has subsequently spread to describe an approach to decision-making that is used at virtually every level of health care as well as other fields (evidence-based practice).\n\nWhether applied to medical education, decisions about individuals, guidelines and policies applied to populations, or administration of health services in general, evidence-based medicine advocates that to the greatest extent possible, decisions and policies should be based on evidence, not just the beliefs of practitioners, experts, or administrators. It thus tries to assure that a clinician's opinion, which may be limited by knowledge gaps or biases, is supplemented with all available knowledge from the scientific literature so that best practice can be determined and applied. It promotes the use of formal, explicit methods to analyze evidence and makes it available to decision makers. It promotes programs to teach the methods to medical students, practitioners, and policy makers.\n\nIn its broadest form, evidence-based medicine is the application of the scientific method into healthcare decision-making. Medicine has a long tradition of both basic and clinical research that dates back at least to Avicenna. An early critique of statistical methods in medicine was published in 1835.\nHowever, until recently, the process by which research results were incorporated in medical decisions was highly subjective. Called \"clinical judgment\" and \"the art of medicine\", the traditional approach to making decisions about individual patients depended on having each individual physician determine what research evidence, if any, to consider, and how to merge that evidence with personal beliefs and other factors. In the case of decisions that applied to groups of patients or populations, the guidelines and policies would usually be developed by committees of experts, but there was no formal process for determining the extent to which research evidence should be considered or how it should be merged with the beliefs of the committee members. There was an implicit assumption that decision makers and policy makers would incorporate evidence in their thinking appropriately, based on their education, experience, and ongoing study of the applicable literature.\n\nBeginning in the late 1960s, several flaws became apparent in the traditional approach to medical decision-making. Alvan Feinstein's publication of \"Clinical Judgment\" in 1967 focused attention on the role of clinical reasoning and identified biases that can affect it. In 1972, Archie Cochrane published \"Effectiveness and Efficiency\", which described the lack of controlled trials supporting many practices that had previously been assumed to be effective. In 1973, John Wennberg began to document wide variations in how physicians practiced. Through the 1980s, David M. Eddy described errors in clinical reasoning and gaps in evidence. In the mid 1980s, Alvin Feinstein, David Sackett and others published textbooks on clinical epidemiology, which translated epidemiological methods to physician decision making. Toward the end of the 1980s, a group at RAND showed that large proportions of procedures performed by physicians were considered inappropriate even by the standards of their own experts. These areas of research increased awareness of the weaknesses in medical decision making at the level of both individual patients and populations, and paved the way for the introduction of evidence-based methods.\n\nThe term \"evidence-based medicine\", as it is currently used, has two main tributaries. Chronologically, the first is the insistence on explicit evaluation of evidence of effectiveness when issuing clinical practice guidelines and other population-level policies. The second is the introduction of epidemiological methods into medical education and individual patient-level decision-making.\n\nThe term \"evidence-based\" was first used by David M. Eddy in the course of his work on population-level policies such as clinical practice guidelines and insurance coverage of new technologies. He first began to use the term \"evidence-based\" in 1987 in workshops and a manual commissioned by the Council of Medical Specialty Societies to teach formal methods for designing clinical practice guidelines. The manual was widely available in unpublished form in the late 1980s and eventually published by the American College of Medicine. Eddy first published the term \"evidence-based\" in March, 1990 in an article in the \"Journal of the American Medical Association\" that laid out the principles of evidence-based guidelines and population-level policies, which Eddy described as \"explicitly describing the available evidence that pertains to a policy and tying the policy to evidence. Consciously anchoring a policy, not to current practices or the beliefs of experts, but to experimental evidence. The policy must be consistent with and supported by evidence. The pertinent evidence must be identified, described, and analyzed. The policymakers must determine whether the policy is justified by the evidence. A rationale must be written.\" He discussed \"evidence-based\" policies in several other papers published in \"JAMA\" in the spring of 1990. Those papers were part of a series of 28 published in \"JAMA\" between 1990 and 1997 on formal methods for designing population-level guidelines and policies.\n\nThe term \"evidence-based medicine\" was introduced slightly later, in the context of medical education. This branch of evidence-based medicine has its roots in clinical epidemiology. In the autumn of 1990, Gordon Guyatt used it in an unpublished description of a program at McMaster University for prospective or new medical students. Guyatt and others first published the term two years later (1992) to describe a new approach to teaching the practice of medicine. In 1996, David Sackett and colleagues clarified the definition of this tributary of evidence-based medicine as \"the conscientious, explicit and judicious use of current best evidence in making decisions about the care of individual patients. ... [It] means integrating individual clinical expertise with the best available external clinical evidence from systematic research.\" This branch of evidence-based medicine aims to make individual decision making more structured and objective by better reflecting the evidence from research. It requires the application of population-based data to the care of an individual patient, while respecting the fact that practitioners have clinical expertise reflected in effective and efficient diagnosis and thoughtful identification and compassionate use of individual patients' predicaments, rights, and preferences. This tributary of evidence-based medicine had its foundations in clinical epidemiology, a discipline that teaches medical students and physicians how to apply clinical and epidemiological research studies to their practices. The methods were published to a broad physician audience in a series of 25 \"Users’ Guides to the Medical Literature\" published in \"JAMA\" between 1993 and 2000 by the Evidence-based Medicine Working Group at McMaster University. Other definitions for individual level evidence-based medicine have been put forth. For example, in 1995 Rosenberg and Donald defined it as \"the process of finding, appraising, and using contemporaneous research findings as the basis for medical decisions.\" In 2010, Greenhalgh used a definition that emphasized quantitative methods: \"the use of mathematical estimates of the risk of benefit and harm, derived from high-quality research on population samples, to inform clinical decision-making in the diagnosis, investigation or management of individual patients.\" Many other definitions have been offered for individual level evidence-based medicine, but the one by Sackett and colleagues is the most commonly cited.\n\nThe two original definitions highlight important differences in how evidence-based medicine is applied to populations versus individuals. When designing policies such as guidelines that will be applied to large groups of people in settings where there is relatively little opportunity for modification by individual physicians, evidence-based policymaking stresses that there be good evidence documenting that the effectiveness of the test or treatment under consideration. In the setting of individual decision-making there is additional information about the individual patients. Practitioners can be given greater latitude in how they interpret research and combine it with their clinical judgment. Recognizing the two branches of EBM, in 2005 Eddy offered an umbrella definition: \"Evidence-based medicine is a set of principles and methods intended to ensure that to the greatest extent possible, medical decisions, guidelines, and other types of policies are based on and consistent with good evidence of effectiveness and benefit.\"\n\nBoth branches of evidence-based medicine spread rapidly. On the evidence-based guidelines and policies side, explicit insistence on evidence of effectiveness was introduced by the American Cancer Society in 1980. The U.S. Preventive Services Task Force (USPSTF) began issuing guidelines for preventive interventions based on evidence-based principles in 1984. In 1985, the Blue Cross Blue Shield Association applied strict evidence-based criteria for covering new technologies. Beginning in 1987, specialty societies such as the American College of Physicians, and voluntary health organizations such as the American Heart Association, wrote many evidence-based guidelines. In 1991, Kaiser Permanente, a managed care organization in the US, began an evidence-based guidelines program. In 1991, Richard Smith wrote an editorial in the \"British Medical Journal\" and introduced the ideas of evidence-based policies in the UK. In 1993, the Cochrane Collaboration created a network of 13 countries to produce of systematic reviews and guidelines. In 1997, the US Agency for Healthcare Research and Quality (then known as the Agency for Health Care Policy and Research, or AHCPR) established Evidence-based Practice Centers (EPCs) to produce evidence reports and technology assessments to support the development of guidelines. In the same year, a National Guideline Clearinghouse that followed the principles of evidence-based policies was created by AHRQ, the AMA, and the American Association of Health Plans (now America's Health Insurance Plans). In 1999, the National Institute for Clinical Excellence (NICE) was created in the UK. A central idea of this branch of evidence-based medicine is that evidence should be classified according to the rigor of its experimental design, and the strength of a recommendation should depend on the strength of the evidence.\n\nOn the medical education side, programs to teach evidence-based medicine have been created in medical schools in Canada, the US, the UK, Australia, and other countries. A 2009 study of UK programs found the more than half of UK medical schools offered some training in evidence-based medicine, although there was considerable variation in the methods and content, and EBM teaching was restricted by lack of curriculum time, trained tutors and teaching materials. Many programs have been developed to help individual physicians gain better access to evidence. For example, Up-to-date was created in the early 1990s. The Cochrane Center began publishing evidence reviews in 1993. BMJ Publishing Group launched a 6-monthly periodical in 1995 called Clinical Evidence that provided brief summaries of the current state of evidence about important clinical questions for clinicians. Since then many other programs have been developed to make evidence more accessible to practitioners.\n\nThe term evidence-based medicine is now applied to both the programs that are designing evidence-based guidelines and the programs that teach evidence-based medicine to practitioners. By 2000, \"evidence-based medicine\" had become an umbrella term for the emphasis on evidence in both population-level and individual-level decisions. In subsequent years, use of the term \"evidence-based\" had extended to other levels of the health care system. An example is \"evidence-based health services\", which seek to increase the competence of health service decision makers and the practice of evidence-based medicine at the organizational or institutional level. The concept has also spread outside of healthcare; for example, in his 1996 inaugural speech as President of the Royal Statistical Society, Adrian Smith proposed that \"evidence-based policy\" should be established for education, prisons and policing policy and all areas of government work.\n\nThe multiple tributaries of evidence-based medicine share an emphasis on the importance of incorporating evidence from formal research in medical policies and decisions. However they differ on the extent to which they require good evidence of effectiveness before promulgating a guideline or payment policy, and they differ on the extent to which it is feasible to incorporate individual-level information in decisions. Thus, evidence-based guidelines and policies may not readily 'hybridise' with experience-based practices orientated towards ethical clinical judgement, and can lead to contradictions, contest, and unintended crises. The most effective 'knowledge leaders' (managers and clinical leaders) use a broad range of management knowledge in their decision making, rather than just formal evidence. Evidence-based guidelines may provide the basis for governmentality in health care and consequently play a central role in the distant governance of contemporary health care systems.\n\nThe steps for designing explicit, evidence-based guidelines were described in the late 1980s: Formulate the question (population, intervention, comparison intervention, outcomes, time horizon, setting); search the literature to identify studies that inform the question; interpret each study to determine precisely what it says about the question; if several studies address the question, synthesize their results (meta-analysis); summarize the evidence in \"evidence tables\"; compare the benefits, harms and costs in a \"balance sheet\"; draw a conclusion about the preferred practice; write the guideline; write the rationale for the guideline; have others review each of the previous steps; implement the guideline.\n\nFor the purposes of medical education and individual-level decision making, five steps of EBM in practice were described in 1992 and the experience of delegates attending the 2003 Conference of Evidence-Based Health Care Teachers and Developers was summarized into five steps and published in 2005. This five step process can broadly be categorized as:\n\n\nSystematic reviews of published research studies is a major part of the evaluation of particular treatments. The Cochrane Collaboration is one of the best-known programs that conducts systematic reviews. Like other collections of systematic reviews, it requires authors to provide a detailed and repeatable plan of their literature search and evaluations of the evidence. Once all the best evidence is assessed, treatment is categorized as (1) likely to be beneficial, (2) likely to be harmful, or (3) evidence did not support either benefit or harm.\n\nA 2007 analysis of 1,016 systematic reviews from all 50 Cochrane Collaboration Review Groups found that 44% of the reviews concluded that the intervention was likely to be beneficial, 7% concluded that the intervention was likely to be harmful, and 49% concluded that evidence did not support either benefit or harm. 96% recommended further research. A 2001 review of 160 Cochrane systematic reviews (excluding complementary treatments) in the 1998 database revealed that, according to two readers, 41.3% concluded positive or possibly positive effect, 20% concluded evidence of no effect, 8.1% concluded net harmful effects, and 21.3% of the reviews concluded insufficient evidence. A review of 145 alternative medicine Cochrane reviews using the 2004 database revealed that 38.4% concluded positive effect or possibly positive (12.4%) effect, 4.8% concluded no effect, 0.69% concluded harmful effect, and 56.6% concluded insufficient evidence.\n\nEvidence quality can be assessed based on the source type (from meta-analyses and systematic reviews of triple-blind randomized clinical trials with concealment of allocation and no attrition at the top end, down to conventional wisdom at the bottom), as well as other factors including statistical validity, clinical relevance, currency, and peer-review acceptance. Evidence-based medicine categorizes different types of clinical evidence and rates or grades them according to the strength of their freedom from the various biases that beset medical research. For example, the strongest evidence for therapeutic interventions is provided by systematic review of randomized, triple-blind, placebo-controlled trials with allocation concealment and complete follow-up involving a homogeneous patient population and medical condition. In contrast, patient testimonials, case reports, and even expert opinion (however some critics have argued that expert opinion \"does not belong in the rankings of the quality of empirical evidence because it does not represent a form of empirical evidence\" and continue that \"expert opinion would seem to be a separate, complex type of knowledge that would not fit into hierarchies otherwise limited to empirical evidence alone.\") have little value as proof because of the placebo effect, the biases inherent in observation and reporting of cases, difficulties in ascertaining who is an expert and more.\n\nSeveral organizations have developed grading systems for assessing the quality of evidence. An example that put forth by the U.S. Preventive Services Task Force (USPSTF).\n\n\nAnother example of a system for grading evidence is the Oxford (UK) CEBM Levels of Evidence. Most of the evidence ranking schemes grade evidence for therapy and prevention, but not for diagnostic tests, prognostic markers, or harm. The Oxford CEBM Levels of Evidence addresses this issue and provides 'Levels' of evidence for claims about prognosis, diagnosis, treatment benefits, treatment harms, and screening. The original CEBM Levels was first released in September 2000 for Evidence-Based On Call to make the process of finding evidence feasible and its results explicit. In 2011, the Oxford CEBM Levels were redesigned by an international team to make it more understandable and to take into account recent developments in evidence ranking schemes. The Oxford CEBM Levels of Evidence have been used by patients, clinicians and also to develop clinical guidelines including recommendations for the optimal use of phototherapy and topical therapy in psoriasis and guidelines for the use of the BCLC staging system for diagnosing and monitoring hepatocellular carcinoma in Canada.\n\nIn guidelines and other publications, recommendation for a clinical service is classified by the balance of risk versus benefit of the service and the level of evidence on which this information is based. The U.S. Preventive Services Task Force uses:\n\n\nA system was developed by the GRADE working group and takes into account more dimensions than just the quality of medical research. It requires users of GRADE (short for Grading of Recommendations Assessment, Development and Evaluation) who are performing an assessment of the quality of evidence, usually as part of a systematic review, to consider the impact of different factors on their confidence in the results. Authors of GRADE tables, grade the quality of evidence into four levels, on the basis of their confidence in the observed effect (a numerical value) being close to what the true effect is. The confidence value is based on judgements assigned in five different domains in a structured manner. The GRADE working group defines 'quality of evidence' and 'strength of recommendations' based on the quality as two different concepts which are commonly confused with each other.\n\nSystematic reviews may include Randomized Controlled trials that have low risk of bias, or, observational studies that have high risk of bias. In the case of Randomized controlled trials, the quality of evidence is high, but can be downgraded in five different domains.\n\n\nIn the case of observational studies, the quality of evidence starts of lower and may be upgraded in three domains in addition to being subject to downgrading.\n\n\nMeaning of the levels of quality of evidence as per GRADE:\n\nGuideline panelists may make strong or weak recommendations on the basis of further criteria. Some of the important criteria are:\n\nDespite the differences between systems, the purposes are the same: to guide users of clinical research information on which studies are likely to be most valid. However, the individual studies still require careful critical appraisal.\n\nEvidence-based medicine attempts to express clinical benefits of tests and treatments using mathematical methods. Tools used by practitioners of evidence-based medicine include:\n\n\nEvidence-based medicine attempts to objectively evaluate the quality of clinical research by critically assessing techniques reported by researchers in their publications.\n\n\nAlthough evidence-based medicine is regarded as the gold standard of clinical practice, there are a number of limitations and criticisms of its use. Two widely cited categorization schemes for the various published critiques of EBM include the three-fold division of Straus and McAlister (\"limitations universal to the practice of medicine, limitations unique to evidence-based medicine and misperceptions of evidence-based-medicine\") and the five-point categorization of Cohen, Stavri and Hersh (EBM is a poor philosophic basis for medicine, defines evidence too narrowly, is not evidence-based, is limited in usefulness when applied to individual patients, or reduces the autonomy of the doctor/patient relationship).\n\nIn no particular order, some published objections include:\n\n\nOne of the ongoing challenges with evidence-based medicine is that some healthcare providers do not follow the evidence. This happens partly because the current balance of evidence for and against treatments shifts constantly, and it is impossible to learn about every change. Even when the evidence is unequivocally against a treatment, it usually takes ten years for other treatments to be adopted. In other cases, significant change can require a generation of physicians to retire or die, and be replaced by physicians who were trained with more recent evidence.\n\nAnother major cause of physicians and other healthcare providers treating patients in ways unsupported by the evidence is that the these healthcare providers are subject to the same cognitive biases as all other humans. They may reject the evidence because they have a vivid memory of a rare but shocking outcome (the availability heuristic), such as a patient dying after refusing treatment. They may overtreat to \"do something\" or to address a patient's emotional needs. They may worry about malpractice charges based on a discrepancy between what the patient expects and what the evidence recommends. They may also overtreat or provide ineffective treatments because the treatment feels biologically plausible.\n\nThe Berlin questionnaire and the Fresno Test are the most validated instruments for assessing the effectiveness of education in evidence-based medicine. These questionnaires have been used in diverse settings.\n\n", "id": "10013", "title": "Evidence-based medicine"}
{"url": "https://en.wikipedia.org/wiki?curid=10016", "text": "End zone\n\nThe end zone refers to the scoring area on the field, according to gridiron-based codes of football. It is the area between the end line and goal line bounded by the sidelines. There are two end zones, each being on an opposite side of the field. It is bordered on all sides by a white line indicating its beginning and end points, with orange, square pylons placed at each of the four corners as a visual aid. Canadian rule books use the terms \"goal area\" and \"dead line\" instead of \"end zone\" and \"end line\" respectively, but the latter terms are the more common in colloquial Canadian English. Unlike sports like association football and ice hockey which require the ball/puck to pass completely over the goal line to count as a score, both Canadian and American football merely need the nose of the ball to break the vertical plane of the outer edge of the goal line.\n\nA similar concept exists in both rugby football codes, where it is known as the \"in-goal area\". The difference between rugby and gridiron-based codes is that in rugby, the ball must be touched to the ground in the in-goal area to count as a try (the rugby equivalent of a touchdown), whereas in the gridiron-based games, simply possessing the ball while it is in the end zone is sufficient to count it as a touchdown.\n\nUltimate frisbee also uses an end zone scoring area. Scores in this sport are counted when a pass is received in the end zone.\n\nThe end zones were invented as a result of the creation of the forward pass. Prior to this, the goal line and end line were the same, and players scored a touchdown by leaving the field of play through that line. Goal posts were placed on the goal line, and any kicks that did not result in field goals but left the field through the end lines were simply recorded as touchbacks (or, in the Canadian game, singles; it was during the pre-end zone era that Hugh Gall set the record for most singles in a game, with eight).\n\nIn the earliest days of the forward pass, the pass had to be caught in-bounds and could not be thrown across the goal line (as the receiver would be out of bounds). This also made it difficult to pass the ball when very close to one's own goal line, since merely dropping back to pass or kick would result in a safety (rules of the forward pass at the time required the passer to be five yards behind the line of scrimmage, which would make throwing the forward pass when the ball was snapped from behind one's own five-yard line illegal in itself).\n\nThus, in 1912, the end zone was introduced in American football. In an era when professional football was still in its early years and college football dominated the game, the resulting enlargement of the field was constrained by fact that many college teams were already playing in well-developed stadiums, complete with stands and other structures at the ends of the fields, thereby making any substantial enlargement of the field unfeasible at many schools. Eventually, a compromise was reached: 12 yards of end zone were added to each end of the field, but in return, the playing field was shortened from 110 yards to 100, resulting in the physical size of the field being only slightly longer than before. Goal posts were originally kept on the goal lines, but after they began to interfere with play, they moved back to the end lines in 1927, where they have remained in college football ever since. The National Football League moved the goal posts up to the goal line again in 1933, then back again to the end line in 1974.\n\nAs with many other aspects of gridiron football, Canadian football adopted the forward pass and end zones much later than American football. The forward pass and end zones were adopted in 1929. In Canada, college football never reached a level of prominence comparable to U.S. college football, and professional football was still in its infancy in the 1920s. As a result, Canadian football was still being played in rudimentary facilities in the late 1920s. A further consideration was that the Canadian Rugby Union (the governing body of Canadian football at the time) wanted to reduce the prominence of single points (then called \"rouges\") in the game. Therefore, the CRU simply appended 25-yard end zones to the ends of the existing 110-yard field, creating a much larger field of play. Since moving the goal posts back 25 yards would have made the scoring of field goals excessively difficult, and since the CRU did not want to reduce the prominence of field goals, the goal posts were left on the goal line where they remain today. However, the rules governing the scoring of singles were changed: teams were required to either kick the ball out of bounds through the end zone or force the opposition to down a kicked ball in their own end zone in order to be awarded a point. By 1986, at which point CFL stadiums were becoming bigger and comparable in development to their American counterparts in an effort to stay financially competitive, the CFL reduced the depth of the end zone to 20 yards.\n\nA team scores a touchdown by entering its opponent's end zone while carrying the ball or catching the ball while being within the end zone. If the ball is carried by a player, it is considered a score when any part of the ball is directly above or beyond any part of the goal line between the pylons. In addition, a two-point conversion may be scored after a touchdown by similar means.\n\nIn Ultimate Frisbee, a goal is scored by completing a pass into the end zone.\n\nThe end zone in American football is 10 yards long by yards (160 feet) wide. Each corner is marked with a pylon.\n\nA full-sized end zone in Canadian football is 20 yards long by 65 yards wide. Prior to the 1980s, the Canadian end zone was 25 yards long. The first stadium to use the 20 yard long end zone was B.C. Place in Vancouver, which was completed in 1983. The floor of B.C. Place was (and is) too short to accommodate a field 160 yards in length. The shorter end zone proved popular enough that the CFL adopted it league-wide in 1986. At BMO Field, home to the Toronto Argonauts, the end zones are only 18 yards. \n\nIn Canadian football stadiums that also feature a running track, it is usually necessary to truncate the back corners of the end zones, since a rectangular field 150 yards long and 65 yards wide will not fit completely inside an oval-shaped running track. Such truncations are marked as straight diagonal lines, resulting in an end zone with six corners and six pylons. As of 2016, Montreal's Percival Molson Stadium is the only CFL stadium that have the rounded-off style end zones.\n\nDuring the CFL's American expansion in the mid-1990s, several stadiums, by necessity, used 15-yard end zones (some even shorter than 15).\n\nUltimate Frisbee uses an end zone 40 yards wide and 20 yards deep (37 m × 18 m).\n\nThe location and dimensions of a goal post differ from league to league, but it is usually within the boundaries of the end zone. In earlier football games (both professional and collegiate), the goal post began at the goal line, and was usually an H-shaped bar. Nowadays, for player safety reasons, almost all goal posts in the professional and collegiate levels of American football are T-shaped, and reside just outside the rear of both end zones.\n\nThe goal posts in Canadian football still reside on the goal line instead of the back of the end zones, partly because the number of field goal attempts would dramatically decrease if the posts were moved 20 yards back in that sport, and also because the larger end zone and wider field makes the resulting interference in play by the goal post a less serious problem.\n\nAt the high school level, it is not uncommon to see multi-purpose goal posts that include football goal posts at the top and a soccer net at the bottom; these are usually seen at smaller schools and in multi-purpose stadiums where facilities are used for multiple sports. When these or H-shaped goal posts are used in football, the lower portions of the posts are covered with several inches of heavy foam padding to protect the safety of the players.\n\nMost professional and collegiate teams have their logo, team name, or both painted on the surface of the end zone, with team colors filling the background. Many championship and bowl games at college and professional level are commemorated by the names of the opposing teams each being painted in one of the opposite end zones. In some leagues, along with bowl games, local, national, or bowl game sponsors may also have their logos placed in the end zone. In the CFL, fully painted endzones are nonexistent, though some feature club logos or sponsors. Additionally, the Canadian endzone, being a live-ball part of the field, often features yardage dashes, not unlike the field of play itself.\n\nIn many places, particularly in smaller high schools and colleges, end zones are undecorated, or have plain white diagonal stripes spaced several yards apart, in lieu of colors and decorations. One notable use of this design in higher levels is with the Pittsburgh Steelers, who kept their diagonal-line end zone decoration at Heinz Field after positive fan reaction.\n\nOne of the quirks of the American Football League was its use of unusual patterns such as argyle in its end zones, a tradition revived in 2009 by the Denver Broncos, itself a former AFL team. The XFL standardized its playing fields so that all eight of its teams had uniform fields with the XFL logo in each end zone and no team identification.\n\n", "id": "10016", "title": "End zone"}
{"url": "https://en.wikipedia.org/wiki?curid=10017", "text": "Ettore Ximenes\n\nEttore Ximenes (April 11, 1855, Palermo – December 20, 1926, Rome) was an Italian sculptor.\n\nSon of Antonio Ximenes and Giulia Tolentino, a sicilian noble woman, Ettore Ximenes initially embarked on literary studies but then took up sculpture and attended the courses at the Palermo Academy of Fine Arts. After 1872, he continued training at the Naples Academy under Domenico Morelli and Stanislao Lista. He also established a close relationship with Vincenzo Gemito.\n\nHe returned to Palermo in 1874 and won a competition for a four-year grant, which enabled him to study and open a studio for sculpture in Florence. In 1873 at Vienna, he exhibited \"Work without Genius\". In 1877 at Naples, he exhibited a life-size statue titled \"The Equilibrium\" about a gymnast walking on a sphere. He would make copies of this work in small marble and bronze statuettes.\n\nHe exhibited a stucco \"Christ and the Adultress\" and \"Il cuore del re (Heart of the King)\", the latter depicting an oft-repeated story of King Vittorio Emanuele during one of his frequent hunts, encountering and offering charity to a peasant child. At the 1878 Paris World Exposition he displayed: \"The Brawl\" and \"il Marmiton\". In Paris, he met with Auguste Rodin and Jean-Baptiste Carpeaux.\n\nIn 1878, he also completed a life-size stucco of \"il Ciceruacchio\", a statue of the Italian patriot Angelo Brunetti and his thirteen-year-old son, depicting them at the moment of their execution in 1849 by Austrian troops. The Cicervacchio statue, with its tinge of revolutionary zeal, did not find commissions for completing the work in marble.\n\nHe then completed a nude statue of \"Nanà\" based on the novel by Émile Zola; the statue was exhibited at the 1879 Salon di Paris. The next year at the Paris Salon, he displayed \"La Pesca meravigliosa\", where a fisherman rescues a bathing maiden. Returning to Italy, he displayed the bust del minister Giuseppe Zanardelli. At the Mostra of Rome, he displayed \"The assassination of Julius Caeser\"; and at the Exposition of Venice, \"Ragazzi messi in fila\". Ximenes' realism gave way to Symbolist and Neo-Renaissance elements. In addition to sculpture, he also produced illustrations for the works of Edmondo De Amicis published by the Treves publishing house.\n\nXimenes was involved in many of the major official monumental projects in Italy from the 1880s on and devoted his energies as from 1911 primarily to commissions for important public works in São Paulo, Kiev, New York and Buenos Aires.\n\n\n\n\n\n", "id": "10017", "title": "Ettore Ximenes"}
{"url": "https://en.wikipedia.org/wiki?curid=10018", "text": "Edsger W. Dijkstra\n\nEdsger Wybe Dijkstra (; 11 May 1930 – 6 August 2002) was a Dutch computer scientist and an early pioneer in many research areas of computing science. A theoretical physicist by training, he worked as a programmer at the Mathematisch Centrum (Amsterdam) from 1952 to 1962. He was a professor of mathematics at the Eindhoven University of Technology (1962–1984) and a research fellow at the Burroughs Corporation (1973–1984). He held the Schlumberger Centennial Chair in Computer Sciences at the University of Texas at Austin from 1984 until 1999, and retired as Professor Emeritus in 1999.\n\nOne of the most influential members of computing science's founding generation, Dijkstra helped shape the new discipline from both an engineering and a theoretical perspective. Many of his papers are the source of new research areas. Several concepts and problems that are now standard in computer science were first identified by Dijkstra or bear names coined by him.\n\nComputer programming in the 1950s to 1960s was not recognized as an academic discipline; Dijkstra, who had a background in mathematics and physics, was one of the driving forces behind the acceptance of computer programming as a scientific discipline. Dijkstra coined the phrase \"structured programming\" and during the 1970s this became the new programming orthodoxy. Dijkstra's ideas about structured programming helped lay the foundations for the birth and development of the professional discipline of software engineering, enabling programmers to organize and manage increasingly complex software projects.\n\nThe academic study of concurrent computing started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving the mutual exclusion problem. He was also one of the early pioneers of the research on principles of distributed computing. His foundational work on concurrency, semaphores, mutual exclusion (mutex), deadlock (deadly embrace), finding shortest paths in graphs, fault-tolerance, self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built. Shortly before his death in 2002, he received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.\n\nEdsger W. Dijkstra was born in Rotterdam. His father was a chemist who was president of the Dutch Chemical Society; he taught chemistry at a secondary school and was later its superintendent. His mother was a mathematician, but never had a formal job.\n\nDijkstra had considered a career in law and had hoped to represent the Netherlands in the United Nations. However, after graduating from school in 1948, at his parents' suggestion he studied mathematics and physics and then theoretical physics at the University of Leiden.\n\nIn the early 1950s, electronic computers were a novelty. Dijkstra stumbled on his career quite by accident, and through his supervisor, Professor A. Haantjes, he met Adriaan van Wijngaarden, the director of the Computation Department at the Mathematical Center in Amsterdam, who offered Dijkstra a job; he officially became the Netherlands' first \"programmer\" in March 1952.\n\nFor some time Dijkstra remained committed to physics, working on it in Leiden three days out of each week. With increasing exposure to computing, however, his focus began to shift. As he recalled:\nWhen Dijkstra married Maria (Ria) C. Debets in 1957, he was required as a part of the marriage rites to state his profession. He stated that he was a programmer, which was unacceptable to the authorities, there being no such profession at that time in The Netherlands.\nIn 1959 he received his PhD from the University of Amsterdam for a thesis entitled 'Communication with an Automatic Computer', devoted to a description of the assembly language designed for the first commercial computer developed in the Netherlands, the X1. His thesis supervisor was van Wijngaarden.\n\nFrom 1952 until 1962 Dijkstra worked at the Mathematisch Centrum in Amsterdam, where he worked closely with Bram Jan Loopstra and Carel S. Scholten, who had been hired to build a computer. Their mode of interaction was disciplined: They would first decide upon the interface between the hardware and the software, by writing a programming manual. Then the hardware designers would have to be faithful to their part of the contract, while Dijkstra, the programmer, would write software for the nonexistent machine. Two of the lessons he learned from this experience were the importance of clear documentation, and that program debugging can be largely avoided through careful design.\nDijkstra formulated and solved the shortest path problem for a demonstration at the official inauguration of the ARMAC computer in 1956, but—because of the absence of journals dedicated to automatic computing—did not publish the result until 1959.\n\nAt the Mathematical Center, Dijkstra and his colleague developed a compiler for the programming language ALGOL 60; it had a profound influence on his later thinking on programming as a scientific activity. He and Zonneveld had completed the implementation of the first ALGOL 60 compiler by August 1960, more than a year before a compiler was produced by another group.\n\nIn 1962 Dijkstra moved to Eindhoven, and later to Nuenen, in the south of the Netherlands, where he became a professor in the Mathematics Department at the Eindhoven University of Technology. The university did not have a separate computer science department and the culture of the mathematics department did not particularly suit him. Dijkstra tried to build a group of computer scientists who could collaborate on solving problems. This was an unusual model of research for the Mathematics Department. In the late 1960s he built the THE operating system (named for the university, then known as Technische Hogeschool Eindhoven), which has influenced the designs of subsequent operating systems.\n\nDijkstra joined Burroughs Corporation, a company known at that time for the production of computers based on an innovative hardware architecture, as its Research Fellow in August 1973. His duties consisted of visiting some of the company's research centers a few times a year and carrying on his own research, which he did in the smallest Burroughs research facility, namely, his study on the second floor of his house in Nuenen. In fact, Dijkstra was the only research fellow of Burroughs Corporation and worked for it from home, occasionally travelling to its branches in the United States. As a result, he reduced his appointment at the university to one day a week. That day, Tuesday, soon became known as the day of the famous 'Tuesday Afternoon Club', a seminar during which he discussed with his colleagues scientific articles, looking at all aspects – notation, organisation, presentation, language, content, etc. Shortly after he moved in 1984 to the University of Texas at Austin (USA), a new 'branch' of the Tuesday Afternoon Club emerged in Austin.\n\nThe Burroughs years saw him at his most prolific in output of research articles. He wrote nearly 500 documents in the EWD series (described below), most of them technical reports, for private circulation within a select group.\n\nDijkstra accepted the Schlumberger Centennial Chair in the Computer Science Department at the University of Texas at Austin in 1984.\n\nDijkstra worked in Austin until his retirement in November 1999. To mark the occasion and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, which took place on his 70th birthday in May 2000.\n\nDijkstra and his wife returned from Austin to his original house in Nuenen (Netherlands) where he found that he had only months to live. He said that he wanted to retire in Austin, Texas, but to die in the Netherlands. Dijkstra died on 6 August 2002 after a long struggle with cancer. He and his wife Maria (Ria) Debets were survived by their three children: Marcus, Femke and the computer scientist Rutger M. Dijkstra.\n\nAs an early theoretical pioneer in many research areas of computing science, Dijkstra helped shape the new discipline from both an engineering and an academic perspective. Many of his papers are the source of new research areas. Many concepts that are now standard in computer science were first identified by Dijkstra or bear names coined by him. Several important problems were also first formulated and solved by him. A 1994 survey of over a thousand professors of computer science was conducted to obtain a list of 38 most influential scholarly papers in the field, and Dijkstra is the author of five papers.\n\nDijkstra's algorithmic work (especially graph algorithms, concurrent algorithms, and distributed algorithms) plays an important role in many areas of computing science. According to Leslie Lamport (2002), Dijkstra \"started the field of concurrent and distributed algorithms with his 1965 CACM paper \"Solution of a Problem in Concurrent Programming Control\", in which he first stated and solved the mutual exclusion problem.\" As Lamport explains, \"that paper is probably why PODC exists (...). It remains to this day the most influential paper in the field. That it did not win a PODC Influential Paper Award reflects an artificial separation between concurrent and distributed algorithms–a separation that has never existed in Dijkstra's work.\"\n\nIn 1959 Dijkstra published in a 3-page article 'A note on two problems in connexion with graphs' the algorithm to find the shortest path in a graph between any two given nodes, now called Dijkstra's algorithm. Its impact over the next 40 years is summarised from the article of Mikkel Thorup, 'Undirected Single Source Shortest Paths with Positive Integer Weights in Linear Time' (1999): \"Since 1959, all theoretical developments in SSSP [Single-Source Shortest Paths] for general directed and undirected graphs have been based on Dijkstra's algorithm.\" Dijkstra's algorithm is used in SPF, Shortest Path First, which is used in the routing protocols OSPF and IS-IS. Various modifications to Dijkstra's algorithm have been proposed by many authors using heuristics to reduce the run time of shortest path search. One of the most used heuristic algorithms is the A* search algorithm, the main goal is to reduce the run time by reducing the search space. A* search algorithm (first described by Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute in 1968) is an extension of Dijkstra's 1959 algorithm. Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate capabilities of a new computer called ARMAC. His objective was to choose both a problem as well as an answer (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm in about 20 minutes without aid of paper and pen and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (so that 6 bits would suffice to represent the city in the algorithm).\n\nA year later, he came across another problem from hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim's minimal spanning tree algorithm. The Prim's algorithm was originally developed in 1930 by Czech mathematician Vojtěch Jarník and later independently rediscovered and republished by Robert C. Prim in 1957 and Dijkstra in 1959. Therefore, it is also sometimes called the DJP algorithm.\n\nIn 1961 Dijkstra first described the shunting-yard algorithm, a method for parsing mathematical expressions specified in infix notation, in the Mathematisch Centrum report. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was named the \"shunting yard\" algorithm because its operation resembles that of a railroad shunting yard. The shunting-yard algorithm is commonly used to implement operator-precedence parsers.\n\nIn 1962 or 1963 Dijkstra proposed the semaphore mechanism for mutual exclusion algorithm for n processes (a generalization of Dekker's algorithm), which was probably the first published concurrent algorithm and which introduced a new area of algorithmic research. He also identified the deadlock problem and proposed the banker's algorithm that prevents deadlock.\n\nIn 1974 Dijkstra presented three self-stabilizing algorithms for mutual exclusion on a ring. Dijkstra's work is considered to be the first to introduce and demonstrate the self-stabilization concept.\n\nIn the mid-1970s Dijkstra (together with other authors) introduced two useful abstractions (mutator and collector) to the study of garbage collection. The mutator abstracts the process that performs the computation, including allocation of a new storage cell. The collector is the process that automatically reclaims garbage. Furthermore, this paper gives a formalization of \"tri-color marking\" that is basic to incremental garbage collection.\n\nIn the early 1980s Dijkstra and Carel S. Scholten proposed the Dijkstra–Scholten algorithm for detecting termination in distributed systems.\n\nIn 1981 Dijkstra developed smoothsort, a comparison-based sorting algorithm and a variation of heapsort.\n\nDijkstra was known to be a fan of ALGOL 60, and worked on the team that implemented the first compiler for that language. He was closely involved in the ALGOL 60 development, realisation and popularisation. As discussed by Peter Naur in the article 'The European side of the last phase of the development of ALGOL 60', in the \"Proceedings of the First ACM SIGPLAN Conference on History of Programming Languages\", January 1978, Dijkstra took part in the period 1958–1959 in a number of meetings that culminated in the publication of the report defining the ALGOL 60 language. Dijkstra's name does not appear in the list of 13 authors of the final report. Apparently, he eventually left the committee because he could not agree with the majority opinions. Still, while at the Mathematisch Centrum (Amsterdam), he wrote jointly with Jaap Zonneveld the first ALGOL 60 compiler. Dijkstra and Zonneveld, who collaborated on the compiler, agreed not to shave until the project was completed; while Zonneveld shaved shortly thereafter, Dijkstra kept his beard for the rest of his life.\n\nALGOL was the result of a collaboration of American and European committees. ALGOL 60 (short for ALGOrithmic Language 1960) is a member of the ALGOL family of computer programming languages. It followed on from ALGOL 58 and inspired many languages that followed it. It gave rise to many other programming languages, including BCPL, B, Pascal, Simula and C. Algol 60 was a sophisticatedly designed computer language and it provided a large number of hitherto unknown implementation challenges. As Bjarne Stroustrup notes, \"one problem with Algol60 was that no one knew how to implement it.\" A major new challenge in Algol 60 implementation was the run-time allocation and management of data. In 1960 Dijkstra and Zonneveld showed how recursive procedures could be executed using a run-time stack of activation records, and how to efficiently access identifiers from statically enclosing scopes using the so-called 'display'. The ALGOL 60 compiler was one of the first to support recursion employing a novel method to do so. Dijkstra's short book \"Primer of Algol 60 Programming\", originally published in 1962, was the standard reference for the language for several years.\n\nComputer programming in the 1950s to 1960s was not recognized as an academic discipline and unlike mature sciences there were no theoretical concepts or coding systems. Programming as a professional activity was poorly understood in those years.\n\nIn the late 1960s computer programming was in state of crisis. Software crisis is a term used in the early days of computing science for the difficulty of writing useful and efficient computer programs in the required time. The software crisis was due to the rapid increases in computer power and the complexity of the problems that could be tackled. With the increase in the complexity of the software, many software problems arose because existing methods were neither sufficient nor up to the mark. The term \"software crisis\" was coined by some attendees at the first NATO Software Engineering Conference in 1968 at Garmisch, Germany. His 1972 ACM Turing Award Lecture makes reference to this same problem: \"The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.\"\n\nWhile Dijkstra had programmed extensively in machine code in the 1950s, he came to the conclusion that in high-level languages frequent use of the GOTO statement was usually symptomatic of poor structure. In 1968 he wrote a private paper \"A Case against the GO TO Statement\", which was then published as a letter in CACM. Editor Niklaus Wirth gave this letter the heading \"Go To Statement Considered Harmful\", which introduced the phrase \"considered harmful\" into computing.\n\nDijkstra argued that the programming statement GOTO, found in many high-level programming languages, is a major source of errors, and should therefore be eliminated. This letter caused a huge debate in the programming community. Some went to the length of equating good programming with the elimination of GO TO. Dijkstra refused to mention the debate, or even the GO TO statement, in his article \"Notes on Structured Programming\". The debate has long since died down; programming languages provide alternatives to the GO TO, few programmers today use it liberally, and most never use it at all.\n\nDijkstra's thesis was that departures from linear control flow were clearer if allowed only in disciplined higher-level structures such as the if-then-else statement and the while loop. This methodology was developed into structured programming movement, the title of his 1972 book, coauthored with C.A.R. Hoare and Ole-Johan Dahl. Considered by many as the first significant movement in history of computer programming, structured programming became the new programming orthodoxy during the 1970s. Bertrand Meyer remarked that, \"The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming.\"\n\nStructured programming is often regarded as \"goto-less programming\". But as Bertrand Meyer notes, \"As the first book on the topic [\"Structured Programming\" by Dijkstra, Dahl, and Hoare] shows, structured programming is about much more than control structures and the goto. Its principal message is that programming should be considered a scientific discipline based on mathematical rigor.\" , structured programming – especially in the 1970s and 1980s – significantly influenced the birth of many modern programming languages such as Pascal, C, Modula-2, and Ada. The Fortran 77 version which incorporates the concepts of structured programming, was released in 1978. The C++ language was a considerably extended and enhanced version of the popular C (see also: list of C-based programming languages). Since C++ was developed from a more traditional , it is a 'hybrid language', rather than a pure object-oriented programming language.\n\nDijkstra's ideas about programming methodology (especially the structured programming movement) helped lay the foundations for the birth and development of the professional discipline of software engineering (in particular the software design and development), enabling programmers to organize and manage increasingly complex software projects. In the late 1960s Dijkstra discussed the concept of program families. And in the mid 1970s David Parnas and others clarified the idea and showed how to apply it in software engineering principles.\n\nThe rise of the structured programming movement led to many other \"structured\" approaches applied to software design. The techniques of structured analysis and structured design are outgrowths of structured programming concepts and techniques, and of the early ideas about modular design. Principles of modularity were strengthened by Larry Constantine's concepts of coupling (to be minimized between modules) and cohesion (to be maximized within modules), by David Parnas's techniques of information hiding, and by abstract data types. A number of tools and methods employing structured concepts were developed, such as Structured Design, Jackson's Structured Programming, Ross' Structured Analysis and Design Technique (SADT), Yourdon's Structured Method, Structured Systems Analysis and Design Method (SSADM), and James Martin's Information Engineering. The field of software metrics is often considered as a direct influence of the structured programming movement on software engineering in the 1970s.\n\nSeparation of concerns (SoC), one of the basic principles in software engineering, is a design principle for separating a computer program into distinct sections, such that each section addresses a separate concern. The term \"separation of concerns\" was coined by Dijkstra in his 1974 paper \"On the role of scientific thought\".\n\nIn the 1960s Dijkstra and his colleagues in Eindhoven designed and implemented THE (standing for 'Technische Hogeschool Eindhoven') operating system, which was organised into clearly identified layers. His 1968 article on this subject provided the foundation for subsequent designs of the operating systems.\n\nDijkstra organized the design of the system in layers in order to reduce the overall complexity of the software. Though the term 'architecture' had not yet been used to describe software design, this was certainly considered the first glimpse of software architecture. It introduced a number of design principles which have become part of the working vocabulary of every professional programmer: levels of abstraction, programming in layers, the semaphore, and cooperating sequential processes. His original paper on the THE operating system was reprinted in the 25th Anniversary issue of Communications of the ACM, in January 1983. By way of introduction, the Editor-in-Chief says, \"This project initiated a long line of research in multilevel systems architecture — a line that continues to the present day because hierarchical modularity is a powerful approach to organizing large systems.\"\n\nIn a one-page paper from 1965 Dijkstra introduced the 'mutual exclusion problem' for n processes and discussed a solution to it. It was probably the first published concurrent algorithm. The notion, standard by now, of a 'critical section' was also introduced in this paper. Per Brinch Hansen, a pioneer in the field of concurrent computing, considers Dijkstra's \"Cooperating Sequential Processes\" (1965) to be the first classic paper in concurrent programming. As Brinch Hansen notes, 'Dijkstra lays the conceptual foundation for abstract concurrent programming' with that paper.\nIn 1968 Dijkstra published his seminal paper 'Cooperating sequential processes', a 70-page essay that originated the field of concurrent programming. He discussed in it the notion of mutual exclusion (mutex) and the criteria a satisfactory solution should satisfy. He also redressed the historical perspective left out of his 1965 paper by including the first known correct solution to the mutual exclusion problem, for two processes, due to Theodorus Dekker. Dijkstra subsequently generalized Dekker's solution to n processes. Further, he proposed the first synchronisation mechanism for concurrent processes, the semaphore with its two operations, P and V. He also identified the 'deadlock problem' (called there 'the problem of the deadly embrace') and proposed an elegant 'Banker's algorithm' that prevents deadlock. The deadlock detection and prevention became perennial research problems in the field of concurrent programming.\nThe dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them. It was originally formulated in 1965 by Dijkstra as a student exam exercise, presented in terms of computers competing for access to tape drive peripherals. Soon after, Tony Hoare gave the problem its present formulation. The sleeping barber problem is also attributed to Dijkstra.\n\nDijkstra was one of the very early pioneers of the research on principles of distributed computing. As the citation for the Dijkstra Prize recognizes, \"no other individual has had a larger influence on research in principles of distributed computing.\" Some of his papers are even considered to be those that established the field. Dijkstra's 1965 paper, \"Solution of a Problem in Concurrent Programming Control\" was the first to present the correct solution to the mutual exclusion problem. Leslie Lamport writes that this work \"is probably why PODC exists\" and it \"started the field of concurrent and distributed algorithms\".\n\nIn particular, his paper \"Self-stabilizing Systems in Spite of Distributed Control\" (1974) started the sub-field of self-stabilization. It is also considered as the first scientific examination of fault-tolerant systems. Dijkstra's paper was not widely noticed until Leslie Lamport's invited talk at the ACM Symposium on Principles of Distributed Computing (PODC) in 1983. In his report on Dijkstra's work on self-stabilizing distributed systems, Lamport regard it to be 'a milestone in work on fault tolerance' and 'a very fertile field for research'.\n\nFrom the 1970s, Dijkstra's chief interest was formal verification. In 1976 Dijkstra published a seminal book, \"A Discipline of Programming\", which put forward his method of systematic development of programs together with their correctness proofs. In his exposition he used his 'Guarded Command Language'. The language, with its reliance on non-determinism, the adopted weakest precondition semantics and the proposed development method has had a considerable impact on the field to this day. The refinement calculus, originally proposed by Ralph-Johan Back and developed by Carroll Morgan, is an extension of Dijkstra's weakest precondition calculus, where program statements are modeled as predicate transformers.\n\nIn 1984, to add further support to this approach to programming, he published jointly with Wim Feijen an introductory textbook for first-year students of computer science. The book, first published in Dutch, was entitled \"Een methode van programmeren\". The English edition appeared in 1988 as \"A Method of Programming\".\n\nMany of his opinions on computer science and programming have become widespread. For example, the programming phrase \"two or more, use a for\" (a rule of thumb when to use a loop) is sometimes attributed to him.\n\nHe was the first to make the claim that programming is so inherently complex that, in order to manage it successfully, programmers need to harness every trick and abstraction possible.\n\nDijkstra was one of the most famous opponents of the engineering view of computing science. Like Peter Naur and Kristen Nygaard, Dijkstra disliked the very term 'computer science'. Computer science, as Dijkstra pointed out, deserves a better name. He suggests it can be called 'computing science'. Instead of the computer, or computing technology, Dijkstra wanted to emphasize the abstract mechanisms that computing science uses to master complexity. When expressing the abstract nature of computing science, he wrote,\n\nIn \"The Humble Programmer\" (1972), Dijkstra wrote: \"We must not forget that it is not our [computing scientists'] business to make programs, it is our business to design classes of computations that will display a desired behaviour.\"\n\nDijkstra also opposed the inclusion of software engineering under the umbrella of academic computer science. He wrote that, \"As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory.\" And \"software engineering has accepted as its charter \"How to program if you cannot.\".\"\n\nIn the world of computing science, Dijkstra is well known as a \"character\". In the preface of his book \"A Discipline of Programming\" (1976) he stated the following: \"For the absence of a bibliography I offer neither explanation nor apology.\" In fact, most of his articles and books have no references at all. This approach to references was deplored by some researchers. But Dijkstra chose this way of working to preserve his self-reliance.\n\nAs a university professor for much of his life, Dijkstra saw teaching not just as a required activity but as a serious research endeavor. His approach to teaching was unconventional. His lecturing style has been described as idiosyncratic. When lecturing, the long pauses between sentences have often been attributed to the fact that English is not Dijkstra's first language. However the pauses also served as a way for him to think on his feet and he was regarded as a quick and deep thinker while engaged in the act of lecturing. His courses for students in Austin had little to do with computer science but they dealt with the presentation of mathematical proofs. At the beginning of each semester he would take a photo of each of the students, in order to memorize their names. He never followed a textbook, with the possible exception of his own while it was under preparation. When lecturing, he would write proofs in chalk on a blackboard rather than using overhead foils. He invited the students to suggest ideas, which he then explored, or refused to explore because they violated some of his tenets. He assigned challenging homework problems, and would study his students' solutions thoroughly. He conducted his final examinations orally, over a whole week. Each student was examined in Dijkstra's office or home, and an exam lasted several hours.\n\nHe was also highly original in his way of assessing people's capacity for a job. When Vladimir Lifschitz came to Austin in 1990 for a job interview, Dijkstra gave him a puzzle. Vladimir solved it and has been working in Austin since then.\n\nDespite having invented much of the technology of software, Dijkstra eschewed the use of computers in his own work for many decades. Even after he succumbed to his UT colleagues' encouragement and acquired a Macintosh computer, he used it only for e-mail and for browsing the World Wide Web. Dijkstra never wrote his articles using a computer. He preferred to rely on his typewriter and later on his Montblanc pen. Dijkstra's favorite writing instrument was the Montblanc Meisterstück fountain pen. He repeatedly tried other pens, but none ever displaced the Montblanc.\n\nHe had no use for word processors, believing that one should be able to write a letter or article without rough drafts, rewriting, or any significant editing. He would work it all out in his head before putting pen to paper, and once mentioned that when he was a physics student he would solve his homework problems in his head while walking the streets of Leiden.\nMost of Dijkstra's publications were written by him alone. He never had a secretary and took care of all his correspondence alone. When colleagues prepared a Festschrift for his sixtieth birthday, published by Springer-Verlag, he took the trouble to thank each of the 61 contributors separately, in a hand-written letter.\n\nThroughout Dijkstra's career, his work was characterized by elegance and economy. A prolific writer (especially as an essayist), Dijkstra authored more than 1,300 papers, many written by hand in his precise script. They were essays and parables; fairy tales and warnings; comprehensive explanation and pedagogical pretext. Most were about mathematics and computer science; others were trip reports that are more revealing about their author than about the people and places visited. It was his habit to copy each paper and circulate it to a small group of colleagues who would copy and forward the papers to another limited group of scientists. His love affair with simplicity came at an early age and under his mother's guidance. He once said he had asked his mother whether trigonometry was a difficult topic. She replied that he must learn all the formulas and that furthermore if he required more than five lines to prove something, he was on the wrong track.\n\nDijkstra was famous for his wit, eloquence, and way with words, such as in his remark, \"The question of whether Machines Can Think (…) is about as relevant as the question of whether Submarines Can Swim.\"; his advice to a promising researcher, who asked how to select a topic for research, \"Do only what only you can do\". Dijkstra was also known for his vocal criticism. As an outspoken and critical visionary, he strongly opposed the teaching of BASIC.\n\nIn many of his more humorous essays, Dijkstra described a fictional company of which he served as chairman. The company was called Mathematics, Inc., a company that he imagined having commercialized the production of mathematical theorems in the same way that software companies had commercialized the production of computer programs. He invented a number of activities and challenges of Mathematics Inc. and documented them in several papers in the EWD series. The imaginary company had produced a proof of the Riemann Hypothesis but then had great difficulties collecting royalties from mathematicians who had proved results assuming the Riemann Hypothesis. The proof itself was a trade secret. Many of the company's proofs were rushed out the door and then much of the company's effort had to be spent on maintenance. A more successful effort was the Standard Proof for Pythagoras' Theorem, that replaced the more than 100 incompatible existing proofs. Dijkstra described Mathematics Inc. as \"the most exciting and most miserable business ever conceived\". EWD 443 (1974) describes his fictional company as having over 75 percent of the world's market share.\n\nDijkstra was well known for his habit of carefully composing manuscripts with his fountain pen. The manuscripts are called EWDs, since Dijkstra numbered them with \"EWD\", his initials, as a prefix. According to Dijkstra himself, the EWDs started when he moved from the Mathematical Centre in Amsterdam to the Eindhoven University of Technology (then Technische Hogeschool Eindhoven). After going to Eindhoven, Dijkstra experienced a writer's block for more than a year. Dijkstra distributed photocopies of a new EWD among his colleagues. Many recipients photocopied and forwarded their copies, so the EWDs spread throughout the international computer science community. The topics were computer science and mathematics, and included trip reports, letters, and speeches. These short articles span a period of 40 years. Almost all EWDs appearing after 1972 were hand-written. They are rarely longer than 15 pages and are consecutively numbered. The last one, No. 1318, is from 14 April 2002. Within computer science they are known as the EWD reports, or, simply the EWDs. More than 1300 EWDs have been scanned, with a growing number transcribed to facilitate search, and are available online at the Dijkstra archive of the University of Texas.\n\nDijkstra's self-confidence went together with a remarkably modest lifestyle, to the point of being spartan. His and his wife's house in Nuenen was simple, small and unassuming. He did not own a TV, a VCR or a mobile telephone, and did not go to the movies. In contrast, he played the piano well and, while in Austin, liked to go to concerts. An enthusiastic listener of classical music, Dijkstra's favorite composer was Mozart.\n\nIn 1972 the Association for Computing Machinery (ACM) acknowledged Dijkstra's seminal contributions to the field by awarding him the distinguished Turing Award. The citation for the award reads:\nThe introduction given at the awards ceremony is a tribute to Dijkstra:\nIn the words of Sir Tony Hoare, FRS, delivered by him at Dijkstra's funeral:\nIn March 2003, the following email was sent to the distributed computing community:\nFormer ACM President Peter J. Denning wrote about Dijkstra:\nAmong Dijkstra's awards and honors are:\n\nThe Distinguished Fellowship of the British Computer Society (BCS) is awarded under bylaw 7 of the BCS's Royal Charter. The award was first approved in 1969 and the first election was made in 1971 to Dijkstra.\n\nOn the occasion of Dijkstra's 60th birthday in 1990, The Department of Computer Science (UTCS) at the University of Texas at Austin organized a two-day seminar in his honor. Speakers came from all over the United States and Europe, and a group of computer scientists contributed research articles which were edited into a book.\n\nIn 2002, the C&C Foundation of Japan recognized Dijkstra \"for his pioneering contributions to the establishment of the scientific basis for computer software through creative research in basic software theory, algorithm theory, structured programming, and semaphores.\" Dijkstra was alive to receive notice of the award, but it was accepted by his family in an award ceremony after his death.\n\nShortly before his death in 2002, Dijkstra received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.\n\nThe Dijkstra Award for Outstanding Academic Achievement in Computer Science (Loyola University Chicago, Department of Computer Science) is named for Edger W. Dijkstra. Beginning in 2005, this award recognizes the top academic performance by a graduating computer science major. Selection is based on GPA in all major courses and election by department faculty.\n\nThe Department of Computer Science (UTCS) at the University of Texas at Austin hosted the inaugural Edsger W. Dijkstra Memorial Lecture on October 12, 2010. Tony Hoare, Emeritus Professor at Oxford and Principal Researcher at Microsoft Research, was the speaker for the event. This lecture series was made possible by a generous grant from Schlumberger to honor the memory of Dijkstra.\n\n\nBooks:\n\nSelected articles:\n\n\n", "id": "10018", "title": "Edsger W. Dijkstra"}
{"url": "https://en.wikipedia.org/wiki?curid=10021", "text": "Educational perennialism\n\nEducational perennialism is a normative educational philosophy. Perennialists believe that one should teach the things that are of everlasting pertinence to all people everywhere, and that the emphasis should be on principles, not facts. Since people are human, one should teach first about humans, rather than machines or techniques and liberal rather than vocational topics.\n\nAlthough perennialism may appear similar to essentialism, perennialism focuses first on personal development, while essentialism focuses first on essential skills. Essentialist curricula thus tend to be much more vocational and fact-based, and far less liberal and principle-based. Both philosophies are typically considered to be teacher-centered, as opposed to student-centered philosophies of education such as progressivism. However, since the teachers associated with perennialism are in a sense the authors of the Western masterpieces themselves, these teachers may be open to student criticism through the associated Socratic method, which, if carried out as true dialogue, is a balance between students, including the teacher promoting the discussion.\n\nThe word perennial in secular perennialism suggests something that lasts an indefinitely long time, recurs again and again, or is self-renewing. As promoted primarily by Robert Hutchins and Mortimer Adler, a universal curriculum based upon the common and essential nature of all human beings is recommended. This form of perennialism comprises the humanist and scientific traditions. Hutchins and Adler implemented these ideas with great success at the University of Chicago, where they still strongly influence the curriculum in the form of the undergraduate Common Core. Other notable figures in the movement include Stringfellow Barr and Scott Buchanan (who together initiated the Great Books program at St. John's College in Annapolis, Maryland), Mark Van Doren, Alexander Meiklejohn, and Sir Richard Livingstone, an English classicist with an American following.\n\nSecular perennialists espouse the idea that education should focus on the historical development of a continually developing common oriented base of human knowledge and art, the timeless value of classic thought on central human issues by landmark thinkers, and revolutionary ideas critical to historical paradigm shifts or changes in world view. A program of studies which is highly general, nonspecialized, and nonvocational is advocated. They firmly believe that exposure of all citizens to the development of thought by those most responsible for the evolution of the occidental oriented tradition is integral to the survival of the freedoms, human rights and responsibilities inherent to a true Democracy.\n\nAdler states: \n... our political democracy depends upon the reconstitution of our schools. Our schools are not turning out young people prepared for the high office and the duties of citizenship in a democratic republic. Our political institutions cannot thrive, they may not even survive, if we do not produce a greater number of thinking citizens, from whom some statesmen of the type we had in the 18th century might eventually emerge. We are, indeed, a nation at risk, and nothing but radical reform of our schools can save us from impending disaster... Whatever the price... the price we will pay for not doing it will be much greater.\nHutchins writes in the same vein: \nThe business of saying ... that people are not capable of achieving a good education is too strongly reminiscent of the opposition of every extension of democracy. This opposition has always rested on the allegation that the people were incapable of exercising the power they demanded. Always the historic statement has been verified: you cannot expect the slave to show the virtues of the free man unless you first set him free. When the slave has been set free, he has, in the passage of time, become indistinguishable from those who have always been free ... There appears to be an innate human tendency to underestimate the capacity of those who do not belong to \"our\" group. Those who do not share our background cannot have our ability. Foreigners, people who are in a different economic status, and the young seem invariably to be regarded as intellectually backward ...\nAs with the essentialists, perennialists are educationally conservative in the requirement of a curriculum focused upon fundamental subject areas, but stress that the overall aim should be exposure to history's finest thinkers as models for discovery. The student should be taught such basic subjects as English, languages, history, mathematics, natural science, philosophy, and fine arts. Adler states: \"The three R's, which always signified the formal disciplines, are the essence of liberal or general education.\"\n\nSecular perennialists agree with progressivists that memorization of vast amounts of factual information and a focus on second-hand information in textbooks and lectures does not develop rational thought. They advocate learning through the development of meaningful conceptual thinking and judgement by means of a directed reading list of the profound, aesthetic, and meaningful great books of the Western canon. These books, secular perennialists argue, are written by the world's finest thinkers, and cumulatively comprise the \"Great Conversation\" of mankind with regard to the central human questions. Their basic argument for the use of original works (abridged translations being acceptable as well) is that these are the products of \"genius\". Hutchins remarks:\n\nGreat books are great teachers; they are showing us every day what ordinary people are capable of. These books come out of ignorant, inquiring humanity. They are usually the first announcements for success in learning. Most of them were written for, and addressed to, ordinary people.\n\nIt is important to note that the Great Conversation is not static, which is the impression that one might obtain from some descriptions of perennialism, a confusion with religious perennialism, or even the term perennialism itself. The Great Conversation and the set of related great books changes as the representative thought of man changes or progresses, and is therefore representative of an evolution of thought, but is not based upon the whim or fancy of the latest cultural fads. Hutchins makes this point very clear:\nIn the course of history... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation. ...the West needs to recapture and reemphasize and bring to bear upon its present problems the wisdom that lies in the works of its greatest thinkers and in the discussion that they have carried on.\nPerennialism was a solution proposed in response to what was considered by many to be a failing educational system. Again Hutchins writes:\nThe products of American high schools are illiterate; and a degree from a famous college or university is no guarantee that the graduate is in any better case. One of the most remarkable features of American society is that the difference between the \"uneducated\" and the \"educated\" is so slight.\nIn this regard John Dewey and Hutchins were in agreement. Hutchins's book \"The Higher Learning in America\" deplored the \"plight of higher learning\" that had turned away from cultivation of the intellect and toward anti-intellectual practicality due in part, to a lust for money. In a highly negative review of the book, Dewey wrote a series of articles in \"The Social Frontier\" which began by applauding Hutchins' attack on \"the aimlessness of our present educational scheme.\n\nPerennialists believe that reading is to be supplemented with mutual investigations (between the teacher and the student) and minimally-directed discussions through the Socratic method in order to develop a historically oriented understanding of concepts. They argue that accurate, independent reasoning distinguishes the developed or educated mind and they thus stress the development of this faculty. A skilled teacher would keep discussions on topic and correct errors in reasoning, but it would be the class, not the teacher, who would reach the conclusions. While not directing or leading the class to a conclusion, the teacher may work to accurately formulate problems within the scope of the texts being studied.\n\nWhile the standard argument for utilizing a modern text supports distillation of information into a form relevant to modern society, perennialists argue that many of the historical debates and the development of ideas presented by the great books are relevant to any society, at any time, and thus that the suitability of the great books for instructional use is unaffected by their age.\n\nPerennialists freely acknowledge that any particular selection of great books will disagree on many topics; however, they see this as an advantage, rather than a detriment. They believe that the student must learn to recognize such disagreements, which often reflect current debates. The student becomes responsible for thinking about the disagreements and reaching a reasoned, defensible conclusion. This is a major goal of the Socratic discussions. They do not advocate teaching a settled scholarly interpretation of the books, which would cheat the student of the opportunity to learn rational criticism and to know his own mind.\n\nPerennialism was originally religious in nature, developed first by Thomas Aquinas in the thirteenth century in his work \"De Magistro\" (\"The Teacher\").\n\nIn the nineteenth century, John Henry Newman presented a defense of religious perennialism in \"The Idea of a University\". Discourse 5 of that work, \"Knowledge Its Own End\", is a recent statement of a Christian educational perennialism.\n\nThere are several epistemological options, which affect the pedagogical options. The possibilities may be surveyed by considering four extreme positions, as indicated in the following table:\n\n\n", "id": "10021", "title": "Educational perennialism"}
{"url": "https://en.wikipedia.org/wiki?curid=10024", "text": "MDMA\n\n3,4-Methylenedioxymethamphetamine (MDMA), commonly known as ecstasy (E), is a psychoactive drug used primarily as a recreational drug. Desired effects include increased empathy, euphoria, and heightened sensations. When taken by mouth, effects begin after 30–45 minutes and last 3–6 hours. It is also sometimes snorted or smoked. , MDMA has no accepted medical uses.\nAdverse effects of MDMA use include addiction, memory problems, paranoia, difficulty sleeping, teeth grinding, blurred vision, sweating, and a rapid heartbeat. Use may also lead to depression and fatigue. Deaths have been reported due to increased body temperature and dehydration. MDMA increases the release and slows the reuptake of the neurotransmitters serotonin, dopamine, and norepinephrine in parts of the brain. It has stimulant and psychedelic effects. The initial increase is followed by a short-term decrease in the neurotransmitters. MDMA belongs to the substituted methylenedioxyphenethylamine and substituted amphetamine classes of drugs.\nMDMA was first made in 1912. It was used to improve psychotherapy beginning in the 1970s and became popular as a street drug in the 1980s. MDMA is commonly associated with dance parties, raves, and electronic dance music. It is often sold mixed with other substances such as ephedrine, amphetamine, and methamphetamine. In 2014, between 9 and 29 million people between the ages of 15 and 64 used ecstasy (0.2% to 0.6% of the world population). This was broadly similar to the percentage of people who use cocaine, amphetamines, and opioids, but fewer than for cannabis. In the United States, about 0.9 million people used ecstasy in 2010.\nMDMA is generally illegal in most countries. Limited exceptions are sometimes made for research. Researchers are investigating whether a few low doses of MDMA may assist in treating severe, treatment-resistant posttraumatic stress disorder (PTSD). In November 2016, phase 3 clinical trials for PTSD were approved by the United States Food and Drug Administration to assess effectiveness and safety.\n\n, MDMA has no accepted medical indications. Before it was widely banned, it saw limited use in therapy.\n\nA small number of therapists continue to use MDMA in therapy despite its illegal status.\n\nMDMA is often considered the drug of choice within the rave culture and is also used at clubs, festivals and house parties. In the rave environment, the sensory effects from the music and lighting are often highly synergistic with the drug. The psychedelic amphetamine quality of MDMA offers multiple reasons for its appeal to users in the rave setting. Some users enjoy the feeling of mass communion from the inhibition-reducing effects of the drug, while others use it as party fuel because of the drug's stimulatory effects. MDMA is used less frequently than other stimulants, typically less than once per week.\n\nMDMA is sometimes taken in conjunction with other psychoactive drugs, such as LSD, psilocybin mushrooms, and ketamine. Users sometimes use mentholated products while taking MDMA for its cooling sensation.\nMDMA has become widely known as ecstasy (shortened \"E\", \"X\", or \"XTC\"), usually referring to its tablet form, although this term may also include the presence of possible adulterants or dilutants. The UK term \"mandy\" and the US term \"molly\" colloquially refer to MDMA in a crystalline powder form that is thought to be free of adulterants. MDMA is also sold in the form of the hydrochloride salt, either as loose crystals or in gelcaps.\n\nIn part due to the global supply shortage of sassafras oil, substances that are sold as molly frequently contain no MDMA and instead contain methylone, ethylone, MDPV, mephedrone, or any other of the group of compounds commonly known as bath salts. Powdered MDMA is typically 30–40% pure, due to bulking agents that are added to dilute the drug and increase profits (e.g., lactose) and binding agents. Tablets sold as ecstasy sometimes contain 3,4-methylenedioxyamphetamine (MDA), 3,4-methylenedioxyethylamphetamine (MDEA), other amphetamine derivatives, caffeine, opiates, or painkillers. Some tablets contain little or no MDMA. The proportion of seized ecstasy tablets with MDMA-like impurities has varied annually and by country. The average content of MDMA in a preparation is 70 to 120 mg with the purity having increased since the 1990s.\n\nIn general, MDMA users begin reporting subjective effects within 30 to 60 minutes of consumption, reaching the peak at about 75 to 120 minutes which plateaus for about 3.5 hours.\n\nThe desired short-term psychoactive effects of MDMA have been reported to include:\n\nThe experience elicited by MDMA depends on the dose, setting, and user. The variability of the induced altered state by MDMA is lower compared to other psychedelics. For example, MDMA used at parties is associated with high motor activity, reduced sense of self-identity as well as poor awareness of the background surroundings. Use of MDMA individually or in a small groups in a quiet environment and when concentrating, is associated with increased lucidity, capability of concentration, sensitivity of aesthetic aspects of the background and emotions, as well as greater capability of communication with others. In psychotherapeutic settings MDMA effects have been described by infantile ideas, alternating phases of mood, sometimes memories and moods connected with childhood experiences.\n\nSometimes MDMA is labelled as an “empathogenic” drug, because of its empathy-producing effects. Results of different studies show its effects of powerful empathy with others. When testing the MDMA for medium and high dosage ranges it showed increase on hedonic as well as arousal continuum. The effect of MDMA increasing sociability is consistent, however effects on empathy have been more mixed.\n\nThe most serious short-term physical health risks of MDMA are hyperthermia and dehydration. Cases of life-threatening or fatal hyponatremia (excessively low sodium concentration in the blood) have developed in MDMA users attempting to prevent dehydration by consuming excessive amounts of water without replenishing electrolytes.\n\nThe immediate adverse effects of MDMA use can include:\n\nThe adverse effects that last up to a week following cessation of moderate MDMA use include:\n\n\n\n\n, the long-term effects of MDMA on human brain structure and function have not been fully determined. However, there is consistent evidence of structural and functional deficits in MDMA users with a high lifetime exposure. In contrast, there is no evidence of structural or functional changes in MDMA users with only a moderate (<50 doses used and <100 tablets consumed) lifetime exposure. MDMA use at high doses has been shown to produce brain lesions, a form of brain damage, in the serotonergic neural pathways of humans and animals. It is unclear if typical MDMA users may develop neurotoxic brain lesions. Long-term exposure to MDMA in humans has been shown to produce marked neurodegeneration in striatal, hippocampal, prefrontal, and occipital serotonergic axon terminals. Neurotoxic damage to serotonergic axon terminals has been shown to persist for more than two years. Elevations in brain temperature from MDMA use are positively correlated with MDMA-induced neurotoxicity. Adverse neuroplastic changes to brain microvasculature and white matter also occur in humans using low doses of MDMA. Reduced gray matter density in certain brain structures has also been noted in human MDMA users. Global reductions in gray matter volume, thinning of the parietal and orbitofrontal cortices, and decreased hippocampal activity have been observed in long term users. The effects established so far for recreational use of ecstasy lie in the range of moderate to large effects for SERT reduction.\n\nMDMA can also produce cognitive impairments in humans. Impairments in multiple aspects of cognition, including attention, learning, memory, visual processing, and sleep have been found in regular MDMA users. The magnitude of these impairments is correlated with lifetime MDMA usage and are partially reversible with abstinence. MDMA use is also associated with increased impulsivity and depression. Several forms of memory are impaired by chronic ecstasy use; however, the effect sizes for memory impairments in ecstasy users are generally small overall.\n\nSerotonin depletion following MDMA use can cause depression in subsequent days. In some cases depressive symptoms persist for longer. Some studies indicate repeated recreational users of ecstasy have increased rates of depression and anxiety, even after quitting the drug. Depression is one of the main factors for cessation of use.\n\nAt high doses, MDMA induces a neuroimmune response which, through several mechanisms, increases the permeability of the blood-brain barrier, thereby making the brain more susceptible to environmental toxins and pathogens. In addition, MDMA has immunosuppressive effects in the peripheral nervous system and pro-inflammatory effects in the central nervous system.\n\nMDMA is a moderately teratogenic drug (i.e., it is toxic to the fetus). In utero exposure to MDMA is associated with a neuro- and cardiotoxicity and impaired motor functioning. Motor delays may be temporary during infancy or long-term. The severity of these developmental delays increases with heavier MDMA use.\n\nApproximately 60% of MDMA users experience withdrawal symptoms when they stop taking MDMA. Some of these symptoms include fatigue, loss of appetite, depression, and trouble concentrating. Tolerance to some of the desired and adverse effects of MDMA is expected to occur with consistent MDMA use. A 2007 analysis estimated MDMA to have a psychological dependence and physical dependence potential roughly three fourths and four fifths that of cannabis.\nMDMA has been shown to induce ΔFosB in the nucleus accumbens. Since MDMA releases dopamine in the striatum, the mechanisms by which it induces ΔFosB in the nucleus accumbens are analogous to other dopaminergic psychostimulants. Therefore, chronic use of MDMA at high doses can result in altered brain structure and drug addiction, which occur as a consequence of ΔFosB overexpression in the nucleus accumbens. MDMA is less addictive than other stimulants such as methamphetamine and cocaine. Compared with amphetamine, MDMA and its metabolite MDA are less reinforcing.\nOne study found approximately 15% of chronic MDMA users met the DSM-IV diagnostic criteria for substance dependence. However, there is little evidence for a specific diagnosable MDMA dependence syndrome since MDMA is typically used relatively infrequently.\nThere are currently no medications to treat MDMA addiction.\n\nA 2007 UK study ranked MDMA 18th in harmfulness out of 20 recreational drugs. Rankings for each drug were based on the risk for acute physical harm, the propensity for physical and psychological dependency on the drug, and the negative familial and societal impacts of the drug. The authors did not evaluate or rate the negative impact of 'ecstasy' on the cognitive health of ecstasy users, e.g., impaired memory and concentration\n\nMDMA overdose symptoms vary widely due to the involvement of multiple organ systems. Some of the more overt overdose symptoms are listed in the table below. The number of instances of fatal MDMA intoxication is low relative to its usage rates. In most fatalities MDMA was not the only drug involved. Acute toxicity is mainly caused by serotonin syndrome and sympathomimetic effects. MDMA's toxicity in overdose may be exacerbated by caffeine, with which it is frequently cut (mixed with to increase volume).\n\nA number of drug interactions can occur between MDMA and other drugs, including serotonergic drugs. MDMA also interacts with drugs which inhibit CYP450 enzymes, like ritonavir (Norvir), particularly CYP2D6 inhibitors. Concurrent use of MDMA high dosages with another serotonergic drug can result in a life-threatening condition called serotonin syndrome. Severe overdose resulting in death has also been reported in people who took MDMA in combination with certain monoamine oxidase inhibitors, such as phenelzine (Nardil), tranylcypromine (Parnate), or moclobemide (Aurorix, Manerix).\n\nMDMA acts primarily as a presynaptic releasing agent of serotonin, norepinephrine, and dopamine, which arises from its activity at trace amine-associated receptor 1 (TAAR1) and vesicular monoamine transporter 2 (VMAT2). MDMA is a monoamine transporter substrate (i.e., a substrate for DAT, NET, and SERT), so it enters monoamine neurons via these neuronal membrane transport proteins; by acting as a monoamine transporter substrate, MDMA produces competitive reuptake inhibition at the neuronal membrane transporters (i.e., it competes with endogenous monoamines for reuptake). MDMA inhibits both vesicular monoamine transporters (VMATs), the second of which (VMAT2) is highly expressed within monoamine neurons at vesicular membranes. Once inside a monoamine neuron, MDMA acts as a VMAT2 inhibitor and a TAAR1 agonist.\n\nInhibition of VMAT2 by MDMA results in increased concentrations of the associated neurotransmitter (serotonin, norepinephrine, or dopamine) in the cytosol of a monoamine neuron. Activation of TAAR1 by MDMA triggers protein kinase A and protein kinase C signaling events which then phosphorylates the associated monoamine transporters – DAT, NET, or SERT – of the neuron. In turn, these phosphorylated monoamine transporters either reverse transport direction – i.e., move neurotransmitters from the cytosol to the synaptic cleft – or withdraw into the neuron, respectively producing neurotransmitter efflux and noncompetitive reuptake inhibition at the neuronal membrane transporters. MDMA has ten times more affinity for uptake at serotonin transporters compared to dopamine and norepinephrine transporters and consequently has mainly serotonergic effects.\n\nIn summary, MDMA enters monoamine neurons by acting as a monoamine transporter substrate. MDMA activity at VMAT2 moves neurotransmitters out from synaptic vesicles and into the cytosol; MDMA activity at TAAR1 moves neurotransmitters out of the cytosol and into the synaptic cleft.\n\nMDMA also has weak agonist activity at postsynaptic serotonin receptors 5-HT and 5-HT receptors, and its more efficacious metabolite MDA likely augments this action. Cortisol, prolactin, and oxytocin quantities in serum are increased by MDMA. A placebo-controlled study in 15 human volunteers found 100 mg MDMA increased blood levels of oxytocin, and the amount of oxytocin increase was correlated with the subjective prosocial effects of MDMA.(\"S\")-MDMA is more effective in eliciting 5-HT, NE, and DA release, while (\"D\")-MDMA is overall less effective, and more selective for 5-HT and NE release (having only a very faint efficacy on DA release).\n\nMDMA is a ligand at both sigma receptor subtypes, though its efficacies at the receptors have not yet been elucidated.\n\nMDMA reaches maximal concentrations in the blood stream between 1.5 and 3 hr after ingestion. It is then slowly metabolized and excreted, with levels of MDMA and its metabolites decreasing to half their peak concentration over the next several hours. The duration of action of MDMA is usually four to six hours, after which serotonin levels in the brain are depleted. Serotonin levels typically return to normal within 24–48 hours.\n\nMetabolites of MDMA that have been identified in humans include 3,4-methylenedioxyamphetamine (MDA), 4-hydroxy-3-methoxymethamphetamine (HMMA), 4-hydroxy-3-methoxyamphetamine (HMA), 3,4-dihydroxyamphetamine (DHA) (also called alpha-methyldopamine (α-Me-DA)), 3,4-methylenedioxyphenylacetone (MDP2P), and 3,4-Methylenedioxy-N-hydroxyamphetamine (MDOH). The contributions of these metabolites to the psychoactive and toxic effects of MDMA are an area of active research. 80% of MDMA is metabolised in the liver, and about 20% is excreted unchanged in the urine.\n\nMDMA is known to be metabolized by two main metabolic pathways: (1) \"O\"-demethylenation followed by catechol-\"O\"-methyltransferase (COMT)-catalyzed methylation and/or glucuronide/sulfate conjugation; and (2) \"N\"-dealkylation, deamination, and oxidation to the corresponding benzoic acid derivatives conjugated with glycine. The metabolism may be primarily by cytochrome P450 (CYP450) enzymes CYP2D6 and CYP3A4 and COMT. Complex, nonlinear pharmacokinetics arise via autoinhibition of CYP2D6 and CYP2D8, resulting in zeroth order kinetics at higher doses. It is thought that this can result in sustained and higher concentrations of MDMA if the user takes consecutive doses of the drug.\n\nMDMA and metabolites are primarily excreted as conjugates, such as sulfates and glucuronides. MDMA is a chiral compound and has been almost exclusively administered as a racemate. However, the two enantiomers have been shown to exhibit different kinetics. The disposition of MDMA may also be stereoselective, with the S-enantiomer having a shorter elimination half-life and greater excretion than the R-enantiomer. Evidence suggests that the area under the blood plasma concentration versus time curve (AUC) was two to four times higher for the (\"R\")-enantiomer than the (\"S\")-enantiomer after a 40 mg oral dose in human volunteers. Likewise, the plasma half-life of (\"R\")-MDMA was significantly longer than that of the (\"S\")-enantiomer (5.8 ± 2.2 hours vs 3.6 ± 0.9 hours). However, because MDMA excretion and metabolism have nonlinear kinetics, the half-lives would be higher at more typical doses (100 mg is sometimes considered a typical dose).\n\nMDMA is in the substituted methylenedioxyphenethylamine and substituted amphetamine classes of chemicals. As a free base, MDMA is a colorless oil insoluble in water. The most common salt of MDMA is the hydrochloride salt; pure MDMA hydrochloride is water-soluble and appears as a white or off-white powder or crystal.\n\nThere are numerous methods available in the literature to synthesize MDMA via different intermediates. The original MDMA synthesis described in Merck's patent involves brominating safrole to 1-(3,4-methylenedioxyphenyl)-2-bromopropane and then reacting this adduct with methylamine. Most illicit MDMA is synthesized using MDP2P (3,4-methylenedioxyphenyl-2-propanone) as a precursor. MDP2P in turn is generally synthesized from piperonal, safrole or isosafrole. One method is to isomerize safrole to isosafrole in the presence of a strong base, and then oxidize isosafrole to MDP2P. Another method uses the Wacker process to oxidize safrole directly to the MDP2P intermediate with a palladium catalyst. Once the MDP2P intermediate has been prepared, a reductive amination leads to racemic MDMA (an equal parts mixture of (\"R\")-MDMA and (\"S\")-MDMA). Relatively small quantities of essential oil are required to make large amounts of MDMA. The essential oil of \"Ocotea cymbarum\", for example, typically contains between 80 and 94% safrole. This allows 500 ml of the oil to produce between 150 and 340 grams of MDMA.\n\nMDMA and MDA may be quantitated in blood, plasma or urine to monitor for use, confirm a diagnosis of poisoning or assist in the forensic investigation of a traffic or other criminal violation or a sudden death. Some drug abuse screening programs rely on hair, saliva, or sweat as specimens. Most commercial amphetamine immunoassay screening tests cross-react significantly with MDMA or its major metabolites, but chromatographic techniques can easily distinguish and separately measure each of these substances. The concentrations of MDA in the blood or urine of a person who has taken only MDMA are, in general, less than 10% those of the parent drug.\n\nMDMA was first synthesized in 1912 by Merck chemist Anton Köllisch. At the time, Merck was interested in developing substances that stopped abnormal bleeding. Merck wanted to avoid an existing patent held by Bayer for one such compound: hydrastinine. Köllisch developed a preparation of a hydrastinine analogue, methylhydrastinine, at the request of fellow lab members, Walther Beckh and Otto Wolfes. MDMA (called methylsafrylamin, safrylmethylamin or N-Methyl-a-Methylhomopiperonylamin in Merck laboratory reports) was an intermediate compound in the synthesis of methylhydrastinine. Merck was not interested in MDMA itself at the time. On 24 December 1912, Merck filed two patent applications that described the synthesis and some chemical properties of MDMA and its subsequent conversion to methylhydrastinine.\n\nMerck records indicate its researchers returned to the compound sporadically. A 1920 Merck patent describes a chemical modification to MDMA. In 1927, Max Oberlin studied the pharmacology of MDMA while searching for substances with effects similar to adrenaline or ephedrine, the latter being structurally similar to MDMA. Compared to ephedrine, Oberlin observed that it had similar effects on vascular smooth muscle tissue, stronger effects at the uterus, and no \"local effect at the eye\". MDMA was also found to have effects on blood sugar levels comparable to high doses of ephedrine. Oberlin concluded that the effects of MDMA were not limited to the sympathetic nervous system. Research was stopped \"particularly due to a strong price increase of safrylmethylamine\", which was still used as an intermediate in methylhydrastinine synthesis. Albert van Schoor performed simple toxicological tests with the drug in 1952, most likely while researching new stimulants or circulatory medications. After pharmacological studies, research on MDMA was not continued. In 1959, Wolfgang Fruhstorfer synthesized MDMA for pharmacological testing while researching stimulants. It is unclear if Fruhstorfer investigated the effects of MDMA in humans.\n\nOutside of Merck, other researchers began to investigate MDMA. In 1953 and 1954, the United States Army commissioned a study of toxicity and behavioral effects in animals injected with mescaline and several analogues, including MDMA. Conducted at the University of Michigan in Ann Arbor, these investigations were declassified in October 1969 and published in 1973. A 1960 Polish paper by Biniecki and Krajewski describing the synthesis of MDMA as an intermediate was the first published scientific paper on the substance.\n\nMDMA may have been in non-medical use in the western United States in 1968. An August 1970 report at a meeting of crime laboratory chemists indicates MDMA was being used recreationally in the Chicago area by 1970. MDMA likely emerged as a substitute for its analog methylenedioxyamphetamine (MDA), a drug at the time popular among users of psychedelics which was made a Schedule 1 substance in the United States in 1970.\n\nAmerican chemist and psychopharmacologist Alexander Shulgin reported he synthesized MDMA in 1965 while researching methylenedioxy compounds at Dow Chemical Company, but did not test the psychoactivity of the compound at this time. Around 1970, Shulgin sent instructions for N-methylated MDA (MDMA) synthesis to the founder of a Los Angeles chemical company who had requested them. This individual later provided these instructions to a client in the Midwest. Shulgin may have suspected he played a role in the emergence of MDMA in Chicago.\n\nShulgin first heard of the psychoactive effects of N-methylated MDA around 1975 from a young student who reported \"amphetamine-like content\". Around 30 May 1976, Shulgin again heard about the effects of N-methylated MDA, this time from a graduate student in a medicinal chemistry group he advised at San Francisco State University who directed him to the University of Michigan study. She and two close friends had consumed 100 mg of MDMA and reported positive emotional experiences. Following the self-trials of a colleague at the University of San Francisco, Shulgin synthesized MDMA and tried it himself in September and October 1976. Shulgin first reported on MDMA in a presentation at a conference in Bethesda, Maryland in December 1976. In 1978, he and David E. Nichols published a report on the drug's psychoactive effect in humans. They described MDMA as inducing \"an easily controlled altered state of consciousness with emotional and sensual overtones\" comparable \"to marijuana, to psilocybin devoid of the hallucinatory component, or to low levels of MDA\".\n\nWhile not finding his own experiences with MDMA particularly powerful, Shulgin was impressed with the drug's disinhibiting effects and thought it could be useful in therapy. Believing MDMA allowed users to strip away habits and perceive the world clearly, Shulgin called the drug \"window\". Shulgin occasionally used MDMA for relaxation, referring to it as \"my low-calorie martini\", and gave the drug to friends, researchers, and others who he thought could benefit from it. One such person was Leo Zeff, a psychotherapist who had been known to use psychedelic substances in his practice. When he tried the drug in 1977, Zeff was impressed with the effects of MDMA and came out of his semi-retirement to promote its use in therapy. Over the following years, Zeff traveled around the US and occasionally to Europe, eventually training an estimated four thousand psychotherapists in the therapeutic use of MDMA. Zeff named the drug \"Adam\", believing it put users in a state of primordial innocence.\n\nPsychotherapists who used MDMA believed the drug eliminated the typical fear response and increased communication. Sessions were usually held in the home of the patient or the therapist. The role therapist was minimized in favor of patient self-discovery accompanied by MDMA induced feelings of empathy. Depression, substance abuse, relationship problems, premenstrual syndrome, and autism were among several psychiatric disorders MDMA assisted therapy was reported to treat. According to psychiatrist George Greer, therapists who used MDMA in their practice were impressed by the results. Anecdotally, MDMA was said to greatly accelerate therapy.\n\nIn the late 1970s and early 1980s, \"Adam\" spread through personal networks of psychotherapists, psychiatrists, users of psychedelics, and yuppies. Hoping MDMA could avoid criminalization like LSD and mescaline, psychotherapists and experimenters attempted to limit the spread of MDMA and information about it while conducting informal research. Early MDMA distributors were deterred from large scale operations by the threat of possible legislation. Between the 1970s and the mid-1980s, this network of MDMA users consumed an estimated 500,000 doses.\n\nA small recreational market for MDMA developed by the late 1970s, consuming perhaps 10,000 doses in 1976. By the early 1980s MDMA was being used in Boston and New York City nightclubs such as Studio 54 and Paradise Garage. Into the early 1980s, as the recreational market slowly expanded, production of MDMA was dominated by a small group of therapeutically minded Boston chemists. Having commenced production in 1976, this \"Boston Group\" did not keep up with growing demand and shortages frequently occurred.\n\nPerceiving a business opportunity, Michael Clegg, the Southwest distributor for the Boston Group, started his own \"Texas Group\" backed financially by Texas friends. In 1981, Clegg had coined \"Ecstasy\" as a slang term for MDMA to increase its marketability. Starting in 1983, the Texas Group mass-produced MDMA in a Texas lab or imported it from California and marketed tablets using pyramid sales structures and toll-free numbers. MDMA could be purchased via credit card and taxes were paid on sales. Under the brand name \"Sassyfras\", MDMA tablets were sold in brown bottles. The Texas Group advertised \"Ecstasy parties\" at bars and discos, describing MDMA as a \"fun drug\" and \"good to dance to\". MDMA was openly distributed in Austin and Dallas-Fort Worth area bars and nightclubs, becoming popular with yuppies, college students, and gays.\n\nRecreational use also increased after several cocaine dealers switched to distributing MDMA following experiences with the drug. A California laboratory that analyzed confidentially submitted drug samples first detected MDMA in 1975. Over the following years the number of MDMA samples increased, eventually exceeding the number of MDA samples in the early 1980s. By the mid-1980s, MDMA use had spread to colleges around the United States.\n\nIn an early media report on MDMA published in 1982, a Drug Enforcement Administration (DEA) spokesman stated the agency would ban the drug if enough evidence for abuse could be found. By mid-1984, MDMA use was becoming more noticed. Bill Mandel reported on \"Adam\" in a 10 June San Francisco Chronicle article, but misidentified the drug as methyloxymethylenedioxyamphetamine (MMDA). In the next month, the World Health Organization identified MDMA as the only substance out of twenty phenethylamines to be seized a significant number of times.\n\nAfter a year of planning and data collection, MDMA was proposed for scheduling by the DEA on 27 July 1984 with a request for comments and objections. The DEA was surprised when a number of psychiatrists, psychotherapists, and researchers objected to the proposed scheduling and requested a hearing. In a Newsweek article published the next year, a DEA pharmacologist stated that the agency had been unaware of its use among psychiatrists. An initial hearing was held on 1 February 1985 at the DEA offices in Washington, D.C. with administrative law judge Francis L. Young presiding. It was decided there to hold three more hearings that year: Los Angeles on 10 June, Kansas City, Missouri on 10–11 July, and Washington, D.C. on 8–11 October.\n\nSensational media attention was given to the proposed criminalization and the reaction of MDMA proponents, effectively advertising the drug. In response to the proposed scheduling, the Texas Group increased production from 1985 estimates of 30,000 tablets a month to as many as 8,000 per day, potentially making two million ecstasy tablets in the months before MDMA was made illegal. By some estimates the Texas Group distributed 500,000 tablets per month in Dallas alone. According to one participant in an ethnographic study, the Texas Group produced more MDMA in eighteen months than all other distribution networks combined across their entire histories. By May 1985, MDMA use was widespread in California, Texas, southern Florida, and the northeastern United States. According to the DEA there was evidence of use in twenty-eight states and Canada. Urged by Senator Lloyd Bentsen, the DEA announced an emergency Schedule I classification of MDMA on 31 May 1985. The agency cited increased distribution in Texas, escalating street use, and new evidence of MDA (an analog of MDMA) neurotoxicity as reasons for the emergency measure. The ban took effect one month later on 1 July 1985 in the midst of Nancy Reagan's \"Just Say No\" campaign.\n\nAs a result of several expert witnesses testifying that MDMA had an accepted medical usage, the administrative law judge presiding over the hearings recommended that MDMA be classified as a Schedule III substance. Despite this, DEA administrator John C. Lawn overruled and classified the drug as Schedule I. Later Harvard psychiatrist Lester Grinspoon sued the DEA, claiming that the DEA had ignored the medical uses of MDMA, and the federal court sided with Grinspoon, calling Lawn's argument \"strained\" and \"unpersuasive\", and vacated MDMA's Schedule I status. Despite this, less than a month later Lawn reviewed the evidence and reclassified MDMA as Schedule I again, claiming that the expert testimony of several psychiatrists claiming over 200 cases where MDMA had been used in a therapeutic context with positive results could be dismissed because they weren't published in medical journals. No double blind studies had yet been conducted as to the efficacy of MDMA for therapy.\n\nWhile engaged in scheduling debates in the United States, the DEA also pushed for international scheduling. In 1985 the World Health Organization's Expert Committee on Drug Dependence recommended that MDMA be placed in Schedule I of the 1971 United Nations Convention on Psychotropic Substances. The committee made this recommendation on the basis of the pharmacological similarity of MDMA to previously scheduled drugs, reports of illicit trafficking in Canada, drug seizures in the United States, and lack of well-defined therapeutic use. While intrigued by reports of psychotherapeutic uses for the drug, the committee viewed the studies as lacking appropriate methodological design and encouraged further research. Committee chairman Paul Grof dissented, believing international control was not warranted at the time and a recommendation should await further therapeutic data. The Commission on Narcotic Drugs added MDMA to Schedule I of the convention on 11 February 1986.\n\nThe use of MDMA in Texas clubs declined rapidly after criminalization, although by 1991 the drug remained popular among young middle-class whites and in nightclubs. In 1985, MDMA use became associated with Acid House on the Spanish island of Ibiza. Thereafter in the late 1980s, the drug spread alongside rave culture to the UK and then to other European and American cities. Illicit MDMA use became increasingly widespread among young adults in universities and later, in high schools. Since the mid-1990s, MDMA has become the most widely used amphetamine-type drug by college students and teenagers. MDMA became one of the four most widely used illicit drugs in the US, along with cocaine, heroin, and cannabis.\nAccording to some estimates as of 2004, only marijuana attracts more first time users in the US.\n\nAfter MDMA was criminalized, most medical use stopped, although some therapists continued to prescribe the drug illegally. Later, Charles Grob initiated an ascending-dose safety study in healthy volunteers. Subsequent legally-approved MDMA studies in humans have taken place in the US. in Detroit (Wayne State University), Chicago (University of Chicago), San Francisco (UCSF and California Pacific Medical Center), Baltimore (NIDA–NIH Intramural Program), and South Carolina, as well as in Switzerland (University Hospital of Psychiatry, Zürich), the Netherlands (Maastricht University), and Spain (Universitat Autònoma de Barcelona).\n\n\"Molly\", short for 'molecule', was recognized as a slang term for crystalline or powder MDMA in the 2000s.\n\nIn 2010, the BBC reported that use of MDMA had decreased in the UK in previous years. This may be due to increased seizures during use and decreased production of the precursor chemicals used to manufacture MDMA. Unwitting substitution with other drugs, such as mephedrone and methamphetamine, as well as legal alternatives to MDMA, such as BZP, MDPV, and methylone, are also thought to have contributed to its decrease in popularity.\nMDMA is legally controlled in most of the world under the UN Convention on Psychotropic Substances and other international agreements, although exceptions exist for research and limited medical use. In general, the unlicensed use, sale or manufacture of MDMA are all criminal offences.\n\nIn Australia, MDMA was declared an illegal substance in 1986 because of its harmful effects and potential for abuse. It is classed as a Schedule 9 Prohibited Substance in the country, meaning it is available for scientific research purposes only. Any other type of sale, use or manufacture is strictly prohibited by law. Permits for research uses on humans must be approved by a recognized ethics committee on human research.\n\nIn Western Australia under the Misuse of Drugs Act 1981 4.0g of MDMA is the amount required determining a court of trial, 2.0g is considered a presumption with intent to sell or supply and 28.0g is considered trafficking under Australian law.\n\nIn the United Kingdom, MDMA was made illegal in 1977 by a modification order to the existing Misuse of Drugs Act 1971. Although MDMA was not named explicitly in this legislation, the order extended the definition of Class A drugs to include various ring-substituted phenethylamines. The drug is therefore illegal to sell, buy, or possess the drug without a licence in the UK. Penalties include a maximum of seven years and/or unlimited fine for possession; life and/or unlimited fine for production or trafficking.\n\nSome researchers such as David Nutt have criticized the current scheduling of MDMA, what he determined to be a relatively harmless drug. An editorial he wrote in the Journal of Psychopharmacology, where he compared the risk of harm for horse riding (1 adverse event in 350) to that of ecstasy (1 in 10,000) resulted in his dismissal as well as the resignation of his colleagues from the ACMD.\n\nIn the United States, MDMA is currently placed in Schedule I of the Controlled Substances Act. In a 2011 federal court hearing the American Civil Liberties Union successfully argued that the sentencing guideline for MDMA/ecstasy is based on outdated science, leading to excessive prison sentences. Other courts have upheld the sentencing guidelines. The United States District Court for the Eastern District of Tennessee explained its ruling by noting that \"an individual federal district court judge simply cannot marshal resources akin to those available to the Commission for tackling the manifold issues involved with determining a proper drug equivalency.\"\n\nIn the Netherlands, the Expert Committee on the List (Expertcommissie Lijstensystematiek Opiumwet) issued a report in June 2011 which discussed the evidence for harm and the legal status of MDMA, arguing in favor of maintaining it on List I.\n\nIn Canada, MDMA is listed as a Schedule 1 as it is an analogue of amphetamine. The CDSA was updated as a result of the Safe Streets Act changing amphetamines from Schedule III to Schedule I in March 2012.\n\nIn 2014, 3.5% of 18 to 25 year-olds had used MDMA in the United States. In Europe, an estimated 37% of regular club-goers aged 14 to 35 used MDMA in the past year according to the 2015 European Drug report. The highest one-year prevalence of MDMA use in Germany in 2012 was 1.7% among people aged 25 to 29 compared with a population average of 0.4%. Among adolescent users in the United States between 1999 and 2008, girls were more likely to use MDMA than boys.\n\nIn 2008 the European Monitoring Centre for Drugs and Drug Addiction noted that although there were some reports of tablets being sold for as little as €1, most countries in Europe then reported typical retail prices in the range of €3 to €9 per tablet, typically containing 25–65 mg of MDMA. By 2014 the EMCDDA reported that the range was more usually between €5 and €10 per tablet, typically containing 57–102 mg of MDMA, although MDMA in powder form was becoming more common.\n\nThe United Nations Office on Drugs and Crime stated in its 2014 World Drug Report that US ecstasy retail prices range from US$1 to $70 per pill, or from $15,000 to $32,000 per kilogram. A new research area named Drug Intelligence aims to automatically monitor distribution networks based on image processing and machine learning techniques, in which an Ecstasy pill picture is analyzed to detect correlations among different production batches. These novel techniques allow police scientists to facilitate the monitoring of illicit distribution networks.\n\n, most of the MDMA in the United States is produced in British Columbia, Canada and imported by Canada-based Asian transnational criminal organizations. The market for MDMA in the United States is relatively small compared to methamphetamine, cocaine, and heroin.\n\nMDMA is particularly expensive in Australia, costing A$15–A$30 per tablet. In terms of purity data for Australian MDMA, the average is around 34%, ranging from less than 1% to about 85%. The majority of tablets contain 70–85 mg of MDMA. Most MDMA enters Australia from the Netherlands, the UK, Asia, and the US.\n\nA number of ecstasy manufacturers brand their pills with a logo, often being a corporate logo, to help distinguish between suppliers; one of the most notable logos which appeared on pills is the Mitsubishi logo which was popular. Some pills depict logos of products or shows popular with children, such as Shaun the Sheep.\n\nThe Multidisciplinary Association for Psychedelic Studies (MAPS) is currently funding pilot studies or clinical trials investigating the use of MDMA in psychotherapy to treat posttraumatic stress disorder (PTSD), social anxiety in autistic adults, and anxiety in terminal illness. In November 2016, the United States Food and Drug Administration (FDA) approved large-scale phase 3 clinical trials involving the use of MDMA for the treatment of PTSD in individuals who do not respond to traditional prescription drugs or psychotherapy — a final step before the possible approval of MDMA as a prescription drug in the United States. MDMA has also been proposed as an adjunct to substance abuse treatment. MDMA is taken only a few times in a therapeutic setting, unlike approved psychiatric medications which require ongoing treatment.\n\nThe potential for MDMA to be used as a rapid-acting antidepressant has been studied in clinical trials, but as of 2017 the evidence on efficacy and safety were insufficient to reach a conclusion. A 2014 review of the safety and efficacy of MDMA as a treatment for various disorders, particularly PTSD, indicated that MDMA has therapeutic efficacy in some patients; however, it emphasized that issues regarding the control-ability of MDMA-induced experiences and neurochemical recovery must be addressed. The author noted that oxytocin and -cycloserine are potentially safer co-drugs in PTSD treatment, albeit with limited evidence of efficacy. This review and a second corroborating review by a different author both concluded that, because of MDMA's demonstrated potential to cause lasting harm in humans (e.g., serotonergic neurotoxicity and persistent memory impairment), \"considerably more research must be performed\" on its efficacy in PTSD treatment to determine if the potential treatment benefits outweigh its potential to harm to a patient.\n\n", "id": "10024", "title": "MDMA"}
{"url": "https://en.wikipedia.org/wiki?curid=10025", "text": "Flag of Europe\n\nThe Flag of Europe, or European Flag is an official symbol of two separate organizations—the Council of Europe (CoE) and the European Union (EU). When representing the latter, it is also known as the Flag of the European Union. \nIt consists of a circle of twelve five-pointed yellow (gold) stars on a blue (azure) field. \n\nThe flag was designed in 1955, and officially launched later that year by the Council of Europe as a symbol for the whole of Europe. \nThe Council of Europe urged it to be adopted by other European organisations, and in 1985 the European Communities (EC) adopted it.\n\nThe EU has inherited the flag's use when it was formed in 1993, being the successor organisation to the EC. It has been in wide official by the EU since the 1990s, but it has never been given official status in any of the EU's treaties.\nIts adoption as an official symbol of the EU was planned as part of the proposed European Constitution, which failed to be ratified in 2005.\n\nSince its adoption by the European Union, it has become broadly associated with the supranational organisation due to its high profile and heavy usage of the emblem. It has also been used by pro-EU protestors in the colour revolutions of the 2000s, e.g. in Belarus (2004) or Moldova.\nThere are also a number of derivative designs used as logos or flags of other European organisations, and in the flags of the Republic of Kosovo (2008) and of Bosnia and Herzegovina (1998).\n\nThe flag is rectangular with 2:3 proportions: its fly (width) is one and a half times the length of its hoist (height). Twelve gold (or yellow) stars are centred in a circle (the radius of which is a third of the length of the hoist) upon a blue background. All the stars are upright (one point straight up), have five points and are spaced equally according to the hour positions on the face of a clock. The diameter of each star is equal to one-ninth of the height of the hoist.\n\nThe graphical specifications given by the EU describe the design as: \"On an azure field a circle of twelve golden mullets, their points not touching.\" The Council of Europe gives the flag a symbolic description in the following terms:\n\nThe base colour of the flag is a dark blue (reflex blue, a mix of cyan and magenta), while the golden stars are portrayed in Yellow. The colours are regulated according to the Pantone colouring system (see table for specifications).\n\nA large number of designs were proposed for the flag before the current flag was agreed. The rejected proposals are preserved in the Council of Europe Archives. One of these consists of a design of white stars on a light blue field, as a gesture to the peace and internationalism of the United Nations. An official website makes a reference to blue and gold being the original colours of Richard von Coudenhove-Kalergi, who proposed a Pan European Union in 1923, and was an active proponent of the early Community.\n\nThe number of stars on the flag is fixed at 12, and is not related to the number of member states of the EU (although the EU did have 12 member states from 1986 to 1994). This is because it originally was the flag of the Council of Europe. In 1953, the Council of Europe had 15 members; it was proposed that the future flag should have one star for each member, and would not change based on future members. West Germany objected to this as one of the members was the disputed area of Saarland, and to have its own star would imply sovereignty for the region. Twelve was eventually adopted as a number with no political connotations and as a symbol of unity. While 12 is the correct number of stars, sometimes flags or emblems can be found that incorrectly show 15 (as of the rejected proposal) or 25 (as suggested by some after the expansion of the EU to 25 member states in 2004). However, the flag also remains that of the Council of Europe, which now has 47 member states.\n\nThe search for a symbol began in 1950 when a committee was set up in order to look into the question of a European flag. There were numerous proposals but a clear theme for stars and circles emerged. Richard von Coudenhove-Kalergi proposed that they adopt the flag of his International Paneuropean Union, which was a blue field, with a red cross inside an orange circle at the centre, which he had himself recently adopted for the European Parliamentary Union. Due to the cross symbolism, this was rejected by Turkey (a member of the Council of Europe since 1949). Kalergi then suggested adding a crescent to the cross design, to overcome the Muslim objections. Another organisation's flag was the European Movement, which had a large green E on a white background. A further design was one based on the Olympic rings: eight silver rings on a blue background. It was rejected due to the rings' similarity with \"dial\", \"chain\" and \"zeros\". One proposal had a large yellow star on a blue background, but it was rejected due to its similarity with the so-called Burnet flag and the flag of the Belgian Congo.\n\nThe Consultative Assembly narrowed their choice to two designs. One was by Salvador de Madariaga, the founder of the College of Europe, who suggested a constellation of stars on a blue background (positioned according to capital cities, with a large star for Strasbourg, the seat of the Council). He had circulated his flag round many European capitals and the concept had found favour. The second was a variant by Arsène Heitz, who worked for the Council's postal service and had submitted dozens of designs; the design of his that was accepted by the Assembly was similar to Salvador de Madariaga's, but rather than a constellation, the stars were arranged in a circle. In 1987, Heitz claimed that his inspiration had been the crown of twelve stars of the Woman of the Apocalypse, often found in Marian iconography (see below).\n\nThe Consultative Assembly favoured Heitz's design. However, the flag the Assembly chose had fifteen stars, reflecting the number of states of the Council of Europe. The Consultative Assembly chose this flag and recommended the Council of Europe to adopt it. The Committee of Ministers (the Council's main decision making body) agreed with the Assembly that the flag should be a circle of stars, but the number was a source of contention. The number twelve was chosen, and Paul M. G. Lévy drew up the exact design of the new flag as it is today. The Parliamentary Assembly of the Council of Europe approved it on 25 October 1955. Adopted on 8 December 1955, the flag was unveiled at the Château de la Muette in Paris on 13 December 1955.\n\nFollowing Expo 58 in Brussels, the flag caught on and the Council of Europe lobbied for other European organisations to adopt the flag as a sign of European unity. \nThe European Parliament took the initiative in seeking a flag to be adopted by the European Communities. Shortly after the first direct elections in 1979 a draft resolution was put forward on the issue. The resolution proposed that the Communities' flag should be that of the Council of Europe and it was adopted by the Parliament on 11 April 1983.\n\nThe June 1984 European Council (the Communities' leaders) summit in Fontainebleau stressed the importance of promoting a European image and identity to citizens and the world. The following year, meeting in Milan, the 28–29 June European Council approved a proposal from the Committee on a People’s Europe (Adonnino Committee) in favour of the flag and adopted it. Following the permission of the Council of Europe, the Communities began to use it from 1986, with it being raised outside the Berlaymont building (the seat of the European Commission) for the first time on 29 May 1986. \n\nPrior to development of political institutions, flags representing Europe were limited to unification movements. The most popular were the European Movement's large green 'E' on a white background, and the \"Pan European flag\" (see \"Creation\" below). With the development of institutions, aside from the Council of Europe, came other emblems and flags. None were intended to represent wider Europe and have since been replaced by the current flag of Europe.\n\nThe first major organisation to adopt one was the European Coal and Steel Community (ECSC), which merged into the European Communities. The ECSC was created in 1952 and the flag of the ECSC was unveiled in 1958 Expo in Brussels.\n\nThe flag had two stripes, blue at the top, black at the bottom with six gold (silver after 1973) stars, three on each stripe. Blue was for steel, black for coal and the stars were the six member-states. The stars increased with the members until 1986 when they were fixed at twelve. When the ECSC treaty expired in 2002, the flag was lowered outside the European Commission in Brussels and replaced with the European flag.\n\nThe European Parliament also used its own flag from 1973, but never formally adopted it. It fell out of use with the adoption of the twelve star flag by the Parliament in 1983. The flag followed the yellow and blue colour scheme however instead of twelve stars there were the letters EP and PE (initials of the European Parliament in the six community languages at the time) surrounded by a wreath.\n\nIn 2002, Dutch architect Rem Koolhaas and his architecture firm Office for Metropolitan Architecture (OMA) designed a new flag in response to Commission President Romano Prodi's request to find ways of rebranding the Union in a way that represents Europe's \"diversity and unity\". The proposed new design was dubbed the \"barcode\", as it displays the colours of every European flag (of the then 15 members) as vertical stripes. As well as the barcode comparison, it had been compared unfavourably to wallpaper, a TV test card, and deckchair fabric. Unlike the current flag, it would change to reflect the member states.\n\nIt was never officially adopted by the EU or any organisation; however, it was used as the logo of the Austrian EU Presidency in 2006. It had been updated with the colours of the 10 members who had joined since the proposal, and was designed by Koolhaas's firm. Its described aim is \"to portray Europe as the common effort of different nations, with each retaining its own unique cultural identity\". There were initially some complaints, as the stripes of the flag of Estonia were displayed incorrectly.\n\nThe European Union, which was established by the Maastricht Treaty in 1992 to replace the European Communities and encompass its\nfunctions, has retained use of the flag.\n\nA Framework Agreement establishing the legal basis for cooperation between the European Space Agency and the European Union came into force in May 2004; already in April 2004, the European flag was flown on behalf of the European Space Agency, by astronaut André Kuipers while on board the International Space Station.\nFollowing the 2004 Summer Olympics, President Romano Prodi pointed out that the combined medal total of the European Union was far greater than that of any other country and called for EU athletes to fly the European flag at the following games alongside their own as a sign of solidarity. Use of the flag has also been reported as representing the European team at the Ryder Cup golf competition in the early 2000s, although most European participants preferred to use their own national flags.\n\nThe official status of the flag as a symbol of the European Union was to be formalised as part of the Constitution of the European Union. However, as the proposed constitutio failed ratification, the mention of all state-like emblems, including the flag, were removed from the replacement Treaty of Lisbon of 2007.\nInstead, a separate declaration by sixteen Member States was included in the final act of the Treaty of Lisbon stating that the flag, the anthem, the motto and the currency and Europe Day \"will for them continue as symbols to express the sense of community of the people in the European Union and their allegiance to it.\"\n\nIn reaction to the removal of the flag from the treaty, the European Parliament, which had supported the inclusion of such symbols, backed a proposal to use these symbols \"more often\" on behalf of the Parliament itself; Jo Leinen, MEP for Germany, suggested that the Parliament should \"take the \"avant-garde\"\" in their use. \nIn September 2008, the Parliament's Committee on Constitutional Affairs proposed a formal change in the institution's rules of procedure to make \"better use of the symbols\". Specifically, the flag would be present in all meeting rooms (not just the hemicycle) and at all official events. The proposal was passed on 8 October 2008 by 503 votes to 96 (15 abstentions).\n\nThe flag was used as a banner for \"pro-Europeanism\" outside the Union, for example in several of the \"colour revolutions\" during the 2000s.\nIn Belarus, it was used on protest marches alongside the banned former national flag and flags of opposition movements during the protests of 2004–2006. \nThe flag was used widely in a 2007 European March in Minsk as protesters rallied in support of democracy and accession to the EU. \n\nIn Georgia, the flag was on most government buildings since the coming to power of Mikhail Saakashvili (2007), who used it during his inauguration, stating: \"[the European] flag is Georgia’s flag as well, as far as it embodies our civilisation, our culture, the essence of our history and perspective, and our vision for the future of Georgia.\" \n\nIt was used in 2008 by pro-western Serbian voters ahead of an election.\n\nThe flag became a symbol of European integration of Ukraine in the 2010s, particularly after Euromaidan. Ukraine is not a part of the EU but is a member of the Council of Europe. The flag is used by the Cabinet of Ukraine, Prime Minister of Ukraine, and MFA UA during official meetings.\nIt was flown during the 2013 Euromaidan protests in Ukraine.\n\nThe Council of Europe, and in a web page archived in 2002 expressed its satisfaction with the \"growing awareness of the European flag and emblem among European citizens\", stating that with the adoption of the flag by the European Union, both \"[t]he European Commission and the Council of Europe are responsible for ensuring that all uses of this symbol respect the dignity of the European flag and emblem\".\nAccording to the EU web portal, the flag should be taken to symbolise \"both the European Union and, more broadly, the identity and unity of Europe\". All EU institutions, bodies and agencies have their own logo or emblem, albeit often inspired by the flag's design and colours. As part of the EU's usage, the flag appears on the euro banknotes. Euro coins also display the twelve stars of the flag on both the national and common sides and the flag is sometimes used as an indication of the currency or the eurozone (a collective name for those countries that use the Euro). The flag appears also on many driving licences and vehicle registration plates issued in the Union.\n\nIt is mandatory for the flag to be used in every official speech made by the President of the European Council and it is often used at official meetings between the leaders of an EU state and a non-EU state (the national flag and European flag appearing together). \nWhile normally the national flag takes precedence over the European flag in the national context, meetings between EU leaders sometimes differ. For example, the Italian flag code as of 2008 expressly replaces the Italian flag with the European flag in precedence when dignitaries from other EU countries visit – for example the EU flag would be in the middle of a group of three flags rather than the Italian flag.\nThe flag is usually flown by the government of the country holding the rotating presidency Council of Ministers.\n\nThe design of the European flag was displayed on the Eiffel Tower in Paris to celebrate the French presidency of the EU Council in the second half of 2008.\nIn 2009, the Czech President Václav Klaus, a eurosceptic, refused to fly the flag from his castle. In response, Greenpeace projected an image of the flag onto the castle and attempted to fly the flag from the building themselves.\n\nSome members also have their own rules regarding the use of the flag alongside their national flag on domestic occasions, for example the obligatory use alongside national flags outside police stations or local government buildings. As an example according to the Italian laws it is mandatory for most public offices and buildings to hoist the European Flag alongside the Italian national Flag (Law 22/2000 and Presidential Decree 121/2000). Outside official use, the flag may not be used for aims incompatible with European values.\n\nIn national usage, national protocol usually demands the national flag takes precedence over the European flag (which is usually displayed to the right of the national flag from the observer's perspective). On occasions where the European flag is flown alongside all national flags (for example, at a European Council meeting), the national flags are placed in alphabetical order (according to their name in the main language of that state) with the European flag either at the head, or the far right, of the order of flags.\n\nExtraordinary flying of the flag is common on the EU's flag day, known as Europe Day, which is celebrated annually on 9 May. On Europe Day 2008, the flag was flown for the first time above the German Reichstag.\n\nIn addition to the flags use by the government and people, the flag is also used in EU military operations; however, it is not used as a civil ensign. In 2003, a member of the European Parliament tabled a proposal in a temporary committee of the European Parliament that national civil ensigns be defaced with the European flag. This proposal was rejected by Parliament in 2004, and hence the European flag is not used as a European civil ensign.\n\nDespite not having a civil ensign, the EU's Fishery Inspection teams display a blue and yellow pennant. The pennant is flown by inspection vessels in EU waters. The flag is triangular and quartered blue and yellow and was adopted according to \"EEC Regulation #1382/87\" on 20 May 1978. There are no other variants or alternative flags used by the EU (in contrast to countries which have presidential, naval and military variants).\n\nThe design of the European flag has been used in a variation, such as that of the Council of Europe mentioned above, and also to a greater extent such as the flag of the Western European Union (WEU; now defunct), which uses the same colours and the stars but has a number of stars based on membership and in a semicircle rather than a circle. It is also defaced with the initials of the former Western European Union in two languages.\n\nThe flag of Bosnia and Herzegovina does not have such a strong connection as the WEU flag, but was partly inspired by the European involvement in, and aspirations of, Bosnia and Herzegovina. It uses the same blue and yellow colours and the stars, although of a different number and colour, are a direct reference to those of the European flag.\n\nLikewise, the Republic of Kosovo uses blue, yellow and stars in its flag, which has been mocked as \"a none too subtle nod to the flag of the European Union, which is about to become Kosovo's new best friend as it takes over protector status from the United Nations\". \n\nThe flag of the Brussels-Capital Region consists of a yellow iris with a white outline upon a blue background. Its colours are based on the colours of the Flag of Europe, because Brussels is considered the unofficial capital of the EU.\n\nThe national flag of Cape Verde also shows similarity to the flag of the European Union. The flag is made of a circular formation of ten yellow stars on a dark blue background and a band of white and red. The stars represent the main islands of the nation (a chain of islands off the coast of West Africa). The blue represents the ocean and the sky. The band of white and red represents the road toward the construction of the nation, and the colours stand for peace (white) and effort (red). The flag was adopted on 22 September 1992.\n\nOther labels take reference to the European flag such as the EU organic food label that uses the twelve stars but reorders them into the shape of a leaf on a green background. The original logo of the European Broadcasting Union used the twelve stars on a blue background adding ray beams to connect the countries.\n\nIn 1987, following the adoption of the flag by the EEC, Arsène Heitz (1908–89), one of the designers who had submitted proposals for the flag's design, suggested a religious inspiration for it. He claimed that the circle of stars was based on the iconographic tradition of showing the Blessed Virgin Mary as the Woman of the Apocalypse, wearing a \"crown of twelve stars\".\nThe French satirical magazine ' reacted to Heitz's statement with an article entitled ' (\"Europe Raped by the Blessed Virgin\") in the 20 December 1989 edition.\nHeitz also made a connection to the date of the flag's adoption, 8 December 1955, coinciding with the Catholic Feast of the Immaculate Conception of the Blessed Virgin Mary.\n\nPaul M. G. Lévy, then Director of Information at the Council of Europe responsible for designing the flag, in a 1989 statement maintained that he had not been aware of any religious connotations.\n\nIn an interview given 26 February 1998, Lévy denied not only awareness of the \"Marian\" connection, but also denied that the final design of a circle of twelve stars was Heitz's. To the question \"Who really designed the flag?\" Lévy replied:\n\nCarlo Curti Gialdino (2005) has reconstructed the design process to the effect that Heitz's proposal contained varying numbers of stars, from which the version with twelve stars was chosen by the Committee of Ministers meeting at Deputy level in January 1955 as one out of two remaining candidate designs.\n\nLévy's 1998 interview apparently gave rise to a new variant of the \"Marian\" anecdote. An article published in \"\" in August 1998 alleged that it was Lévy himself who was inspired to introduce a \"Marian\" element as he walked past a statue of the Blessed Virgin Mary.\n\nAn article posted in ' in February 2000 further connected the donation of a stained glass window for Strasbourg Cathedral by the Council of Europe on 21 October 1956. This window, a work by Parisian master Max Ingrand, shows a blessing Madonna underneath a circle of 12 stars on dark blue ground. The overall design of the Madonna is inspired by the banner of the cathedral's ', and the twelve stars are found on the statue venerated by this congregation inside the cathedral (twelve is also the number of members of the congregation's council).\n\n", "id": "10025", "title": "Flag of Europe"}
{"url": "https://en.wikipedia.org/wiki?curid=10026", "text": "Anthem of Europe\n\n\"Anthem of Europe\" is the anthem of the Council of Europe and the European Union. It is based on \"Ode to Joy\" from the final movement of Beethoven's 9th Symphony composed in 1823, and is played on official occasions by both organisations.\n\nFriedrich Schiller wrote the poem \"An die Freude\" (\"To Joy\") in 1785 as a \"celebration of the brotherhood of man\". In later life, the poet was contemptuous of this popularity and dismissed the poem as typical of \"the bad taste of the age\" in which it had been written. After Schiller's death, the poem provided the words for the choral movement of Ludwig van Beethoven's 9th Symphony.\n\nIn 1971 the Parliamentary Assembly of the Council of Europe decided to propose adopting the prelude to the \"Ode to Joy\" from Beethoven's 9th Symphony as the European anthem, taking up a suggestion made by Richard von Coudenhove-Kalergi in 1955. Beethoven was generally seen as the natural choice for a European anthem. The Committee of Ministers of the Council of Europe officially announced the European Anthem on 19 January 1972 at Strasbourg: the prelude to \"Ode to Joy\", 4th movement of Ludwig van Beethoven's 9th symphony. In 1974 the same piece of music was adopted as the National Anthem of Rhodesia.\n\nConductor Herbert von Karajan was asked to write three instrumental arrangements – for solo piano, for wind instruments and for symphony orchestra and he conducted the performance used to make the official recording. He wrote his decisions on the score, notably those concerning the tempo. Karajan decided on minim (half note) = 80 whereas Beethoven had written crotchet (quarter note) = 120.\n\nThe anthem was launched via a major information campaign on Europe Day in 1972. In 1985, it was adopted by EU heads of State and government as the official anthem of the then European Community – since 1993 the European Union. It is not intended to replace the national anthems of the member states but rather to celebrate the values they all share and their unity in diversity. It expresses the ideals of a united Europe: freedom, peace, and solidarity.\n\nIt was to have been included in the European Constitution along with the other European symbols; however, the treaty failed ratification and was replaced by the Treaty of Lisbon, which does not include any symbols. A declaration was attached to the treaty, in which sixteen member states formally recognised the proposed symbols. In response, the European Parliament decided that it would make greater use of the anthem, for example at official occasions. In October 2008, the Parliament changed its rules of procedure to have the anthem played at the opening of Parliament after elections and at formal sittings.\n\n\"Ode to Joy\" is the anthem of the Council of Europe and the European Union, promoted as a symbol for the whole of Europe as are the other European symbols. It is used on occasions such as Europe Day and formal events such as the signing of treaties. The European Parliament seeks to make greater use of the music, then-Parliament President Hans-Gert Pöttering stated he was moved when the anthem was played for him on his visit to Israel and ought to be used in Europe more often.\n\nIn 2008 it was used by Kosovo as its national anthem until it adopted its own, and it was played at its declaration of independence, as a nod to the EU's role in its independence from Serbia.\n\nAt the 2007 signing ceremony for the Treaty of Lisbon, the plenipotentiaries of the European Union's twenty-seven member states stood in attendance while the \"Ode to Joy\" was played and a choir of 26 Portuguese children sang the original German lyrics.\n\nIn 1992 the anthem was used by CIS national football team at the 1992 UEFA European Football Championship.\n\nOn 4 October 2010 the anthem was used when a European team beat a team representing the United States of America to win the Ryder Cup golf tournament. The European Ryder Cup captain Colin Montgomerie decided to break with tradition and play the European anthem by itself instead of the individual anthems from participating European nations. It was similarly employed at the 2014 Ryder Cup prizegiving ceremony on 28 September, after Europe had beaten America under its captain, Paul McGinley.\n\n\"Ode to Joy\" is used as the theme song to the 2016 UEFA Euro qualifying and the European qualifying of the 2018 FIFA World Cup football competition at the introduction of every match.\n\n\"Ode to Joy\" , automatically orchestrated in seven different styles, has been used on 18 June 2015 during the ceremony celebrating the 5000th ERC grantee as anthem of the European Research Council to represent achievements of European research.\n\nIn 2017, MPs from Scotland National Party first whistled and then sang \"Ode to Joy\" at the House of Commons to protest against the Brexit referendum. \n\nDue to the large number of languages used in the European Union, the anthem is purely instrumental, and the German lyrics that Friedrich Schiller wrote and on which Beethoven based the melody have no official status. Despite this, the German lyrics are often sung by choirs or ordinary people when the anthem is played: for example, at the 2004 enlargement on the German-Polish border, the crowd watching the ceremony playing the music sang along with the German lyrics.\n\nAside from this, several translations of the poem used by Beethoven as well as original works have attempted to provide lyrics to the anthem in various languages. Versions of the anthem including lyrics have been sung outside official EU occasions.\n\nIn France, several adaptations of Beethoven's composition were known long before the onset of European Union. A version by the librettist Maurice Bouchor (1855–1929) entitled \"Hymn to Universal Humanity\" (\"\") adding several verses to a preceding version of Jean Ruault, was published. This version and another by Maurice Bouchor, published with Julien Thiersot under the title \"Hymn for future times\" (\"Hymne des temps futurs\") in a music book which was widespread among basic schools, is performed unofficially by school choirs during European events. Another version by the Catholic writer Joseph Folliet (1903–1972) is also known.\n\n\n", "id": "10026", "title": "Anthem of Europe"}
{"url": "https://en.wikipedia.org/wiki?curid=10029", "text": "Timeline of the evolutionary history of life\n\nThis timeline of the evolutionary history of life represents the current scientific theory outlining the major events during the development of life on planet Earth. In biology, evolution is any change across successive generations in the heritable characteristics of biological populations. Evolutionary processes give rise to diversity at every level of biological organization, from kingdoms to species, and individual organisms and molecules, such as DNA and proteins. The similarities between all present day organisms indicate the presence of a common ancestor from which all known species, living and extinct, have diverged through the process of evolution. More than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described.\n\nWhile the dates given in this article are estimates based on scientific evidence, there has been controversy between more traditional views of increased biodiversity through a cone of diversity with the passing of time and the view that the basic pattern on Earth has been one of annihilation and diversification and that in certain past times, such as the Cambrian explosion, there was great diversity.\n\nSpecies go extinct constantly as environments change, organisms compete for environmental niches, and genetic mutation leads to the rise of new species from older ones. Occasionally biodiversity on the planet takes a hit in the form of a mass extinction in which the extinction rate is much higher than usual. A large extinction event often represents an accumulation of smaller extinction events that take place in a relatively brief period of time.\n\nThe first known mass extinction in earth's history was the Great Oxygenation Event 2.4 billion years ago. The event led to the loss of most of the planet's obligate anaerobes. The five largest extinction events in earth's history since are these:\n\nSmaller extinction events have occurred in the periods between these larger catastrophes, with some standing at the delineation points of the periods and epochs recognized by scientists in geologic time. The Holocene extinction event is currently under way.\n\nFactors in mass extinctions include continental drift, changes in atmospheric and marine chemistry, volcanism and other aspects of mountain formation, changes in glaciation, changes in sea level, and impact events.\n\nIn this timeline, Ma (for \"megaannum\") means \"million years ago,\" ka (for \"kiloannum\") means \"thousand years ago,\" and ya means \"years ago.\"\n\n4000 Ma and earlier.\n\n4000 Ma – 2500 Ma\n\n2500 Ma – 542 Ma. Contains the Palaeoproterozoic, Mesoproterozoic and Neoproterozoic eras.\n\n542 Ma – present\n\nThe Phanerozoic Eon, literally the \"period of well-displayed life,\" marks the appearance in the fossil record of abundant, shell-forming and/or trace-making organisms. It is subdivided into three eras, the Paleozoic, Mesozoic and Cenozoic, which are divided by major mass extinctions.\n\n542 Ma – 251.0 Ma and contains the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian periods.\n\nFrom 251.4 Ma to 66 Ma and containing the Triassic, Jurassic and Cretaceous periods.\n\n66 Ma – present\n\n\n", "id": "10029", "title": "Timeline of the evolutionary history of life"}
